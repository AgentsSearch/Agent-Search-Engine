[
  {
    "agent_id": "6a006decc42e1955",
    "name": "ai.aliengiraffe/spotdb",
    "source": "mcp",
    "source_url": "https://github.com/aliengiraffe/spotdb",
    "description": "Ephemeral data sandbox for AI workflows with guardrails and security",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-09T17:05:17.793149Z",
    "indexed_at": "2026-02-18T04:01:16.536943",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# SpotDB\n\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=aliengiraffe_env&metric=alert_status&token=a564584b18c4d708580b4825a1d8c270b18a3f3f)](https://sonarcloud.io/summary/new_code?id=aliengiraffe_env)\n\n### Lightweight data sandbox for AI workflows and data exploration, enabled with guardrails and security to keep your data safe.\n\nThis project provides a lightweight, **ephemeral data sandbox** designed for large language models (LLMs) and agentic workflows. By providing a secure, isolated environment, it allows AI agents and scripts to analyze data without direct access to production databases. This setup prevents accidental data modification, ensures data privacy, and enforces guardrails for safe data exploration.\n\n## Features\n\n- üèñÔ∏è **Ephemeral Data Sandbox**: Create temporary databases for AI workflows and data exploration.\n- üì∏ **Snapshot**: Capture and store data snapshots, recover point-in-time data states or continue from a previous state.\n- üß† **MCP API**: Access data through a Model Context Protocol for seamless integration with AI models and agentic workflows.\n- ‚öôÔ∏è **REST API**: Access data through a RESTful API for integration with traditional systems and workflows.\n- üöÇ **Guardrails**: Enforce rules and constraints to ensure data safety and privacy.\n- üõ°Ô∏è **Security**: Protect data from unauthorized access and modification.\n\n## Quick Start\n\n1. Tap the repository and install the package:\n\n```bash\nbrew tap aliengiraffe/spaceship && \\\\\nbrew install spotdb\n```\n\n2. Start the server:\n\n```bash\nspotdb\n```\n\n3. Upload a CSV file:\n\n```bash\ncurl -X POST \\\n  http://localhost:8080/api/v1/upload \\\n  -F \"table_name=mytable\" \\\n  -F \"has_header=true\" \\\n  -F \"csv_file=@data.csv\"\n```\n\n4. Query the data:\n\n```bash\ncurl -X POST \\\n  http://localhost:8080/api/v1/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"SELECT * FROM mytable LIMIT 10\"}'\n```\n\n5. Setup Claude Code\n   You must have the `claude` command installed.\n\nThen, you can add the `spotdb` mcp:\n\n```bash\nclaude mcp add spotdb -s user -- npx -y mcp-remote http://localhost:8081/stream\n```\n\n## Use Explorer UI\n\nOpen the Explorer UI in your browser and upload files and query the data:\n\n```bash\nopen http://localhost:8080/explorer\n```\n\n## Full Documentation\n\nüëâ [https://github.com/aliengiraffe/spotdb/blob/main/DOCS.md](https://github.com/aliengiraffe/spotdb/blob/main/DOCS.md)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Create temporary ephemeral databases for AI workflows and data exploration",
        "Capture and store data snapshots for point-in-time recovery or continuation",
        "Provide data access through a Model Context Protocol (MCP) API for AI model integration",
        "Provide data access through a RESTful API for traditional system integration",
        "Enforce guardrails to ensure data safety and privacy during exploration",
        "Protect data from unauthorized access and modification",
        "Upload CSV files to create tables in the sandbox environment",
        "Execute SQL queries against uploaded data tables"
      ],
      "limitations": [],
      "requirements": [
        "Installation via Homebrew package manager",
        "Running the spotdb server locally",
        "Having the 'claude' command installed for MCP integration"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, usage examples, feature descriptions, security considerations, and integration details, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# SpotDB\n\n[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=aliengiraffe_env&metric=alert_status&token=a564584b18c4d708580b4825a1d8c270b18a3f3f)](https://sonarcloud.io/summary/new_code?id=aliengiraffe_env)\n\n### Lightweight data sandbox for AI workflows and data exploration, enabled with guardrails and security to keep your data safe.\n\nThis project provides a lightweight, **ephemeral data sandbox** designed for large language models (LLMs) and agentic workflows. By providing a secure, isolated environment, it allows AI agents and scripts to analyze data without direct access to production databases. This setup prevents accidental data modification, ensures data privacy, and enforces guardrails for safe data exploration.\n\n## Features\n\n- üèñÔ∏è **Ephemeral Data Sandbox**: Create temporary databases for AI workflows and data exploration.\n- üì∏ **Snapshot**: Capture and store data snapshots, recover point-in-time data states or continue from a previous state.\n- üß† **MCP API**: Access data through a Model Context Protocol for seamless integration with AI models and agentic workflows.\n- ‚öôÔ∏è **REST API**: Access data through a RESTful API for integration with traditional systems and workflows.\n- üöÇ **Guardrails**: Enforce rules and constraints to ensure data safety and privacy.\n- üõ°Ô∏è **Security**: Protect data from unauthorized access and modification.\n\n## Quick Start\n\n1. Tap the repository and install the package:\n\n```bash\nbrew tap aliengiraffe/spaceship && \\\\\nbrew install spotdb\n```\n\n2. Start the server:\n\n```bash\nspotdb\n```\n\n3. Upload a CSV file:\n\n```bash\ncurl -X POST \\\n  http://localhost:8080/api/v1/upload \\\n  -F \"table_name=mytable\" \\\n  -F \"has_header=true\" \\\n  -F \"csv_file=@data.csv\"\n```\n\n4. Query the data:\n\n```bash\ncurl -X POST \\\n  http://localhost:8080/api/v1/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"SELECT * FROM mytable LIMIT 10\"}'\n```\n\n5. Setup Claude Code\n   You must have the `claude` command installed.",
        "start_pos": 0,
        "end_pos": 1983,
        "token_count_estimate": 495,
        "source_type": "readme",
        "agent_id": "6a006decc42e1955"
      },
      {
        "chunk_id": 1,
        "text": "tp://localhost:8080/api/v1/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"SELECT * FROM mytable LIMIT 10\"}'\n```\n\n5. Setup Claude Code\n   You must have the `claude` command installed.\n\nThen, you can add the `spotdb` mcp:\n\n```bash\nclaude mcp add spotdb -s user -- npx -y mcp-remote http://localhost:8081/stream\n```\n\n## Use Explorer UI\n\nOpen the Explorer UI in your browser and upload files and query the data:\n\n```bash\nopen http://localhost:8080/explorer\n```\n\n## Full Documentation\n\nüëâ [https://github.com/aliengiraffe/spotdb/blob/main/DOCS.md](https://github.com/aliengiraffe/spotdb/blob/main/DOCS.md)",
        "start_pos": 1783,
        "end_pos": 2401,
        "token_count_estimate": 154,
        "source_type": "readme",
        "agent_id": "6a006decc42e1955"
      }
    ]
  },
  {
    "agent_id": "78e7f7ed4b813999",
    "name": "ai.alpic.test/test-mcp-server",
    "source": "mcp",
    "source_url": "https://test.alpic.ai/",
    "description": "Alpic Test MCP Server - great server!",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-10T13:57:43.256739Z",
    "indexed_at": "2026-02-18T04:01:18.871571",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.1,
    "quality_rationale": "The documentation consists of a single vague sentence with no details on capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "592a33ed36398975",
    "name": "ai.appdeploy/deploy-app",
    "source": "mcp",
    "source_url": "https://api-v2.appdeploy.ai/mcp",
    "description": "AppDeploy turns app ideas described in AI chat into live full-stack web applications",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-09T15:16:30.60739Z",
    "indexed_at": "2026-02-18T04:01:18.952425",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Turn app ideas described in AI chat into live full-stack web applications"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic capability but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "bbd0225288aea080",
    "name": "ai.auteng/docs",
    "source": "mcp",
    "source_url": "https://auteng.ai",
    "description": "Publish markdown documents as public share links with mermaid diagram support. Built by AutEng.ai",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2026-02-06T20:56:39.150934Z",
    "indexed_at": "2026-02-18T04:01:20.299150",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "AI native workspace\nfor technical documentation\nMarkdown, Mermaid diagrams, and KaTeX math√¢¬Ä¬îall in one place. Test the editor now, no account required.\nTry AutEng Editor\nNo account required\nCopy\nOpen In AutEng\nShare\n# Welcome to Your AI Documentation Assistant! √∞¬ü¬ë¬ã\n\nI'm here to help you create comprehensive, professional documentation for software engineering, mathematics, and data science projects. Let me show you what I can do.\n\n## √∞¬ü¬é¬Ø Core Capabilities\n\nI specialize in creating well-structured technical documents with rich formatting capabilities:\n\n- **Markdown Documentation** - Professional technical writing with clear structure\n- **Visual Diagrams** - Architecture, flowcharts, sequences, and more using Mermaid\n- **Mathematical Notation** - LaTeX equations for algorithms, proofs, and analysis\n- **Code Examples** - Syntax-highlighted code blocks in multiple languages\n- **Web Research** - I can search and fetch current information to ensure accuracy\n\n---\n\n## √∞¬ü¬ì¬ä Visual Diagrams with Mermaid\n\nI can create various types of diagrams to illustrate complex concepts:\n\n### System Architecture Example\n\n```mermaid\ngraph TB\n    A[User Interface] --> B[API Gateway]\n    B --> C[Authentication Service]\n    B --> D[Business Logic]\n    D --> E[(Database)]\n    D --> F[Cache Layer]\n    F --> E\n    D --> G[Message Queue]\n    G --> H[Background Workers]\n```\n\n### Sequence Diagram Example\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant App\n    participant API\n    participant DB\n    \n    User->>App: Request Data\n    App->>API: GET /data\n    API->>DB: Query\n    DB-->>API: Results\n    API-->>App: JSON Response\n    App-->>User: Display Data\n```\n\n---\n\n## √∞¬ü¬î¬¢ Mathematical Notation\n\nI can express complex mathematical concepts using LaTeX notation:\n\n### Inline Math\nThe quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ solves equations of the form $ax^2 + bx + c = 0$.\n\n### Display Math\nHere's the gradient descent update rule:\n\n$$\n\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t)\n$$\n\nAnd the softmax function used in neural networks:\n\n$$\n\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n$$\n\n---\n\n## √∞¬ü¬í¬ª Code Examples\n\nI provide syntax-highlighted code blocks for multiple languages:\n\n### Python Example\n```python\ndef fibonacci(n: int) -> int:\n    \"\"\"Calculate the nth Fibonacci number using dynamic programming.\"\"\"\n    if n <= 1:\n        return n\n    \n    dp = [0] * (n + 1)\n    dp[1] = 1\n    \n    for i in range(2, n + 1):\n        dp[i] = dp[i-1] + dp[i-2]\n    \n    return dp[n]\n```\n\n### TypeScript Example\n```typescript\ninterface User {\n    id: string;\n    name: string;\n    email: string;\n}\n\nasync function fetchUser(id: string): Promise<User> {\n    const response = await fetch(`/api/users/${id}`);\n    return response.json();\n}\n```\n\n---\n\n## √∞¬ü¬ì¬ã What I Can Create For You\n\n### Documentation Types\n- **Architecture Documents** - System design, component diagrams, data flows\n- **API Documentation** - Endpoints, request/response formats, authentication\n- **Algorithm Explanations** - With mathematical proofs and complexity analysis\n- **Tutorial Guides** - Step-by-step instructions with code examples\n- **Technical Specifications** - Requirements, constraints, and implementation details\n- **Data Pipeline Documentation** - ETL processes, transformations, and workflows\n\n### Special Features\n- **Research Integration** - I can search the web for current best practices and information\n- **Interactive Clarification** - I'll ask questions if I need more details\n- **Professional Quality** - Suitable for team documentation, academic papers, or technical blogs\n\n---\n\n## √∞¬ü¬ö¬Ä Getting Started with AI\n\nSimply describe what you need, and I'll create it for you. Here are some example requests:\n\n> *\"Create an OAuth 2.0 implementation guide with sequence diagrams\"*\n\n> *\"Document a microservices architecture for an e-commerce platform\"*\n\n> *\"Explain the mathematical foundations of gradient descent with proofs\"*\n\n> *\"Create API documentation for a REST API with authentication\"*\n\n> *\"Write a tutorial on implementing a binary search tree in Python\"*\n\n---\n\n## √∞¬ü¬é¬® Formatting Quick Reference\n\n| Feature | Syntax | Use Case |\n|---------|--------|----------|\n| **Mermaid Diagrams** | ` ```mermaid ` | Architecture, flows, sequences |\n| **Inline Math** | `$...$` | Equations within text |\n| **Display Math** | `$$...$$` | Centered equations |\n| **Code Blocks** | ` ```language ` | Code examples |\n| **Tables** | `| ... |` | Structured data |\n\n---\n\n## √∞¬ü¬í¬° Tips for Best Results\n\n1. **Be Specific** - The more details you provide, the better I can tailor the document\n2. **Mention Visuals** - Tell me if you want diagrams, equations, or code examples\n3. **Specify Audience** - Let me know the technical level (beginner, intermediate, expert)\n4. **Iterate** - I can refine and improve based on your feedback\n\n---\n\n## Ready to Begin?\n\nI'm here to transform your ideas into professional, comprehensive documentation. What would you like to create today?\nWelcome to Your AI Documentation Assistant! √∞¬ü¬ë¬ã\nI'm here to help you create comprehensive, professional documentation for software engineering, mathematics, and data science projects. Let me show you what I can do.\n√∞¬ü¬é¬Ø Core Capabilities\nI specialize in creating well-structured technical documents with rich formatting capabilities:\nMarkdown Documentation\n- Professional technical writing with clear structure\nVisual Diagrams\n- Architecture, flowcharts, sequences, and more using Mermaid\nMathematical Notation\n- LaTeX equations for algorithms, proofs, and analysis\nCode Examples\n- Syntax-highlighted code blocks in multiple languages\nWeb Research\n- I can search and fetch current information to ensure accuracy\n√∞¬ü¬ì¬ä Visual Diagrams with Mermaid\nI can create various types of diagrams to illustrate complex concepts:\nSystem Architecture Example\nRendering diagram...\nSequence Diagram Example\nRendering diagram...\n√∞¬ü¬î¬¢ Mathematical Notation\nI can express complex mathematical concepts using LaTeX notation:\nInline Math\nThe quadratic formula\nx\n=\n√¢¬à¬í\nb\n√Ç¬±\nb\n2\n√¢¬à¬í\n4\na\nc\n2\na\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\nx\n=\n2\na\n√¢¬à¬í\nb\n√Ç¬±\nb\n2\n√¢¬à¬í\n4\na\nc\n√¢¬Ä¬ã\n√¢¬Ä¬ã\nsolves equations of the form\na\nx\n2\n+\nb\nx\n+\nc\n=\n0\nax^2 + bx + c = 0\na\nx\n2\n+\nb\nx\n+\nc\n=\n0\n.\nDisplay Math\nHere's the gradient descent update rule:\n√é¬∏\nt\n+\n1\n=\n√é¬∏\nt\n√¢¬à¬í\n√é¬±\n√¢¬à¬á\nJ\n(\n√é¬∏\nt\n)\n\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t)\n√é¬∏\nt\n+\n1\n√¢¬Ä¬ã\n=\n√é¬∏\nt\n√¢¬Ä¬ã\n√¢¬à¬í\n√é¬±\n√¢¬à¬á\nJ\n(\n√é¬∏\nt\n√¢¬Ä¬ã\n)\nAnd the softmax function used in neural networks:\n√è¬É\n(\nz\n)\ni\n=\ne\nz\ni\n√¢¬à¬ë\nj\n=\n1\nK\ne\nz\nj\n\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n√è¬É\n(\nz\n)\ni\n√¢¬Ä¬ã\n=\n√¢¬à¬ë\nj\n=\n1\nK\n√¢¬Ä¬ã\ne\nz\nj\n√¢¬Ä¬ã\ne\nz\ni\n√¢¬Ä¬ã\n√¢¬Ä¬ã\n√∞¬ü¬í¬ª Code Examples\nI provide syntax-highlighted code blocks for multiple languages:\nPython Example\ndef\nfibonacci\n(\nn\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Calculate the nth Fibonacci number using dynamic programming.\"\"\"\nif\nn\n<=\n1\n:\nreturn\nn\ndp\n=\n[\n0\n]\n*\n(\nn\n+\n1\n)\ndp\n[\n1\n]\n=\n1\nfor\ni\nin\nrange\n(\n2\n,\nn\n+\n1\n)\n:\ndp\n[\ni\n]\n=\ndp\n[\ni\n-\n1\n]\n+\ndp\n[\ni\n-\n2\n]\nreturn\ndp\n[\nn\n]\nTypeScript Example\ninterface\nUser\n{\nid\n:\nstring\n;\nname\n:\nstring\n;\nemail\n:\nstring\n;\n}\nasync\nfunction\nfetchUser\n(\nid\n:\nstring\n)\n:\nPromise\n<\nUser\n>\n{\nconst\nresponse\n=\nawait\nfetch\n(\n`\n/api/users/\n${\nid\n}\n`\n)\n;\nreturn\nresponse\n.\njson\n(\n)\n;\n}\n√∞¬ü¬ì¬ã What I Can Create For You\nDocumentation Types\nArchitecture Documents\n- System design, component diagrams, data flows\nAPI Documentation\n- Endpoints, request/response formats, authentication\nAlgorithm Explanations\n- With mathematical proofs and complexity analysis\nTutorial Guides\n- Step-by-step instructions with code examples\nTechnical Specifications\n- Requirements, constraints, and implementation details\nData Pipeline Documentation\n- ETL processes, transformations, and workflows\nSpecial Features\nResearch Integration\n- I can search the web for current best practices and information\nInteractive Clarification\n- I'll ask questions if I need more details\nProfessional Quality\n- Suitable for team documentation, academic papers, or technical blogs\n√∞¬ü¬ö¬Ä Getting Started with AI\nSimply describe what you need, and I'll create it for you. Here are some example requests:\n\"Create an OAuth 2.0 implementation guide with sequence diagrams\"\n\"Document a microservices architecture for an e-commerce platform\"\n\"Explain the mathematical foundations of gradient descent with proofs\"\n\"Create API documentation for a REST API with authentication\"\n\"Write a tutorial on implementing a binary search tree in Python\"\n√∞¬ü¬é¬® Formatting Quick Reference\nFeature\nSyntax\nUse Case\nMermaid Diagrams\n```mermaid\nArchitecture, flows, sequences\nInline Math\n$...$\nEquations within text\nDisplay Math\n$$...$$\nCentered equations\nCode Blocks\n```language\nCode examples\nTables\n`\n...\n√∞¬ü¬í¬° Tips for Best Results\nBe Specific\n- The more details you provide, the better I can tailor the document\nMention Visuals\n- Tell me if you want diagrams, equations, or code examples\nSpecify Audience\n- Let me know the technical level (beginner, intermediate, expert)\nIterate\n- I can refine and improve based on your feedback\nReady to Begin?\nI'm here to transform your ideas into professional, comprehensive documentation. What would you like to create today?\n4755 characters\nReady\nEdit markdown live\nRun AI tools\nDownload as .md\nMarkdown, Mermaid, Math\nFirst-class support for GitHub Flavored Markdown, Mermaid diagrams, and KaTeX math. Write docs the way developers actually want to.\nAI-Powered Generation\n\"Generate a sequence diagram for this flow.\" \"Draft an architecture doc.\" AI understands technical context and creates docs instantly.\nSave & Share\nOnce you're ready, create an account to save your work and generate public share links. Beautiful docs that are easy to share with your team.\nBuilt for technical teams\nFrom entity diagrams to math equations√¢¬Ä¬îeverything renders beautifully.\nAI-Powered Math Verification\nWatch the AI derive the quadratic formula step-by-step, calling Computer Algebra System (CAS) tools to verify each transformation√¢¬Ä¬îthen generate a document with embedded verification blocks.\nYour browser does not support the video tag.\nComplete Quadratic Formula Derivation\nView full tutorial\nAll steps verified together using\nmode=solve\nto ensure solution-set equivalence at each transformation.\nCAS Verification Block\nCopy\nVerify\nOpen In AutEng\nNot Verified\nAssumptions:\na != 0\n1\na\nx\n2\n+\nb\nx\n+\nc\n=\n0\nax^2 + bx + c = 0\na\nx\n2\n+\nb\nx\n+\nc\n=\n0\n2\na\nx\n2\n+\nb\nx\n=\n√¢¬à¬í\nc\nax^2 + bx = -c\na\nx\n2\n+\nb\nx\n=\n√¢¬à¬í\nc\n3\nx\n2\n+\nb\na\nx\n=\n√¢¬à¬í\nc\na\nx^2 + \\frac{b}{a}x = -\\frac{c}{a}\nx\n2\n+\na\nb\n√¢¬Ä¬ã\nx\n=\n√¢¬à¬í\na\nc\n√¢¬Ä¬ã\n4\nx\n2\n+\nb\na\nx\n+\nb\n2\n4\na\n2\n=\nb\n2\n√¢¬à¬í\n4\na\nc\n4\na\n2\nx^2 + \\frac{b}{a}x + \\frac{b^2}{4a^2} = \\frac{b^2 - 4ac}{4a^2}\nx\n2\n+\na\nb\n√¢¬Ä¬ã\nx\n+\n4\na\n2\nb\n2\n√¢¬Ä¬ã\n=\n4\na\n2\nb\n2\n√¢¬à¬í\n4\na\nc\n√¢¬Ä¬ã\n5\n(\nx\n+\nb\n2\na\n)\n2\n=\nb\n2\n√¢¬à¬í\n4\na\nc\n4\na\n2\n\\left(x + \\frac{b}{2a}\\right)^2 = \\frac{b^2 - 4ac}{4a^2}\n(\nx\n+\n2\na\nb\n√¢¬Ä¬ã\n)\n2\n=\n4\na\n2\nb\n2\n√¢¬à¬í\n4\na\nc\n√¢¬Ä¬ã\n6\nx\n=\n√¢¬à¬í\nb\n√Ç¬±\nb\n2\n√¢¬à¬í\n4\na\nc\n2\na\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\nx\n=\n2\na\n√¢¬à¬í\nb\n√Ç¬±\nb\n2\n√¢¬à¬í\n4\na\nc\n√¢¬Ä¬ã\n√¢¬Ä¬ã\nClick\nVerify\nor\nOpen In AutEng\nto run CAS verification on this derivation\nto edit and customize it.\nFormal Theorem Proving with Lean 4\nMachine-checked mathematical proofs using Lean 4 and Mathlib. When you need absolute certainty, not just heuristic verification.\nCAS (SymPy)\nAlgebraic Verification\nQuick identity checks (sin√Ç¬≤x + cos√Ç¬≤x = 1)\nSymbolic simplification\n~1-5 seconds verification\nBest for: Quick algebraic checks\nLean 4 + Mathlib\nFormal Proof Verification\nInduction, case analysis, complex reasoning\nMachine-checked proof terms\n~2-5 seconds (warm server)\nBest for: Rigorous formal proofs\nExample: Sum of First n Odd Numbers = n√Ç¬≤\nView verification docs\nA classic proof by induction: 1 + 3 + 5 + ... + (2n-1) = n√Ç¬≤. Verified using Lean 4's\ninduction\ntactic and Mathlib's\nring\ntactic.\nLean 4 Proof Block\nCopy\nVerify\nOpen In AutEng\nNot Verified\nLean 4 + Mathlib\n1\nimport Mathlib.Tactic\n2\nimport Mathlib.Algebra.BigOperators.Group.Finset.Basic\n3\n4\nopen Finset BigOperators\n5\n6\ntheorem sum_odd_numbers (n : √¢¬Ñ¬ï) : √¢¬à¬ë i √¢¬à¬à range n, (2 * i + 1) = n^2 := by\n7\ninduction n with\n8\n| zero => simp\n9\n| succ n ih =>\n10\nrw [sum_range_succ, ih]\n11\nring\nClick\nVerify\nto run formal verification on this proof, or\nOpen In AutEng\nto edit and customize it.\nAI-Assisted Proof Generation\nOur AI can generate Lean proofs and automatically verify them. Ask it to \"prove that the sum of the first n natural numbers is n(n+1)/2\" and watch it construct a machine-checked proof using induction and Mathlib tactics.\nStop fighting your docs tool.\nConfluence wasn't built for engineers. Notion doesn't speak Markdown natively. Google Docs can't render Mermaid diagrams or math equations.\nAutEng is the workspace technical teams actually want to use.\nArchitecture Docs\nADRs & RFCs\nAPI Specs\nRunbooks"
    },
    "llm_extracted": {
      "capabilities": [
        "Create professional technical documentation using Markdown",
        "Generate visual diagrams including architecture, flowcharts, and sequence diagrams using Mermaid",
        "Render mathematical notation with LaTeX for algorithms, proofs, and analysis",
        "Provide syntax-highlighted code examples in multiple programming languages",
        "Perform web research to fetch current information and best practices",
        "Produce various documentation types such as architecture documents, API documentation, algorithm explanations, tutorial guides, technical specifications, and data pipeline documentation",
        "Integrate interactive clarification by asking questions to refine documentation",
        "Verify mathematical derivations using Computer Algebra System (CAS) tools and formal theorem proving with Lean 4"
      ],
      "limitations": [],
      "requirements": [
        "No account required for testing the editor",
        "Account creation required to save work and generate public share links"
      ]
    },
    "documentation_quality": 0.55,
    "quality_rationale": "The documentation provides detailed descriptions of capabilities, examples of diagrams, math notation, and code, but lacks explicit installation instructions and detailed limitations.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "AI native workspace\nfor technical documentation\nMarkdown, Mermaid diagrams, and KaTeX math√¢¬Ä¬îall in one place. Test the editor now, no account required.\nTry AutEng Editor\nNo account required\nCopy\nOpen In AutEng\nShare\n# Welcome to Your AI Documentation Assistant! √∞¬ü¬ë¬ã\n\nI'm here to help you create comprehensive, professional documentation for software engineering, mathematics, and data science projects. Let me show you what I can do.\n\n## √∞¬ü¬é¬Ø Core Capabilities\n\nI specialize in creating well-structured technical documents with rich formatting capabilities:\n\n- **Markdown Documentation** - Professional technical writing with clear structure\n- **Visual Diagrams** - Architecture, flowcharts, sequences, and more using Mermaid\n- **Mathematical Notation** - LaTeX equations for algorithms, proofs, and analysis\n- **Code Examples** - Syntax-highlighted code blocks in multiple languages\n- **Web Research** - I can search and fetch current information to ensure accuracy\n\n---\n\n## √∞¬ü¬ì¬ä Visual Diagrams with Mermaid\n\nI can create various types of diagrams to illustrate complex concepts:\n\n### System Architecture Example\n\n```mermaid\ngraph TB\n    A[User Interface] --> B[API Gateway]\n    B --> C[Authentication Service]\n    B --> D[Business Logic]\n    D --> E[(Database)]\n    D --> F[Cache Layer]\n    F --> E\n    D --> G[Message Queue]\n    G --> H[Background Workers]\n```\n\n### Sequence Diagram Example\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant App\n    participant API\n    participant DB\n    \n    User->>App: Request Data\n    App->>API: GET /data\n    API->>DB: Query\n    DB-->>API: Results\n    API-->>App: JSON Response\n    App-->>User: Display Data\n```\n\n---\n\n## √∞¬ü¬î¬¢ Mathematical Notation\n\nI can express complex mathematical concepts using LaTeX notation:\n\n### Inline Math\nThe quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ solves equations of the form $ax^2 + bx + c = 0$.",
        "start_pos": 0,
        "end_pos": 1903,
        "token_count_estimate": 475,
        "source_type": "detail_page",
        "agent_id": "bbd0225288aea080"
      },
      {
        "chunk_id": 1,
        "text": "ion\n\nI can express complex mathematical concepts using LaTeX notation:\n\n### Inline Math\nThe quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ solves equations of the form $ax^2 + bx + c = 0$.\n\n### Display Math\nHere's the gradient descent update rule:\n\n$$\n\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t)\n$$\n\nAnd the softmax function used in neural networks:\n\n$$\n\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n$$\n\n---\n\n## √∞¬ü¬í¬ª Code Examples\n\nI provide syntax-highlighted code blocks for multiple languages:\n\n### Python Example\n```python\ndef fibonacci(n: int) -> int:\n    \"\"\"Calculate the nth Fibonacci number using dynamic programming.\"\"\"\n    if n <= 1:\n        return n\n    \n    dp = [0] * (n + 1)\n    dp[1] = 1\n    \n    for i in range(2, n + 1):\n        dp[i] = dp[i-1] + dp[i-2]\n    \n    return dp[n]\n```\n\n### TypeScript Example\n```typescript\ninterface User {\n    id: string;\n    name: string;\n    email: string;\n}\n\nasync function fetchUser(id: string): Promise<User> {\n    const response = await fetch(`/api/users/${id}`);\n    return response.json();\n}\n```\n\n---\n\n## √∞¬ü¬ì¬ã What I Can Create For You\n\n### Documentation Types\n- **Architecture Documents** - System design, component diagrams, data flows\n- **API Documentation** - Endpoints, request/response formats, authentication\n- **Algorithm Explanations** - With mathematical proofs and complexity analysis\n- **Tutorial Guides** - Step-by-step instructions with code examples\n- **Technical Specifications** - Requirements, constraints, and implementation details\n- **Data Pipeline Documentation** - ETL processes, transformations, and workflows\n\n### Special Features\n- **Research Integration** - I can search the web for current best practices and information\n- **Interactive Clarification** - I'll ask questions if I need more details\n- **Professional Quality** - Suitable for team documentation, academic papers, or technical blogs\n\n---\n\n## √∞¬ü¬ö¬Ä Getting Started with AI\n\nSimply describe what you need, and I'll create it for you.",
        "start_pos": 1703,
        "end_pos": 3705,
        "token_count_estimate": 500,
        "source_type": "detail_page",
        "agent_id": "bbd0225288aea080"
      },
      {
        "chunk_id": 2,
        "text": "ails\n- **Professional Quality** - Suitable for team documentation, academic papers, or technical blogs\n\n---\n\n## √∞¬ü¬ö¬Ä Getting Started with AI\n\nSimply describe what you need, and I'll create it for you. Here are some example requests:\n\n> *\"Create an OAuth 2.0 implementation guide with sequence diagrams\"*\n\n> *\"Document a microservices architecture for an e-commerce platform\"*\n\n> *\"Explain the mathematical foundations of gradient descent with proofs\"*\n\n> *\"Create API documentation for a REST API with authentication\"*\n\n> *\"Write a tutorial on implementing a binary search tree in Python\"*\n\n---\n\n## √∞¬ü¬é¬® Formatting Quick Reference\n\n| Feature | Syntax | Use Case |\n|---------|--------|----------|\n| **Mermaid Diagrams** | ` ```mermaid ` | Architecture, flows, sequences |\n| **Inline Math** | `$...$` | Equations within text |\n| **Display Math** | `$$...$$` | Centered equations |\n| **Code Blocks** | ` ```language ` | Code examples |\n| **Tables** | `| ... |` | Structured data |\n\n---\n\n## √∞¬ü¬í¬° Tips for Best Results\n\n1. **Be Specific** - The more details you provide, the better I can tailor the document\n2. **Mention Visuals** - Tell me if you want diagrams, equations, or code examples\n3. **Specify Audience** - Let me know the technical level (beginner, intermediate, expert)\n4. **Iterate** - I can refine and improve based on your feedback\n\n---\n\n## Ready to Begin?\n\nI'm here to transform your ideas into professional, comprehensive documentation. What would you like to create today?\nWelcome to Your AI Documentation Assistant! √∞¬ü¬ë¬ã\nI'm here to help you create comprehensive, professional documentation for software engineering, mathematics, and data science projects. Let me show you what I can do.",
        "start_pos": 3505,
        "end_pos": 5206,
        "token_count_estimate": 425,
        "source_type": "detail_page",
        "agent_id": "bbd0225288aea080"
      },
      {
        "chunk_id": 3,
        "text": "ofessional technical writing with clear structure\nVisual Diagrams\n- Architecture, flowcharts, sequences, and more using Mermaid\nMathematical Notation\n- LaTeX equations for algorithms, proofs, and analysis\nCode Examples\n- Syntax-highlighted code blocks in multiple languages\nWeb Research\n- I can search and fetch current information to ensure accuracy\n√∞¬ü¬ì¬ä Visual Diagrams with Mermaid\nI can create various types of diagrams to illustrate complex concepts:\nSystem Architecture Example\nRendering diagram...\nSequence Diagram Example\nRendering diagram...\n√∞¬ü¬î¬¢ Mathematical Notation\nI can express complex mathematical concepts using LaTeX notation:\nInline Math\nThe quadratic formula\nx\n=\n√¢¬à¬í\nb\n√Ç¬±\nb\n2\n√¢¬à¬í\n4\na\nc\n2\na\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\nx\n=\n2\na\n√¢¬à¬í\nb\n√Ç¬±\nb\n2\n√¢¬à¬í\n4\na\nc\n√¢¬Ä¬ã\n√¢¬Ä¬ã\nsolves equations of the form\na\nx\n2\n+\nb\nx\n+\nc\n=\n0\nax^2 + bx + c = 0\na\nx\n2\n+\nb\nx\n+\nc\n=\n0\n.\nDisplay Math\nHere's the gradient descent update rule:\n√é¬∏\nt\n+\n1\n=\n√é¬∏\nt\n√¢¬à¬í\n√é¬±\n√¢¬à¬á\nJ\n(\n√é¬∏\nt\n)\n\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t)\n√é¬∏\nt\n+\n1\n√¢¬Ä¬ã\n=\n√é¬∏\nt\n√¢¬Ä¬ã\n√¢¬à¬í\n√é¬±\n√¢¬à¬á\nJ\n(\n√é¬∏\nt\n√¢¬Ä¬ã\n)\nAnd the softmax function used in neural networks:\n√è¬É\n(\nz\n)\ni\n=\ne\nz\ni\n√¢¬à¬ë\nj\n=\n1\nK\ne\nz\nj\n\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n√è¬É\n(\nz\n)\ni\n√¢¬Ä¬ã\n=\n√¢¬à¬ë\nj\n=\n1\nK\n√¢¬Ä¬ã\ne\nz\nj\n√¢¬Ä¬ã\ne\nz\ni\n√¢¬Ä¬ã\n√¢¬Ä¬ã\n√∞¬ü¬í¬ª Code Examples\nI provide syntax-highlighted code blocks for multiple languages:\nPython Example\ndef\nfibonacci\n(\nn\n:\nint\n)\n-\n>\nint\n:\n\"\"\"Calculate the nth Fibonacci number using dynamic programming.\"\"\"\nif\nn\n<=\n1\n:\nreturn\nn\ndp\n=\n[\n0\n]\n*\n(\nn\n+\n1\n)\ndp\n[\n1\n]\n=\n1\nfor\ni\nin\nrange\n(\n2\n,\nn\n+\n1\n)\n:\ndp\n[\ni\n]\n=\ndp\n[\ni\n-\n1\n]\n+\ndp\n[\ni\n-\n2\n]\nreturn\ndp\n[\nn\n]\nTypeScript Example\ninterface\nUser\n{\nid\n:\nstring\n;\nname\n:\nstring\n;\nemail\n:\nstring\n;\n}\nasync\nfunction\nfetchUser\n(\nid\n:\nstring\n)\n:\nPromise\n<\nUser\n>\n{\nconst\nresponse\n=\nawait\nfetch\n(\n`\n/api/users/\n${\nid\n}\n`\n)\n;\nreturn\nresponse\n.",
        "start_pos": 5353,
        "end_pos": 7189,
        "token_count_estimate": 459,
        "source_type": "detail_page",
        "agent_id": "bbd0225288aea080"
      },
      {
        "chunk_id": 4,
        "text": "}\n√∞¬ü¬ì¬ã What I Can Create For You\nDocumentation Types\nArchitecture Documents\n- System design, component diagrams, data flows\nAPI Documentation\n- Endpoints, request/response formats, authentication\nAlgorithm Explanations\n- With mathematical proofs and complexity analysis\nTutorial Guides\n- Step-by-step instructions with code examples\nTechnical Specifications\n- Requirements, constraints, and implementation details\nData Pipeline Documentation\n- ETL processes, transformations, and workflows\nSpecial Features\nResearch Integration\n- I can search the web for current best practices and information\nInteractive Clarification\n- I'll ask questions if I need more details\nProfessional Quality\n- Suitable for team documentation, academic papers, or technical blogs\n√∞¬ü¬ö¬Ä Getting Started with AI\nSimply describe what you need, and I'll create it for you. Here are some example requests:\n\"Create an OAuth 2.0 implementation guide with sequence diagrams\"\n\"Document a microservices architecture for an e-commerce platform\"\n\"Explain the mathematical foundations of gradient descent with proofs\"\n\"Create API documentation for a REST API with authentication\"\n\"Write a tutorial on implementing a binary search tree in Python\"\n√∞¬ü¬é¬® Formatting Quick Reference\nFeature\nSyntax\nUse Case\nMermaid Diagrams\n```mermaid\nArchitecture, flows, sequences\nInline Math\n$...$\nEquations within text\nDisplay Math\n$$...$$\nCentered equations\nCode Blocks\n```language\nCode examples\nTables\n`\n...\n√∞¬ü¬í¬° Tips for Best Results\nBe Specific\n- The more details you provide, the better I can tailor the document\nMention Visuals\n- Tell me if you want diagrams, equations, or code examples\nSpecify Audience\n- Let me know the technical level (beginner, intermediate, expert)\nIterate\n- I can refine and improve based on your feedback\nReady to Begin?\nI'm here to transform your ideas into professional, comprehensive documentation. What would you like to create today?",
        "start_pos": 7201,
        "end_pos": 9114,
        "token_count_estimate": 478,
        "source_type": "detail_page",
        "agent_id": "bbd0225288aea080"
      },
      {
        "chunk_id": 5,
        "text": "expert)\nIterate\n- I can refine and improve based on your feedback\nReady to Begin?\nI'm here to transform your ideas into professional, comprehensive documentation. What would you like to create today?\n4755 characters\nReady\nEdit markdown live\nRun AI tools\nDownload as .md\nMarkdown, Mermaid, Math\nFirst-class support for GitHub Flavored Markdown, Mermaid diagrams, and KaTeX math. Write docs the way developers actually want to.\nAI-Powered Generation\n\"Generate a sequence diagram for this flow.\" \"Draft an architecture doc.\" AI understands technical context and creates docs instantly.\nSave & Share\nOnce you're ready, create an account to save your work and generate public share links. Beautiful docs that are easy to share with your team.\nBuilt for technical teams\nFrom entity diagrams to math equations√¢¬Ä¬îeverything renders beautifully.\nAI-Powered Math Verification\nWatch the AI derive the quadratic formula step-by-step, calling Computer Algebra System (CAS) tools to verify each transformation√¢¬Ä¬îthen generate a document with embedded verification blocks.\nYour browser does not support the video tag.\nComplete Quadratic Formula Derivation\nView full tutorial\nAll steps verified together using\nmode=solve\nto ensure solution-set equivalence at each transformation.",
        "start_pos": 8914,
        "end_pos": 10178,
        "token_count_estimate": 315,
        "source_type": "detail_page",
        "agent_id": "bbd0225288aea080"
      },
      {
        "chunk_id": 6,
        "text": "2\n√¢¬à¬í\n4\na\nc\n√¢¬Ä¬ã\n6\nx\n=\n√¢¬à¬í\nb\n√Ç¬±\nb\n2\n√¢¬à¬í\n4\na\nc\n2\na\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\nx\n=\n2\na\n√¢¬à¬í\nb\n√Ç¬±\nb\n2\n√¢¬à¬í\n4\na\nc\n√¢¬Ä¬ã\n√¢¬Ä¬ã\nClick\nVerify\nor\nOpen In AutEng\nto run CAS verification on this derivation\nto edit and customize it.\nFormal Theorem Proving with Lean 4\nMachine-checked mathematical proofs using Lean 4 and Mathlib. When you need absolute certainty, not just heuristic verification.\nCAS (SymPy)\nAlgebraic Verification\nQuick identity checks (sin√Ç¬≤x + cos√Ç¬≤x = 1)\nSymbolic simplification\n~1-5 seconds verification\nBest for: Quick algebraic checks\nLean 4 + Mathlib\nFormal Proof Verification\nInduction, case analysis, complex reasoning\nMachine-checked proof terms\n~2-5 seconds (warm server)\nBest for: Rigorous formal proofs\nExample: Sum of First n Odd Numbers = n√Ç¬≤\nView verification docs\nA classic proof by induction: 1 + 3 + 5 + ... + (2n-1) = n√Ç¬≤. Verified using Lean 4's\ninduction\ntactic and Mathlib's\nring\ntactic.\nLean 4 Proof Block\nCopy\nVerify\nOpen In AutEng\nNot Verified\nLean 4 + Mathlib\n1\nimport Mathlib.Tactic\n2\nimport Mathlib.Algebra.BigOperators.Group.Finset.Basic\n3\n4\nopen Finset BigOperators\n5\n6\ntheorem sum_odd_numbers (n : √¢¬Ñ¬ï) : √¢¬à¬ë i √¢¬à¬à range n, (2 * i + 1) = n^2 := by\n7\ninduction n with\n8\n| zero => simp\n9\n| succ n ih =>\n10\nrw [sum_range_succ, ih]\n11\nring\nClick\nVerify\nto run formal verification on this proof, or\nOpen In AutEng\nto edit and customize it.\nAI-Assisted Proof Generation\nOur AI can generate Lean proofs and automatically verify them. Ask it to \"prove that the sum of the first n natural numbers is n(n+1)/2\" and watch it construct a machine-checked proof using induction and Mathlib tactics.\nStop fighting your docs tool.\nConfluence wasn't built for engineers. Notion doesn't speak Markdown natively. Google Docs can't render Mermaid diagrams or math equations.\nAutEng is the workspace technical teams actually want to use.\nArchitecture Docs\nADRs & RFCs\nAPI Specs\nRunbooks",
        "start_pos": 10762,
        "end_pos": 12672,
        "token_count_estimate": 477,
        "source_type": "detail_page",
        "agent_id": "bbd0225288aea080"
      },
      {
        "chunk_id": 7,
        "text": "n't speak Markdown natively. Google Docs can't render Mermaid diagrams or math equations.\nAutEng is the workspace technical teams actually want to use.\nArchitecture Docs\nADRs & RFCs\nAPI Specs\nRunbooks",
        "start_pos": 12472,
        "end_pos": 12672,
        "token_count_estimate": 50,
        "source_type": "detail_page",
        "agent_id": "bbd0225288aea080"
      }
    ]
  },
  {
    "agent_id": "8fb20e3c81a992eb",
    "name": "ai.auteng/mcp",
    "source": "mcp",
    "source_url": "https://github.com/auteng/auteng",
    "description": "Publish markdown as a publicly shareable AutEng document",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-06T16:52:00.816324Z",
    "indexed_at": "2026-02-18T04:01:20.523905",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Publish markdown as a publicly shareable document"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with minimal detail, providing only a basic capability without examples or additional context.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "fe2e184b5b9948c9",
    "name": "ai.autoblocks/contextlayer-mcp",
    "source": "mcp",
    "source_url": "https://github.com/autoblocksai/ctxl",
    "description": "Personal context management for AI assistants",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-12T03:52:52.134106Z",
    "indexed_at": "2026-02-18T04:01:24.834369",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage personal context for AI assistants"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with minimal detail, providing only a basic idea of the server's purpose.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "efe1f330fb406df0",
    "name": "ai.cirra/salesforce-mcp",
    "source": "mcp",
    "source_url": "https://github.com/cirra-ai/mcp-server",
    "description": "Comprehensive Salesforce administration and data management capabilities",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-12-13T01:53:10.483779Z",
    "indexed_at": "2026-02-18T04:01:29.552630",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide Salesforce administration functionalities",
        "Manage Salesforce data comprehensively"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence providing a basic overview without details, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "79836362f8a60ef4",
    "name": "ai.com.mcp/contabo",
    "source": "mcp",
    "source_url": "https://github.com/la-rebelion/hapimcp",
    "description": "Contabo API (v1.0.0) as MCP tools for cloud provisioning, and management. Powered by HAPI MCP server",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-01-08T13:19:46.430479Z",
    "indexed_at": "2026-02-18T04:01:34.162320",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# HAPI MCP ‚Äì Headless API for Model Context Protocol\n\n**Stop rewriting systems for AI. Instantly turn your APIs into MCP servers.**\n\nHAPI MCP lifts your OpenAPI catalog into MCP tools automatically, keeping design (OAS/Arazzo) and runtime (agents/LLMs) cleanly separated. Your existing APIs become AI-ready tools‚Äîno new business logic, no sidecar servers, no rework.\n\n## Key Features\n\n- **No rewrites:** Reuse 100% of your API logic. OpenAPI in, MCP tools out. No parallel codebase or shadow services.\n- **Faster time-to-value:** Turn specs into agent-ready tools in minutes. Update the spec, ship new tools instantly.\n- **Reduced risk:** Permissions, auth, rate limits, auditability are inherited from your APIs‚Äîgovernance without bolt-ons.\n- **Scales with you:** [runMCP](https://run.mcp.com.ai) delivers serverless-like elasticity with long-running when needed. Cold-start fast, stay warm for throughput.\n- **Deterministic orchestration:** OrcA plans and executes multi-tool tasks predictably‚Äîno brittle prompt chaining.\n- **Vendor-neutral:** Works with any MCP client: ChatGPT, Claude, [QBot](https://qbot.mcp.com.ai), [chatMCP](https://chat.mcp.com.ai). OAS + MCP + Arazzo stay portable.\n\n## How It Works\n\n1. **Pick your API spec**\n   - OpenAPI, Swagger, REST‚ÄîHAPI CLI works with any API specification format.\n   - Supports OpenAPI 3.0+, REST APIs, OAuth 2.0 Dynamic Client Registration.\n\n2. **Run a single command**\n   - One simple CLI command transforms your API into a usable MCP Server.\n   - No complex setup, zero configuration, works anywhere, cross-platform.\n\n3. **Use instantly**\n   - Your API is now ready as a tool, AI agent, or testing interface.\n   - MCP server ready, AI agent compatible, testing interface, developer experience friendly.\n\n## Visual Pipeline\n\n```\nAPI Spec (swagger.json) ‚Üí HAPI CLI ‚Üí Usable Tool (AI-ready) ‚Üê MCP Clients\n```\n\n```\n[ OpenAPI / Swagger ]       [ HAPI CLI ]        [ MCP Server ]\n       (Your API)         ‚Üí   (Transform)   ‚Üí   (Usable Tool)\n                                                 ‚Üë          ‚Üë\n                                                 |          |\n                                          [ ChatGPT ]  [ runMCP ]\n                                          [ Claude  ]  [ OrcA   ]\n                                          [ QBot    ]  [ chatMCP]\n                                          [ Agents  ]\n```\n\n## Who Wins With HAPI\n\n- **Executives:** Ship AI initiatives without ballooning cost. Keep teams focused on outcomes, not rewrites and integration sprawl.\n- **Architects & PMs:** Design with OpenAPI/Arazzo, run with MCP. Clear contracts, policy inheritance, and versioned workflows keep risk low.\n- **Engineers & Ops:** Deploy once. HAPI Server + runMCP scale tools; QBot/chatMCP give fast feedback; OrcA keeps executions deterministic.\n\n## Your Stack, Already Wired\n\n- [**HAPI Server:**](https://hapi.mcp.com.ai) Turns OpenAPI into MCP tools automatically‚Äîcontracts stay in sync with your source of truth.\n- [**runMCP:**](https://run.mcp.com.ai) Autoscaling execution and testing for MCP tools; cold-start fast, stay warm when workflows run long.\n- [**QBot:**](https://qbot.mcp.com.ai) CLI TUI for power users to interact with MCP tools directly from terminal or scripts.\n- [**chatMCP:**](https://chat.mcp.com.ai) Conversational client that speaks MCP natively for support, ops, and internal assistants.\n- [**OrcA:**](https://orca.mcp.com.ai) Deterministic planning and orchestration for multi-tool tasks; no brittle prompt spaghetti.\n- **Agents:** Build agentic systems from standard APIs‚Äîno custom glue. Connect MCP clients across platforms.\n\n## Demos\n\n- [Playlist with demos](https://www.youtube.com/playlist?list=PL7wYqDMFQYFOZuB1nNOfisbVzb1uHw1fz)\n- New [HAPI MCP YT Channel](https://youtube.com/@hapi-mcp) with demos API-First and MCP\n\n## Documentation\n\n- [Explore the Docs](https://docs.mcp.com.ai)\n- [See HAPI Server](https://hapi.mcp.com.ai)\n\n## FAQ\n\n- **Do we need to rewrite our services?**  \n  No. HAPI MCP lifts your existing OpenAPI specs directly into MCP tools. Your auth, validation, and business rules remain unchanged.\n\n- **Is this just another MCP server?**  \n  It‚Äôs the **Headless API** model: your API becomes the runtime. HAPI Server reflects it as MCP; runMCP scales it; OrcA orchestrates it. No duplicate logic.\n\n- **Which clients can consume the tools?**  \n  Any MCP client: ChatGPT, Claude, QBot, chatMCP, bespoke orchestrators‚Äîvendor-neutral by design.\n\n- **How do we keep control and compliance?**  \n  Your API remains the single source of truth. Policies, RBAC, rate limits, and audit logs flow through automatically; **no shadow logic**.\n\n- **What about security and privacy?**  \n  Scoped credentials, per-tool permissions, and auditable calls are inherited from your API layer. HAPI adds guardrails and observability for regulated environments.\n\n- **How fast can we get started?**  \n  You can transform your first API into an MCP tool in less than five minutes. The HAPI CLI makes it quick and painless.\n\n## Get Started\n\n1. Install HAPI CLI from latest release: https://github.com/la-rebelion/hapimcp/releases\n2. Transform your OpenAPI spec into an MCP server with a single command:\n   ```bash\n   hapi serve path/to/your/swagger.json --headless\n   ```\n3. Start using your MCP tools with any compatible client!\n\n4. Explore the [documentation](https://docs.mcp.com.ai) for advanced usage and best practices.\n\n## Contributing\n\nContributions are welcome! Open a discussion or submit a pull request on GitHub: https://github.com/la-rebelion/hapimcp\n\n## License\n\nHAPI MCP is licensed under the MIT License. See LICENSE for details."
    },
    "llm_extracted": {
      "capabilities": [
        "Transform OpenAPI specifications into MCP servers automatically",
        "Reuse existing API logic without rewriting or creating shadow services",
        "Enable AI agents and testing interfaces to interact with APIs as MCP tools",
        "Inherit permissions, authentication, rate limits, and auditability from existing APIs",
        "Scale MCP tools elastically with runMCP for serverless-like execution",
        "Orchestrate multi-tool tasks deterministically using OrcA",
        "Support any MCP client including ChatGPT, Claude, QBot, and chatMCP",
        "Provide a CLI for quick transformation of API specs into MCP servers",
        "Maintain vendor-neutral and portable API and MCP tool design"
      ],
      "limitations": [
        "No custom business logic or sidecar servers are created; only existing API logic is exposed",
        "Requires APIs to be defined with OpenAPI 3.0+ or compatible REST specifications",
        "Does not handle API design or runtime beyond transforming specs into MCP tools",
        "Dependent on existing API security and governance mechanisms; HAPI adds guardrails but does not replace them"
      ],
      "requirements": [
        "An existing API defined with OpenAPI 3.0+ or Swagger specification",
        "HAPI CLI installed from the official GitHub releases",
        "Compatible MCP clients to consume the generated MCP tools",
        "Access to API credentials and permissions as managed by the original API",
        "Cross-platform environment capable of running the HAPI CLI"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed feature descriptions, supported tools, limitations, and requirements, making it excellent for users.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# HAPI MCP ‚Äì Headless API for Model Context Protocol\n\n**Stop rewriting systems for AI. Instantly turn your APIs into MCP servers.**\n\nHAPI MCP lifts your OpenAPI catalog into MCP tools automatically, keeping design (OAS/Arazzo) and runtime (agents/LLMs) cleanly separated. Your existing APIs become AI-ready tools‚Äîno new business logic, no sidecar servers, no rework.\n\n## Key Features\n\n- **No rewrites:** Reuse 100% of your API logic. OpenAPI in, MCP tools out. No parallel codebase or shadow services.\n- **Faster time-to-value:** Turn specs into agent-ready tools in minutes. Update the spec, ship new tools instantly.\n- **Reduced risk:** Permissions, auth, rate limits, auditability are inherited from your APIs‚Äîgovernance without bolt-ons.\n- **Scales with you:** [runMCP](https://run.mcp.com.ai) delivers serverless-like elasticity with long-running when needed. Cold-start fast, stay warm for throughput.\n- **Deterministic orchestration:** OrcA plans and executes multi-tool tasks predictably‚Äîno brittle prompt chaining.\n- **Vendor-neutral:** Works with any MCP client: ChatGPT, Claude, [QBot](https://qbot.mcp.com.ai), [chatMCP](https://chat.mcp.com.ai). OAS + MCP + Arazzo stay portable.\n\n## How It Works\n\n1. **Pick your API spec**\n   - OpenAPI, Swagger, REST‚ÄîHAPI CLI works with any API specification format.\n   - Supports OpenAPI 3.0+, REST APIs, OAuth 2.0 Dynamic Client Registration.\n\n2. **Run a single command**\n   - One simple CLI command transforms your API into a usable MCP Server.\n   - No complex setup, zero configuration, works anywhere, cross-platform.\n\n3. **Use instantly**\n   - Your API is now ready as a tool, AI agent, or testing interface.\n   - MCP server ready, AI agent compatible, testing interface, developer experience friendly.",
        "start_pos": 0,
        "end_pos": 1756,
        "token_count_estimate": 439,
        "source_type": "readme",
        "agent_id": "79836362f8a60ef4"
      },
      {
        "chunk_id": 1,
        "text": "Clients\n```\n\n```\n[ OpenAPI / Swagger ]       [ HAPI CLI ]        [ MCP Server ]\n       (Your API)         ‚Üí   (Transform)   ‚Üí   (Usable Tool)\n                                                 ‚Üë          ‚Üë\n                                                 |          |\n                                          [ ChatGPT ]  [ runMCP ]\n                                          [ Claude  ]  [ OrcA   ]\n                                          [ QBot    ]  [ chatMCP]\n                                          [ Agents  ]\n```\n\n## Who Wins With HAPI\n\n- **Executives:** Ship AI initiatives without ballooning cost. Keep teams focused on outcomes, not rewrites and integration sprawl.\n- **Architects & PMs:** Design with OpenAPI/Arazzo, run with MCP. Clear contracts, policy inheritance, and versioned workflows keep risk low.\n- **Engineers & Ops:** Deploy once. HAPI Server + runMCP scale tools; QBot/chatMCP give fast feedback; OrcA keeps executions deterministic.\n\n## Your Stack, Already Wired\n\n- [**HAPI Server:**](https://hapi.mcp.com.ai) Turns OpenAPI into MCP tools automatically‚Äîcontracts stay in sync with your source of truth.\n- [**runMCP:**](https://run.mcp.com.ai) Autoscaling execution and testing for MCP tools; cold-start fast, stay warm when workflows run long.\n- [**QBot:**](https://qbot.mcp.com.ai) CLI TUI for power users to interact with MCP tools directly from terminal or scripts.\n- [**chatMCP:**](https://chat.mcp.com.ai) Conversational client that speaks MCP natively for support, ops, and internal assistants.\n- [**OrcA:**](https://orca.mcp.com.ai) Deterministic planning and orchestration for multi-tool tasks; no brittle prompt spaghetti.\n- **Agents:** Build agentic systems from standard APIs‚Äîno custom glue. Connect MCP clients across platforms.",
        "start_pos": 1848,
        "end_pos": 3615,
        "token_count_estimate": 441,
        "source_type": "readme",
        "agent_id": "79836362f8a60ef4"
      },
      {
        "chunk_id": 2,
        "text": "MFQYFOZuB1nNOfisbVzb1uHw1fz)\n- New [HAPI MCP YT Channel](https://youtube.com/@hapi-mcp) with demos API-First and MCP\n\n## Documentation\n\n- [Explore the Docs](https://docs.mcp.com.ai)\n- [See HAPI Server](https://hapi.mcp.com.ai)\n\n## FAQ\n\n- **Do we need to rewrite our services?**  \n  No. HAPI MCP lifts your existing OpenAPI specs directly into MCP tools. Your auth, validation, and business rules remain unchanged.\n\n- **Is this just another MCP server?**  \n  It‚Äôs the **Headless API** model: your API becomes the runtime. HAPI Server reflects it as MCP; runMCP scales it; OrcA orchestrates it. No duplicate logic.\n\n- **Which clients can consume the tools?**  \n  Any MCP client: ChatGPT, Claude, QBot, chatMCP, bespoke orchestrators‚Äîvendor-neutral by design.\n\n- **How do we keep control and compliance?**  \n  Your API remains the single source of truth. Policies, RBAC, rate limits, and audit logs flow through automatically; **no shadow logic**.\n\n- **What about security and privacy?**  \n  Scoped credentials, per-tool permissions, and auditable calls are inherited from your API layer. HAPI adds guardrails and observability for regulated environments.\n\n- **How fast can we get started?**  \n  You can transform your first API into an MCP tool in less than five minutes. The HAPI CLI makes it quick and painless.\n\n## Get Started\n\n1. Install HAPI CLI from latest release: https://github.com/la-rebelion/hapimcp/releases\n2. Transform your OpenAPI spec into an MCP server with a single command:\n   ```bash\n   hapi serve path/to/your/swagger.json --headless\n   ```\n3. Start using your MCP tools with any compatible client!\n\n4. Explore the [documentation](https://docs.mcp.com.ai) for advanced usage and best practices.\n\n## Contributing\n\nContributions are welcome! Open a discussion or submit a pull request on GitHub: https://github.com/la-rebelion/hapimcp\n\n## License\n\nHAPI MCP is licensed under the MIT License. See LICENSE for details.",
        "start_pos": 3696,
        "end_pos": 5629,
        "token_count_estimate": 483,
        "source_type": "readme",
        "agent_id": "79836362f8a60ef4"
      },
      {
        "chunk_id": 3,
        "text": "ontributions are welcome! Open a discussion or submit a pull request on GitHub: https://github.com/la-rebelion/hapimcp\n\n## License\n\nHAPI MCP is licensed under the MIT License. See LICENSE for details.",
        "start_pos": 5429,
        "end_pos": 5629,
        "token_count_estimate": 50,
        "source_type": "readme",
        "agent_id": "79836362f8a60ef4"
      }
    ]
  },
  {
    "agent_id": "b5aa00649d39db21",
    "name": "ai.com.mcp/hapi-mcp",
    "source": "mcp",
    "source_url": "https://github.com/larebelion/hapimcp",
    "description": "HAPI MCP server: Dynamically exposes OpenAPI REST APIs as MCP tools for AI assistants",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-07T00:32:01.41626Z",
    "indexed_at": "2026-02-18T04:01:35.437916",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Dynamically expose OpenAPI REST APIs as MCP tools for AI assistants"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's function but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "bb3a53083b9f3000",
    "name": "ai.com.mcp/registry",
    "source": "mcp",
    "source_url": "https://github.com/modelcontextprotocol/registry",
    "description": "Publish and discover MCP servers via the official MCP Registry. Powered by HAPI MCP server.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-01-10T22:33:54.03436Z",
    "indexed_at": "2026-02-18T04:01:40.069099",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# MCP Registry\n\nThe MCP registry provides MCP clients with a list of MCP servers, like an app store for MCP servers.\n\n[**üì§ Publish my MCP server**](docs/modelcontextprotocol-io/quickstart.mdx) | [**‚ö°Ô∏è Live API docs**](https://registry.modelcontextprotocol.io/docs) | [**üëÄ Ecosystem vision**](docs/design/ecosystem-vision.md) | üìñ **[Full documentation](./docs)**\n\n## Development Status\n\n**2025-10-24 update**: The Registry API has entered an **API freeze (v0.1)** üéâ. For the next month or more, the API will remain stable with no breaking changes, allowing integrators to confidently implement support. This freeze applies to v0.1 while development continues on v0. We'll use this period to validate the API in real-world integrations and gather feedback to shape v1 for general availability. Thank you to everyone for your contributions and patience‚Äîyour involvement has been key to getting us here!\n\n**2025-09-08 update**: The registry has launched in preview üéâ ([announcement blog post](https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/)). While the system is now more stable, this is still a preview release and breaking changes or data resets may occur. A general availability (GA) release will follow later. We'd love your feedback in [GitHub discussions](https://github.com/modelcontextprotocol/registry/discussions/new?category=ideas) or in the [#registry-dev Discord](https://discord.com/channels/1358869848138059966/1369487942862504016) ([joining details here](https://modelcontextprotocol.io/community/communication)).\n\nCurrent key maintainers:\n- **Adam Jones** (Anthropic) [@domdomegg](https://github.com/domdomegg)  \n- **Tadas Antanavicius** (PulseMCP) [@tadasant](https://github.com/tadasant)\n- **Toby Padilla** (GitHub) [@toby](https://github.com/toby)\n- **Radoslav (Rado) Dimitrov** (Stacklok) [@rdimitrov](https://github.com/rdimitrov)\n\n## Contributing\n\nWe use multiple channels for collaboration - see [modelcontextprotocol.io/community/communication](https://modelcontextprotocol.io/community/communication).\n\nOften (but not always) ideas flow through this pipeline:\n\n- **[Discord](https://modelcontextprotocol.io/community/communication)** - Real-time community discussions\n- **[Discussions](https://github.com/modelcontextprotocol/registry/discussions)** - Propose and discuss product/technical requirements\n- **[Issues](https://github.com/modelcontextprotocol/registry/issues)** - Track well-scoped technical work  \n- **[Pull Requests](https://github.com/modelcontextprotocol/registry/pulls)** - Contribute work towards issues\n\n### Quick start:\n\n#### Pre-requisites\n\n- **Docker**\n- **Go 1.24.x**\n- **ko** - Container image builder for Go ([installation instructions](https://ko.build/install/))\n- **golangci-lint v2.4.0**\n\n#### Running the server\n\n```bash\n# Start full development environment\nmake dev-compose\n```\n\nThis starts the registry at [`localhost:8080`](http://localhost:8080) with PostgreSQL. The database uses ephemeral storage and is reset each time you restart the containers, ensuring a clean state for development and testing.\n\n**Note:** The registry uses [ko](https://ko.build) to build container images. The `make dev-compose` command automatically builds the registry image with ko and loads it into your local Docker daemon before starting the services.\n\nBy default, the registry seeds from the production API with a filtered subset of servers (to keep startup fast). This ensures your local environment mirrors production behavior and all seed data passes validation. For offline development you can seed from a file without validation with `MCP_REGISTRY_SEED_FROM=data/seed.json MCP_REGISTRY_ENABLE_REGISTRY_VALIDATION=false make dev-compose`.\n\nThe setup can be configured with environment variables in [docker-compose.yml](./docker-compose.yml) - see [.env.example](./.env.example) for a reference.\n\n<details>\n<summary>Alternative: Running a pre-built Docker image</summary>\n\nPre-built Docker images are automatically published to GitHub Container Registry:\n\n```bash\n# Run latest stable release\ndocker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:latest\n\n# Run latest from main branch (continuous deployment)\ndocker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main\n\n# Run specific release version\ndocker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:v1.0.0\n\n# Run development build from main branch\ndocker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main-20250906-abc123d\n```\n\n**Available tags:** \n- **Releases**: `latest`, `v1.0.0`, `v1.1.0`, etc.\n- **Continuous**: `main` (latest main branch build)\n- **Development**: `main-<date>-<sha>` (specific commit builds)\n\n</details>\n\n#### Publishing a server\n\nTo publish a server, we've built a simple CLI. You can use it with:\n\n```bash\n# Build the latest CLI\nmake publisher\n\n# Use it!\n./bin/mcp-publisher --help\n```\n\nSee [the publisher guide](./docs/modelcontextprotocol-io/quickstart.mdx) for more details.\n\n#### Other commands\n\n```bash\n# Run lint, unit tests and integration tests\nmake check\n```\n\nThere are also a few more helpful commands for development. Run `make help` to learn more, or look in [Makefile](./Makefile).\n\n<!--\nFor Claude and other AI tools: Always prefer make targets over custom commands where possible.\n-->\n\n## Architecture\n\n### Project Structure\n\n```\n‚îú‚îÄ‚îÄ cmd/                     # Application entry points\n‚îÇ   ‚îî‚îÄ‚îÄ publisher/           # Server publishing tool\n‚îú‚îÄ‚îÄ data/                    # Seed data\n‚îú‚îÄ‚îÄ deploy/                  # Deployment configuration (Pulumi)\n‚îú‚îÄ‚îÄ docs/                    # Documentation\n‚îú‚îÄ‚îÄ internal/                # Private application code\n‚îÇ   ‚îú‚îÄ‚îÄ api/                 # HTTP handlers and routing\n‚îÇ   ‚îú‚îÄ‚îÄ auth/                # Authentication (GitHub OAuth, JWT, namespace blocking)\n‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration management\n‚îÇ   ‚îú‚îÄ‚îÄ database/            # Data persistence (PostgreSQL)\n‚îÇ   ‚îú‚îÄ‚îÄ service/             # Business logic\n‚îÇ   ‚îú‚îÄ‚îÄ telemetry/           # Metrics and monitoring\n‚îÇ   ‚îî‚îÄ‚îÄ validators/          # Input validation\n‚îú‚îÄ‚îÄ pkg/                     # Public packages\n‚îÇ   ‚îú‚îÄ‚îÄ api/                 # API types and structures\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v0/              # Version 0 API types\n‚îÇ   ‚îî‚îÄ‚îÄ model/               # Data models for server.json\n‚îú‚îÄ‚îÄ scripts/                 # Development and testing scripts\n‚îú‚îÄ‚îÄ tests/                   # Integration tests\n‚îî‚îÄ‚îÄ tools/                   # CLI tools and utilities\n    ‚îî‚îÄ‚îÄ validate-*.sh        # Schema validation tools\n```\n\n### Authentication\n\nPublishing supports multiple authentication methods:\n- **GitHub OAuth** - For publishing by logging into GitHub\n- **GitHub OIDC** - For publishing from GitHub Actions\n- **DNS verification** - For proving ownership of a domain and its subdomains\n- **HTTP verification** - For proving ownership of a domain\n\nThe registry validates namespace ownership when publishing. E.g. to publish...:\n- `io.github.domdomegg/my-cool-mcp` you must login to GitHub as `domdomegg`, or be in a GitHub Action on domdomegg's repos\n- `me.adamjones/my-cool-mcp` you must prove ownership of `adamjones.me` via DNS or HTTP challenge\n\n## Community Projects\n\nCheck out [community projects](docs/community-projects.md) to explore notable registry-related work created by the community.\n\n## More documentation\n\nSee the [documentation](./docs) for more details if your question has not been answered here!\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide MCP clients with a list of MCP servers",
        "Serve as a registry or app store for MCP servers",
        "Allow publishing of MCP servers with authentication",
        "Support multiple authentication methods including GitHub OAuth, GitHub OIDC, DNS verification, and HTTP verification",
        "Validate namespace ownership during server publishing",
        "Seed the registry database from production API or local files",
        "Offer a CLI tool for publishing MCP servers",
        "Provide a stable API with versioning and an API freeze period",
        "Support local development with Docker and Go environment"
      ],
      "limitations": [
        "Preview release status with potential breaking changes or data resets",
        "Ephemeral database storage in local development resets on container restart",
        "API freeze applies only to v0.1, with ongoing development for v0 and future v1",
        "Publishing requires proof of ownership or authentication tied to namespaces"
      ],
      "requirements": [
        "Docker installed for running the server",
        "Go version 1.24.x for development",
        "ko container image builder installed",
        "golangci-lint version 2.4.0 for linting",
        "GitHub account or domain ownership for publishing servers",
        "Environment variables configuration for deployment and development"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed architecture, authentication methods, and clearly states limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# MCP Registry\n\nThe MCP registry provides MCP clients with a list of MCP servers, like an app store for MCP servers.\n\n[**üì§ Publish my MCP server**](docs/modelcontextprotocol-io/quickstart.mdx) | [**‚ö°Ô∏è Live API docs**](https://registry.modelcontextprotocol.io/docs) | [**üëÄ Ecosystem vision**](docs/design/ecosystem-vision.md) | üìñ **[Full documentation](./docs)**\n\n## Development Status\n\n**2025-10-24 update**: The Registry API has entered an **API freeze (v0.1)** üéâ. For the next month or more, the API will remain stable with no breaking changes, allowing integrators to confidently implement support. This freeze applies to v0.1 while development continues on v0. We'll use this period to validate the API in real-world integrations and gather feedback to shape v1 for general availability. Thank you to everyone for your contributions and patience‚Äîyour involvement has been key to getting us here!\n\n**2025-09-08 update**: The registry has launched in preview üéâ ([announcement blog post](https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/)). While the system is now more stable, this is still a preview release and breaking changes or data resets may occur. A general availability (GA) release will follow later. We'd love your feedback in [GitHub discussions](https://github.com/modelcontextprotocol/registry/discussions/new?category=ideas) or in the [#registry-dev Discord](https://discord.com/channels/1358869848138059966/1369487942862504016) ([joining details here](https://modelcontextprotocol.io/community/communication)).",
        "start_pos": 0,
        "end_pos": 1554,
        "token_count_estimate": 388,
        "source_type": "readme",
        "agent_id": "bb3a53083b9f3000"
      },
      {
        "chunk_id": 1,
        "text": "https://github.com/rdimitrov)\n\n## Contributing\n\nWe use multiple channels for collaboration - see [modelcontextprotocol.io/community/communication](https://modelcontextprotocol.io/community/communication).\n\nOften (but not always) ideas flow through this pipeline:\n\n- **[Discord](https://modelcontextprotocol.io/community/communication)** - Real-time community discussions\n- **[Discussions](https://github.com/modelcontextprotocol/registry/discussions)** - Propose and discuss product/technical requirements\n- **[Issues](https://github.com/modelcontextprotocol/registry/issues)** - Track well-scoped technical work  \n- **[Pull Requests](https://github.com/modelcontextprotocol/registry/pulls)** - Contribute work towards issues\n\n### Quick start:\n\n#### Pre-requisites\n\n- **Docker**\n- **Go 1.24.x**\n- **ko** - Container image builder for Go ([installation instructions](https://ko.build/install/))\n- **golangci-lint v2.4.0**\n\n#### Running the server\n\n```bash\n# Start full development environment\nmake dev-compose\n```\n\nThis starts the registry at [`localhost:8080`](http://localhost:8080) with PostgreSQL. The database uses ephemeral storage and is reset each time you restart the containers, ensuring a clean state for development and testing.\n\n**Note:** The registry uses [ko](https://ko.build) to build container images. The `make dev-compose` command automatically builds the registry image with ko and loads it into your local Docker daemon before starting the services.\n\nBy default, the registry seeds from the production API with a filtered subset of servers (to keep startup fast). This ensures your local environment mirrors production behavior and all seed data passes validation. For offline development you can seed from a file without validation with `MCP_REGISTRY_SEED_FROM=data/seed.json MCP_REGISTRY_ENABLE_REGISTRY_VALIDATION=false make dev-compose`.\n\nThe setup can be configured with environment variables in [docker-compose.yml](./docker-compose.yml) - see [.env.example](./.env.example) for a reference.",
        "start_pos": 1848,
        "end_pos": 3866,
        "token_count_estimate": 504,
        "source_type": "readme",
        "agent_id": "bb3a53083b9f3000"
      },
      {
        "chunk_id": 2,
        "text": "REGISTRY_VALIDATION=false make dev-compose`.\n\nThe setup can be configured with environment variables in [docker-compose.yml](./docker-compose.yml) - see [.env.example](./.env.example) for a reference.\n\n<details>\n<summary>Alternative: Running a pre-built Docker image</summary>\n\nPre-built Docker images are automatically published to GitHub Container Registry:\n\n```bash\n# Run latest stable release\ndocker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:latest\n\n# Run latest from main branch (continuous deployment)\ndocker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main\n\n# Run specific release version\ndocker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:v1.0.0\n\n# Run development build from main branch\ndocker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main-20250906-abc123d\n```\n\n**Available tags:** \n- **Releases**: `latest`, `v1.0.0`, `v1.1.0`, etc.\n- **Continuous**: `main` (latest main branch build)\n- **Development**: `main-<date>-<sha>` (specific commit builds)\n\n</details>\n\n#### Publishing a server\n\nTo publish a server, we've built a simple CLI. You can use it with:\n\n```bash\n# Build the latest CLI\nmake publisher\n\n# Use it!\n./bin/mcp-publisher --help\n```\n\nSee [the publisher guide](./docs/modelcontextprotocol-io/quickstart.mdx) for more details.\n\n#### Other commands\n\n```bash\n# Run lint, unit tests and integration tests\nmake check\n```\n\nThere are also a few more helpful commands for development. Run `make help` to learn more, or look in [Makefile](./Makefile).\n\n<!--\nFor Claude and other AI tools: Always prefer make targets over custom commands where possible.",
        "start_pos": 3666,
        "end_pos": 5279,
        "token_count_estimate": 403,
        "source_type": "readme",
        "agent_id": "bb3a53083b9f3000"
      },
      {
        "chunk_id": 3,
        "text": "loyment configuration (Pulumi)\n‚îú‚îÄ‚îÄ docs/                    # Documentation\n‚îú‚îÄ‚îÄ internal/                # Private application code\n‚îÇ   ‚îú‚îÄ‚îÄ api/                 # HTTP handlers and routing\n‚îÇ   ‚îú‚îÄ‚îÄ auth/                # Authentication (GitHub OAuth, JWT, namespace blocking)\n‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration management\n‚îÇ   ‚îú‚îÄ‚îÄ database/            # Data persistence (PostgreSQL)\n‚îÇ   ‚îú‚îÄ‚îÄ service/             # Business logic\n‚îÇ   ‚îú‚îÄ‚îÄ telemetry/           # Metrics and monitoring\n‚îÇ   ‚îî‚îÄ‚îÄ validators/          # Input validation\n‚îú‚îÄ‚îÄ pkg/                     # Public packages\n‚îÇ   ‚îú‚îÄ‚îÄ api/                 # API types and structures\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v0/              # Version 0 API types\n‚îÇ   ‚îî‚îÄ‚îÄ model/               # Data models for server.json\n‚îú‚îÄ‚îÄ scripts/                 # Development and testing scripts\n‚îú‚îÄ‚îÄ tests/                   # Integration tests\n‚îî‚îÄ‚îÄ tools/                   # CLI tools and utilities\n    ‚îî‚îÄ‚îÄ validate-*.sh        # Schema validation tools\n```\n\n### Authentication\n\nPublishing supports multiple authentication methods:\n- **GitHub OAuth** - For publishing by logging into GitHub\n- **GitHub OIDC** - For publishing from GitHub Actions\n- **DNS verification** - For proving ownership of a domain and its subdomains\n- **HTTP verification** - For proving ownership of a domain\n\nThe registry validates namespace ownership when publishing. E.g. to publish...:\n- `io.github.domdomegg/my-cool-mcp` you must login to GitHub as `domdomegg`, or be in a GitHub Action on domdomegg's repos\n- `me.adamjones/my-cool-mcp` you must prove ownership of `adamjones.me` via DNS or HTTP challenge\n\n## Community Projects\n\nCheck out [community projects](docs/community-projects.md) to explore notable registry-related work created by the community.\n\n## More documentation\n\nSee the [documentation](./docs) for more details if your question has not been answered here!",
        "start_pos": 5514,
        "end_pos": 7389,
        "token_count_estimate": 468,
        "source_type": "readme",
        "agent_id": "bb3a53083b9f3000"
      },
      {
        "chunk_id": 4,
        "text": "y-projects.md) to explore notable registry-related work created by the community.\n\n## More documentation\n\nSee the [documentation](./docs) for more details if your question has not been answered here!",
        "start_pos": 7189,
        "end_pos": 7389,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "bb3a53083b9f3000"
      }
    ]
  },
  {
    "agent_id": "5169ce075110bfc3",
    "name": "ai.com.mcp/skills-search",
    "source": "mcp",
    "source_url": "https://github.com/agentskills/agentskills",
    "description": "Search and discover Agent Skills from the skills.sh registry. Powered by HAPI MCP server.",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-04T12:46:39.858109Z",
    "indexed_at": "2026-02-18T04:01:41.384055",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Agent Skills\n\n[Agent Skills](https://agentskills.io) are a simple, open format for giving agents new capabilities and expertise.\n\nSkills are folders of instructions, scripts, and resources that agents can discover and use to perform better at specific tasks. Write once, use everywhere.\n\n## Getting Started\n\n- [Documentation](https://agentskills.io) - Guides and tutorials\n- [Specification](https://agentskills.io/specification) - Format details\n- [Example Skills](https://github.com/anthropics/skills) - See what's possible\n\nThis repo contains the specification, documentation, and reference SDK. Also see a list of example skills [here](https://github.com/anthropics/skills).\n\n## About\n\nAgent Skills is an open format maintained by [Anthropic](https://anthropic.com) and open to contributions from the community.\n\n## License\n\nCode in this repository is licensed under [Apache 2.0](LICENSE). Documentation is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). See individual directories for details."
    },
    "llm_extracted": {
      "capabilities": [
        "Provide an open format for defining agent skills",
        "Allow agents to discover and use skills to improve task performance",
        "Support writing skills once and using them across different agents",
        "Offer a specification and reference SDK for skill development",
        "Enable community contributions to the skill format"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The documentation provides a basic description and links to further resources but lacks detailed usage examples, installation instructions, or explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Agent Skills\n\n[Agent Skills](https://agentskills.io) are a simple, open format for giving agents new capabilities and expertise.\n\nSkills are folders of instructions, scripts, and resources that agents can discover and use to perform better at specific tasks. Write once, use everywhere.\n\n## Getting Started\n\n- [Documentation](https://agentskills.io) - Guides and tutorials\n- [Specification](https://agentskills.io/specification) - Format details\n- [Example Skills](https://github.com/anthropics/skills) - See what's possible\n\nThis repo contains the specification, documentation, and reference SDK. Also see a list of example skills [here](https://github.com/anthropics/skills).\n\n## About\n\nAgent Skills is an open format maintained by [Anthropic](https://anthropic.com) and open to contributions from the community.\n\n## License\n\nCode in this repository is licensed under [Apache 2.0](LICENSE). Documentation is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). See individual directories for details.",
        "start_pos": 0,
        "end_pos": 1025,
        "token_count_estimate": 256,
        "source_type": "readme",
        "agent_id": "5169ce075110bfc3"
      }
    ]
  },
  {
    "agent_id": "79dadcf1013f16bc",
    "name": "ai.exa/exa",
    "source": "mcp",
    "source_url": "https://github.com/exa-labs/exa-mcp-server",
    "description": "Fast, intelligent web search and web crawling.\n\nNew mcp tool: Exa-code is a context tool for coding ",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-30T10:19:26.892476Z",
    "indexed_at": "2026-02-18T04:01:43.305700",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Exa MCP Server\n\n[![Install in Cursor](https://img.shields.io/badge/Install_in-Cursor-000000?style=flat-square&logoColor=white)](https://cursor.com/en/install-mcp?name=exa&config=eyJuYW1lIjoiZXhhIiwidHlwZSI6Imh0dHAiLCJ1cmwiOiJodHRwczovL21jcC5leGEuYWkvbWNwIn0=)\n[![Install in VS Code](https://img.shields.io/badge/Install_in-VS_Code-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://vscode.dev/redirect/mcp/install?name=exa&config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.exa.ai%2Fmcp%22%7D)\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\nConnect AI assistants to Exa's search capabilities: web search, code search, and company research.\n\n**[Full Documentation](https://docs.exa.ai/reference/exa-mcp)** | **[npm Package](https://www.npmjs.com/package/exa-mcp-server)** | **[Get Your Exa API Key](https://dashboard.exa.ai/api-keys)**\n\n## Installation\n\nConnect to Exa's hosted MCP server:\n\n```\nhttps://mcp.exa.ai/mcp\n```\n\n[Get your API key](https://dashboard.exa.ai/api-keys)\n\n<details>\n<summary><b>Cursor</b></summary>\n\nAdd to `~/.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>VS Code</b></summary>\n\nAdd to `.vscode/mcp.json`:\n\n```json\n{\n  \"servers\": {\n    \"exa\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Claude Code</b></summary>\n\n```bash\nclaude mcp add --transport http exa https://mcp.exa.ai/mcp\n```\n</details>\n\n<details>\n<summary><b>Claude Desktop</b></summary>\n\nAdd to your config file:\n- **macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows:** `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.exa.ai/mcp\"]\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Codex</b></summary>\n\n```bash\ncodex mcp add exa --url https://mcp.exa.ai/mcp\n```\n</details>\n\n<details>\n<summary><b>OpenCode</b></summary>\n\nAdd to your `opencode.json`:\n\n```json\n{\n  \"mcp\": {\n    \"exa\": {\n      \"type\": \"remote\",\n      \"url\": \"https://mcp.exa.ai/mcp\",\n      \"enabled\": true\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Antigravity</b></summary>\n\nOpen the MCP Store panel (from the \"...\" dropdown in the side panel), then add a custom server with:\n\n```\nhttps://mcp.exa.ai/mcp\n```\n</details>\n\n<details>\n<summary><b>Windsurf</b></summary>\n\nAdd to `~/.codeium/windsurf/mcp_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"serverUrl\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Zed</b></summary>\n\nAdd to your Zed settings:\n\n```json\n{\n  \"context_servers\": {\n    \"exa\": {\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Gemini CLI</b></summary>\n\nAdd to `~/.gemini/settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"httpUrl\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>v0 by Vercel</b></summary>\n\nIn v0, select **Prompt Tools** > **Add MCP** and enter:\n\n```\nhttps://mcp.exa.ai/mcp\n```\n</details>\n\n<details>\n<summary><b>Warp</b></summary>\n\nGo to **Settings** > **MCP Servers** > **Add MCP Server** and add:\n\n```json\n{\n  \"exa\": {\n    \"url\": \"https://mcp.exa.ai/mcp\"\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Kiro</b></summary>\n\nAdd to `~/.kiro/settings/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Roo Code</b></summary>\n\nAdd to your Roo Code MCP config:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Other Clients</b></summary>\n\nFor clients that support remote MCP:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n\nFor clients that need mcp-remote:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.exa.ai/mcp\"]\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Via npm Package</b></summary>\n\nUse the npm package with your API key. [Get your API key](https://dashboard.exa.ai/api-keys).\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"exa-mcp-server\"],\n      \"env\": {\n        \"EXA_API_KEY\": \"your_api_key\"\n      }\n    }\n  }\n}\n```\n</details>\n\n## Available Tools\n\n**Enabled by Default:**\n| Tool | Description |\n| ---- | ----------- |\n| `web_search_exa` | Search the web for any topic and get clean, ready-to-use content |\n| `get_code_context_exa` | Find code examples, documentation, and programming solutions from GitHub, Stack Overflow, and docs |\n| `company_research_exa` | Research any company to get business information, news, and insights |\n\n**Off by Default:**\n| Tool | Description |\n| ---- | ----------- |\n| `web_search_advanced_exa` | Advanced web search with full control over filters, domains, dates, and content options |\n| `crawling_exa` | Get the full content of a specific webpage from a known URL |\n| `people_search_exa` | Find people and their professional profiles |\n| `deep_researcher_start` | Start an AI research agent that searches, reads, and writes a detailed report |\n| `deep_researcher_check` | Check status and get results from a deep research task |\n\nEnable all tools with the `tools` parameter:\n\n```\nhttps://mcp.exa.ai/mcp?tools=web_search_exa,web_search_advanced_exa,get_code_context_exa,crawling_exa,company_research_exa,people_search_exa,deep_researcher_start,deep_researcher_check\n```\n\n## Agent Skills (Claude Skills)\n\nReady-to-use skills for Claude Code. Each skill teaches Claude how to use Exa search for a specific task. Copy the content inside a dropdown and paste it into Claude Code ‚Äî it handles the rest.\n\n<details>\n<summary><b>Company Research</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: company-research\ndescription: Company research using Exa search. Finds company info, competitors, news, tweets, financials, LinkedIn profiles, builds company lists. Use when researching companies, doing competitor analysis, market research, or building company lists.\ncontext: fork\n---\n\n# Company Research\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa`. Do NOT use `web_search_exa` or any other Exa tools.\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent runs Exa search internally\n- Agent processes results using LLM intelligence\n- Agent returns only distilled output (compact JSON or brief markdown)\n- Main context stays clean regardless of search volume\n\n## Dynamic Tuning\n\nNo hardcoded numResults. Tune to user intent:\n- User says \"a few\" ‚Üí 10-20\n- User says \"comprehensive\" ‚Üí 50-100\n- User specifies number ‚Üí match it\n- Ambiguous? Ask: \"How many companies would you like?\"\n\n## Query Variation\n\nExa returns different results for different phrasings. For coverage:\n- Generate 2-3 query variations\n- Run in parallel\n- Merge and deduplicate\n\n## Categories\n\nUse appropriate Exa `category` depending on what you need:\n- `company` ‚Üí homepages, rich metadata (headcount, location, funding, revenue)\n- `news` ‚Üí press coverage, announcements\n- `tweet` ‚Üí social presence, public commentary\n- `people` ‚Üí LinkedIn profiles (public data)\n- No category (`type: \"auto\"`) ‚Üí general web results, deep dives, broader context\n\nStart with `category: \"company\"` for discovery, then use other categories or no category with `livecrawl: \"fallback\"` for deeper research.\n\n### Category-Specific Filter Restrictions\n\nWhen using `category: \"company\"`, these parameters cause 400 errors:\n- `includeDomains` / `excludeDomains`\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n\nWhen searching without a category (or with `news`), domain and date filters work fine.\n\n**Universal restriction:** `includeText` and `excludeText` only support **single-item arrays**. Multi-item arrays cause 400 errors across all categories.\n\n## LinkedIn\n\nPublic LinkedIn via Exa: `category: \"people\"`, no other filters.\nAuth-required LinkedIn ‚Üí use Claude in Chrome browser fallback.\n\n## Browser Fallback\n\nAuto-fallback to Claude in Chrome when:\n- Exa returns insufficient results\n- Content is auth-gated\n- Dynamic pages need JavaScript\n\n## Examples\n\n### Discovery: find companies in a space\n```\nweb_search_advanced_exa {\n  \"query\": \"AI infrastructure startups San Francisco\",\n  \"category\": \"company\",\n  \"numResults\": 20,\n  \"type\": \"auto\"\n}\n```\n\n### Deep dive: research a specific company\n```\nweb_search_advanced_exa {\n  \"query\": \"Anthropic funding rounds valuation 2024\",\n  \"type\": \"deep\",\n  \"livecrawl\": \"fallback\",\n  \"numResults\": 10,\n  \"includeDomains\": [\"techcrunch.com\", \"crunchbase.com\", \"bloomberg.com\"]\n}\n```\n\n### News coverage\n```\nweb_search_advanced_exa {\n  \"query\": \"Anthropic AI safety\",\n  \"category\": \"news\",\n  \"numResults\": 15,\n  \"startPublishedDate\": \"2024-01-01\"\n}\n```\n\n### LinkedIn profiles\n```\nweb_search_advanced_exa {\n  \"query\": \"VP Engineering AI infrastructure\",\n  \"category\": \"people\",\n  \"numResults\": 20\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (structured list; one company per row)\n2) Sources (URLs; 1-line relevance each)\n3) Notes (uncertainty/conflicts)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>Code Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=get_code_context_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: get-code-context-exa\ndescription: Code context using Exa. Finds real snippets and docs from GitHub, StackOverflow, and technical docs. Use when searching for code examples, API syntax, library documentation, or debugging help.\ncontext: fork\n---\n\n# Code Context (Exa)\n\n## Tool Restriction (Critical)\n\nONLY use `get_code_context_exa`. Do NOT use other Exa tools.\n\n## Token Isolation (Critical)\n\nNever run Exa in main context. Always spawn Task agents:\n- Agent calls `get_code_context_exa`\n- Agent extracts the minimum viable snippet(s) + constraints\n- Agent deduplicates near-identical results (mirrors, forks, repeated StackOverflow answers) before presenting\n- Agent returns copyable snippets + brief explanation\n- Main context stays clean regardless of search volume\n\n## When to Use\n\nUse this tool for ANY programming-related request:\n- API usage and syntax\n- SDK/library examples\n- config and setup patterns\n- framework \"how to\" questions\n- debugging when you need authoritative snippets\n\n## Inputs (Supported)\n\n`get_code_context_exa` supports:\n- `query` (string, required)\n- `tokensNum` (number, optional; default ~5000; typical range 1000‚Äì50000)\n\n## Query Writing Patterns (High Signal)\n\nTo reduce irrelevant results and cross-language noise:\n- Always include the **programming language** in the query.\n  - Example: use **\"Go generics\"** instead of just **\"generics\"**.\n- When applicable, also include **framework + version** (e.g., \"Next.js 14\", \"React 19\", \"Python 3.12\").\n- Include exact identifiers (function/class names, config keys, error messages) when you have them.\n\n## Dynamic Tuning\n\nToken strategy:\n- Focused snippet needed ‚Üí tokensNum 1000‚Äì3000\n- Most tasks ‚Üí tokensNum 5000\n- Complex integration ‚Üí tokensNum 10000‚Äì20000\n- Only go larger when necessary (avoid dumping large context)\n\n## Output Format (Recommended)\n\nReturn:\n1) Best minimal working snippet(s) (keep it copy/paste friendly)\n2) Notes on version / constraints / gotchas\n3) Sources (URLs if present in returned context)\n\nBefore presenting:\n- Deduplicate similar results and keep only the best representative snippet per approach.\n\n## MCP Configuration\n\n```json\n{\n  \"servers\": {\n    \"exa\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.exa.ai/mcp?tools=get_code_context_exa\"\n    }\n  }\n}\n```\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>People Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: people-research\ndescription: People research using Exa search. Finds LinkedIn profiles, professional backgrounds, experts, team members, and public bios across the web. Use when searching for people, finding experts, or looking up professional profiles.\ncontext: fork\n---\n\n# People Research\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa`. Do NOT use `web_search_exa` or any other Exa tools.\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent runs Exa search internally\n- Agent processes results using LLM intelligence\n- Agent returns only distilled output (compact JSON or brief markdown)\n- Main context stays clean regardless of search volume\n\n## Dynamic Tuning\n\nNo hardcoded numResults. Tune to user intent:\n- User says \"a few\" ‚Üí 10-20\n- User says \"comprehensive\" ‚Üí 50-100\n- User specifies number ‚Üí match it\n- Ambiguous? Ask: \"How many profiles would you like?\"\n\n## Query Variation\n\nExa returns different results for different phrasings. For coverage:\n- Generate 2-3 query variations\n- Run in parallel\n- Merge and deduplicate\n\n## Categories\n\nUse appropriate Exa `category` depending on what you need:\n- `people` ‚Üí LinkedIn profiles, public bios (primary for discovery)\n- `personal site` ‚Üí personal blogs, portfolio sites, about pages\n- `news` ‚Üí press mentions, interviews, speaker bios\n- No category (`type: \"auto\"`) ‚Üí general web results, broader context\n\nStart with `category: \"people\"` for profile discovery, then use other categories or no category with `livecrawl: \"fallback\"` for deeper research on specific individuals.\n\n### Category-Specific Filter Restrictions\n\nWhen using `category: \"people\"`, these parameters cause errors:\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n- `includeText` / `excludeText`\n- `excludeDomains`\n- `includeDomains` ‚Äî **LinkedIn domains only** (e.g., \"linkedin.com\")\n\nWhen searching without a category, all parameters are available (but `includeText`/`excludeText` still only support single-item arrays).\n\n## LinkedIn\n\nPublic LinkedIn via Exa: `category: \"people\"`, no other filters.\nAuth-required LinkedIn ‚Üí use Claude in Chrome browser fallback.\n\n## Browser Fallback\n\nAuto-fallback to Claude in Chrome when:\n- Exa returns insufficient results\n- Content is auth-gated\n- Dynamic pages need JavaScript\n\n## Examples\n\n### Discovery: find people by role\n```\nweb_search_advanced_exa {\n  \"query\": \"VP Engineering AI infrastructure\",\n  \"category\": \"people\",\n  \"numResults\": 20,\n  \"type\": \"auto\"\n}\n```\n\n### With query variations\n```\nweb_search_advanced_exa {\n  \"query\": \"machine learning engineer San Francisco\",\n  \"category\": \"people\",\n  \"additionalQueries\": [\"ML engineer SF\", \"AI engineer Bay Area\"],\n  \"numResults\": 25,\n  \"type\": \"deep\"\n}\n```\n\n### Deep dive: research a specific person\n```\nweb_search_advanced_exa {\n  \"query\": \"Dario Amodei Anthropic CEO background\",\n  \"type\": \"auto\",\n  \"livecrawl\": \"fallback\",\n  \"numResults\": 15\n}\n```\n\n### News mentions\n```\nweb_search_advanced_exa {\n  \"query\": \"Dario Amodei interview\",\n  \"category\": \"news\",\n  \"numResults\": 10,\n  \"startPublishedDate\": \"2024-01-01\"\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (name, title, company, location if available)\n2) Sources (Profile URLs)\n3) Notes (profile completeness, verification status)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>Financial Report Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: web-search-advanced-financial-report\ndescription: Search for financial reports using Exa advanced search. Near-full filter support for finding SEC filings, earnings reports, and financial documents. Use when searching for 10-K filings, quarterly earnings, or annual reports.\ncontext: fork\n---\n\n# Web Search Advanced - Financial Report Category\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa` with `category: \"financial report\"`. Do NOT use other categories or tools.\n\n## Filter Restrictions (Critical)\n\nThe `financial report` category has one known restriction:\n\n- `excludeText` - NOT SUPPORTED (causes 400 error)\n\n## Supported Parameters\n\n### Core\n- `query` (required)\n- `numResults`\n- `type` (\"auto\", \"fast\", \"deep\", \"neural\")\n\n### Domain filtering\n- `includeDomains` (e.g., [\"sec.gov\", \"investor.apple.com\"])\n- `excludeDomains`\n\n### Date filtering (ISO 8601) - Very useful for financial reports!\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n\n### Text filtering\n- `includeText` (must contain ALL) - **single-item arrays only**; multi-item causes 400\n- ~~`excludeText`~~ - NOT SUPPORTED\n\n### Content extraction\n- `textMaxCharacters` / `contextMaxCharacters`\n- `enableSummary` / `summaryQuery`\n- `enableHighlights` / `highlightsNumSentences` / `highlightsPerUrl` / `highlightsQuery`\n\n### Additional\n- `additionalQueries`\n- `livecrawl` / `livecrawlTimeout`\n- `subpages` / `subpageTarget`\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent calls `web_search_advanced_exa` with `category: \"financial report\"`\n- Agent merges + deduplicates results before presenting\n- Agent returns distilled output (brief markdown or compact JSON)\n- Main context stays clean regardless of search volume\n\n## When to Use\n\nUse this category when you need:\n- SEC filings (10-K, 10-Q, 8-K, S-1)\n- Quarterly earnings reports\n- Annual reports\n- Investor presentations\n- Financial statements\n\n## Examples\n\nSEC filings for a company:\n```\nweb_search_advanced_exa {\n  \"query\": \"Anthropic SEC filing S-1\",\n  \"category\": \"financial report\",\n  \"numResults\": 10,\n  \"type\": \"auto\"\n}\n```\n\nRecent earnings reports:\n```\nweb_search_advanced_exa {\n  \"query\": \"Q4 2025 earnings report technology\",\n  \"category\": \"financial report\",\n  \"startPublishedDate\": \"2025-10-01\",\n  \"numResults\": 20,\n  \"type\": \"auto\"\n}\n```\n\nSpecific filing type:\n```\nweb_search_advanced_exa {\n  \"query\": \"10-K annual report AI companies\",\n  \"category\": \"financial report\",\n  \"includeDomains\": [\"sec.gov\"],\n  \"startPublishedDate\": \"2025-01-01\",\n  \"numResults\": 15,\n  \"type\": \"deep\"\n}\n```\n\nRisk factors analysis:\n```\nweb_search_advanced_exa {\n  \"query\": \"risk factors cybersecurity\",\n  \"category\": \"financial report\",\n  \"includeText\": [\"cybersecurity\"],\n  \"numResults\": 10,\n  \"enableHighlights\": true,\n  \"highlightsQuery\": \"What are the main cybersecurity risks?\"\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (company name, filing type, date, key figures/highlights)\n2) Sources (Filing URLs)\n3) Notes (reporting period, any restatements, auditor notes)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>Research Paper Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: web-search-advanced-research-paper\ndescription: Search for research papers and academic content using Exa advanced search. Full filter support including date ranges and text filtering. Use when searching for academic papers, arXiv preprints, or scientific research.\ncontext: fork\n---\n\n# Web Search Advanced - Research Paper Category\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa` with `category: \"research paper\"`. Do NOT use other categories or tools.\n\n## Full Filter Support\n\nThe `research paper` category supports ALL available parameters:\n\n### Core\n- `query` (required)\n- `numResults`\n- `type` (\"auto\", \"fast\", \"deep\", \"neural\")\n\n### Domain filtering\n- `includeDomains` (e.g., [\"arxiv.org\", \"openreview.net\"])\n- `excludeDomains`\n\n### Date filtering (ISO 8601)\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n\n### Text filtering\n- `includeText` (must contain ALL)\n- `excludeText` (exclude if ANY match)\n\n**Array size restriction:** `includeText` and `excludeText` only support **single-item arrays**. Multi-item arrays (2+ items) cause 400 errors. To match multiple terms, put them in the `query` string or run separate searches.\n\n### Content extraction\n- `textMaxCharacters` / `contextMaxCharacters`\n- `enableSummary` / `summaryQuery`\n- `enableHighlights` / `highlightsNumSentences` / `highlightsPerUrl` / `highlightsQuery`\n\n### Additional\n- `userLocation`\n- `moderation`\n- `additionalQueries`\n- `livecrawl` / `livecrawlTimeout`\n- `subpages` / `subpageTarget`\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent calls `web_search_advanced_exa` with `category: \"research paper\"`\n- Agent merges + deduplicates results before presenting\n- Agent returns distilled output (brief markdown or compact JSON)\n- Main context stays clean regardless of search volume\n\n## When to Use\n\nUse this category when you need:\n- Academic papers from arXiv, OpenReview, PubMed, etc.\n- Scientific research on specific topics\n- Literature reviews with date filtering\n- Papers containing specific methodologies or terms\n\n## Examples\n\nRecent papers on a topic:\n```\nweb_search_advanced_exa {\n  \"query\": \"transformer attention mechanisms efficiency\",\n  \"category\": \"research paper\",\n  \"startPublishedDate\": \"2024-01-01\",\n  \"numResults\": 15,\n  \"type\": \"auto\"\n}\n```\n\nPapers from specific venues:\n```\nweb_search_advanced_exa {\n  \"query\": \"large language model agents\",\n  \"category\": \"research paper\",\n  \"includeDomains\": [\"arxiv.org\", \"openreview.net\"],\n  \"includeText\": [\"LLM\"],\n  \"numResults\": 20,\n  \"type\": \"deep\"\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (structured list with title, authors, date, abstract summary)\n2) Sources (URLs with publication venue)\n3) Notes (methodology differences, conflicting findings)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>Personal Site Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: web-search-advanced-personal-site\ndescription: Search personal websites and blogs using Exa advanced search. Full filter support for finding individual perspectives, portfolios, and personal blogs. Use when searching for personal sites, blog posts, or portfolio websites.\ncontext: fork\n---\n\n# Web Search Advanced - Personal Site Category\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa` with `category: \"personal site\"`. Do NOT use other categories or tools.\n\n## Full Filter Support\n\nThe `personal site` category supports ALL available parameters:\n\n### Core\n- `query` (required)\n- `numResults`\n- `type` (\"auto\", \"fast\", \"deep\", \"neural\")\n\n### Domain filtering\n- `includeDomains`\n- `excludeDomains` (e.g., exclude Medium if you want independent blogs)\n\n### Date filtering (ISO 8601)\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n\n### Text filtering\n- `includeText` (must contain ALL)\n- `excludeText` (exclude if ANY match)\n\n**Array size restriction:** `includeText` and `excludeText` only support **single-item arrays**. Multi-item arrays (2+ items) cause 400 errors. To match multiple terms, put them in the `query` string or run separate searches.\n\n### Content extraction\n- `textMaxCharacters` / `contextMaxCharacters`\n- `enableSummary` / `summaryQuery`\n- `enableHighlights` / `highlightsNumSentences` / `highlightsPerUrl` / `highlightsQuery`\n\n### Additional\n- `additionalQueries`\n- `livecrawl` / `livecrawlTimeout`\n- `subpages` / `subpageTarget` - useful for exploring portfolio sites\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent calls `web_search_advanced_exa` with `category: \"personal site\"`\n- Agent merges + deduplicates results before presenting\n- Agent returns distilled output (brief markdown or compact JSON)\n- Main context stays clean regardless of search volume\n\n## When to Use\n\nUse this category when you need:\n- Individual expert opinions and experiences\n- Personal blog posts on technical topics\n- Portfolio websites\n- Independent analysis (not corporate content)\n- Deep dives and tutorials from practitioners\n\n## Examples\n\nTechnical blog posts:\n```\nweb_search_advanced_exa {\n  \"query\": \"building production LLM applications lessons learned\",\n  \"category\": \"personal site\",\n  \"numResults\": 15,\n  \"type\": \"deep\",\n  \"enableSummary\": true\n}\n```\n\nRecent posts on a topic:\n```\nweb_search_advanced_exa {\n  \"query\": \"Rust async runtime comparison\",\n  \"category\": \"personal site\",\n  \"startPublishedDate\": \"2025-01-01\",\n  \"numResults\": 10,\n  \"type\": \"auto\"\n}\n```\n\nExclude aggregators:\n```\nweb_search_advanced_exa {\n  \"query\": \"startup founder lessons\",\n  \"category\": \"personal site\",\n  \"excludeDomains\": [\"medium.com\", \"substack.com\"],\n  \"numResults\": 15,\n  \"type\": \"auto\"\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (title, author/site name, date, key insights)\n2) Sources (URLs)\n3) Notes (author expertise, potential biases, depth of coverage)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>X/Twitter Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: web-search-advanced-tweet\ndescription: Search tweets and Twitter/X content using Exa advanced search. Limited filter support - text and domain filters are NOT supported. Use when searching for tweets, Twitter/X discussions, or social media sentiment.\ncontext: fork\n---\n\n# Web Search Advanced - Tweet Category\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa` with `category: \"tweet\"`. Do NOT use other categories or tools.\n\n## Filter Restrictions (Critical)\n\nThe `tweet` category has **LIMITED filter support**. The following parameters are **NOT supported** and will cause 400 errors:\n\n- `includeText` - NOT SUPPORTED\n- `excludeText` - NOT SUPPORTED\n- `includeDomains` - NOT SUPPORTED\n- `excludeDomains` - NOT SUPPORTED\n- `moderation` - NOT SUPPORTED (causes 500 server error)\n\n## Supported Parameters\n\n### Core\n- `query` (required)\n- `numResults`\n- `type` (\"auto\", \"fast\", \"deep\", \"neural\")\n\n### Date filtering (ISO 8601) - Use these instead of text filters!\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n\n### Content extraction\n- `textMaxCharacters` / `contextMaxCharacters`\n- `enableHighlights` / `highlightsNumSentences` / `highlightsPerUrl` / `highlightsQuery`\n- `enableSummary` / `summaryQuery`\n\n### Additional\n- `additionalQueries` - useful for hashtag variations\n- `livecrawl` / `livecrawlTimeout` - use \"preferred\" for recent tweets\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent calls `web_search_advanced_exa` with `category: \"tweet\"`\n- Agent merges + deduplicates results before presenting\n- Agent returns distilled output (brief markdown or compact JSON)\n- Main context stays clean regardless of search volume\n\n## When to Use\n\nUse this category when you need:\n- Social discussions on a topic\n- Product announcements from company accounts\n- Developer opinions and experiences\n- Trending topics and community sentiment\n- Expert takes and threads\n\n## Examples\n\nRecent tweets on a topic:\n```\nweb_search_advanced_exa {\n  \"query\": \"Claude Code MCP experience\",\n  \"category\": \"tweet\",\n  \"startPublishedDate\": \"2025-01-01\",\n  \"numResults\": 20,\n  \"type\": \"auto\",\n  \"livecrawl\": \"preferred\"\n}\n```\n\nSearch with specific keywords (put keywords in query, not includeText):\n```\nweb_search_advanced_exa {\n  \"query\": \"launching announcing new open source release\",\n  \"category\": \"tweet\",\n  \"startPublishedDate\": \"2025-12-01\",\n  \"numResults\": 15,\n  \"type\": \"auto\"\n}\n```\n\nDeveloper sentiment (use specific query terms instead of excludeText):\n```\nweb_search_advanced_exa {\n  \"query\": \"developer experience DX frustrating painful\",\n  \"category\": \"tweet\",\n  \"numResults\": 20,\n  \"type\": \"deep\",\n  \"livecrawl\": \"preferred\"\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (tweet content, author handle, date, engagement if visible)\n2) Sources (Tweet URLs)\n3) Notes (sentiment summary, notable accounts, threads vs single tweets)\n\nImportant: Be aware that tweet content can be informal, sarcastic, or context-dependent.\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n## Links\n\n- [Documentation](https://docs.exa.ai/reference/exa-mcp)\n- [npm Package](https://www.npmjs.com/package/exa-mcp-server)\n- [Get Your Exa API Key](https://dashboard.exa.ai/api-keys)\n\n\n<br>\n\nBuilt with ‚ù§Ô∏è by Exa\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search the web for any topic and provide clean, ready-to-use content",
        "Find code examples, documentation, and programming solutions from GitHub, Stack Overflow, and technical docs",
        "Research companies to obtain business information, news, and insights",
        "Perform advanced web searches with control over filters, domains, dates, and content options",
        "Retrieve full content of specific webpages from known URLs",
        "Find people and their professional profiles",
        "Start and manage AI research agents that search, read, and write detailed reports"
      ],
      "limitations": [
        "Certain parameters cause errors when used with specific categories (e.g., includeDomains/excludeDomains and date filters cause 400 errors with category 'company')",
        "includeText and excludeText filters only support single-item arrays; multi-item arrays cause errors",
        "Public LinkedIn data is accessible, but authenticated LinkedIn data requires browser fallback",
        "Exa searches should not be run in the main context; must use spawned task agents to isolate tokens and manage search volume",
        "Browser fallback is required for auth-gated content and dynamic pages needing JavaScript"
      ],
      "requirements": [
        "An Exa API key is required to authenticate requests",
        "Clients must configure the MCP server URL as https://mcp.exa.ai/mcp",
        "For npm package usage, environment variable EXA_API_KEY must be set",
        "Clients must support remote MCP servers or use mcp-remote tooling",
        "Claude Code users must add specific skills and configure MCP with appropriate tool parameters"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, tool descriptions, agent skill setups, limitations, and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Exa MCP Server\n\n[![Install in Cursor](https://img.shields.io/badge/Install_in-Cursor-000000?style=flat-square&logoColor=white)](https://cursor.com/en/install-mcp?name=exa&config=eyJuYW1lIjoiZXhhIiwidHlwZSI6Imh0dHAiLCJ1cmwiOiJodHRwczovL21jcC5leGEuYWkvbWNwIn0=)\n[![Install in VS Code](https://img.shields.io/badge/Install_in-VS_Code-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://vscode.dev/redirect/mcp/install?name=exa&config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmcp.exa.ai%2Fmcp%22%7D)\n[![npm version](https://badge.fury.io/js/exa-mcp-server.svg)](https://www.npmjs.com/package/exa-mcp-server)\n[![smithery badge](https://smithery.ai/badge/exa)](https://smithery.ai/server/exa)\n\nConnect AI assistants to Exa's search capabilities: web search, code search, and company research.",
        "start_pos": 0,
        "end_pos": 826,
        "token_count_estimate": 206,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 1,
        "text": "%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.exa.ai/mcp\"]\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Codex</b></summary>\n\n```bash\ncodex mcp add exa --url https://mcp.exa.ai/mcp\n```\n</details>\n\n<details>\n<summary><b>OpenCode</b></summary>\n\nAdd to your `opencode.json`:\n\n```json\n{\n  \"mcp\": {\n    \"exa\": {\n      \"type\": \"remote\",\n      \"url\": \"https://mcp.exa.ai/mcp\",\n      \"enabled\": true\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Antigravity</b></summary>\n\nOpen the MCP Store panel (from the \"...\" dropdown in the side panel), then add a custom server with:\n\n```\nhttps://mcp.exa.ai/mcp\n```\n</details>\n\n<details>\n<summary><b>Windsurf</b></summary>\n\nAdd to `~/.codeium/windsurf/mcp_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"serverUrl\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Zed</b></summary>\n\nAdd to your Zed settings:\n\n```json\n{\n  \"context_servers\": {\n    \"exa\": {\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Gemini CLI</b></summary>\n\nAdd to `~/.gemini/settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"httpUrl\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>v0 by Vercel</b></summary>\n\nIn v0, select **Prompt Tools** > **Add MCP** and enter:\n\n```\nhttps://mcp.exa.ai/mcp\n```\n</details>\n\n<details>\n<summary><b>Warp</b></summary>\n\nGo to **Settings** > **MCP Servers** > **Add MCP Server** and add:\n\n```json\n{\n  \"exa\": {\n    \"url\": \"https://mcp.exa.ai/mcp\"\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Kiro</b></summary>\n\nAdd to `~/.kiro/settings/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Roo Code</b></summary>\n\nAdd to your Roo Code MCP config:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</de",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 2,
        "text": "ummary><b>Roo Code</b></summary>\n\nAdd to your Roo Code MCP config:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Other Clients</b></summary>\n\nFor clients that support remote MCP:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"url\": \"https://mcp.exa.ai/mcp\"\n    }\n  }\n}\n```\n\nFor clients that need mcp-remote:\n\n```json\n{\n  \"mcpServers\": {\n    \"exa\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.exa.ai/mcp\"]\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary><b>Via npm Package</b></summary>\n\nUse the npm package with your API key. [Get your API key](https://dashboard.exa.ai/api-keys).",
        "start_pos": 3696,
        "end_pos": 4424,
        "token_count_estimate": 182,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 3,
        "text": "rch task |\n\nEnable all tools with the `tools` parameter:\n\n```\nhttps://mcp.exa.ai/mcp?tools=web_search_exa,web_search_advanced_exa,get_code_context_exa,crawling_exa,company_research_exa,people_search_exa,deep_researcher_start,deep_researcher_check\n```\n\n## Agent Skills (Claude Skills)\n\nReady-to-use skills for Claude Code. Each skill teaches Claude how to use Exa search for a specific task. Copy the content inside a dropdown and paste it into Claude Code ‚Äî it handles the rest.\n\n<details>\n<summary><b>Company Research</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: company-research\ndescription: Company research using Exa search. Finds company info, competitors, news, tweets, financials, LinkedIn profiles, builds company lists. Use when researching companies, doing competitor analysis, market research, or building company lists.\ncontext: fork\n---\n\n# Company Research\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa`. Do NOT use `web_search_exa` or any other Exa tools.\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent runs Exa search internally\n- Agent processes results using LLM intelligence\n- Agent returns only distilled output (compact JSON or brief markdown)\n- Main context stays clean regardless of search volume\n\n## Dynamic Tuning\n\nNo hardcoded numResults. Tune to user intent:\n- User says \"a few\" ‚Üí 10-20\n- User says \"comprehensive\" ‚Üí 50-100\n- User specifies number ‚Üí match it\n- Ambiguous? Ask: \"How many companies would you like?\"\n\n## Query Variation\n\nExa returns different results for different phrasings.",
        "start_pos": 5544,
        "end_pos": 7586,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 4,
        "text": "User says \"comprehensive\" ‚Üí 50-100\n- User specifies number ‚Üí match it\n- Ambiguous? Ask: \"How many companies would you like?\"\n\n## Query Variation\n\nExa returns different results for different phrasings. For coverage:\n- Generate 2-3 query variations\n- Run in parallel\n- Merge and deduplicate\n\n## Categories\n\nUse appropriate Exa `category` depending on what you need:\n- `company` ‚Üí homepages, rich metadata (headcount, location, funding, revenue)\n- `news` ‚Üí press coverage, announcements\n- `tweet` ‚Üí social presence, public commentary\n- `people` ‚Üí LinkedIn profiles (public data)\n- No category (`type: \"auto\"`) ‚Üí general web results, deep dives, broader context\n\nStart with `category: \"company\"` for discovery, then use other categories or no category with `livecrawl: \"fallback\"` for deeper research.\n\n### Category-Specific Filter Restrictions\n\nWhen using `category: \"company\"`, these parameters cause 400 errors:\n- `includeDomains` / `excludeDomains`\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n\nWhen searching without a category (or with `news`), domain and date filters work fine.\n\n**Universal restriction:** `includeText` and `excludeText` only support **single-item arrays**. Multi-item arrays cause 400 errors across all categories.\n\n## LinkedIn\n\nPublic LinkedIn via Exa: `category: \"people\"`, no other filters.\nAuth-required LinkedIn ‚Üí use Claude in Chrome browser fallback.",
        "start_pos": 7386,
        "end_pos": 8800,
        "token_count_estimate": 353,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 5,
        "text": "{\n  \"query\": \"Anthropic funding rounds valuation 2024\",\n  \"type\": \"deep\",\n  \"livecrawl\": \"fallback\",\n  \"numResults\": 10,\n  \"includeDomains\": [\"techcrunch.com\", \"crunchbase.com\", \"bloomberg.com\"]\n}\n```\n\n### News coverage\n```\nweb_search_advanced_exa {\n  \"query\": \"Anthropic AI safety\",\n  \"category\": \"news\",\n  \"numResults\": 15,\n  \"startPublishedDate\": \"2024-01-01\"\n}\n```\n\n### LinkedIn profiles\n```\nweb_search_advanced_exa {\n  \"query\": \"VP Engineering AI infrastructure\",\n  \"category\": \"people\",\n  \"numResults\": 20\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (structured list; one company per row)\n2) Sources (URLs; 1-line relevance each)\n3) Notes (uncertainty/conflicts)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>Code Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=get_code_context_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: get-code-context-exa\ndescription: Code context using Exa. Finds real snippets and docs from GitHub, StackOverflow, and technical docs. Use when searching for code examples, API syntax, library documentation, or debugging help.\ncontext: fork\n---\n\n# Code Context (Exa)\n\n## Tool Restriction (Critical)\n\nONLY use `get_code_context_exa`. Do NOT use other Exa tools.\n\n## Token Isolation (Critical)\n\nNever run Exa in main context.",
        "start_pos": 9234,
        "end_pos": 10997,
        "token_count_estimate": 440,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 6,
        "text": "minimum viable snippet(s) + constraints\n- Agent deduplicates near-identical results (mirrors, forks, repeated StackOverflow answers) before presenting\n- Agent returns copyable snippets + brief explanation\n- Main context stays clean regardless of search volume\n\n## When to Use\n\nUse this tool for ANY programming-related request:\n- API usage and syntax\n- SDK/library examples\n- config and setup patterns\n- framework \"how to\" questions\n- debugging when you need authoritative snippets\n\n## Inputs (Supported)\n\n`get_code_context_exa` supports:\n- `query` (string, required)\n- `tokensNum` (number, optional; default ~5000; typical range 1000‚Äì50000)\n\n## Query Writing Patterns (High Signal)\n\nTo reduce irrelevant results and cross-language noise:\n- Always include the **programming language** in the query.\n  - Example: use **\"Go generics\"** instead of just **\"generics\"**.\n- When applicable, also include **framework + version** (e.g., \"Next.js 14\", \"React 19\", \"Python 3.12\").\n- Include exact identifiers (function/class names, config keys, error messages) when you have them.\n\n## Dynamic Tuning\n\nToken strategy:\n- Focused snippet needed ‚Üí tokensNum 1000‚Äì3000\n- Most tasks ‚Üí tokensNum 5000\n- Complex integration ‚Üí tokensNum 10000‚Äì20000\n- Only go larger when necessary (avoid dumping large context)\n\n## Output Format (Recommended)\n\nReturn:\n1) Best minimal working snippet(s) (keep it copy/paste friendly)\n2) Notes on version / constraints / gotchas\n3) Sources (URLs if present in returned context)\n\nBefore presenting:\n- Deduplicate similar results and keep only the best representative snippet per approach.\n\n## MCP Configuration\n\n```json\n{\n  \"servers\": {\n    \"exa\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.exa.ai/mcp?tools=get_code_context_exa\"\n    }\n  }\n}\n```\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>People Search</b></summary>\n\nCopy the content below and paste it into Claude Code.",
        "start_pos": 11082,
        "end_pos": 13099,
        "token_count_estimate": 504,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 7,
        "text": "d ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>People Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: people-research\ndescription: People research using Exa search. Finds LinkedIn profiles, professional backgrounds, experts, team members, and public bios across the web. Use when searching for people, finding experts, or looking up professional profiles.\ncontext: fork\n---\n\n# People Research\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa`. Do NOT use `web_search_exa` or any other Exa tools.\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent runs Exa search internally\n- Agent processes results using LLM intelligence\n- Agent returns only distilled output (compact JSON or brief markdown)\n- Main context stays clean regardless of search volume\n\n## Dynamic Tuning\n\nNo hardcoded numResults. Tune to user intent:\n- User says \"a few\" ‚Üí 10-20\n- User says \"comprehensive\" ‚Üí 50-100\n- User specifies number ‚Üí match it\n- Ambiguous? Ask: \"How many profiles would you like?\"\n\n## Query Variation\n\nExa returns different results for different phrasings.",
        "start_pos": 12899,
        "end_pos": 14538,
        "token_count_estimate": 409,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 8,
        "text": "primary for discovery)\n- `personal site` ‚Üí personal blogs, portfolio sites, about pages\n- `news` ‚Üí press mentions, interviews, speaker bios\n- No category (`type: \"auto\"`) ‚Üí general web results, broader context\n\nStart with `category: \"people\"` for profile discovery, then use other categories or no category with `livecrawl: \"fallback\"` for deeper research on specific individuals.\n\n### Category-Specific Filter Restrictions\n\nWhen using `category: \"people\"`, these parameters cause errors:\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n- `includeText` / `excludeText`\n- `excludeDomains`\n- `includeDomains` ‚Äî **LinkedIn domains only** (e.g., \"linkedin.com\")\n\nWhen searching without a category, all parameters are available (but `includeText`/`excludeText` still only support single-item arrays).\n\n## LinkedIn\n\nPublic LinkedIn via Exa: `category: \"people\"`, no other filters.\nAuth-required LinkedIn ‚Üí use Claude in Chrome browser fallback.",
        "start_pos": 14747,
        "end_pos": 15717,
        "token_count_estimate": 242,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 9,
        "text": "tegory\": \"news\",\n  \"numResults\": 10,\n  \"startPublishedDate\": \"2024-01-01\"\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (name, title, company, location if available)\n2) Sources (Profile URLs)\n3) Notes (profile completeness, verification status)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>Financial Report Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: web-search-advanced-financial-report\ndescription: Search for financial reports using Exa advanced search. Near-full filter support for finding SEC filings, earnings reports, and financial documents. Use when searching for 10-K filings, quarterly earnings, or annual reports.\ncontext: fork\n---\n\n# Web Search Advanced - Financial Report Category\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa` with `category: \"financial report\"`. Do NOT use other categories or tools.\n\n## Filter Restrictions (Critical)\n\nThe `financial report` category has one known restriction:\n\n- `excludeText` - NOT SUPPORTED (causes 400 error)\n\n## Supported Parameters\n\n### Core\n- `query` (required)\n- `numResults`\n- `type` (\"auto\", \"fast\", \"deep\", \"neural\")\n\n### Domain filtering\n- `includeDomains` (e.g., [\"sec.gov\", \"investor.apple.com\"])\n- `excludeDomains`\n\n### Date filtering (ISO 8601) - Very useful for financial reports!",
        "start_pos": 16595,
        "end_pos": 18442,
        "token_count_estimate": 461,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 10,
        "text": "- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n\n### Text filtering\n- `includeText` (must contain ALL) - **single-item arrays only**; multi-item causes 400\n- ~~`excludeText`~~ - NOT SUPPORTED\n\n### Content extraction\n- `textMaxCharacters` / `contextMaxCharacters`\n- `enableSummary` / `summaryQuery`\n- `enableHighlights` / `highlightsNumSentences` / `highlightsPerUrl` / `highlightsQuery`\n\n### Additional\n- `additionalQueries`\n- `livecrawl` / `livecrawlTimeout`\n- `subpages` / `subpageTarget`\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context.",
        "start_pos": 18443,
        "end_pos": 19037,
        "token_count_estimate": 148,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 11,
        "text": "Text\": [\"cybersecurity\"],\n  \"numResults\": 10,\n  \"enableHighlights\": true,\n  \"highlightsQuery\": \"What are the main cybersecurity risks?\"\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (company name, filing type, date, key figures/highlights)\n2) Sources (Filing URLs)\n3) Notes (reporting period, any restatements, auditor notes)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>Research Paper Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: web-search-advanced-research-paper\ndescription: Search for research papers and academic content using Exa advanced search. Full filter support including date ranges and text filtering. Use when searching for academic papers, arXiv preprints, or scientific research.\ncontext: fork\n---\n\n# Web Search Advanced - Research Paper Category\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa` with `category: \"research paper\"`. Do NOT use other categories or tools.",
        "start_pos": 20291,
        "end_pos": 21772,
        "token_count_estimate": 370,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 12,
        "text": "wlDate` / `endCrawlDate`\n\n### Text filtering\n- `includeText` (must contain ALL)\n- `excludeText` (exclude if ANY match)\n\n**Array size restriction:** `includeText` and `excludeText` only support **single-item arrays**. Multi-item arrays (2+ items) cause 400 errors. To match multiple terms, put them in the `query` string or run separate searches.\n\n### Content extraction\n- `textMaxCharacters` / `contextMaxCharacters`\n- `enableSummary` / `summaryQuery`\n- `enableHighlights` / `highlightsNumSentences` / `highlightsPerUrl` / `highlightsQuery`\n\n### Additional\n- `userLocation`\n- `moderation`\n- `additionalQueries`\n- `livecrawl` / `livecrawlTimeout`\n- `subpages` / `subpageTarget`\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent calls `web_search_advanced_exa` with `category: \"research paper\"`\n- Agent merges + deduplicates results before presenting\n- Agent returns distilled output (brief markdown or compact JSON)\n- Main context stays clean regardless of search volume\n\n## When to Use\n\nUse this category when you need:\n- Academic papers from arXiv, OpenReview, PubMed, etc.",
        "start_pos": 22139,
        "end_pos": 23269,
        "token_count_estimate": 282,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 13,
        "text": "ract summary)\n2) Sources (URLs with publication venue)\n3) Notes (methodology differences, conflicting findings)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>Personal Site Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: web-search-advanced-personal-site\ndescription: Search personal websites and blogs using Exa advanced search. Full filter support for finding individual perspectives, portfolios, and personal blogs. Use when searching for personal sites, blog posts, or portfolio websites.\ncontext: fork\n---\n\n# Web Search Advanced - Personal Site Category\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa` with `category: \"personal site\"`. Do NOT use other categories or tools.\n\n## Full Filter Support\n\nThe `personal site` category supports ALL available parameters:\n\n### Core\n- `query` (required)\n- `numResults`\n- `type` (\"auto\", \"fast\", \"deep\", \"neural\")\n\n### Domain filtering\n- `includeDomains`\n- `excludeDomains` (e.g., exclude Medium if you want independent blogs)\n\n### Date filtering (ISO 8601)\n- `startPublishedDate` / `endPublishedDate`\n- `startCrawlDate` / `endCrawlDate`\n\n### Text filtering\n- `includeText` (must contain ALL)\n- `excludeText` (exclude if ANY match)\n\n**Array size restriction:** `includeText` and `excludeText` only support **single-item arrays**. Multi-item arrays (2+ items) cause 400 errors. To match multiple terms, put them in the `query` string or run separate searches.",
        "start_pos": 23987,
        "end_pos": 25983,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 14,
        "text": "** `includeText` and `excludeText` only support **single-item arrays**. Multi-item arrays (2+ items) cause 400 errors. To match multiple terms, put them in the `query` string or run separate searches.\n\n### Content extraction\n- `textMaxCharacters` / `contextMaxCharacters`\n- `enableSummary` / `summaryQuery`\n- `enableHighlights` / `highlightsNumSentences` / `highlightsPerUrl` / `highlightsQuery`\n\n### Additional\n- `additionalQueries`\n- `livecrawl` / `livecrawlTimeout`\n- `subpages` / `subpageTarget` - useful for exploring portfolio sites\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context.",
        "start_pos": 25783,
        "end_pos": 26393,
        "token_count_estimate": 152,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 15,
        "text": "or/site name, date, key insights)\n2) Sources (URLs)\n3) Notes (author expertise, potential biases, depth of coverage)\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n<details>\n<summary><b>X/Twitter Search</b></summary>\n\nCopy the content below and paste it into Claude Code. It will set up the MCP connection and skill for you.\n\n````\nStep 1: Install or update Exa MCP\n\nIf Exa MCP already exists in your MCP configuration, either uninstall it first and install the new one, or update your existing MCP config with this endpoint. Run this command in your terminal:\n\nclaude mcp add --transport http exa \"https://mcp.exa.ai/mcp?tools=web_search_advanced_exa\"\n\n\nStep 2: Add this Claude skill\n\n---\nname: web-search-advanced-tweet\ndescription: Search tweets and Twitter/X content using Exa advanced search. Limited filter support - text and domain filters are NOT supported. Use when searching for tweets, Twitter/X discussions, or social media sentiment.\ncontext: fork\n---\n\n# Web Search Advanced - Tweet Category\n\n## Tool Restriction (Critical)\n\nONLY use `web_search_advanced_exa` with `category: \"tweet\"`. Do NOT use other categories or tools.\n\n## Filter Restrictions (Critical)\n\nThe `tweet` category has **LIMITED filter support**. The following parameters are **NOT supported** and will cause 400 errors:\n\n- `includeText` - NOT SUPPORTED\n- `excludeText` - NOT SUPPORTED\n- `includeDomains` - NOT SUPPORTED\n- `excludeDomains` - NOT SUPPORTED\n- `moderation` - NOT SUPPORTED (causes 500 server error)\n\n## Supported Parameters\n\n### Core\n- `query` (required)\n- `numResults`\n- `type` (\"auto\", \"fast\", \"deep\", \"neural\")\n\n### Date filtering (ISO 8601) - Use these instead of text filters!",
        "start_pos": 27631,
        "end_pos": 29405,
        "token_count_estimate": 443,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 16,
        "text": "lDate`\n\n### Content extraction\n- `textMaxCharacters` / `contextMaxCharacters`\n- `enableHighlights` / `highlightsNumSentences` / `highlightsPerUrl` / `highlightsQuery`\n- `enableSummary` / `summaryQuery`\n\n### Additional\n- `additionalQueries` - useful for hashtag variations\n- `livecrawl` / `livecrawlTimeout` - use \"preferred\" for recent tweets\n\n## Token Isolation (Critical)\n\nNever run Exa searches in main context. Always spawn Task agents:\n- Agent calls `web_search_advanced_exa` with `category: \"tweet\"`\n- Agent merges + deduplicates results before presenting\n- Agent returns distilled output (brief markdown or compact JSON)\n- Main context stays clean regardless of search volume\n\n## When to Use\n\nUse this category when you need:\n- Social discussions on a topic\n- Product announcements from company accounts\n- Developer opinions and experiences\n- Trending topics and community sentiment\n- Expert takes and threads\n\n## Examples\n\nRecent tweets on a topic:\n```\nweb_search_advanced_exa {\n  \"query\": \"Claude Code MCP experience\",\n  \"category\": \"tweet\",\n  \"startPublishedDate\": \"2025-01-01\",\n  \"numResults\": 20,\n  \"type\": \"auto\",\n  \"livecrawl\": \"preferred\"\n}\n```\n\nSearch with specific keywords (put keywords in query, not includeText):\n```\nweb_search_advanced_exa {\n  \"query\": \"launching announcing new open source release\",\n  \"category\": \"tweet\",\n  \"startPublishedDate\": \"2025-12-01\",\n  \"numResults\": 15,\n  \"type\": \"auto\"\n}\n```\n\nDeveloper sentiment (use specific query terms instead of excludeText):\n```\nweb_search_advanced_exa {\n  \"query\": \"developer experience DX frustrating painful\",\n  \"category\": \"tweet\",\n  \"numResults\": 20,\n  \"type\": \"deep\",\n  \"livecrawl\": \"preferred\"\n}\n```\n\n## Output Format\n\nReturn:\n1) Results (tweet content, author handle, date, engagement if visible)\n2) Sources (Tweet URLs)\n3) Notes (sentiment summary, notable accounts, threads vs single tweets)\n\nImportant: Be aware that tweet content can be informal, sarcastic, or context-dependent.",
        "start_pos": 29479,
        "end_pos": 31443,
        "token_count_estimate": 491,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      },
      {
        "chunk_id": 17,
        "text": "t if visible)\n2) Sources (Tweet URLs)\n3) Notes (sentiment summary, notable accounts, threads vs single tweets)\n\nImportant: Be aware that tweet content can be informal, sarcastic, or context-dependent.\n\n\nStep 3: Ask User to Restart Claude Code\n\nYou should ask the user to restart Claude Code to have the config changes take effect.\n````\n\n</details>\n\n## Links\n\n- [Documentation](https://docs.exa.ai/reference/exa-mcp)\n- [npm Package](https://www.npmjs.com/package/exa-mcp-server)\n- [Get Your Exa API Key](https://dashboard.exa.ai/api-keys)\n\n\n<br>\n\nBuilt with ‚ù§Ô∏è by Exa",
        "start_pos": 31243,
        "end_pos": 31810,
        "token_count_estimate": 141,
        "source_type": "readme",
        "agent_id": "79dadcf1013f16bc"
      }
    ]
  },
  {
    "agent_id": "56d9adffc6a661b3",
    "name": "ai.explorium/mcp-explorium",
    "source": "mcp",
    "source_url": "https://mcp-github-registry.explorium.ai/sse",
    "description": "Access live company and contact data from Explorium's AgentSource B2B platform.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-16T21:06:15.352229Z",
    "indexed_at": "2026-02-18T04:01:45.011702",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Access live company data",
        "Access live contact data",
        "Retrieve data from Explorium's AgentSource B2B platform"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's function but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "56d9adffc6a661b3",
    "name": "ai.explorium/mcp-explorium",
    "source": "mcp",
    "source_url": "https://github.com/explorium-ai/mcp-explorium",
    "description": "Access live company and contact data from Explorium's AgentSource B2B platform.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-11-03T13:21:46.054906Z",
    "indexed_at": "2026-02-18T04:01:45.242068",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "## Explorium Business Data Hub\n\n\n<p>\n  <a href=\"https://github.com/explorium-ai/mcp-explorium/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"></a>\n  <img src=\"https://img.shields.io/badge/Node.js-v24+-green.svg\" alt=\"Node.js Version\">\n  <img src=\"https://img.shields.io/badge/MCP-Compatible-blueviolet\" alt=\"MCP Compatible\">\n  <img src=\"https://img.shields.io/badge/Claude-Ready-orange\" alt=\"Claude Ready\">\n  <img src=\"https://img.shields.io/badge/OpenAI-Compatible-lightgrey\" alt=\"OpenAI Compatible\">\n  <img src=\"https://img.shields.io/badge/TypeScript-Powered-blue\" alt=\"TypeScript\">\n</p>\n\n<img src=\"logo.png\" alt=\"Explorium Logo\" width=\"90\">\n\n**Discover companies, contacts, and business insights‚Äîpowered by dozens of trusted external data sources.**\n\nThis repository contains the configuration and setup files for connecting to Explorium's Model Context Protocol (MCP) server, enabling AI tools to access comprehensive business intelligence data.\n\n## Overview\n\nThe **Explorium Business Data Hub** provides AI tools with access to:\n\n- **Company Search & Enrichment**: Find companies by name, domain, or attributes with detailed firmographics\n- **Contact Discovery**: Locate and enrich professional contact information\n- **Business Intelligence**: Access technology stack, funding history, growth signals, and business events\n- **Real-Time Data**: Up-to-date information from dozens of trusted external data sources\n- **Workflow Integration**: Seamlessly integrate business data into AI-powered workflows\n\nSearch any company or professional for everything from emails and phone numbers to roles, growth signals, tech stack, business events, website changes, and more. Find qualified leads, research prospects, identify talent, or craft personalized outreach‚Äîall without leaving your AI tool.\n\n### Examples\n\n**Example 1: Partnership Opportunity Research**\n```\nWho should I contact for partnership with monday.com? Get anyone who can promote a partnership with them. Bring me all the contact details you can find\n```\n\n**Example 2: Business Challenge Analysis**\n```\nWhat are the business challenges of amazon?\n```\n\n**Example 3: Leadership Team Discovery**\n```\nGet the engineering leadership team at Palo Alto Networks\n```\n\n## Installation\n\n<details>\n<summary><b>Install in Claude Desktop</b></summary>\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to Settings > Connectors > Add Custom Connector. Enter the name as `Explorium` and the remote MCP server URL as `https://mcp.explorium.ai/mcp`.\n\n#### Local Server Connection\n\nOpen Claude Desktop developer settings and edit your `claude_desktop_config.json` file to add the following configuration. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Cursor</b></summary>\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n#### Cursor Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"url\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Windsurf</b></summary>\n\nAdd this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Windsurf Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"serverUrl\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n#### Windsurf Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in VS Code</b></summary>\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### VS Code Remote Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"explorium\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n#### VS Code Local Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"explorium\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Zed</b></summary>\n\nIt can be installed via [Zed Extensions](https://zed.dev/extensions?query=Explorium) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.\n\n```json\n{\n  \"context_servers\": {\n    \"Explorium\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Cline</b></summary>\n\nYou can easily install Explorium through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Use the search bar within the **Marketplace** tab to find _Explorium_.\n4. Click the **Install** button.\n\n</details>\n\n\n\n<details>\n<summary><b>Install in Roo Code</b></summary>\n\nAdd this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.\n\n#### Roo Code Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n#### Roo Code Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in JetBrains AI Assistant</b></summary>\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.\n\n1. In JetBrains IDEs go to `Settings` -> `Tools` -> `AI Assistant` -> `Model Context Protocol (MCP)`\n2. Click `+ Add`.\n3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list\n4. Add this configuration and click `OK`\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n5. Click `Apply` to save changes.\n6. The same way explorium could be added for JetBrains Junie in `Settings` -> `Tools` -> `Junie` -> `MCP Settings`\n\n</details>\n\n<details>\n<summary><b>Install in Kiro</b></summary>\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` > `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n</details>\n\n\n\n## Connecting to Explorium MCP\n\nFor advanced users or other MCP clients, you can connect using these methods:\n\nYou can connect your AI tool to Explorium using the Model Context Protocol (MCP) through several methods:\n\n### Streamable HTTP (Recommended)\n\n- **URL**: `https://mcp.explorium.ai/mcp`\n- **JSON config**:\n```json\n{\n  \"mcpServers\": {\n    \"Explorium\": {\n      \"url\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n### SSE (Server-Sent Events)\n\n- **URL**: `https://mcp.explorium.ai/sse`\n- **JSON config**:\n```json\n{\n  \"mcpServers\": {\n    \"Explorium\": {\n      \"url\": \"https://mcp.explorium.ai/sse\"\n    }\n  }\n}\n```\n\n### STDIO (Local Server)\n\n- **JSON config**:\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n## API Key Requirements\n\n**Important**: Different connection methods have different authentication requirements:\n\n- ‚úÖ **Claude Desktop Extension** - No API key required\n- ‚úÖ **MCP Remote Connections** (Streamable HTTP/SSE/STDIO) - No API key required\n- üîë **Docker Self-Hosting** - Requires API key\n\n### Getting Your API Key\n\nFor Docker deployment, you'll need an API access token. Get yours at: [https://admin.explorium.ai/api-key](https://admin.explorium.ai/api-key)\n\n## Docker Deployment\n\nThis repository includes Docker configuration for self-hosting:\n\n```bash\n# Build the Docker image\ndocker build -t explorium-mcp .\n\n# Run the container with API access token\ndocker run -e API_ACCESS_TOKEN=your_explorium_access_token explorium-mcp\n```\n\n**Required Environment Variables:**\n- `API_ACCESS_TOKEN` - Your Explorium API access token for authentication (get it [here](https://admin.explorium.ai/api-key))\n\nYou can also use a `.env` file or docker-compose for easier management:\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\nservices:\n  explorium-mcp:\n    build: .\n    ports:\n      - \"44280:44280\"\n    environment:\n      - API_ACCESS_TOKEN=${API_ACCESS_TOKEN}\n```\n\n## Available Tools\n\nOnce connected, your AI tool will have access to tools for:\n\n- **Business Matching**: Find companies by name, domain, or business ID\n- **Business Enrichment**: Get detailed firmographics, technographics, and business intelligence\n- **Business Events**: Track funding rounds, office changes, hiring trends, and company developments\n- **Prospect Discovery**: Search for professionals and contacts within companies\n- **Prospect Enrichment**: Access contact information, work history, and professional profiles\n- **Prospect Events**: Track role changes, company moves, and career milestones\n\n## Troubleshooting Connection Issues\n\nIf you're experiencing issues connecting your AI tool to Explorium MCP:\n\n1. **Check MCP Client Support**\n   Verify that your AI tool supports MCP clients and can connect to MCP servers. Not all AI tools have this capability built-in yet.\n\n2. **Verify Remote Server Support**\n   Some AI tools have MCP clients but don't support remote connections. If this is the case, you may still be able to connect using our Docker configuration or local server setup.\n\n3. **Request MCP Support**\n   If your AI tool doesn't support MCP at all, we recommend reaching out to the tool's developers to request MCP server connection support.\n\n## Configuration Files\n\nThis repository contains:\n\n- `package.json` - Node.js dependencies and scripts\n- `manifest.json` - Extension metadata and configuration\n- `Dockerfile` - Container configuration for self-hosting\n- `server/index.js` - Placeholder file (does not contain actual MCP implementation)\n- `entrypoint.sh` - Docker container entry point\n\n**Important Note**: The `server/index.js` file in this repository is just a placeholder and does not contain the actual MCP server implementation. To use Explorium MCP, you need to connect to the remote server at `https://mcp.explorium.ai/mcp` using `mcp-remote` or through the connection methods described above. The actual MCP server is hosted by Explorium and accessible via the remote URLs.\n\n## Documentation & Support\n\n- [API Documentation](https://developers.explorium.ai/reference/agentsource-mcp)\n- [Support & Help Center](https://developers.explorium.ai/reference/support-help-center)\n- [Explorium Homepage](https://www.explorium.ai/mcp/)\n\nFor technical support, contact [support@explorium.ai](mailto:support@explorium.ai).\n\n\n\n## License\n\nThis project is licensed under the MIT License. See [LICENSE](LICENSE) for details.\n\n---\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Find companies by name, domain, or business ID with detailed firmographics",
        "Locate and enrich professional contact information",
        "Access business intelligence including technology stack, funding history, and growth signals",
        "Track business events such as funding rounds, office changes, and hiring trends",
        "Discover prospects and professionals within companies",
        "Track prospect events including role changes and career milestones",
        "Integrate real-time business data into AI-powered workflows",
        "Support multiple MCP connection methods including Streamable HTTP, SSE, and STDIO",
        "Enable self-hosting via Docker with API key authentication"
      ],
      "limitations": [
        "Docker self-hosting requires an API access token; remote connections do not",
        "The repository's server/index.js is a placeholder and does not contain the actual MCP server implementation",
        "Some AI tools may not support MCP clients or remote MCP server connections",
        "Users must connect to the hosted remote MCP server for actual data access unless self-hosting with Docker"
      ],
      "requirements": [
        "No API key required for Claude Desktop Extension and MCP remote connections",
        "API access token required for Docker self-hosting deployment",
        "Node.js v24+ environment for local server usage",
        "MCP-compatible AI tools or clients to connect to the server",
        "Access to the remote MCP server URL https://mcp.explorium.ai/mcp for remote connections"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions across multiple platforms, detailed usage examples, clear capability descriptions, explicit limitations, and environment requirements including API key usage and Docker deployment.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "## Explorium Business Data Hub\n\n\n<p>\n  <a href=\"https://github.com/explorium-ai/mcp-explorium/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"></a>\n  <img src=\"https://img.shields.io/badge/Node.js-v24+-green.svg\" alt=\"Node.js Version\">\n  <img src=\"https://img.shields.io/badge/MCP-Compatible-blueviolet\" alt=\"MCP Compatible\">\n  <img src=\"https://img.shields.io/badge/Claude-Ready-orange\" alt=\"Claude Ready\">\n  <img src=\"https://img.shields.io/badge/OpenAI-Compatible-lightgrey\" alt=\"OpenAI Compatible\">\n  <img src=\"https://img.shields.io/badge/TypeScript-Powered-blue\" alt=\"TypeScript\">\n</p>\n\n<img src=\"logo.png\" alt=\"Explorium Logo\" width=\"90\">\n\n**Discover companies, contacts, and business insights‚Äîpowered by dozens of trusted external data sources.**\n\nThis repository contains the configuration and setup files for connecting to Explorium's Model Context Protocol (MCP) server, enabling AI tools to access comprehensive business intelligence data.\n\n## Overview\n\nThe **Explorium Business Data Hub** provides AI tools with access to:\n\n- **Company Search & Enrichment**: Find companies by name, domain, or attributes with detailed firmographics\n- **Contact Discovery**: Locate and enrich professional contact information\n- **Business Intelligence**: Access technology stack, funding history, growth signals, and business events\n- **Real-Time Data**: Up-to-date information from dozens of trusted external data sources\n- **Workflow Integration**: Seamlessly integrate business data into AI-powered workflows\n\nSearch any company or professional for everything from emails and phone numbers to roles, growth signals, tech stack, business events, website changes, and more. Find qualified leads, research prospects, identify talent, or craft personalized outreach‚Äîall without leaving your AI tool.\n\n### Examples\n\n**Example 1: Partnership Opportunity Research**\n```\nWho should I contact for partnership with monday.com? Get anyone who can promote a partnership with them.",
        "start_pos": 0,
        "end_pos": 2016,
        "token_count_estimate": 504,
        "source_type": "readme",
        "agent_id": "56d9adffc6a661b3"
      },
      {
        "chunk_id": 1,
        "text": "thout leaving your AI tool.\n\n### Examples\n\n**Example 1: Partnership Opportunity Research**\n```\nWho should I contact for partnership with monday.com? Get anyone who can promote a partnership with them. Bring me all the contact details you can find\n```\n\n**Example 2: Business Challenge Analysis**\n```\nWhat are the business challenges of amazon?\n```\n\n**Example 3: Leadership Team Discovery**\n```\nGet the engineering leadership team at Palo Alto Networks\n```\n\n## Installation\n\n<details>\n<summary><b>Install in Claude Desktop</b></summary>\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to Settings > Connectors > Add Custom Connector. Enter the name as `Explorium` and the remote MCP server URL as `https://mcp.explorium.ai/mcp`.\n\n#### Local Server Connection\n\nOpen Claude Desktop developer settings and edit your `claude_desktop_config.json` file to add the following configuration. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Cursor</b></summary>\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n#### Cursor Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"url\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Windsurf</b></summary>\n\nAdd this to your Windsurf MCP config file.",
        "start_pos": 1816,
        "end_pos": 3860,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "56d9adffc6a661b3"
      },
      {
        "chunk_id": 2,
        "text": "px\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Windsurf</b></summary>\n\nAdd this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Windsurf Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"serverUrl\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n#### Windsurf Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in VS Code</b></summary>\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### VS Code Remote Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"explorium\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n#### VS Code Local Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"explorium\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Zed</b></summary>\n\nIt can be installed via [Zed Extensions](https://zed.dev/extensions?query=Explorium) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.\n\n```json\n{\n  \"context_servers\": {\n    \"Explorium\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Cline</b></summary>\n\nYou can easily install Explorium through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.",
        "start_pos": 3660,
        "end_pos": 5706,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "56d9adffc6a661b3"
      },
      {
        "chunk_id": 3,
        "text": "gh the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Use the search bar within the **Marketplace** tab to find _Explorium_.\n4. Click the **Install** button.\n\n</details>\n\n\n\n<details>\n<summary><b>Install in Roo Code</b></summary>\n\nAdd this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.\n\n#### Roo Code Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n#### Roo Code Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in JetBrains AI Assistant</b></summary>\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.\n\n1. In JetBrains IDEs go to `Settings` -> `Tools` -> `AI Assistant` -> `Model Context Protocol (MCP)`\n2. Click `+ Add`.\n3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list\n4. Add this configuration and click `OK`\n\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n5. Click `Apply` to save changes.\n6. The same way explorium could be added for JetBrains Junie in `Settings` -> `Tools` -> `Junie` -> `MCP Settings`\n\n</details>\n\n<details>\n<summary><b>Install in Kiro</b></summary>\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` > `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3.",
        "start_pos": 5506,
        "end_pos": 7457,
        "token_count_estimate": 487,
        "source_type": "readme",
        "agent_id": "56d9adffc6a661b3"
      },
      {
        "chunk_id": 4,
        "text": ">\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` > `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n</details>\n\n\n\n## Connecting to Explorium MCP\n\nFor advanced users or other MCP clients, you can connect using these methods:\n\nYou can connect your AI tool to Explorium using the Model Context Protocol (MCP) through several methods:\n\n### Streamable HTTP (Recommended)\n\n- **URL**: `https://mcp.explorium.ai/mcp`\n- **JSON config**:\n```json\n{\n  \"mcpServers\": {\n    \"Explorium\": {\n      \"url\": \"https://mcp.explorium.ai/mcp\"\n    }\n  }\n}\n```\n\n### SSE (Server-Sent Events)\n\n- **URL**: `https://mcp.explorium.ai/sse`\n- **JSON config**:\n```json\n{\n  \"mcpServers\": {\n    \"Explorium\": {\n      \"url\": \"https://mcp.explorium.ai/sse\"\n    }\n  }\n}\n```\n\n### STDIO (Local Server)\n\n- **JSON config**:\n```json\n{\n  \"mcpServers\": {\n    \"explorium\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.explorium.ai/mcp\"]\n    }\n  }\n}\n```\n\n## API Key Requirements\n\n**Important**: Different connection methods have different authentication requirements:\n\n- ‚úÖ **Claude Desktop Extension** - No API key required\n- ‚úÖ **MCP Remote Connections** (Streamable HTTP/SSE/STDIO) - No API key required\n- üîë **Docker Self-Hosting** - Requires API key\n\n### Getting Your API Key\n\nFor Docker deployment, you'll need an API access token. Get yours at: [https://admin.explorium.ai/api-key](https://admin.explorium.ai/api-key)\n\n## Docker Deployment\n\nThis repository includes Docker configuration for self-hosting:\n\n```bash\n# Build the Docker image\ndocker build -t explorium-mcp .",
        "start_pos": 7257,
        "end_pos": 9211,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "56d9adffc6a661b3"
      },
      {
        "chunk_id": 5,
        "text": "ai/api-key](https://admin.explorium.ai/api-key)\n\n## Docker Deployment\n\nThis repository includes Docker configuration for self-hosting:\n\n```bash\n# Build the Docker image\ndocker build -t explorium-mcp .\n\n# Run the container with API access token\ndocker run -e API_ACCESS_TOKEN=your_explorium_access_token explorium-mcp\n```\n\n**Required Environment Variables:**\n- `API_ACCESS_TOKEN` - Your Explorium API access token for authentication (get it [here](https://admin.explorium.ai/api-key))\n\nYou can also use a `.env` file or docker-compose for easier management:\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\nservices:\n  explorium-mcp:\n    build: .\n    ports:\n      - \"44280:44280\"\n    environment:\n      - API_ACCESS_TOKEN=${API_ACCESS_TOKEN}\n```\n\n## Available Tools\n\nOnce connected, your AI tool will have access to tools for:\n\n- **Business Matching**: Find companies by name, domain, or business ID\n- **Business Enrichment**: Get detailed firmographics, technographics, and business intelligence\n- **Business Events**: Track funding rounds, office changes, hiring trends, and company developments\n- **Prospect Discovery**: Search for professionals and contacts within companies\n- **Prospect Enrichment**: Access contact information, work history, and professional profiles\n- **Prospect Events**: Track role changes, company moves, and career milestones\n\n## Troubleshooting Connection Issues\n\nIf you're experiencing issues connecting your AI tool to Explorium MCP:\n\n1. **Check MCP Client Support**\n   Verify that your AI tool supports MCP clients and can connect to MCP servers. Not all AI tools have this capability built-in yet.\n\n2. **Verify Remote Server Support**\n   Some AI tools have MCP clients but don't support remote connections. If this is the case, you may still be able to connect using our Docker configuration or local server setup.\n\n3. **Request MCP Support**\n   If your AI tool doesn't support MCP at all, we recommend reaching out to the tool's developers to request MCP server connection support.",
        "start_pos": 9011,
        "end_pos": 11021,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "56d9adffc6a661b3"
      },
      {
        "chunk_id": 6,
        "text": "iguration or local server setup.\n\n3. **Request MCP Support**\n   If your AI tool doesn't support MCP at all, we recommend reaching out to the tool's developers to request MCP server connection support.\n\n## Configuration Files\n\nThis repository contains:\n\n- `package.json` - Node.js dependencies and scripts\n- `manifest.json` - Extension metadata and configuration\n- `Dockerfile` - Container configuration for self-hosting\n- `server/index.js` - Placeholder file (does not contain actual MCP implementation)\n- `entrypoint.sh` - Docker container entry point\n\n**Important Note**: The `server/index.js` file in this repository is just a placeholder and does not contain the actual MCP server implementation. To use Explorium MCP, you need to connect to the remote server at `https://mcp.explorium.ai/mcp` using `mcp-remote` or through the connection methods described above. The actual MCP server is hosted by Explorium and accessible via the remote URLs.\n\n## Documentation & Support\n\n- [API Documentation](https://developers.explorium.ai/reference/agentsource-mcp)\n- [Support & Help Center](https://developers.explorium.ai/reference/support-help-center)\n- [Explorium Homepage](https://www.explorium.ai/mcp/)\n\nFor technical support, contact [support@explorium.ai](mailto:support@explorium.ai).\n\n\n\n## License\n\nThis project is licensed under the MIT License. See [LICENSE](LICENSE) for details.\n\n---",
        "start_pos": 10821,
        "end_pos": 12212,
        "token_count_estimate": 347,
        "source_type": "readme",
        "agent_id": "56d9adffc6a661b3"
      }
    ]
  },
  {
    "agent_id": "524c233d886400e2",
    "name": "ai.filegraph/document-processing",
    "source": "mcp",
    "source_url": "https://github.com/filegraph/docconvert",
    "description": "Extract text from documents, manipulate PDFs, and perform OCR via FileGraph API.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-24T15:50:15.237565Z",
    "indexed_at": "2026-02-18T04:01:46.829885",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Extract text from documents",
        "Manipulate PDFs",
        "Perform OCR via FileGraph API"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "16dfcfdc8c8dd150",
    "name": "ai.fodda/mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/fodda/mcp-server",
    "description": "Expert-curated knowledge graphs for AI agents ‚Äî PSFK Retail, Beauty, Sports and more.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-16T19:33:48.522184Z",
    "indexed_at": "2026-02-18T04:01:51.587196",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide expert-curated knowledge graphs",
        "Support AI agents with domain-specific knowledge",
        "Cover multiple domains including PSFK Retail, Beauty, and Sports"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence that outlines the server's purpose and domain coverage but lacks detailed information on usage, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "c9ac2eecc63e1c47",
    "name": "ai.gomarble/mcp-api",
    "source": "mcp",
    "source_url": "https://apps.gomarble.ai/mcp-api/sse",
    "description": "GoMarble MCP API Server",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-14T05:56:54.505554Z",
    "indexed_at": "2026-02-18T04:01:56.408654",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.1,
    "quality_rationale": "The documentation provides only the server name without any description of capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "bbfd8b7e6d22c705",
    "name": "ai.gossiper/shopify-admin-mcp",
    "source": "mcp",
    "source_url": "https://mcp.gossiper.io/mcp",
    "description": "Control Shopify Admin tasks with agents or via prompt. Ultra slim integration, fast and secure.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-06T18:20:42.175634Z",
    "indexed_at": "2026-02-18T04:01:57.888313",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Control Shopify Admin tasks with agents",
        "Control Shopify Admin tasks via prompt",
        "Provide ultra slim integration",
        "Offer fast and secure operations"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence providing a basic overview without detailed capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "1fa80bcec84559f3",
    "name": "ai.klavis/strata",
    "source": "mcp",
    "source_url": "https://github.com/Klavis-AI/klavis",
    "description": "MCP server for progressive tool usage at any scale (see https://klavis.ai)",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-28T19:13:44.307076Z",
    "indexed_at": "2026-02-18T04:01:58.361315",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<div align=\"center\">\n  <picture>\n    <img src=\"./docs/images/logo/cover.png\" width=\"100%\">\n  </picture>\n</div>\n\n<div align=\"center\">\n\n[![Documentation](https://img.shields.io/badge/Documentation-üìñ-green)](https://www.klavis.ai/docs)\n[![Website](https://img.shields.io/badge/Website-üåê-purple)](https://www.klavis.ai)\n[![Discord](https://img.shields.io/badge/Discord-Join-7289DA?logo=discord&logoColor=white)](https://discord.gg/p7TuTEcssn)\n\n<a href=\"https://www.producthunt.com/products/strata-2?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_source=badge-strata&#0045;2\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=1016948&theme=light&period=daily&t=1758639605639\" alt=\"Strata - One&#0032;MCP&#0032;server&#0032;for&#0032;AI&#0032;agents&#0032;to&#0032;handle&#0032;thousands&#0032;of&#0032;tools | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n</div>\n\n## üéØ Choose Your Solution\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\" width=\"33%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>Strata</h2>\n            <p><strong>Intelligent connectors for your AI agent, optimize context window</strong></p>\n          </div>\n          <div>\n            <a href=\"https://www.klavis.ai/docs/concepts/strata\">\n              <img src=\"https://img.shields.io/badge/Explore-Strata-blue?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjIwIiBoZWlnaHQ9IjIwIiByeD0iNCIgcnk9IjQiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8+CjxyZWN0IHg9IjYiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iNiIgeT0iMTQiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjE0IiB3aWR0aD0iNCIgaGVpZ2h0PSI0IiByeD0iMSIgcnk9IjEiIGZpbGw9IndoaXRlIi8+Cjwvc3ZnPg==\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n      <td align=\"center\" width=\"33%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>MCP Integrations</h2>\n            <p><strong>100+ prebuilt integrations out-of-the-box, with OAuth support</strong></p>\n          </div>\n          <div>\n            <a href=\"https://www.klavis.ai/docs/mcp-server/overview\">\n              <img src=\"https://img.shields.io/badge/Explore-MCP%20Servers-purple?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTIwLjUgN0gzLjVDMi42NzE1NyA3IDIgNy42NzE1NyAyIDguNVYxNS41QzIgMTYuMzI4NCAyLjY3MTU3IDE3IDMuNSAxN0gyMC41QzIxLjMyODQgMTcgMjIgMTYuMzI4NCAyMiAxNS41VjguNUMyMiA3LjY3MTU3IDIxLjMyODQgNyAyMC41IDdaIiBzdHJva2U9IndoaXRlIiBzdHJva2Utd2lkdGg9IjIiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIvPgo8cGF0aCBkPSJNNiAxMkgxOCIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8L3N2Zz4=\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n      <td align=\"center\" width=\"33%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>MCP Sandbox</h2>\n            <p><strong>scalable MCP environments for LLM training and RL</strong></p>\n          </div>\n          <div>\n            <a href=\"https://www.klavis.ai/docs/concepts/sandbox\">\n              <img src=\"https://img.shields.io/badge/Explore-Sandbox-orange?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTQgNEgyMFY4TDE4IDEwTDIwIDE2VjIwSDRWMTZMNiAxMEw0IDhWNFoiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8+CjxwYXRoIGQ9Ik00IDhIMjAiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIvPgo8cGF0aCBkPSJNNCAxNkgyMCIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIi8+Cjwvc3ZnPg==\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n    </tr>\n  </table>\n</div>\n\n## Quick Start\n\n### Option 1: Cloud-hosted - [klavis.ai](https://www.klavis.ai)\n\n[Quickstart guide ‚Üí](https://www.klavis.ai/docs/quickstart)\n\n### Option 2: Self-host\n\n```bash\n# Run any MCP Integration\ndocker pull ghcr.io/klavis-ai/github-mcp-server:latest\ndocker run -p 5000:5000 ghcr.io/klavis-ai/github-mcp-server:latest\n\n# Install Open Source Strata locally\npipx install strata-mcp\nstrata add --type stdio playwright npx @playwright/mcp@latest\n```\n\n### Option 3: SDK\n\n```python\n# Python SDK\nfrom klavis import Klavis\nfrom klavis.types import McpServerName\n\nklavis = Klavis(api_key=\"your-key\")\n\n# Create Strata instance\nstrata = klavis_client.mcp_server.create_strata_server(\n    user_id=\"user123\",\n    servers=[McpServerName.GMAIL, McpServerName.SLACK],\n)\n\n# Or use individual MCP servers\ngmail = klavis.mcp_server.create_server_instance(\n    server_name=McpServerName.GMAIL,\n    user_id=\"user123\",\n)\n```\n\n```typescript\n// TypeScript SDK\nimport { KlavisClient, McpServerName } from 'klavis';\n\nconst klavis = new KlavisClient({ apiKey: 'your-api-key' });\n\n// Create Strata instance\nconst strata = await klavis.mcpServer.createStrataServer({\n    userId: \"user123\",\n    servers: [Klavis.McpServerName.Gmail, Klavis.McpServerName.Slack],\n});\n\n// Or use individual MCP servers\nconst gmail = await klavis.mcpServer.createServerInstance({\n    serverName: McpServerName.GMAIL,\n    userId: \"user123\"\n});\n```\n\n### Option 4: REST API\n\n\n```bash\n# Create Strata server\ncurl -X POST \"https://api.klavis.ai/v1/mcp-server/strata\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"user_id\": \"user123\",\n    \"servers\": [\"GMAIL\", \"SLACK\"]\n  }'\n\n# Create individual MCP server\ncurl -X POST \"https://api.klavis.ai/v1/mcp-server/instance\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"server_name\": \"GMAIL\",\n    \"user_id\": \"user123\"\n  }'\n```\n\n\n## Resources\n\n- üìñ [Documentation](https://www.klavis.ai/docs)\n- üí¨ [Discord Community](https://discord.gg/p7TuTEcssn)\n- üêõ [Report Issues](https://github.com/klavis-ai/klavis/issues)\n- üåê [Klavis AI Website](https://www.klavis.ai)\n\n---\n\n<div align=\"center\">\n  <p><strong>Made with ‚ù§Ô∏è by the Klavis Team</strong></p>\n</div>"
    },
    "llm_extracted": {
      "capabilities": [
        "Create and manage Strata MCP servers that act as intelligent connectors for AI agents",
        "Optimize context window usage for AI agents through Strata",
        "Provide over 100 prebuilt MCP integrations with OAuth support",
        "Support scalable MCP sandbox environments for large language model training and reinforcement learning",
        "Allow deployment via cloud-hosted service, self-hosted Docker containers, SDKs, or REST API",
        "Enable creation of individual MCP server instances for specific services like Gmail or Slack",
        "Offer SDKs in Python and TypeScript for programmatic control of MCP servers"
      ],
      "limitations": [],
      "requirements": [
        "API key for authentication when using cloud-hosted or REST API options",
        "Docker environment for self-hosted MCP integration containers",
        "Python environment with pipx for local installation of Strata MCP",
        "Node.js environment for Playwright MCP integration via npx",
        "User ID required to create MCP server instances"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, multiple usage examples (CLI, SDK, REST API), descriptions of key features and tools, and mentions authentication requirements, making it comprehensive and well-structured.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<div align=\"center\">\n  <picture>\n    <img src=\"./docs/images/logo/cover.png\" width=\"100%\">\n  </picture>\n</div>\n\n<div align=\"center\">\n\n[![Documentation](https://img.shields.io/badge/Documentation-üìñ-green)](https://www.klavis.ai/docs)\n[![Website](https://img.shields.io/badge/Website-üåê-purple)](https://www.klavis.ai)\n[![Discord](https://img.shields.io/badge/Discord-Join-7289DA?logo=discord&logoColor=white)](https://discord.gg/p7TuTEcssn)\n\n<a href=\"https://www.producthunt.com/products/strata-2?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_source=badge-strata&#0045;2\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=1016948&theme=light&period=daily&t=1758639605639\" alt=\"Strata - One&#0032;MCP&#0032;server&#0032;for&#0032;AI&#0032;agents&#0032;to&#0032;handle&#0032;thousands&#0032;of&#0032;tools | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n</div>\n\n## üéØ Choose Your Solution\n\n<div align=\"center\">\n  <table>\n    <tr>\n      <td align=\"center\" width=\"33%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>Strata</h2>\n            <p><strong>Intelligent connectors for your AI agent, optimize context window</strong></p>\n          </div>\n          <div>\n            <a href=\"https://www.klavis.ai/docs/concepts/strata\">\n              <img src=\"https://img.shields.io/badge/Explore-Strata-blue?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjIwIiBoZWlnaHQ9IjIwIiByeD0iNCIgcnk9IjQiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8+CjxyZWN0IHg9IjYiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjYiIHdpZHRoPSI0IiBoZWln",
        "start_pos": 0,
        "end_pos": 2048,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "1fa80bcec84559f3"
      },
      {
        "chunk_id": 1,
        "text": "c3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8+CjxyZWN0IHg9IjYiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjYiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iNiIgeT0iMTQiIHdpZHRoPSI0IiBoZWlnaHQ9IjQiIHJ4PSIxIiByeT0iMSIgZmlsbD0id2hpdGUiLz4KPHJlY3QgeD0iMTQiIHk9IjE0IiB3aWR0aD0iNCIgaGVpZ2h0PSI0IiByeD0iMSIgcnk9IjEiIGZpbGw9IndoaXRlIi8+Cjwvc3ZnPg==\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n      <td align=\"center\" width=\"33%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>MCP Integrations</h2>\n            <p><strong>100+ prebuilt integrations out-of-the-box, with OAuth support</strong></p>\n          </div>\n          <div>\n            <a href=\"https://www.klavis.ai/docs/mcp-server/overview\">\n              <img src=\"https://img.shields.io/badge/Explore-MCP%20Servers-purple?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTIwLjUgN0gzLjVDMi42NzE1NyA3IDIgNy42NzE1NyAyIDguNVYxNS41QzIgMTYuMzI4NCAyLjY3MTU3IDE3IDMuNSAxN0gyMC41QzIxLjMyODQgMTcgMjIgMTYuMzI4NCAyMiAxNS41VjguNUMyMiA3LjY3MTU3IDIxLjMyODQgNyAyMC41IDdaIiBzdHJva2U9IndoaXRlIiBzdHJva2Utd2lkdGg9IjIiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCIvPgo8cGF0aCBkPSJNNiAxMkgxOCIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIxIiBmaWxsPSJ3aGl0ZSIvPgo8L3N2Zz4=\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n      <td align=\"center\" width=\"33%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "1fa80bcec84559f3"
      },
      {
        "chunk_id": 2,
        "text": ">\n      <td align=\"center\" width=\"33%\" valign=\"top\" style=\"vertical-align: top; height: 250px;\">\n        <div style=\"height: 100%; display: flex; flex-direction: column; justify-content: space-between;\">\n          <div>\n            <h2>MCP Sandbox</h2>\n            <p><strong>scalable MCP environments for LLM training and RL</strong></p>\n          </div>\n          <div>\n            <a href=\"https://www.klavis.ai/docs/concepts/sandbox\">\n              <img src=\"https://img.shields.io/badge/Explore-Sandbox-orange?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTQgNEgyMFY4TDE4IDEwTDIwIDE2VjIwSDRWMTZMNiAxMEw0IDhWNFoiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8+CjxwYXRoIGQ9Ik00IDhIMjAiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIvPgo8cGF0aCBkPSJNNCAxNkgyMCIgc3Ryb2tlPSJ3aGl0ZSIgc3Ryb2tlLXdpZHRoPSIyIi8+Cjwvc3ZnPg==\" height=\"40\">\n            </a>\n          </div>\n        </div>\n      </td>\n    </tr>\n  </table>\n</div>\n\n## Quick Start\n\n### Option 1: Cloud-hosted - [klavis.ai](https://www.klavis.ai)\n\n[Quickstart guide ‚Üí](https://www.klavis.ai/docs/quickstart)\n\n### Option 2: Self-host\n\n```bash\n# Run any MCP Integration\ndocker pull ghcr.io/klavis-ai/github-mcp-server:latest\ndocker run -p 5000:5000 ghcr.io/klavis-ai/github-mcp-server:latest\n\n# Install Open Source Strata locally\npipx install strata-mcp\nstrata add --type stdio playwright npx @playwright/mcp@latest\n```\n\n### Option 3: SDK\n\n```python\n# Python SDK\nfrom klavis import Klavis\nfrom klavis.types import McpServerName\n\nklavis = Klavis(api_key=\"your-key\")\n\n# Create Strata instance\nstrata = klavis_client.mcp_server.create_strata_server(\n    user_id=\"user123\",\n    servers=[McpServerName.GMAIL, McpServerName.SLACK],\n)\n\n# Or use individual MCP servers\ngmail = klavis.mcp_server.create_server_instance(\n    server_name=McpServerName.GMAIL,\n    user_id=\"user123\",\n)\n```\n\n```typescrip",
        "start_pos": 3696,
        "end_pos": 5744,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "1fa80bcec84559f3"
      },
      {
        "chunk_id": 3,
        "text": "erName.GMAIL, McpServerName.SLACK],\n)\n\n# Or use individual MCP servers\ngmail = klavis.mcp_server.create_server_instance(\n    server_name=McpServerName.GMAIL,\n    user_id=\"user123\",\n)\n```\n\n```typescript\n// TypeScript SDK\nimport { KlavisClient, McpServerName } from 'klavis';\n\nconst klavis = new KlavisClient({ apiKey: 'your-api-key' });\n\n// Create Strata instance\nconst strata = await klavis.mcpServer.createStrataServer({\n    userId: \"user123\",\n    servers: [Klavis.McpServerName.Gmail, Klavis.McpServerName.Slack],\n});\n\n// Or use individual MCP servers\nconst gmail = await klavis.mcpServer.createServerInstance({\n    serverName: McpServerName.GMAIL,\n    userId: \"user123\"\n});\n```\n\n### Option 4: REST API\n\n\n```bash\n# Create Strata server\ncurl -X POST \"https://api.klavis.ai/v1/mcp-server/strata\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"user_id\": \"user123\",\n    \"servers\": [\"GMAIL\", \"SLACK\"]\n  }'\n\n# Create individual MCP server\ncurl -X POST \"https://api.klavis.ai/v1/mcp-server/instance\" \\\n  -H \"Authorization: Bearer your-api-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"server_name\": \"GMAIL\",\n    \"user_id\": \"user123\"\n  }'\n```\n\n\n## Resources\n\n- üìñ [Documentation](https://www.klavis.ai/docs)\n- üí¨ [Discord Community](https://discord.gg/p7TuTEcssn)\n- üêõ [Report Issues](https://github.com/klavis-ai/klavis/issues)\n- üåê [Klavis AI Website](https://www.klavis.ai)\n\n---\n\n<div align=\"center\">\n  <p><strong>Made with ‚ù§Ô∏è by the Klavis Team</strong></p>\n</div>",
        "start_pos": 5544,
        "end_pos": 7068,
        "token_count_estimate": 381,
        "source_type": "readme",
        "agent_id": "1fa80bcec84559f3"
      }
    ]
  },
  {
    "agent_id": "2e08317133c4dc39",
    "name": "ai.kubit/mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/Kubit-AI/mcp-server",
    "description": "Bring Kubit into your AI workflow ‚Äî query your warehouse with natural language",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-12-09T00:18:46.752966Z",
    "indexed_at": "2026-02-18T04:01:59.534598",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Kubit MCP Server\n\n**Warehouse-native analytics meets conversational AI**\n\nBring the full power of Kubit directly into your AI workflow. Query, analyze, and explore your data warehouse through natural language‚Äîno complex syntax required.\n\n---\n\n## What is Kubit MCP?\n\nThe Kubit MCP (Model Context Protocol) server transforms how teams interact with their analytics platform. By connecting your AI assistant to Kubit, you can:\n\n- **Explore schemas** - Discover events, properties, and dimensions in natural language\n- **Generate reports** - Create analytical queries through conversation\n- **Export data** - Pull raw data in CSV format for deep analysis  \n- **Search content** - Find existing reports and dashboards instantly\n- **Ask questions** - Get insights without learning query syntax\n\n> **Beta Notice**\n> \n> This server is under active development. You may encounter bugs, performance issues, or rate limits as we continue to improve the platform.\n\n---\n\n## Quick Start\n\n### What You'll Need\n\n| Requirement | Description |\n|-------------|-------------|\n| **Kubit Account** | Active access to a Kubit organization |\n| **AI Client** | MCP-compatible tool (Claude, Cursor, etc.) |\n| **Permissions** | Schema access in your Kubit workspace |\n\n### Connection Steps\n\nSetting up the Kubit MCP server is straightforward:\n\n1. **Add the MCP server** to your AI client configuration\n2. **Use the server URL**: `https://mcp.kubit.ai/mcp`\n3. **Complete OAuth authentication** when prompted\n4. **Start querying** your Kubit data\n\n> **Note:** Check your AI client's documentation for specific MCP server setup instructions.\n\n### Authentication & Access\n\nThe server uses **OAuth 2.0** authentication and respects your existing Kubit permissions. You'll only see data from schemas you already have access to‚Äîno additional permissions needed.\n\n---\n\n## Tools & Capabilities\n\nYour AI assistant gains access to five powerful tools:\n\n| Tool | Purpose |\n|------|---------|\n| **`getUserContext`** | Initialize session and retrieve available schemas |\n| **`getSchema`** | Explore events, properties, and dimensions in detail |\n| **`createReport`** | Generate and execute analytical queries |\n| **`getRawData`** | Export CSV data from existing reports |\n| **`searchKubit`** | Find reports and dashboards across your org |\n\n---\n\n## Example Conversations\n\n### Understanding User Behavior\n\n```\n\"Show me conversion funnel for mobile app sign-ups in the last quarter\"\n\"What are the most popular features used by premium users?\"\n\"How has user retention changed month-over-month?\"\n```\n\n### Product Performance\n\n```\n\"What are the top events by volume this week?\"\n\"Show me user engagement trends for the last 30 days\"\n\"Compare conversion rates across different traffic sources\"\n```\n\n### Data Discovery\n\n```\n\"What events and properties are available in the mobile app schema?\"\n\"Show me all custom properties for the checkout event\"\n\"What dimensions can I use for user segmentation?\"\n```\n\n---\n\n## Typical Workflow\n\nHere's how most analysis sessions flow:\n\n```\nInitialize ‚Üí Explore ‚Üí Search ‚Üí Create ‚Üí Export\n```\n\n1. **Initialize** - Call `getUserContext` to see available schemas\n2. **Explore** - Use `getSchema` to understand events and properties  \n3. **Search** - Check `searchKubit` for existing analyses\n4. **Create** - Generate new reports with custom queries\n5. **Export** - Pull `getRawData` for external analysis\n\n---\n\n## Best Practices\n\n### Crafting Effective Prompts\n\n**Be Specific**  \nInclude time ranges, events, and segments in your questions.\n\n```diff\n- \"Show me users\"\n+ \"Show me active users in the US who signed up last month\"\n```\n\n**Provide Context**  \nExplain what you're trying to understand.\n\n```diff\n- \"What's the conversion rate?\"\n+ \"What's the conversion rate from free trial to paid for users who engaged with feature X?\"\n```\n\n**Reference Schemas**  \nUse schema names when working with multiple data sources.\n\n```diff\n- \"Show me sign-up events\"\n+ \"In the mobile_events schema, show me sign-up events\"\n```\n\n**Break It Down**  \nComplex analyses work better as multiple focused questions.\n\n```diff\n- \"Show me everything about user behavior across all channels with retention and conversion\"\n+ Start with \"Show me user retention by channel\" then follow up\n```\n\n### Performance Optimization\n\n- **Use `searchKubit` first** - Leverage existing analyses before creating new reports\n- **Specify date ranges** - Narrow time windows improve query performance  \n- **Export selectively** - Only use `getRawData` when you need detailed external analysis\n\n### Security & Compliance\n\n| Consideration | What It Means |\n|---------------|---------------|\n| **Permission Model** | You can only access schemas you're authorized to view |\n| **AI Processing** | Third-party AI models will process your query data |\n| **Policy Review** | Confirm your organization allows AI-assisted data analysis |\n\n---\n\n## Troubleshooting\n\n### Common Issues & Solutions\n\n**Authentication Failures**  \nVerify your Kubit credentials and organization name\n\n**No Schemas Available**  \nCheck that you have access to at least one schema in Kubit\n\n**Connection Errors**  \nConfirm you're using the correct server URL: `https://mcp.kubit.ai/mcp`\n\n**Report Generation Issues**  \nVerify the schema and events you're referencing exist using `getSchema`\n\n### Need Help?\n\n- Test with simple queries first to verify your connection\n- Check schema access through the Kubit web interface  \n- Use `getSchema` to confirm available events and properties\n\n---\n\n## Support & Resources\n\n**Documentation**  \n[docs.kubit.ai](https://docs.kubit.ai/) - Complete platform documentation\n\n**Customer Success**  \nContact your Kubit customer success team for assistance\n\n**About Kubit**  \n[kubit.ai](https://kubit.ai) - Learn more about warehouse-native analytics\n\n**MCP Protocol**  \n[modelcontextprotocol.io](https://modelcontextprotocol.io) - Explore the Model Context Protocol\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Explore schemas to discover events, properties, and dimensions using natural language",
        "Generate and execute analytical queries through conversational interaction",
        "Export raw data in CSV format from existing reports for external analysis",
        "Search for existing reports and dashboards across the organization",
        "Initialize user sessions and retrieve available schemas",
        "Provide insights without requiring knowledge of query syntax"
      ],
      "limitations": [
        "Server is in beta and may have bugs, performance issues, or rate limits",
        "Access restricted to schemas the user has permission to view",
        "Dependent on third-party AI models processing query data",
        "Requires existing Kubit permissions; no additional permissions granted by the server"
      ],
      "requirements": [
        "Active Kubit account with access to an organization",
        "MCP-compatible AI client (e.g., Claude, Cursor)",
        "OAuth 2.0 authentication setup",
        "Schema access permissions within Kubit workspace"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions, usage examples, limitations, and requirements, making it highly informative and practical.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Kubit MCP Server\n\n**Warehouse-native analytics meets conversational AI**\n\nBring the full power of Kubit directly into your AI workflow. Query, analyze, and explore your data warehouse through natural language‚Äîno complex syntax required.\n\n---\n\n## What is Kubit MCP?\n\nThe Kubit MCP (Model Context Protocol) server transforms how teams interact with their analytics platform. By connecting your AI assistant to Kubit, you can:\n\n- **Explore schemas** - Discover events, properties, and dimensions in natural language\n- **Generate reports** - Create analytical queries through conversation\n- **Export data** - Pull raw data in CSV format for deep analysis  \n- **Search content** - Find existing reports and dashboards instantly\n- **Ask questions** - Get insights without learning query syntax\n\n> **Beta Notice**\n> \n> This server is under active development. You may encounter bugs, performance issues, or rate limits as we continue to improve the platform.\n\n---\n\n## Quick Start\n\n### What You'll Need\n\n| Requirement | Description |\n|-------------|-------------|\n| **Kubit Account** | Active access to a Kubit organization |\n| **AI Client** | MCP-compatible tool (Claude, Cursor, etc.) |\n| **Permissions** | Schema access in your Kubit workspace |\n\n### Connection Steps\n\nSetting up the Kubit MCP server is straightforward:\n\n1. **Add the MCP server** to your AI client configuration\n2. **Use the server URL**: `https://mcp.kubit.ai/mcp`\n3. **Complete OAuth authentication** when prompted\n4. **Start querying** your Kubit data\n\n> **Note:** Check your AI client's documentation for specific MCP server setup instructions.\n\n### Authentication & Access\n\nThe server uses **OAuth 2.0** authentication and respects your existing Kubit permissions. You'll only see data from schemas you already have access to‚Äîno additional permissions needed.",
        "start_pos": 0,
        "end_pos": 1829,
        "token_count_estimate": 457,
        "source_type": "readme",
        "agent_id": "2e08317133c4dc39"
      },
      {
        "chunk_id": 1,
        "text": "apabilities\n\nYour AI assistant gains access to five powerful tools:\n\n| Tool | Purpose |\n|------|---------|\n| **`getUserContext`** | Initialize session and retrieve available schemas |\n| **`getSchema`** | Explore events, properties, and dimensions in detail |\n| **`createReport`** | Generate and execute analytical queries |\n| **`getRawData`** | Export CSV data from existing reports |\n| **`searchKubit`** | Find reports and dashboards across your org |\n\n---\n\n## Example Conversations\n\n### Understanding User Behavior\n\n```\n\"Show me conversion funnel for mobile app sign-ups in the last quarter\"\n\"What are the most popular features used by premium users?\"\n\"How has user retention changed month-over-month?\"\n```\n\n### Product Performance\n\n```\n\"What are the top events by volume this week?\"\n\"Show me user engagement trends for the last 30 days\"\n\"Compare conversion rates across different traffic sources\"\n```\n\n### Data Discovery\n\n```\n\"What events and properties are available in the mobile app schema?\"\n\"Show me all custom properties for the checkout event\"\n\"What dimensions can I use for user segmentation?\"\n```\n\n---\n\n## Typical Workflow\n\nHere's how most analysis sessions flow:\n\n```\nInitialize ‚Üí Explore ‚Üí Search ‚Üí Create ‚Üí Export\n```\n\n1. **Initialize** - Call `getUserContext` to see available schemas\n2. **Explore** - Use `getSchema` to understand events and properties  \n3. **Search** - Check `searchKubit` for existing analyses\n4. **Create** - Generate new reports with custom queries\n5. **Export** - Pull `getRawData` for external analysis\n\n---\n\n## Best Practices\n\n### Crafting Effective Prompts\n\n**Be Specific**  \nInclude time ranges, events, and segments in your questions.\n\n```diff\n- \"Show me users\"\n+ \"Show me active users in the US who signed up last month\"\n```\n\n**Provide Context**  \nExplain what you're trying to understand.",
        "start_pos": 1848,
        "end_pos": 3681,
        "token_count_estimate": 458,
        "source_type": "readme",
        "agent_id": "2e08317133c4dc39"
      },
      {
        "chunk_id": 2,
        "text": "at's the conversion rate?\"\n+ \"What's the conversion rate from free trial to paid for users who engaged with feature X?\"\n```\n\n**Reference Schemas**  \nUse schema names when working with multiple data sources.\n\n```diff\n- \"Show me sign-up events\"\n+ \"In the mobile_events schema, show me sign-up events\"\n```\n\n**Break It Down**  \nComplex analyses work better as multiple focused questions.\n\n```diff\n- \"Show me everything about user behavior across all channels with retention and conversion\"\n+ Start with \"Show me user retention by channel\" then follow up\n```\n\n### Performance Optimization\n\n- **Use `searchKubit` first** - Leverage existing analyses before creating new reports\n- **Specify date ranges** - Narrow time windows improve query performance  \n- **Export selectively** - Only use `getRawData` when you need detailed external analysis\n\n### Security & Compliance\n\n| Consideration | What It Means |\n|---------------|---------------|\n| **Permission Model** | You can only access schemas you're authorized to view |\n| **AI Processing** | Third-party AI models will process your query data |\n| **Policy Review** | Confirm your organization allows AI-assisted data analysis |\n\n---\n\n## Troubleshooting\n\n### Common Issues & Solutions\n\n**Authentication Failures**  \nVerify your Kubit credentials and organization name\n\n**No Schemas Available**  \nCheck that you have access to at least one schema in Kubit\n\n**Connection Errors**  \nConfirm you're using the correct server URL: `https://mcp.kubit.ai/mcp`\n\n**Report Generation Issues**  \nVerify the schema and events you're referencing exist using `getSchema`\n\n### Need Help?",
        "start_pos": 3696,
        "end_pos": 5311,
        "token_count_estimate": 403,
        "source_type": "readme",
        "agent_id": "2e08317133c4dc39"
      },
      {
        "chunk_id": 3,
        "text": ".kubit.ai](https://docs.kubit.ai/) - Complete platform documentation\n\n**Customer Success**  \nContact your Kubit customer success team for assistance\n\n**About Kubit**  \n[kubit.ai](https://kubit.ai) - Learn more about warehouse-native analytics\n\n**MCP Protocol**  \n[modelcontextprotocol.io](https://modelcontextprotocol.io) - Explore the Model Context Protocol",
        "start_pos": 5544,
        "end_pos": 5903,
        "token_count_estimate": 89,
        "source_type": "readme",
        "agent_id": "2e08317133c4dc39"
      }
    ]
  },
  {
    "agent_id": "f27a3ecbdfdc2b7d",
    "name": "ai.llmse/mcp",
    "source": "mcp",
    "source_url": "https://llmse.ai/mcp",
    "description": "Public MCP server for the LLM Search Engine",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-21T16:19:36.12182Z",
    "indexed_at": "2026-02-18T04:02:02.409739",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Serve as a public MCP server",
        "Support the LLM Search Engine"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with minimal information about the server's functionality.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "68c60611626c6f53",
    "name": "ai.ludo/game-assets",
    "source": "mcp",
    "source_url": "https://github.com/Ludo-AI/ludo-mcp",
    "description": "Generate game assets with AI: sprites, 3D models, animations, sound effects, music, and voices.",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-01-30T17:59:40.034021Z",
    "indexed_at": "2026-02-18T04:02:03.528782",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Ludo AI MCP Server\n\nGenerate game assets using AI through the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/).\n\n## Features\n\n| Category | Capabilities |\n|----------|-------------|\n| **Images** | Sprites, icons, screenshots, backgrounds, UI assets, textures, background removal |\n| **3D Models** | Convert 2D images to GLB models with PBR textures |\n| **Animation** | Animated spritesheets from static sprites (4-64 frames), motion transfer from video or presets |\n| **Video** | Generate short videos from images (3-10 seconds) |\n| **Audio** | Sound effects, background music, character voices, TTS |\n\n## Quick Start\n\n### 1. Get an API Key\n\nSign up at [ludo.ai](https://ludo.ai) and get your API key from [app.ludo.ai](https://app.ludo.ai).\n\n### 2. Configure Your MCP Client\n\n#### Claude Desktop\n\nAdd to `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"ludo\": {\n      \"url\": \"https://mcp.ludo.ai/mcp\",\n      \"headers\": {\n        \"Authorization\": \"ApiKey YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor\n\nAdd to your MCP settings in Cursor preferences:\n\n```json\n{\n  \"mcpServers\": {\n    \"ludo\": {\n      \"url\": \"https://mcp.ludo.ai/mcp\",\n      \"headers\": {\n        \"Authorization\": \"ApiKey YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n### Image Generation (`createImage`)\n\nGenerate sprites, icons, backgrounds, UI assets, and textures.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `prompt` | Yes | Detailed description of the image |\n| `image_type` | Yes | `sprite`, `icon`, `screenshot`, `art`, `asset`, `sprite-vfx`, `ui_asset`, `fixed_background`, `texture`, `3d`, `generic` |\n| `art_style` | No | `Pixel Art (16-Bit)`, `Pixel Art (8-Bit)`, `Low Poly`, `Cartoonish`, `Stylized 3D`, `Flat Design`, `Anime/Manga`, `Voxel Art`, etc. |\n| `perspective` | No | `Side-Scroll`, `Top-Down`, `Isometric`, `First-Person`, `Third-Person`, `2.5D` |\n| `platform` | No | `Mobile`, `Desktop`, `Web` |\n| `genre` | No | `Hypercasual`, `Action`, `RPG`, `Puzzle`, `Platformer`, etc. |\n| `aspect_ratio` | No | `default`, `ar_1_1`, `ar_4_3`, `ar_16_9`, `ar_9_16` |\n| `n` | No | Number of variations (1-8, default: 1) |\n\n**Credits:** 0.5 per image\n\n---\n\n### Edit Image (`editImage`)\n\nModify an existing image using text instructions (smart editing).\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded image to edit |\n| `prompt` | Yes | Description of changes (e.g., \"remove the background\", \"make it darker\", \"add clouds to the sky\") |\n| `reference_image` | No | URL or base64 reference image for style/content guidance |\n| `n` | No | Number of variations (1-4, default: 1) |\n\n**Credits:** 0.5 per image\n\n---\n\n### Generate with Style (`generateWithStyle`)\n\nGenerate new content while maintaining the visual style of a reference image.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `style_image` | Yes | URL or base64-encoded reference image for style matching |\n| `prompt` | Yes | Description of what to generate (e.g., \"a warrior character\", \"a treasure chest\") |\n| `image_type` | Yes | `sprite`, `icon`, `screenshot`, `art`, `asset`, `sprite-vfx`, `ui_asset`, `fixed_background`, `texture`, `3d`, `generic` |\n| `n` | No | Number of variations (1-4, default: 1) |\n\n**Credits:** 0.5 per image\n\n---\n\n### Generate Pose (`generatePose`)\n\nGenerate a new pose for an existing sprite. **Use this BEFORE `animateSprite`** to get the best animation results - the starting pose should match your intended animation.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded sprite image |\n| `pose` | Yes | Target pose: `Idle (Front)`, `Idle (Back)`, `Walk / Run (Left)`, `Attack Ready`, `Jumping`, `Crouching`, `Flying`, `Defending / Blocking`, or any custom description |\n| `description` | No | Additional instructions to guide pose generation |\n| `n` | No | Number of variations (1-4, default: 1) |\n\n**Returns:** `url`, `pose`, `motion_prompt`\n\n**Example workflow:**\n1. Generate a \"Walk / Run (Left)\" pose with `generatePose`\n2. Use the returned `motion_prompt` directly in `animateSprite` for optimal animation results\n\n**Credits:** 0.5 per image\n\n---\n\n### Remove Background (`removeImageBackground`)\n\nRemove the background from an image, returning a transparent PNG.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded image |\n\n**Returns:** `url` (transparent PNG)\n\n**Credits:** 0.5 per image\n\n---\n\n### 3D Model Generation (`create3DModel`)\n\nConvert a 2D image to a 3D GLB model with textures.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded image |\n| `texture_type` | No | `pbr` (default), `simple`, `none` |\n| `texture_size` | No | `1024`, `2048` (default), `4096` |\n| `target_num_faces` | No | Triangle count 1,000-100,000 (default: 50,000) |\n| `high_detail_shape` | No | Enable for complex shapes (slower) |\n\n**Returns:** `model_url` (GLB file) + 4 snapshot images from different angles\n\n**Credits:** 3 per model\n**Processing time:** 60-120 seconds\n\n---\n\n### Sprite Animation (`animateSprite`)\n\nCreate animated spritesheets from static images.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `initial_image` | Yes | URL or base64 of the starting frame |\n| `motion_prompt` | Yes | Animation description (e.g., \"walking cycle\", \"idle breathing\", \"attack slash\") |\n| `image_type` | No | `sprite`, `sprite-vfx`, `ui_asset` |\n| `frames` | No | `4`, `9`, `16`, `25`, `36` (default), `49`, `64` |\n| `frame_size` | No | `64`, `128`, `256` (default), `0` (max resolution) |\n| `loop` | No | Seamless loop (default: true) |\n| `model` | No | `standard` (default) or `new` (higher quality) |\n| `duration` | No | Standard: 1.2-3s, New: 4s |\n| `final_image` | No | Ending frame for interpolation |\n| `pixel_art_filter` | No | `none`, `small`, `medium`, `large` |\n| `gif` | No | Generate an animated GIF (default: false) |\n| `individual_frames` | No | Extract individual frame images (default: false) |\n\n**Returns:** `spritesheet_url`, `video_url`, `gif_url`, `individual_frame_urls`, `num_frames`, `num_cols`, `num_rows`\n\n**Credits:** 5 per animation\n**Processing time:** 30-90 seconds\n\n---\n\n### Animation Presets (`listAnimationPresets`)\n\nList available animation presets for use with motion transfer. Returns preset animations, perspectives, and directions ‚Äî no video URLs are exposed.\n\n**Returns:**\n- `animations` ‚Äî Array of presets with `id`, `name`, `category`, `description`, `duration`, `preview_url`\n- `perspectives` ‚Äî Array with `id`, `name`, `description` (all animations support all perspectives)\n- `directions` ‚Äî `[\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]` (all animations support all directions)\n\n**Credits:** Free\n\n---\n\n### Motion Transfer (`transferMotion`)\n\nTransfer motion from a video or animation preset onto a static sprite, producing an animated spritesheet.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded sprite image |\n| `video` | No | URL of the video to use as motion source. You can use videos from the animateSprite endpoint or provide your own. Videos up to 4 seconds will produce better results. Either `video` or `preset_id` + `perspective` + `direction` must be provided. |\n| `preset_id` | No | ID of an animation preset to use instead of a video URL. Use the animation-presets endpoint to list available presets. When using a preset, `perspective` and `direction` are required. |\n| `direction` | No | Direction for the animation preset. When using a preset, `direction` is required. Values: `N`, `NE`, `E`, `SE`, `S`, `SW`, `W`, `NW` |\n| `perspective` | No | Perspective ID to use with the animation preset. When using a preset, `perspective` is required. |\n| `num_frames` | No | Number of frames in the output spritesheet |\n| `target_frame_size` | No | Size of each frame in pixels |\n| `loop` | No | Trim animation for seamless loop |\n| `crop` | No | Crop frames to fit content |\n| `pixel_art_filter` | No | `none`, `small`, `medium`, `large` |\n| `margin_ratio` | No | Padding around sprite (0.0-1.0) |\n| `margin_ratio_mode` | No | `manual` (default), `none` |\n| `gif` | No | Generate an animated GIF (default: false) |\n| `individual_frames` | No | Extract individual frame images (default: false) |\n\n**Returns:** `spritesheet_url`, `video_url`, `gif_url`, `individual_frame_urls`, `num_frames`, `num_cols`, `num_rows`\n\n**Credits:** 5 per transfer\n\n---\n\n### Video Generation (`createVideo`)\n\nGenerate short videos from images.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64 starting frame |\n| `prompt` | Yes | Motion description (e.g., \"camera zooms in\", \"character walks forward\") |\n| `duration` | No | `3`, `5` (default), `8`, `10` seconds |\n| `model` | No | `standard` (default) or `new` |\n| `final_image` | No | Ending frame for interpolation |\n\n**Credits:** 3s=5, 5s=8, 8s=12, 10s=15\n\n---\n\n### Sound Effect Generation (`createSoundEffect`)\n\nGenerate game sound effects from text descriptions.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `description` | Yes | Sound description (e.g., \"laser gun firing\", \"footsteps on gravel\", \"coin pickup\") |\n| `duration` | No | 0-10 seconds (0 = automatic) |\n\n**Credits:** 3 per sound\n\n---\n\n### Music Generation (`createMusic`)\n\nGenerate background music and themes.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `description` | Yes | Music description (e.g., \"epic orchestral battle theme\", \"calm piano melody\", \"8-bit chiptune\") |\n| `lyrics` | No | Optional lyrics for vocal tracks |\n\n**Credits:** 3 per track\n\n---\n\n### Voice Generation (`createVoice`)\n\nGenerate unique character voices.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `voice_description` | Yes | Character description (e.g., \"gruff old warrior\", \"cheerful young girl\") |\n| `text` | Yes | Text to speak (max 200 characters) |\n| `type` | No | `human` (default) or `non-human` |\n\n**Credits:** 3 per voice\n\n---\n\n### Text-to-Speech (`createSpeech`)\n\nClone a voice from an audio sample.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `text` | Yes | Text to speak (max 1000 characters) |\n| `sample` | Yes | URL or base64 audio sample for voice cloning |\n\n**Credits:** 3 per generation\n\n---\n\n### Text-to-Speech Preset (`createSpeechPreset`)\n\nUse preset voices for text-to-speech.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `text` | Yes | Text to speak (max 1000 characters) |\n| `voice_preset_id` | Yes | `Serious woman`, `Wise woman`, `Calm woman`, `Patient man`, `Determined man`, `Deep voice man`, `Teen boy`, `Sweet girl`, etc. |\n| `emotion` | No | `Default`, `Happy`, `Sad`, `Angry`, `Fearful`, `Surprised`, `Neutral` |\n| `language` | No | `auto`, `English`, `Spanish`, `French`, `German`, `Japanese`, `Korean`, etc. |\n\n**Credits:** 3 per generation\n\n---\n\n## Example Prompts\n\n### Creating Game Assets\n\n```\nCreate a pixel art knight character with sword and shield, side view, 16-bit style\n```\n\n```\nGenerate an isometric treasure chest icon for a mobile RPG\n```\n\n```\nCreate a low-poly stylized tree for a casual mobile game\n```\n\n### Animations\n\n```\nAnimate this character with a smooth walking cycle, 16 frames\n```\n\n```\nCreate an idle breathing animation for this character sprite\n```\n\n```\nAnimate this fire sprite as a looping VFX effect\n```\n\n### Audio\n\n```\nCreate a satisfying coin pickup sound effect for a platformer\n```\n\n```\nGenerate an epic orchestral boss battle theme, intense and dramatic\n```\n\n```\nCreate a voice for a wise old wizard saying \"The journey begins now\"\n```\n\n## API Documentation\n\nFull API documentation with all parameters and response formats: [api.ludo.ai/api-documentation](https://api.ludo.ai/api-documentation)\n\n## Support\n\n- [Documentation](https://api.ludo.ai/api-documentation)\n- [Ludo AI Website](https://ludo.ai)\n- [GitHub Issues](https://github.com/Ludo-AI/ludo-mcp/issues)\n\n## License\n\nProprietary - See [ludo.ai/terms](https://ludo.ai/terms) for terms of service."
    },
    "llm_extracted": {
      "capabilities": [
        "Generate 2D game images including sprites, icons, backgrounds, UI assets, and textures",
        "Edit existing images using text instructions for smart modifications",
        "Generate new images maintaining the style of a reference image",
        "Create new poses for sprites to prepare for animation",
        "Remove backgrounds from images to produce transparent PNGs",
        "Convert 2D images into 3D GLB models with PBR textures",
        "Create animated spritesheets from static images with customizable frames and effects",
        "List animation presets for motion transfer onto sprites",
        "Transfer motion from videos or presets onto static sprites to produce animations",
        "Generate short videos from images with motion descriptions",
        "Generate game sound effects, background music, character voices, and text-to-speech audio"
      ],
      "limitations": [
        "Video motion transfer works best with videos up to 4 seconds",
        "3D model generation processing time ranges from 60 to 120 seconds",
        "Sprite animation processing time ranges from 30 to 90 seconds",
        "Text-to-speech cloning limited to 1000 characters per generation",
        "Voice generation text limited to 200 characters",
        "Number of variations for image generation and editing capped between 1 and 8 or 4 depending on tool",
        "Credits are required per generation and vary by asset type",
        "Animation presets do not expose video URLs directly"
      ],
      "requirements": [
        "API key obtained by signing up at ludo.ai and accessing app.ludo.ai",
        "Configuration of MCP client with server URL https://mcp.ludo.ai/mcp and Authorization header using the API key",
        "Support for base64 or URL encoded images and audio inputs",
        "Use of supported MCP clients such as Claude Desktop or Cursor with proper configuration"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with parameters and credits, usage examples, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Ludo AI MCP Server\n\nGenerate game assets using AI through the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/).\n\n## Features\n\n| Category | Capabilities |\n|----------|-------------|\n| **Images** | Sprites, icons, screenshots, backgrounds, UI assets, textures, background removal |\n| **3D Models** | Convert 2D images to GLB models with PBR textures |\n| **Animation** | Animated spritesheets from static sprites (4-64 frames), motion transfer from video or presets |\n| **Video** | Generate short videos from images (3-10 seconds) |\n| **Audio** | Sound effects, background music, character voices, TTS |\n\n## Quick Start\n\n### 1. Get an API Key\n\nSign up at [ludo.ai](https://ludo.ai) and get your API key from [app.ludo.ai](https://app.ludo.ai).\n\n### 2. Configure Your MCP Client\n\n#### Claude Desktop\n\nAdd to `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"ludo\": {\n      \"url\": \"https://mcp.ludo.ai/mcp\",\n      \"headers\": {\n        \"Authorization\": \"ApiKey YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor\n\nAdd to your MCP settings in Cursor preferences:\n\n```json\n{\n  \"mcpServers\": {\n    \"ludo\": {\n      \"url\": \"https://mcp.ludo.ai/mcp\",\n      \"headers\": {\n        \"Authorization\": \"ApiKey YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\n### Image Generation (`createImage`)\n\nGenerate sprites, icons, backgrounds, UI assets, and textures.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `prompt` | Yes | Detailed description of the image |\n| `image_type` | Yes | `sprite`, `icon`, `screenshot`, `art`, `asset`, `sprite-vfx`, `ui_asset`, `fixed_background`, `texture`, `3d`, `generic` |\n| `art_style` | No | `Pixel Art (16-Bit)`, `Pixel Art (8-Bit)`, `Low Poly`, `Cartoonish`, `Stylized 3D`, `Flat Design`, `Anime/Manga`, `Voxel Art`, etc.",
        "start_pos": 0,
        "end_pos": 1918,
        "token_count_estimate": 479,
        "source_type": "readme",
        "agent_id": "68c60611626c6f53"
      },
      {
        "chunk_id": 1,
        "text": "fixed_background`, `texture`, `3d`, `generic` |\n| `art_style` | No | `Pixel Art (16-Bit)`, `Pixel Art (8-Bit)`, `Low Poly`, `Cartoonish`, `Stylized 3D`, `Flat Design`, `Anime/Manga`, `Voxel Art`, etc. |\n| `perspective` | No | `Side-Scroll`, `Top-Down`, `Isometric`, `First-Person`, `Third-Person`, `2.5D` |\n| `platform` | No | `Mobile`, `Desktop`, `Web` |\n| `genre` | No | `Hypercasual`, `Action`, `RPG`, `Puzzle`, `Platformer`, etc. |\n| `aspect_ratio` | No | `default`, `ar_1_1`, `ar_4_3`, `ar_16_9`, `ar_9_16` |\n| `n` | No | Number of variations (1-8, default: 1) |\n\n**Credits:** 0.5 per image\n\n---\n\n### Edit Image (`editImage`)\n\nModify an existing image using text instructions (smart editing).\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded image to edit |\n| `prompt` | Yes | Description of changes (e.g., \"remove the background\", \"make it darker\", \"add clouds to the sky\") |\n| `reference_image` | No | URL or base64 reference image for style/content guidance |\n| `n` | No | Number of variations (1-4, default: 1) |\n\n**Credits:** 0.5 per image\n\n---\n\n### Generate with Style (`generateWithStyle`)\n\nGenerate new content while maintaining the visual style of a reference image.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `style_image` | Yes | URL or base64-encoded reference image for style matching |\n| `prompt` | Yes | Description of what to generate (e.g., \"a warrior character\", \"a treasure chest\") |\n| `image_type` | Yes | `sprite`, `icon`, `screenshot`, `art`, `asset`, `sprite-vfx`, `ui_asset`, `fixed_background`, `texture`, `3d`, `generic` |\n| `n` | No | Number of variations (1-4, default: 1) |\n\n**Credits:** 0.5 per image\n\n---\n\n### Generate Pose (`generatePose`)\n\nGenerate a new pose for an existing sprite. **Use this BEFORE `animateSprite`** to get the best animation results - the starting pose should match your intended animation.",
        "start_pos": 1718,
        "end_pos": 3675,
        "token_count_estimate": 489,
        "source_type": "readme",
        "agent_id": "68c60611626c6f53"
      },
      {
        "chunk_id": 2,
        "text": "erate Pose (`generatePose`)\n\nGenerate a new pose for an existing sprite. **Use this BEFORE `animateSprite`** to get the best animation results - the starting pose should match your intended animation.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded sprite image |\n| `pose` | Yes | Target pose: `Idle (Front)`, `Idle (Back)`, `Walk / Run (Left)`, `Attack Ready`, `Jumping`, `Crouching`, `Flying`, `Defending / Blocking`, or any custom description |\n| `description` | No | Additional instructions to guide pose generation |\n| `n` | No | Number of variations (1-4, default: 1) |\n\n**Returns:** `url`, `pose`, `motion_prompt`\n\n**Example workflow:**\n1. Generate a \"Walk / Run (Left)\" pose with `generatePose`\n2. Use the returned `motion_prompt` directly in `animateSprite` for optimal animation results\n\n**Credits:** 0.5 per image\n\n---\n\n### Remove Background (`removeImageBackground`)\n\nRemove the background from an image, returning a transparent PNG.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded image |\n\n**Returns:** `url` (transparent PNG)\n\n**Credits:** 0.5 per image\n\n---\n\n### 3D Model Generation (`create3DModel`)\n\nConvert a 2D image to a 3D GLB model with textures.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded image |\n| `texture_type` | No | `pbr` (default), `simple`, `none` |\n| `texture_size` | No | `1024`, `2048` (default), `4096` |\n| `target_num_faces` | No | Triangle count 1,000-100,000 (default: 50,000) |\n| `high_detail_shape` | No | Enable for complex shapes (slower) |\n\n**Returns:** `model_url` (GLB file) + 4 snapshot images from different angles\n\n**Credits:** 3 per model\n**Processing time:** 60-120 seconds\n\n---\n\n### Sprite Animation (`animateSprite`)\n\nCreate animated spritesheets from static images.",
        "start_pos": 3475,
        "end_pos": 5410,
        "token_count_estimate": 483,
        "source_type": "readme",
        "agent_id": "68c60611626c6f53"
      },
      {
        "chunk_id": 3,
        "text": ") + 4 snapshot images from different angles\n\n**Credits:** 3 per model\n**Processing time:** 60-120 seconds\n\n---\n\n### Sprite Animation (`animateSprite`)\n\nCreate animated spritesheets from static images.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `initial_image` | Yes | URL or base64 of the starting frame |\n| `motion_prompt` | Yes | Animation description (e.g., \"walking cycle\", \"idle breathing\", \"attack slash\") |\n| `image_type` | No | `sprite`, `sprite-vfx`, `ui_asset` |\n| `frames` | No | `4`, `9`, `16`, `25`, `36` (default), `49`, `64` |\n| `frame_size` | No | `64`, `128`, `256` (default), `0` (max resolution) |\n| `loop` | No | Seamless loop (default: true) |\n| `model` | No | `standard` (default) or `new` (higher quality) |\n| `duration` | No | Standard: 1.2-3s, New: 4s |\n| `final_image` | No | Ending frame for interpolation |\n| `pixel_art_filter` | No | `none`, `small`, `medium`, `large` |\n| `gif` | No | Generate an animated GIF (default: false) |\n| `individual_frames` | No | Extract individual frame images (default: false) |\n\n**Returns:** `spritesheet_url`, `video_url`, `gif_url`, `individual_frame_urls`, `num_frames`, `num_cols`, `num_rows`\n\n**Credits:** 5 per animation\n**Processing time:** 30-90 seconds\n\n---\n\n### Animation Presets (`listAnimationPresets`)\n\nList available animation presets for use with motion transfer. Returns preset animations, perspectives, and directions ‚Äî no video URLs are exposed.\n\n**Returns:**\n- `animations` ‚Äî Array of presets with `id`, `name`, `category`, `description`, `duration`, `preview_url`\n- `perspectives` ‚Äî Array with `id`, `name`, `description` (all animations support all perspectives)\n- `directions` ‚Äî `[\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"]` (all animations support all directions)\n\n**Credits:** Free\n\n---\n\n### Motion Transfer (`transferMotion`)\n\nTransfer motion from a video or animation preset onto a static sprite, producing an animated spritesheet.",
        "start_pos": 5210,
        "end_pos": 7167,
        "token_count_estimate": 489,
        "source_type": "readme",
        "agent_id": "68c60611626c6f53"
      },
      {
        "chunk_id": 4,
        "text": "tions support all directions)\n\n**Credits:** Free\n\n---\n\n### Motion Transfer (`transferMotion`)\n\nTransfer motion from a video or animation preset onto a static sprite, producing an animated spritesheet.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64-encoded sprite image |\n| `video` | No | URL of the video to use as motion source. You can use videos from the animateSprite endpoint or provide your own. Videos up to 4 seconds will produce better results. Either `video` or `preset_id` + `perspective` + `direction` must be provided. |\n| `preset_id` | No | ID of an animation preset to use instead of a video URL. Use the animation-presets endpoint to list available presets. When using a preset, `perspective` and `direction` are required. |\n| `direction` | No | Direction for the animation preset. When using a preset, `direction` is required. Values: `N`, `NE`, `E`, `SE`, `S`, `SW`, `W`, `NW` |\n| `perspective` | No | Perspective ID to use with the animation preset. When using a preset, `perspective` is required. |\n| `num_frames` | No | Number of frames in the output spritesheet |\n| `target_frame_size` | No | Size of each frame in pixels |\n| `loop` | No | Trim animation for seamless loop |\n| `crop` | No | Crop frames to fit content |\n| `pixel_art_filter` | No | `none`, `small`, `medium`, `large` |\n| `margin_ratio` | No | Padding around sprite (0.0-1.0) |\n| `margin_ratio_mode` | No | `manual` (default), `none` |\n| `gif` | No | Generate an animated GIF (default: false) |\n| `individual_frames` | No | Extract individual frame images (default: false) |\n\n**Returns:** `spritesheet_url`, `video_url`, `gif_url`, `individual_frame_urls`, `num_frames`, `num_cols`, `num_rows`\n\n**Credits:** 5 per transfer\n\n---\n\n### Video Generation (`createVideo`)\n\nGenerate short videos from images.",
        "start_pos": 6967,
        "end_pos": 8819,
        "token_count_estimate": 463,
        "source_type": "readme",
        "agent_id": "68c60611626c6f53"
      },
      {
        "chunk_id": 5,
        "text": "_url`, `video_url`, `gif_url`, `individual_frame_urls`, `num_frames`, `num_cols`, `num_rows`\n\n**Credits:** 5 per transfer\n\n---\n\n### Video Generation (`createVideo`)\n\nGenerate short videos from images.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `image` | Yes | URL or base64 starting frame |\n| `prompt` | Yes | Motion description (e.g., \"camera zooms in\", \"character walks forward\") |\n| `duration` | No | `3`, `5` (default), `8`, `10` seconds |\n| `model` | No | `standard` (default) or `new` |\n| `final_image` | No | Ending frame for interpolation |\n\n**Credits:** 3s=5, 5s=8, 8s=12, 10s=15\n\n---\n\n### Sound Effect Generation (`createSoundEffect`)\n\nGenerate game sound effects from text descriptions.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `description` | Yes | Sound description (e.g., \"laser gun firing\", \"footsteps on gravel\", \"coin pickup\") |\n| `duration` | No | 0-10 seconds (0 = automatic) |\n\n**Credits:** 3 per sound\n\n---\n\n### Music Generation (`createMusic`)\n\nGenerate background music and themes.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `description` | Yes | Music description (e.g., \"epic orchestral battle theme\", \"calm piano melody\", \"8-bit chiptune\") |\n| `lyrics` | No | Optional lyrics for vocal tracks |\n\n**Credits:** 3 per track\n\n---\n\n### Voice Generation (`createVoice`)\n\nGenerate unique character voices.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `voice_description` | Yes | Character description (e.g., \"gruff old warrior\", \"cheerful young girl\") |\n| `text` | Yes | Text to speak (max 200 characters) |\n| `type` | No | `human` (default) or `non-human` |\n\n**Credits:** 3 per voice\n\n---\n\n### Text-to-Speech (`createSpeech`)\n\nClone a voice from an audio sample.",
        "start_pos": 8619,
        "end_pos": 10453,
        "token_count_estimate": 458,
        "source_type": "readme",
        "agent_id": "68c60611626c6f53"
      },
      {
        "chunk_id": 6,
        "text": "| Required | Description |\n|-----------|----------|-------------|\n| `text` | Yes | Text to speak (max 1000 characters) |\n| `sample` | Yes | URL or base64 audio sample for voice cloning |\n\n**Credits:** 3 per generation\n\n---\n\n### Text-to-Speech Preset (`createSpeechPreset`)\n\nUse preset voices for text-to-speech.\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `text` | Yes | Text to speak (max 1000 characters) |\n| `voice_preset_id` | Yes | `Serious woman`, `Wise woman`, `Calm woman`, `Patient man`, `Determined man`, `Deep voice man`, `Teen boy`, `Sweet girl`, etc. |\n| `emotion` | No | `Default`, `Happy`, `Sad`, `Angry`, `Fearful`, `Surprised`, `Neutral` |\n| `language` | No | `auto`, `English`, `Spanish`, `French`, `German`, `Japanese`, `Korean`, etc. |\n\n**Credits:** 3 per generation\n\n---\n\n## Example Prompts\n\n### Creating Game Assets\n\n```\nCreate a pixel art knight character with sword and shield, side view, 16-bit style\n```\n\n```\nGenerate an isometric treasure chest icon for a mobile RPG\n```\n\n```\nCreate a low-poly stylized tree for a casual mobile game\n```\n\n### Animations\n\n```\nAnimate this character with a smooth walking cycle, 16 frames\n```\n\n```\nCreate an idle breathing animation for this character sprite\n```\n\n```\nAnimate this fire sprite as a looping VFX effect\n```\n\n### Audio\n\n```\nCreate a satisfying coin pickup sound effect for a platformer\n```\n\n```\nGenerate an epic orchestral boss battle theme, intense and dramatic\n```\n\n```\nCreate a voice for a wise old wizard saying \"The journey begins now\"\n```\n\n## API Documentation\n\nFull API documentation with all parameters and response formats: [api.ludo.ai/api-documentation](https://api.ludo.ai/api-documentation)\n\n## Support\n\n- [Documentation](https://api.ludo.ai/api-documentation)\n- [Ludo AI Website](https://ludo.ai)\n- [GitHub Issues](https://github.com/Ludo-AI/ludo-mcp/issues)\n\n## License\n\nProprietary - See [ludo.ai/terms](https://ludo.ai/terms) for terms of service.",
        "start_pos": 10467,
        "end_pos": 12439,
        "token_count_estimate": 493,
        "source_type": "readme",
        "agent_id": "68c60611626c6f53"
      },
      {
        "chunk_id": 7,
        "text": "entation)\n- [Ludo AI Website](https://ludo.ai)\n- [GitHub Issues](https://github.com/Ludo-AI/ludo-mcp/issues)\n\n## License\n\nProprietary - See [ludo.ai/terms](https://ludo.ai/terms) for terms of service.",
        "start_pos": 12239,
        "end_pos": 12439,
        "token_count_estimate": 50,
        "source_type": "readme",
        "agent_id": "68c60611626c6f53"
      }
    ]
  },
  {
    "agent_id": "c336f98e9973c53a",
    "name": "ai.mailjunky/mcp",
    "source": "mcp",
    "source_url": "https://github.com/TheNightProject/tnp.web.mailjunky.ai",
    "description": "Send emails, track events, and manage contacts with MailJunky.",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-24T00:30:06.33677Z",
    "indexed_at": "2026-02-18T04:02:05.369712",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Send emails",
        "Track events",
        "Manage contacts"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of core functions but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "c12237a27b9ddfde",
    "name": "ai.mcpanalytics/analytics",
    "source": "mcp",
    "source_url": "https://api.mcpanalytics.ai/auth0",
    "description": "ML statistical analysis platform for data teams",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-17T02:38:18.073872Z",
    "indexed_at": "2026-02-18T04:02:09.511157",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Perform ML statistical analysis",
        "Support data teams in analytics tasks"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence providing a very basic overview without details on features, usage, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "c12237a27b9ddfde",
    "name": "ai.mcpanalytics/analytics",
    "source": "mcp",
    "source_url": "https://github.com/embeddedlayers/mcp-analytics",
    "description": "MCP Analytics, searchable tools and reports with interactive HTML visualization",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-17T03:17:49.632449Z",
    "indexed_at": "2026-02-18T04:02:10.332183",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# MCP Analytics Suite\n\n<div align=\"center\">\n\n[![Version](https://img.shields.io/badge/version-1.0.3-blue)](https://mcpanalytics.ai)\n[![Platform](https://img.shields.io/badge/Platform-MCP_Compatible-green)](https://mcpanalytics.ai/tools)\n[![License](https://img.shields.io/badge/License-Commercial-orange)](https://mcpanalytics.ai/terms)\n[![Docs](https://img.shields.io/badge/Docs-mcpanalytics.ai-brightgreen)](https://mcpanalytics.ai/docs)\n[![Auth](https://img.shields.io/badge/Auth-OAuth_2.0-purple)](https://mcpanalytics.ai/oauth-setup)\n\n**Every analysis starts with a question. We handle the rest.**\n\n[üöÄ Quick Start](#quick-start) ‚Ä¢ [üîÑ How It Works](#how-it-works) ‚Ä¢ [üõ†Ô∏è MCP Tools](#mcp-tools) ‚Ä¢ [üõ°Ô∏è Security](#security--compliance) ‚Ä¢ [üìñ Documentation](#documentation)\n\n</div>\n\n---\n\n## The Formula\n\n<div align=\"center\">\n  <h3>Question + Dataset = Analytics</h3>\n  <p>Transform business questions into actionable insights through intelligent discovery</p>\n</div>\n\n## Overview\n\nMCP Analytics Suite is an intelligent analytics platform that understands what you want to analyze and automatically selects the right approach. No statistics degree required‚Äîjust describe your business question and let our AI-powered discovery handle the complexity.\n\n### Why MCP Analytics?\n\n- **Intelligent Discovery**: Automatically finds the right analytical approach\n- **Complete Workflow**: From question to insight in one seamless flow\n- **Zero Setup**: Cloud-based processing, works instantly\n- **Enterprise Security**: OAuth2, encryption, isolated processing\n- **Comprehensive Suite**: Full range of analytical capabilities\n- **Interactive Reports**: Shareable visualizations with AI insights\n\n## Quick Start\n\n### Installation\n\n##### For Claude Desktop\n\nAdd to your config file:\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-analytics\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote@latest\", \"https://api.mcpanalytics.ai/auth0\"]\n    }\n  }\n}\n```\n\n##### For Cursor\n\nAdd to `.cursor/config.json` in your project root:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-analytics\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote@latest\", \"https://api.mcpanalytics.ai/auth0\"]\n    }\n  }\n}\n```\n\n##### For VS Code (Continue Extension)\n\nAdd to your Continue config at `~/.continue/config.json`:\n\n```json\n{\n  \"models\": [{\n    \"provider\": \"anthropic\",\n    \"model\": \"claude-3-5-sonnet\",\n    \"mcpServers\": {\n      \"mcp-analytics\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"mcp-remote@latest\", \"https://api.mcpanalytics.ai/auth0\"]\n      }\n    }\n  }]\n}\n```\n\n##### For Claude Code\n\nAdd to `claude_code_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-analytics\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote@latest\", \"https://api.mcpanalytics.ai/auth0\"]\n    }\n  }\n}\n```\n\n## How It Works\n\n### The MCP Analytics Workflow\n\n1. **Ask Your Question** - Describe what you want to analyze in natural language\n2. **Intelligent Discovery** - `tools.discover` finds the right analytical approach\n3. **Data Upload** - `datasets.upload` securely processes your data\n4. **Automated Analysis** - `tools.run` executes with optimal configuration\n5. **Interactive Results** - `reports.view` delivers shareable insights\n\n```\nUser: \"What drives our sales growth?\"\nMCP Analytics:\n  ‚Üí Discovers regression and correlation methods\n  ‚Üí Configures analysis for your data structure\n  ‚Üí Runs multiple analytical approaches\n  ‚Üí Returns comprehensive report with insights\n```\n\n## MCP Tools\n\nThe platform provides a complete suite of MCP tools for end-to-end analytics:\n\n### Core Analytics Tools\n- **`tools.discover`** - Natural language tool discovery\n- **`tools.run`** - Automated analysis execution\n- **`tools.info`** - Get tool documentation\n\n### Data Management\n- **`datasets.upload`** - Secure data upload with encryption\n- **`datasets.list`** - Manage your datasets\n- **`datasets.read`** - Access and preview data\n\n### Reporting & Insights\n- **`reports.view`** - Interactive visualization dashboard\n- **`reports.search`** - Semantic search across analyses\n\n### Platform Tools\n- **`billing()`** - Usage and subscription management\n- **`about()`** - Platform information and status\n- **`manual()`** - Documentation access\n\n## Features\n\n### Natural Language Interface\n\nJust describe what you need:\n\n```\n\"What drives our revenue growth?\"\n\"Find customer segments in our data\"\n\"Forecast next quarter's sales\"\n\"Did our marketing campaign work?\"\n```\n\n### Comprehensive Analysis Suite\n\n<table>\n<tr>\n<td width=\"50%\">\n\n**Statistical Methods**\n- Regression Analysis\n- Advanced Modeling\n- Hypothesis Testing\n- Survival Analysis\n- Bayesian Methods\n\n</td>\n<td width=\"50%\">\n\n**Machine Learning**\n- Ensemble Methods\n- Boosting Algorithms\n- Neural Networks\n- Clustering\n- Dimensionality Reduction\n\n</td>\n</tr>\n<tr>\n<td width=\"50%\">\n\n**Time Series**\n- Forecasting\n- Seasonal Analysis\n- Trend Detection\n- Multivariate Models\n- Causal Analysis\n\n</td>\n<td width=\"50%\">\n\n**Business Analytics**\n- Customer Analytics\n- Market Analysis\n- Pricing Models\n- Predictive Analytics\n- Experimental Design\n\n</td>\n</tr>\n</table>\n\n### Seamless Workflow\n\n```mermaid\ngraph LR\n    A[Ask in Claude/Cursor] --> B[MCP Analytics]\n    B --> C[Secure Processing]\n    C --> D[Interactive Report]\n    D --> E[Share Results]\n```\n\n\n## Example Usage\n\n### Basic Regression\n```\nUser: \"I have a CSV with house prices. Can you predict price based on size and location?\"\nClaude: [Runs linear regression, provides R¬≤, coefficients, and diagnostic plots]\n```\n\n### Customer Segmentation\n```\nUser: \"Segment my customers in sales_data.csv into meaningful groups\"\nClaude: [Performs k-means clustering, creates segment profiles with visualizations]\n```\n\n### Time Series Forecasting\n```\nUser: \"Forecast next quarter's revenue using our historical data\"\nClaude: [Applies ARIMA, generates predictions with confidence intervals]\n```\n\n## Security & Compliance\n\n### Enterprise Security Features\n\n- **Authentication**: OAuth2 via Auth0 with PKCE\n- **Encryption**: TLS 1.3 for all data transfers\n- **Processing**: Isolated Docker containers per analysis\n- **Data Handling**: Ephemeral processing, no persistence\n- **Access Control**: API key management with usage limits\n- **Audit Trail**: Complete logging for compliance\n\n### Privacy & Data Handling\n\n- **Data Privacy**: Ephemeral processing, no data retention\n- **User Rights**: Data deletion upon request\n- **Secure Processing**: Isolated containers per analysis\n- **Enterprise Options**: Contact us for compliance requirements\n\n[**Read full security documentation ‚Üí**](https://mcpanalytics.ai/security)\n\n## Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"Client Integration\"\n        CLI[CLI/SDK]\n        Claude[Claude Desktop]\n        Cursor[Cursor IDE]\n        MCP[MCP Protocol]\n    end\n\n    subgraph \"API Gateway\"\n        LB[Load Balancer]\n        Auth[OAuth 2.0/Auth0]\n        Rate[Rate Limiting]\n    end\n\n    subgraph \"Processing Layer\"\n        Router[Request Router]\n        Queue[Job Queue]\n        Workers[Processing Workers]\n        Docker[Docker Containers]\n    end\n\n    subgraph \"Analytics Engine\"\n        Stats[Statistical Methods]\n        ML[Machine Learning]\n        TS[Time Series]\n        Report[Report Generation]\n    end\n\n    subgraph \"Data Layer\"\n        Cache[Results Cache]\n        Storage[Secure Storage]\n        Encrypt[Encryption Layer]\n    end\n\n    CLI --> LB\n    Claude --> LB\n    Cursor --> LB\n    MCP --> LB\n\n    LB --> Auth\n    Auth --> Rate\n    Rate --> Router\n\n    Router --> Queue\n    Queue --> Workers\n    Workers --> Docker\n\n    Docker --> Stats\n    Docker --> ML\n    Docker --> TS\n\n    Stats --> Report\n    ML --> Report\n    TS --> Report\n\n    Report --> Cache\n    Cache --> Storage\n    Storage --> Encrypt\n\n    style Auth fill:#e8f5e9\n    style Docker fill:#fff3e0\n    style Report fill:#e3f2fd\n```\n\n## Performance\n\n- **Dataset Size**: Handles large datasets\n- **Processing Time**: Fast cloud-based processing\n- **Secure Infrastructure**: Isolated Docker containers\n- **API Access**: RESTful API with authentication\n\n## Getting Started\n\n[**Visit our website for pricing and signup ‚Üí**](https://mcpanalytics.ai)\n\n## Documentation\n\n- [**Quick Start Guide**](https://mcpanalytics.ai/docs/quickstart) - Get running in 30 seconds\n- [**API Reference**](https://api.mcpanalytics.ai/docs) - Complete API documentation\n- [**Platform Overview**](https://mcpanalytics.ai/tools) - How the platform works\n- [**Tutorials**](https://mcpanalytics.ai/tutorials) - Step-by-step guides\n- [**Examples**](https://mcpanalytics.ai/examples) - Real-world use cases\n- [**Security**](https://mcpanalytics.ai/security) - Security & compliance details\n\n## Support\n\n- **Issues**: [GitHub Issues](https://github.com/embeddedlayers/mcp-analytics/issues)\n- **Discord**: [Join our community](https://discord.gg/mcpanalytics)\n- **Email**: support@mcpanalytics.ai\n- **Docs**: [mcpanalytics.ai/docs](https://mcpanalytics.ai/docs)\n- **Enterprise**: sales@mcpanalytics.ai\n\n## Comparison with Other MCP Servers\n\n| Feature | MCP Analytics | Google Analytics MCP | PostgreSQL MCP | Filesystem MCP |\n|---------|--------------|---------------------|----------------|----------------|\n| **Use Case** | Statistical Analysis | Web Metrics | Database Queries | File Access |\n| **Setup Time** | 30 seconds | OAuth + Config | Connection string | Path config |\n| **Data Sources** | Any CSV/JSON/URL | GA4 Only | PostgreSQL Only | Local files |\n| **Analysis Tools** | Full Suite | GA4 Metrics | SQL Only | Read/Write |\n| **Machine Learning** | ‚úÖ Full Suite | ‚ùå | ‚ùå | ‚ùå |\n| **Visualizations** | ‚úÖ Interactive | ‚úÖ Dashboards | ‚ùå | ‚ùå |\n| **Shareable Reports** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |\n\n[**Detailed comparison ‚Üí**](https://mcpanalytics.ai/comparisons)\n\n## About MCP Analytics\n\nMCP Analytics is built by a team of data scientists and engineers passionate about making advanced analytics accessible through AI. We're backed by enterprise customers across finance, healthcare, and e-commerce.\n\n### Coming Soon\n\n- **NPM Package**: Direct installation via `npm install @mcpanalytics/server`\n- **Smithery Integration**: One-click install via Smithery CLI\n- **MCP Registry**: Official listing in the MCP servers directory\n- **More Tools**: Continuously expanding our analytics capabilities\n\n## Testing & Support\n\n### Testing Your Connection\n\nAfter installation, restart your IDE and look for \"MCP Analytics\" in the available tools. On first use, you'll be prompted to authenticate via OAuth 2.0.\n\n```bash\n# To test the connection directly:\nnpx -y mcp-remote@latest https://api.mcpanalytics.ai/auth0\n```\n\n### Troubleshooting\n\nIf MCP Analytics doesn't appear after installation:\n1. Ensure your config file is valid JSON\n2. Restart your IDE completely\n3. Check the IDE's developer console for errors\n4. Verify you have internet connectivity\n\nFor support: support@mcpanalytics.ai\n\n## Contributing\n\nWhile the core server is proprietary, we welcome contributions to:\n\n- Documentation improvements\n- Example notebooks and use cases\n- Bug reports and feature requests\n- Community tools and integrations\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## License\n\nCopyright ¬© 2025 PeopleDrivenAI LLC. All Rights Reserved.\n\nMCP Analytics is a product of PeopleDrivenAI LLC.\n\nThis is commercial software. Use of the MCP Analytics service is subject to our:\n- [Terms of Service](https://mcpanalytics.ai/terms)\n- [Privacy Policy](https://mcpanalytics.ai/privacy)\n- [Acceptable Use Policy](https://mcpanalytics.ai/aup)\n\n---\n\n<div align=\"center\">\n\n**Ready to transform your data analysis workflow?**\n\n[**Get Started Free**](https://mcpanalytics.ai/signup) | [**Read Docs**](https://mcpanalytics.ai/docs) | [**View Demo**](https://mcpanalytics.ai/demo)\n\nBuilt by [MCP Analytics](https://mcpanalytics.ai) | Powered by R & Python\n\n</div>"
    },
    "llm_extracted": {
      "capabilities": [
        "Transform business questions into actionable insights using AI-powered discovery",
        "Automatically discover and select appropriate analytical methods based on natural language queries",
        "Upload and securely process datasets with encryption",
        "Execute a wide range of statistical, machine learning, time series, and business analytics methods",
        "Generate interactive, shareable visualization reports with AI insights",
        "Manage datasets including upload, listing, and previewing",
        "Perform semantic search across previous analyses and reports",
        "Provide usage and subscription management via billing tools",
        "Integrate seamlessly with multiple IDEs and platforms using MCP protocol and OAuth2 authentication"
      ],
      "limitations": [
        "No data persistence beyond ephemeral processing; data is not stored permanently",
        "Requires OAuth2 authentication and API key management with usage limits",
        "Proprietary core server with commercial licensing restricting open-source usage",
        "Dependent on cloud-based infrastructure; no offline or local processing mentioned",
        "No direct support for non-tabular data sources beyond CSV/JSON/URL formats"
      ],
      "requirements": [
        "OAuth2 authentication via Auth0 with PKCE",
        "Valid API key with usage limits",
        "Compatible client environment such as Claude Desktop, Cursor IDE, VS Code with Continue extension, or Claude Code",
        "Internet connectivity for cloud-based processing and API access",
        "Configuration of MCP server in client config files with specified command and arguments"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear descriptions of tools and capabilities, explicit security and compliance information, and outlines limitations and requirements, making it an excellent resource.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# MCP Analytics Suite\n\n<div align=\"center\">\n\n[![Version](https://img.shields.io/badge/version-1.0.3-blue)](https://mcpanalytics.ai)\n[![Platform](https://img.shields.io/badge/Platform-MCP_Compatible-green)](https://mcpanalytics.ai/tools)\n[![License](https://img.shields.io/badge/License-Commercial-orange)](https://mcpanalytics.ai/terms)\n[![Docs](https://img.shields.io/badge/Docs-mcpanalytics.ai-brightgreen)](https://mcpanalytics.ai/docs)\n[![Auth](https://img.shields.io/badge/Auth-OAuth_2.0-purple)](https://mcpanalytics.ai/oauth-setup)\n\n**Every analysis starts with a question. We handle the rest.**\n\n[üöÄ Quick Start](#quick-start) ‚Ä¢ [üîÑ How It Works](#how-it-works) ‚Ä¢ [üõ†Ô∏è MCP Tools](#mcp-tools) ‚Ä¢ [üõ°Ô∏è Security](#security--compliance) ‚Ä¢ [üìñ Documentation](#documentation)\n\n</div>\n\n---\n\n## The Formula\n\n<div align=\"center\">\n  <h3>Question + Dataset = Analytics</h3>\n  <p>Transform business questions into actionable insights through intelligent discovery</p>\n</div>\n\n## Overview\n\nMCP Analytics Suite is an intelligent analytics platform that understands what you want to analyze and automatically selects the right approach. No statistics degree required‚Äîjust describe your business question and let our AI-powered discovery handle the complexity.\n\n### Why MCP Analytics?",
        "start_pos": 0,
        "end_pos": 1270,
        "token_count_estimate": 317,
        "source_type": "readme",
        "agent_id": "c12237a27b9ddfde"
      },
      {
        "chunk_id": 1,
        "text": "**Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-analytics\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote@latest\", \"https://api.mcpanalytics.ai/auth0\"]\n    }\n  }\n}\n```\n\n##### For Cursor\n\nAdd to `.cursor/config.json` in your project root:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-analytics\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote@latest\", \"https://api.mcpanalytics.ai/auth0\"]\n    }\n  }\n}\n```\n\n##### For VS Code (Continue Extension)\n\nAdd to your Continue config at `~/.continue/config.json`:\n\n```json\n{\n  \"models\": [{\n    \"provider\": \"anthropic\",\n    \"model\": \"claude-3-5-sonnet\",\n    \"mcpServers\": {\n      \"mcp-analytics\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"mcp-remote@latest\", \"https://api.mcpanalytics.ai/auth0\"]\n      }\n    }\n  }]\n}\n```\n\n##### For Claude Code\n\nAdd to `claude_code_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-analytics\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-remote@latest\", \"https://api.mcpanalytics.ai/auth0\"]\n    }\n  }\n}\n```\n\n## How It Works\n\n### The MCP Analytics Workflow\n\n1. **Ask Your Question** - Describe what you want to analyze in natural language\n2. **Intelligent Discovery** - `tools.discover` finds the right analytical approach\n3. **Data Upload** - `datasets.upload` securely processes your data\n4. **Automated Analysis** - `tools.run` executes with optimal configuration\n5.",
        "start_pos": 1848,
        "end_pos": 3274,
        "token_count_estimate": 356,
        "source_type": "readme",
        "agent_id": "c12237a27b9ddfde"
      },
      {
        "chunk_id": 2,
        "text": "lytics Tools\n- **`tools.discover`** - Natural language tool discovery\n- **`tools.run`** - Automated analysis execution\n- **`tools.info`** - Get tool documentation\n\n### Data Management\n- **`datasets.upload`** - Secure data upload with encryption\n- **`datasets.list`** - Manage your datasets\n- **`datasets.read`** - Access and preview data\n\n### Reporting & Insights\n- **`reports.view`** - Interactive visualization dashboard\n- **`reports.search`** - Semantic search across analyses\n\n### Platform Tools\n- **`billing()`** - Usage and subscription management\n- **`about()`** - Platform information and status\n- **`manual()`** - Documentation access\n\n## Features\n\n### Natural Language Interface\n\nJust describe what you need:\n\n```\n\"What drives our revenue growth?\"\n\"Find customer segments in our data\"\n\"Forecast next quarter's sales\"\n\"Did our marketing campaign work?\"\n```\n\n### Comprehensive Analysis Suite\n\n<table>\n<tr>\n<td width=\"50%\">\n\n**Statistical Methods**\n- Regression Analysis\n- Advanced Modeling\n- Hypothesis Testing\n- Survival Analysis\n- Bayesian Methods\n\n</td>\n<td width=\"50%\">\n\n**Machine Learning**\n- Ensemble Methods\n- Boosting Algorithms\n- Neural Networks\n- Clustering\n- Dimensionality Reduction\n\n</td>\n</tr>\n<tr>\n<td width=\"50%\">\n\n**Time Series**\n- Forecasting\n- Seasonal Analysis\n- Trend Detection\n- Multivariate Models\n- Causal Analysis\n\n</td>\n<td width=\"50%\">\n\n**Business Analytics**\n- Customer Analytics\n- Market Analysis\n- Pricing Models\n- Predictive Analytics\n- Experimental Design\n\n</td>\n</tr>\n</table>\n\n### Seamless Workflow\n\n```mermaid\ngraph LR\n    A[Ask in Claude/Cursor] --> B[MCP Analytics]\n    B --> C[Secure Processing]\n    C --> D[Interactive Report]\n    D --> E[Share Results]\n```\n\n\n## Example Usage\n\n### Basic Regression\n```\nUser: \"I have a CSV with house prices.",
        "start_pos": 3696,
        "end_pos": 5484,
        "token_count_estimate": 447,
        "source_type": "readme",
        "agent_id": "c12237a27b9ddfde"
      },
      {
        "chunk_id": 3,
        "text": "[Runs linear regression, provides R¬≤, coefficients, and diagnostic plots]\n```\n\n### Customer Segmentation\n```\nUser: \"Segment my customers in sales_data.csv into meaningful groups\"\nClaude: [Performs k-means clustering, creates segment profiles with visualizations]\n```\n\n### Time Series Forecasting\n```\nUser: \"Forecast next quarter's revenue using our historical data\"\nClaude: [Applies ARIMA, generates predictions with confidence intervals]\n```\n\n## Security & Compliance\n\n### Enterprise Security Features\n\n- **Authentication**: OAuth2 via Auth0 with PKCE\n- **Encryption**: TLS 1.3 for all data transfers\n- **Processing**: Isolated Docker containers per analysis\n- **Data Handling**: Ephemeral processing, no persistence\n- **Access Control**: API key management with usage limits\n- **Audit Trail**: Complete logging for compliance\n\n### Privacy & Data Handling\n\n- **Data Privacy**: Ephemeral processing, no data retention\n- **User Rights**: Data deletion upon request\n- **Secure Processing**: Isolated containers per analysis\n- **Enterprise Options**: Contact us for compliance requirements\n\n[**Read full security documentation ‚Üí**](https://mcpanalytics.ai/security)\n\n## Architecture\n\n```mermaid\nflowchart TB\n    subgraph \"Client Integration\"\n        CLI[CLI/SDK]\n        Claude[Claude Desktop]\n        Cursor[Cursor IDE]\n        MCP[MCP Protocol]\n    end\n\n    subgraph \"API Gateway\"\n        LB[Load Balancer]\n        Auth[OAuth 2.0/Auth0]\n        Rate[Rate Limiting]\n    end\n\n    subgraph \"Processing Layer\"\n        Router[Request Router]\n        Queue[Job Queue]\n        Workers[Processing Workers]\n        Docker[Docker Containers]\n    end\n\n    subgraph \"Analytics Engine\"\n        Stats[Statistical Methods]\n        ML[Machine Learning]\n        TS[Time Series]\n        Report[Report Generation]\n    end\n\n    subgraph \"Data Layer\"\n        Cache[Results Cache]\n        Storage[Secure Storage]\n        Encrypt[Encryption Layer]\n    end\n\n    CLI --> LB\n    Claude --> LB\n    Cursor --> LB\n    MCP --> LB\n\n    LB --> Auth\n    Auth --> Rate\n    Rate --> R",
        "start_pos": 5544,
        "end_pos": 7592,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "c12237a27b9ddfde"
      },
      {
        "chunk_id": 4,
        "text": "ts Cache]\n        Storage[Secure Storage]\n        Encrypt[Encryption Layer]\n    end\n\n    CLI --> LB\n    Claude --> LB\n    Cursor --> LB\n    MCP --> LB\n\n    LB --> Auth\n    Auth --> Rate\n    Rate --> Router\n\n    Router --> Queue\n    Queue --> Workers\n    Workers --> Docker\n\n    Docker --> Stats\n    Docker --> ML\n    Docker --> TS\n\n    Stats --> Report\n    ML --> Report\n    TS --> Report\n\n    Report --> Cache\n    Cache --> Storage\n    Storage --> Encrypt\n\n    style Auth fill:#e8f5e9\n    style Docker fill:#fff3e0\n    style Report fill:#e3f2fd\n```\n\n## Performance\n\n- **Dataset Size**: Handles large datasets\n- **Processing Time**: Fast cloud-based processing\n- **Secure Infrastructure**: Isolated Docker containers\n- **API Access**: RESTful API with authentication\n\n## Getting Started\n\n[**Visit our website for pricing and signup ‚Üí**](https://mcpanalytics.ai)\n\n## Documentation\n\n- [**Quick Start Guide**](https://mcpanalytics.ai/docs/quickstart) - Get running in 30 seconds\n- [**API Reference**](https://api.mcpanalytics.ai/docs) - Complete API documentation\n- [**Platform Overview**](https://mcpanalytics.ai/tools) - How the platform works\n- [**Tutorials**](https://mcpanalytics.ai/tutorials) - Step-by-step guides\n- [**Examples**](https://mcpanalytics.ai/examples) - Real-world use cases\n- [**Security**](https://mcpanalytics.ai/security) - Security & compliance details\n\n## Support\n\n- **Issues**: [GitHub Issues](https://github.com/embeddedlayers/mcp-analytics/issues)\n- **Discord**: [Join our community](https://discord.gg/mcpanalytics)\n- **Email**: support@mcpanalytics.ai\n- **Docs**: [mcpanalytics.ai/docs](https://mcpanalytics.ai/docs)\n- **Enterprise**: sales@mcpanalytics.ai\n\n## Comparison with Other MCP Servers\n\n| Feature | MCP Analytics | Google Analytics MCP | PostgreSQL MCP | Filesystem MCP |\n|---------|--------------|---------------------|----------------|----------------|\n| **Use Case** | Statistical Analysis | Web Metrics | Database Queries | File Access |\n| **Setup Time** | 30 seconds | OAuth + Config | Connection string |",
        "start_pos": 7392,
        "end_pos": 9440,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "c12237a27b9ddfde"
      },
      {
        "chunk_id": 5,
        "text": "--------|----------------|----------------|\n| **Use Case** | Statistical Analysis | Web Metrics | Database Queries | File Access |\n| **Setup Time** | 30 seconds | OAuth + Config | Connection string | Path config |\n| **Data Sources** | Any CSV/JSON/URL | GA4 Only | PostgreSQL Only | Local files |\n| **Analysis Tools** | Full Suite | GA4 Metrics | SQL Only | Read/Write |\n| **Machine Learning** | ‚úÖ Full Suite | ‚ùå | ‚ùå | ‚ùå |\n| **Visualizations** | ‚úÖ Interactive | ‚úÖ Dashboards | ‚ùå | ‚ùå |\n| **Shareable Reports** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |\n\n[**Detailed comparison ‚Üí**](https://mcpanalytics.ai/comparisons)\n\n## About MCP Analytics\n\nMCP Analytics is built by a team of data scientists and engineers passionate about making advanced analytics accessible through AI. We're backed by enterprise customers across finance, healthcare, and e-commerce.\n\n### Coming Soon\n\n- **NPM Package**: Direct installation via `npm install @mcpanalytics/server`\n- **Smithery Integration**: One-click install via Smithery CLI\n- **MCP Registry**: Official listing in the MCP servers directory\n- **More Tools**: Continuously expanding our analytics capabilities\n\n## Testing & Support\n\n### Testing Your Connection\n\nAfter installation, restart your IDE and look for \"MCP Analytics\" in the available tools. On first use, you'll be prompted to authenticate via OAuth 2.0.\n\n```bash\n# To test the connection directly:\nnpx -y mcp-remote@latest https://api.mcpanalytics.ai/auth0\n```\n\n### Troubleshooting\n\nIf MCP Analytics doesn't appear after installation:\n1. Ensure your config file is valid JSON\n2. Restart your IDE completely\n3. Check the IDE's developer console for errors\n4. Verify you have internet connectivity\n\nFor support: support@mcpanalytics.ai\n\n## Contributing\n\nWhile the core server is proprietary, we welcome contributions to:\n\n- Documentation improvements\n- Example notebooks and use cases\n- Bug reports and feature requests\n- Community tools and integrations\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## License\n\nCopyright ¬© 2025 PeopleDrivenAI LLC.",
        "start_pos": 9240,
        "end_pos": 11274,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "c12237a27b9ddfde"
      },
      {
        "chunk_id": 6,
        "text": "notebooks and use cases\n- Bug reports and feature requests\n- Community tools and integrations\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n## License\n\nCopyright ¬© 2025 PeopleDrivenAI LLC. All Rights Reserved.\n\nMCP Analytics is a product of PeopleDrivenAI LLC.\n\nThis is commercial software. Use of the MCP Analytics service is subject to our:\n- [Terms of Service](https://mcpanalytics.ai/terms)\n- [Privacy Policy](https://mcpanalytics.ai/privacy)\n- [Acceptable Use Policy](https://mcpanalytics.ai/aup)\n\n---\n\n<div align=\"center\">\n\n**Ready to transform your data analysis workflow?**\n\n[**Get Started Free**](https://mcpanalytics.ai/signup) | [**Read Docs**](https://mcpanalytics.ai/docs) | [**View Demo**](https://mcpanalytics.ai/demo)\n\nBuilt by [MCP Analytics](https://mcpanalytics.ai) | Powered by R & Python\n\n</div>",
        "start_pos": 11074,
        "end_pos": 11902,
        "token_count_estimate": 206,
        "source_type": "readme",
        "agent_id": "c12237a27b9ddfde"
      }
    ]
  },
  {
    "agent_id": "b78965c3d99cb239",
    "name": "ai.mcpcap/mcpcap",
    "source": "mcp",
    "source_url": "https://github.com/mcpcap/mcpcap",
    "description": "An MCP server for analyzing PCAP files.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T01:06:15.729223Z",
    "indexed_at": "2026-02-18T04:02:12.036728",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# mcpcap\n\n<!-- mcp-name: ai.mcpcap/mcpcap -->\n\n![mcpcap logo](https://raw.githubusercontent.com/mcpcap/mcpcap/main/readme-assets/mcpcap-logo.png)\n\nA modular Python MCP (Model Context Protocol) Server for analyzing PCAP files. mcpcap enables LLMs to read and analyze network packet captures with protocol-specific analysis tools that accept local file paths or remote URLs as parameters (no file uploads - provide the path or URL to your PCAP file).\n\n## Overview\n\nmcpcap uses a modular architecture to analyze different network protocols found in PCAP files. Each module provides specialized analysis tools that can be called independently with any PCAP file, making it perfect for integration with Claude Desktop and other MCP clients.\n\n### Key Features\n\n- **Stateless MCP Tools**: Each analysis accepts PCAP file paths or URLs as parameters (no file uploads)\n- **Modular Architecture**: DNS, DHCP, ICMP, TCP, and CapInfos modules with easy extensibility for new protocols  \n- **Advanced TCP Analysis**: Connection tracking, anomaly detection, retransmission analysis, and traffic flow inspection\n- **Local & Remote PCAP Support**: Analyze files from local storage or HTTP URLs\n- **Scapy Integration**: Leverages scapy's comprehensive packet parsing capabilities\n- **Specialized Analysis Prompts**: Security, networking, and forensic analysis guidance\n- **JSON Responses**: Structured data format optimized for LLM consumption\n\n## Installation\n\nmcpcap requires Python 3.10 or greater.\n\n### Using pip\n\n```bash\npip install mcpcap\n```\n\n### Using uv\n\n```bash\nuv add mcpcap\n```\n\n### Using uvx (for one-time usage)\n\n```bash\nuvx mcpcap\n```\n\n## Quick Start\n\n### 1. Start the MCP Server\n\nStart mcpcap as a stateless MCP server:\n\n```bash\n# Default: Start with DNS, DHCP, ICMP, TCP, and CapInfos modules\nmcpcap\n\n# Start with specific modules only\nmcpcap --modules dns,tcp\n\n# With packet analysis limits\nmcpcap --max-packets 1000\n```\n\n### 2. Connect Your MCP Client\n\nConfigure your MCP client (like Claude Desktop) to connect to the mcpcap server:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcpcap\": {\n      \"command\": \"mcpcap\",\n      \"args\": []\n    }\n  }\n}\n```\n\n### 3. Analyze PCAP Files\n\nUse the analysis tools with any PCAP file by providing the file path or URL (not file uploads):\n\n**DNS Analysis:**\n```\nanalyze_dns_packets(\"/path/to/dns.pcap\")\nanalyze_dns_packets(\"https://example.com/remote.pcap\")\n```\n\n**DHCP Analysis:**\n```\nanalyze_dhcp_packets(\"/path/to/dhcp.pcap\")\nanalyze_dhcp_packets(\"https://example.com/dhcp-capture.pcap\")\n```\n\n**ICMP Analysis:**\n```\nanalyze_icmp_packets(\"/path/to/icmp.pcap\")\nanalyze_icmp_packets(\"https://example.com/ping-capture.pcap\")\n```\n\n**TCP Connection Analysis:**\n```\nanalyze_tcp_connections(\"/path/to/capture.pcap\")\nanalyze_tcp_connections(\"/path/to/capture.pcap\", server_ip=\"192.168.1.1\", server_port=80)\n```\n\n**TCP Anomaly Detection:**\n```\nanalyze_tcp_anomalies(\"/path/to/capture.pcap\", server_ip=\"10.0.0.1\")\n```\n\n**TCP Retransmission Analysis:**\n```\nanalyze_tcp_retransmissions(\"/path/to/capture.pcap\")\n```\n\n**Traffic Flow Analysis:**\n```\nanalyze_traffic_flow(\"/path/to/capture.pcap\", server_ip=\"192.168.1.100\")\n```\n\n**CapInfos Analysis:**\n```\nanalyze_capinfos(\"/path/to/any.pcap\")\nanalyze_capinfos(\"https://example.com/capture.pcap\")\n```\n\n## Available Tools\n\n### DNS Analysis Tools\n\n- **`analyze_dns_packets(pcap_file)`**: Complete DNS traffic analysis\n  - Extract DNS queries and responses\n  - Identify queried domains and subdomains\n  - Analyze query types (A, AAAA, MX, CNAME, etc.)\n  - Track query frequency and patterns\n  - Detect potential security issues\n\n### DHCP Analysis Tools\n\n- **`analyze_dhcp_packets(pcap_file)`**: Complete DHCP traffic analysis\n  - Track DHCP transactions (DISCOVER, OFFER, REQUEST, ACK)\n  - Identify DHCP clients and servers\n  - Monitor IP address assignments and lease information\n  - Analyze DHCP options and configurations\n  - Detect DHCP anomalies and security issues\n\n### ICMP Analysis Tools\n\n- **`analyze_icmp_packets(pcap_file)`**: Complete ICMP traffic analysis\n  - Analyze ping requests and replies with response times\n  - Identify network connectivity and reachability issues\n  - Track TTL values and routing paths (traceroute data)\n  - Detect ICMP error messages (unreachable, time exceeded)\n  - Monitor for potential ICMP-based attacks or reconnaissance\n\n### TCP Analysis Tools\n\n- **`analyze_tcp_connections(pcap_file, server_ip=None, server_port=None, detailed=False)`**: TCP connection state analysis\n  - Track TCP three-way handshake (SYN, SYN-ACK, ACK)\n  - Analyze connection lifecycle and termination (FIN, RST)\n  - Identify successful vs failed connections\n  - Filter by server IP and/or port\n  - Detect connection issues and abnormal closures\n\n- **`analyze_tcp_anomalies(pcap_file, server_ip=None, server_port=None)`**: Intelligent TCP anomaly detection\n  - Automatically detect common network problems\n  - Identify client vs server-initiated RST patterns (firewall blocks)\n  - Detect high retransmission rates (network quality issues)\n  - Diagnose handshake failures\n  - Root cause analysis with confidence scoring\n  - Actionable recommendations\n\n- **`analyze_tcp_retransmissions(pcap_file, server_ip=None, threshold=0.02)`**: TCP retransmission analysis\n  - Measure overall and per-connection retransmission rates\n  - Identify connections with quality issues\n  - Compare against configurable thresholds\n  - Detect network congestion and packet loss\n\n- **`analyze_traffic_flow(pcap_file, server_ip, server_port=None)`**: Bidirectional traffic flow analysis\n  - Analyze client-to-server vs server-to-client traffic\n  - Identify traffic asymmetry\n  - Determine RST packet sources\n  - Interpret connection patterns and behaviors\n\n### CapInfos Analysis Tools\n\n- **`analyze_capinfos(pcap_file)`**: PCAP file metadata and statistics\n  - File information (size, name, link layer encapsulation)\n  - Packet statistics (count, data size, average packet size)\n  - Temporal analysis (duration, timestamps, packet rates)\n  - Data throughput metrics (bytes/second, bits/second)\n  - Similar to Wireshark's capinfos(1) utility\n\n## Analysis Prompts\n\nmcpcap provides specialized analysis prompts to guide LLM analysis:\n\n### DNS Prompts\n- **`security_analysis`** - Focus on threat detection, DGA domains, DNS tunneling\n- **`network_troubleshooting`** - Identify DNS performance and configuration issues\n- **`forensic_investigation`** - Timeline reconstruction and evidence collection\n\n### DHCP Prompts  \n- **`dhcp_network_analysis`** - Network administration and IP management\n- **`dhcp_security_analysis`** - Security threats and rogue DHCP detection\n- **`dhcp_forensic_investigation`** - Forensic analysis of DHCP transactions\n\n### ICMP Prompts\n- **`icmp_network_diagnostics`** - Network connectivity and path analysis\n- **`icmp_security_analysis`** - ICMP-based attacks and reconnaissance detection\n- **`icmp_forensic_investigation`** - Timeline reconstruction and network mapping\n\n### TCP Prompts\n- **`tcp_connection_troubleshooting`** - Connection issues, handshake analysis, termination patterns\n- **`tcp_security_analysis`** - Attack detection, firewall analysis, anomaly identification\n\n## Configuration Options\n\n### Module Selection\n\n```bash\n# Load specific modules\nmcpcap --modules dns              # DNS analysis only\nmcpcap --modules tcp              # TCP analysis only\nmcpcap --modules dhcp             # DHCP analysis only\nmcpcap --modules icmp             # ICMP analysis only  \nmcpcap --modules dns,tcp          # DNS and TCP analysis\nmcpcap --modules dns,dhcp,icmp,tcp,capinfos    # All modules (default)\n```\n\n### Analysis Limits\n\n```bash\n# Limit packet analysis for large files\nmcpcap --max-packets 1000\n```\n\n### Complete Configuration Example\n\n```bash\nmcpcap --modules dns,dhcp,icmp,tcp,capinfos --max-packets 500\n```\n\n## CLI Reference\n\n```bash\nmcpcap [--modules MODULES] [--max-packets N]\n```\n\n**Options:**\n- `--modules MODULES`: Comma-separated modules to load (default: `dns,dhcp,icmp,tcp,capinfos`)\n  - Available modules: `dns`, `dhcp`, `icmp`, `tcp`, `capinfos`\n- `--max-packets N`: Maximum packets to analyze per file (default: unlimited)\n\n**Examples:**\n```bash\n# Start with all modules\nmcpcap\n\n# DNS and TCP analysis only\nmcpcap --modules dns,tcp\n\n# TCP analysis for troubleshooting connections\nmcpcap --modules tcp\n\n# With packet limits for large files\nmcpcap --max-packets 1000\n```\n\n## Examples\n\nExample PCAP files are included in the `examples/` directory:\n\n- `dns.pcap` - DNS traffic for testing DNS analysis\n- `dhcp.pcap` - DHCP 4-way handshake capture\n- `icmp.pcap` - ICMP ping and traceroute traffic\n\n### Using with MCP Inspector\n\n```bash\nnpm install -g @modelcontextprotocol/inspector\nnpx @modelcontextprotocol/inspector mcpcap\n```\n\nThen test the tools:\n```javascript\n// In the MCP Inspector web interface\nanalyze_dns_packets(\"./examples/dns.pcap\")\nanalyze_dhcp_packets(\"./examples/dhcp.pcap\")\nanalyze_icmp_packets(\"./examples/icmp.pcap\")\nanalyze_capinfos(\"./examples/dns.pcap\")\n```\n\n## Architecture\n\nmcpcap's modular design supports easy extension:\n\n### Core Components\n1. **BaseModule**: Shared file handling, validation, and remote download\n2. **Protocol Modules**: DNS, DHCP, ICMP, and TCP analysis implementations  \n3. **MCP Interface**: Tool registration and prompt management\n4. **FastMCP Framework**: MCP server implementation\n\n### Tool Flow\n```\nMCP Client Request ‚Üí analyze_*_packets(pcap_file)\n                  ‚Üí BaseModule.analyze_packets()\n                  ‚Üí Module._analyze_protocol_file()\n                  ‚Üí Structured JSON Response\n```\n\n### Adding New Modules\n\nCreate new protocol modules by:\n\n1. Inheriting from `BaseModule`\n2. Implementing `_analyze_protocol_file(pcap_file)`\n3. Registering analysis tools with the MCP server\n4. Adding specialized analysis prompts\n\nFuture modules might include:\n- HTTP/HTTPS traffic analysis\n- UDP connection analysis\n- BGP routing analysis\n- SSL/TLS certificate analysis\n- Network forensics tools\n- Port scan detection\n\n## Remote File Support\n\nBoth analysis tools accept remote PCAP files via HTTP/HTTPS URLs:\n\n```bash\n# Examples of remote analysis\nanalyze_dns_packets(\"https://wiki.wireshark.org/uploads/dns.cap\")\nanalyze_dhcp_packets(\"https://example.com/network-capture.pcap\")\nanalyze_icmp_packets(\"https://example.com/ping-test.pcap\")\nanalyze_capinfos(\"https://example.com/network-metadata.pcap\")\n```\n\n**Features:**\n- Automatic temporary download and cleanup\n- Support for `.pcap`, `.pcapng`, and `.cap` files\n- HTTP/HTTPS protocols supported\n\n## Security Considerations\n\nWhen analyzing PCAP files:\n- Files may contain sensitive network information\n- Remote downloads are performed over HTTPS when possible\n- Temporary files are cleaned up automatically\n- Consider the source and trustworthiness of remote files\n\n## Contributing\n\nContributions welcome! Areas for contribution:\n\n- **New Protocol Modules**: Add support for HTTP, BGP, TCP, etc.\n- **Enhanced Analysis**: Improve existing DNS/DHCP analysis\n- **Security Features**: Add more threat detection capabilities\n- **Performance**: Optimize analysis for large PCAP files\n\n## License\n\nMIT\n\n## Requirements\n\n- Python 3.10+\n- scapy (packet parsing and analysis)\n- requests (remote file access)\n- fastmcp (MCP server framework)\n\n## Documentation\n\n- **GitHub**: [github.com/mcpcap/mcpcap](https://github.com/mcpcap/mcpcap)\n- **Documentation**: [docs.mcpcap.ai](https://docs.mcpcap.ai) \n- **Website**: [mcpcap.ai](https://mcpcap.ai)\n\n## Support\n\nFor questions, issues, or feature requests, please open an issue on GitHub."
    },
    "llm_extracted": {
      "capabilities": [
        "Analyze DNS traffic including queries, responses, and security issues",
        "Analyze DHCP traffic including transactions, IP assignments, and anomalies",
        "Analyze ICMP traffic including ping requests, traceroute data, and error messages",
        "Perform advanced TCP analysis including connection tracking, anomaly detection, retransmission analysis, and traffic flow inspection",
        "Provide PCAP file metadata and statistics similar to Wireshark's capinfos utility",
        "Accept local file paths or remote HTTP/HTTPS URLs for PCAP file analysis",
        "Generate structured JSON responses optimized for LLM consumption",
        "Support modular protocol analysis with selectable modules (DNS, DHCP, ICMP, TCP, CapInfos)",
        "Provide specialized analysis prompts for security, networking, and forensic investigations"
      ],
      "limitations": [
        "Does not support direct file uploads; requires file paths or URLs",
        "Remote file downloads limited to HTTP/HTTPS protocols",
        "Packet analysis can be limited by a maximum packet count parameter",
        "Currently supports only DNS, DHCP, ICMP, TCP, and CapInfos modules; other protocols require future extension",
        "Security depends on trustworthiness of remote PCAP file sources"
      ],
      "requirements": [
        "Python 3.10 or greater",
        "scapy library for packet parsing and analysis",
        "requests library for remote file access",
        "fastmcp framework for MCP server implementation"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for all tools, clear descriptions of capabilities and limitations, configuration options, and explicit requirements, making it excellent for users.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# mcpcap\n\n<!-- mcp-name: ai.mcpcap/mcpcap -->\n\n![mcpcap logo](https://raw.githubusercontent.com/mcpcap/mcpcap/main/readme-assets/mcpcap-logo.png)\n\nA modular Python MCP (Model Context Protocol) Server for analyzing PCAP files. mcpcap enables LLMs to read and analyze network packet captures with protocol-specific analysis tools that accept local file paths or remote URLs as parameters (no file uploads - provide the path or URL to your PCAP file).\n\n## Overview\n\nmcpcap uses a modular architecture to analyze different network protocols found in PCAP files. Each module provides specialized analysis tools that can be called independently with any PCAP file, making it perfect for integration with Claude Desktop and other MCP clients.\n\n### Key Features\n\n- **Stateless MCP Tools**: Each analysis accepts PCAP file paths or URLs as parameters (no file uploads)\n- **Modular Architecture**: DNS, DHCP, ICMP, TCP, and CapInfos modules with easy extensibility for new protocols  \n- **Advanced TCP Analysis**: Connection tracking, anomaly detection, retransmission analysis, and traffic flow inspection\n- **Local & Remote PCAP Support**: Analyze files from local storage or HTTP URLs\n- **Scapy Integration**: Leverages scapy's comprehensive packet parsing capabilities\n- **Specialized Analysis Prompts**: Security, networking, and forensic analysis guidance\n- **JSON Responses**: Structured data format optimized for LLM consumption\n\n## Installation\n\nmcpcap requires Python 3.10 or greater.\n\n### Using pip\n\n```bash\npip install mcpcap\n```\n\n### Using uv\n\n```bash\nuv add mcpcap\n```\n\n### Using uvx (for one-time usage)\n\n```bash\nuvx mcpcap\n```\n\n## Quick Start\n\n### 1. Start the MCP Server\n\nStart mcpcap as a stateless MCP server:\n\n```bash\n# Default: Start with DNS, DHCP, ICMP, TCP, and CapInfos modules\nmcpcap\n\n# Start with specific modules only\nmcpcap --modules dns,tcp\n\n# With packet analysis limits\nmcpcap --max-packets 1000\n```\n\n### 2.",
        "start_pos": 0,
        "end_pos": 1929,
        "token_count_estimate": 482,
        "source_type": "readme",
        "agent_id": "b78965c3d99cb239"
      },
      {
        "chunk_id": 1,
        "text": "Default: Start with DNS, DHCP, ICMP, TCP, and CapInfos modules\nmcpcap\n\n# Start with specific modules only\nmcpcap --modules dns,tcp\n\n# With packet analysis limits\nmcpcap --max-packets 1000\n```\n\n### 2. Connect Your MCP Client\n\nConfigure your MCP client (like Claude Desktop) to connect to the mcpcap server:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcpcap\": {\n      \"command\": \"mcpcap\",\n      \"args\": []\n    }\n  }\n}\n```\n\n### 3.",
        "start_pos": 1729,
        "end_pos": 2148,
        "token_count_estimate": 104,
        "source_type": "readme",
        "agent_id": "b78965c3d99cb239"
      },
      {
        "chunk_id": 2,
        "text": "ty issues\n\n### DHCP Analysis Tools\n\n- **`analyze_dhcp_packets(pcap_file)`**: Complete DHCP traffic analysis\n  - Track DHCP transactions (DISCOVER, OFFER, REQUEST, ACK)\n  - Identify DHCP clients and servers\n  - Monitor IP address assignments and lease information\n  - Analyze DHCP options and configurations\n  - Detect DHCP anomalies and security issues\n\n### ICMP Analysis Tools\n\n- **`analyze_icmp_packets(pcap_file)`**: Complete ICMP traffic analysis\n  - Analyze ping requests and replies with response times\n  - Identify network connectivity and reachability issues\n  - Track TTL values and routing paths (traceroute data)\n  - Detect ICMP error messages (unreachable, time exceeded)\n  - Monitor for potential ICMP-based attacks or reconnaissance\n\n### TCP Analysis Tools\n\n- **`analyze_tcp_connections(pcap_file, server_ip=None, server_port=None, detailed=False)`**: TCP connection state analysis\n  - Track TCP three-way handshake (SYN, SYN-ACK, ACK)\n  - Analyze connection lifecycle and termination (FIN, RST)\n  - Identify successful vs failed connections\n  - Filter by server IP and/or port\n  - Detect connection issues and abnormal closures\n\n- **`analyze_tcp_anomalies(pcap_file, server_ip=None, server_port=None)`**: Intelligent TCP anomaly detection\n  - Automatically detect common network problems\n  - Identify client vs server-initiated RST patterns (firewall blocks)\n  - Detect high retransmission rates (network quality issues)\n  - Diagnose handshake failures\n  - Root cause analysis with confidence scoring\n  - Actionable recommendations\n\n- **`analyze_tcp_retransmissions(pcap_file, server_ip=None, threshold=0.02)`**: TCP retransmission analysis\n  - Measure overall and per-connection retransmission rates\n  - Identify connections with quality issues\n  - Compare against configurable thresholds\n  - Detect network congestion and packet loss\n\n- **`analyze_traffic_flow(pcap_file, server_ip, server_port=None)`**: Bidirectional traffic flow analysis\n  - Analyze client-to-server vs server-to-client traffic\n  - Identify traffic asymmetry",
        "start_pos": 3577,
        "end_pos": 5625,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "b78965c3d99cb239"
      },
      {
        "chunk_id": 3,
        "text": "ss\n\n- **`analyze_traffic_flow(pcap_file, server_ip, server_port=None)`**: Bidirectional traffic flow analysis\n  - Analyze client-to-server vs server-to-client traffic\n  - Identify traffic asymmetry\n  - Determine RST packet sources\n  - Interpret connection patterns and behaviors\n\n### CapInfos Analysis Tools\n\n- **`analyze_capinfos(pcap_file)`**: PCAP file metadata and statistics\n  - File information (size, name, link layer encapsulation)\n  - Packet statistics (count, data size, average packet size)\n  - Temporal analysis (duration, timestamps, packet rates)\n  - Data throughput metrics (bytes/second, bits/second)\n  - Similar to Wireshark's capinfos(1) utility\n\n## Analysis Prompts\n\nmcpcap provides specialized analysis prompts to guide LLM analysis:\n\n### DNS Prompts\n- **`security_analysis`** - Focus on threat detection, DGA domains, DNS tunneling\n- **`network_troubleshooting`** - Identify DNS performance and configuration issues\n- **`forensic_investigation`** - Timeline reconstruction and evidence collection\n\n### DHCP Prompts  \n- **`dhcp_network_analysis`** - Network administration and IP management\n- **`dhcp_security_analysis`** - Security threats and rogue DHCP detection\n- **`dhcp_forensic_investigation`** - Forensic analysis of DHCP transactions\n\n### ICMP Prompts\n- **`icmp_network_diagnostics`** - Network connectivity and path analysis\n- **`icmp_security_analysis`** - ICMP-based attacks and reconnaissance detection\n- **`icmp_forensic_investigation`** - Timeline reconstruction and network mapping\n\n### TCP Prompts\n- **`tcp_connection_troubleshooting`** - Connection issues, handshake analysis, termination patterns\n- **`tcp_security_analysis`** - Attack detection, firewall analysis, anomaly identification\n\n## Configuration Options\n\n### Module Selection\n\n```bash\n# Load specific modules\nmcpcap --modules dns              # DNS analysis only\nmcpcap --modules tcp              # TCP analysis only\nmcpcap --modules dhcp             # DHCP analysis only\nmcpcap --modules icmp             # ICMP analysis only  \nmcpcap --modules dn",
        "start_pos": 5425,
        "end_pos": 7473,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "b78965c3d99cb239"
      },
      {
        "chunk_id": 4,
        "text": "analysis only\nmcpcap --modules tcp              # TCP analysis only\nmcpcap --modules dhcp             # DHCP analysis only\nmcpcap --modules icmp             # ICMP analysis only  \nmcpcap --modules dns,tcp          # DNS and TCP analysis\nmcpcap --modules dns,dhcp,icmp,tcp,capinfos    # All modules (default)\n```\n\n### Analysis Limits\n\n```bash\n# Limit packet analysis for large files\nmcpcap --max-packets 1000\n```\n\n### Complete Configuration Example\n\n```bash\nmcpcap --modules dns,dhcp,icmp,tcp,capinfos --max-packets 500\n```\n\n## CLI Reference\n\n```bash\nmcpcap [--modules MODULES] [--max-packets N]\n```\n\n**Options:**\n- `--modules MODULES`: Comma-separated modules to load (default: `dns,dhcp,icmp,tcp,capinfos`)\n  - Available modules: `dns`, `dhcp`, `icmp`, `tcp`, `capinfos`\n- `--max-packets N`: Maximum packets to analyze per file (default: unlimited)\n\n**Examples:**\n```bash\n# Start with all modules\nmcpcap\n\n# DNS and TCP analysis only\nmcpcap --modules dns,tcp\n\n# TCP analysis for troubleshooting connections\nmcpcap --modules tcp\n\n# With packet limits for large files\nmcpcap --max-packets 1000\n```\n\n## Examples\n\nExample PCAP files are included in the `examples/` directory:\n\n- `dns.pcap` - DNS traffic for testing DNS analysis\n- `dhcp.pcap` - DHCP 4-way handshake capture\n- `icmp.pcap` - ICMP ping and traceroute traffic\n\n### Using with MCP Inspector\n\n```bash\nnpm install -g @modelcontextprotocol/inspector\nnpx @modelcontextprotocol/inspector mcpcap\n```\n\nThen test the tools:\n```javascript\n// In the MCP Inspector web interface\nanalyze_dns_packets(\"./examples/dns.pcap\")\nanalyze_dhcp_packets(\"./examples/dhcp.pcap\")\nanalyze_icmp_packets(\"./examples/icmp.pcap\")\nanalyze_capinfos(\"./examples/dns.pcap\")\n```\n\n## Architecture\n\nmcpcap's modular design supports easy extension:\n\n### Core Components\n1. **BaseModule**: Shared file handling, validation, and remote download\n2. **Protocol Modules**: DNS, DHCP, ICMP, and TCP analysis implementations  \n3. **MCP Interface**: Tool registration and prompt management\n4.",
        "start_pos": 7273,
        "end_pos": 9279,
        "token_count_estimate": 501,
        "source_type": "readme",
        "agent_id": "b78965c3d99cb239"
      },
      {
        "chunk_id": 5,
        "text": "e**: Shared file handling, validation, and remote download\n2. **Protocol Modules**: DNS, DHCP, ICMP, and TCP analysis implementations  \n3. **MCP Interface**: Tool registration and prompt management\n4. **FastMCP Framework**: MCP server implementation\n\n### Tool Flow\n```\nMCP Client Request ‚Üí analyze_*_packets(pcap_file)\n                  ‚Üí BaseModule.analyze_packets()\n                  ‚Üí Module._analyze_protocol_file()\n                  ‚Üí Structured JSON Response\n```\n\n### Adding New Modules\n\nCreate new protocol modules by:\n\n1. Inheriting from `BaseModule`\n2. Implementing `_analyze_protocol_file(pcap_file)`\n3. Registering analysis tools with the MCP server\n4. Adding specialized analysis prompts\n\nFuture modules might include:\n- HTTP/HTTPS traffic analysis\n- UDP connection analysis\n- BGP routing analysis\n- SSL/TLS certificate analysis\n- Network forensics tools\n- Port scan detection\n\n## Remote File Support\n\nBoth analysis tools accept remote PCAP files via HTTP/HTTPS URLs:\n\n```bash\n# Examples of remote analysis\nanalyze_dns_packets(\"https://wiki.wireshark.org/uploads/dns.cap\")\nanalyze_dhcp_packets(\"https://example.com/network-capture.pcap\")\nanalyze_icmp_packets(\"https://example.com/ping-test.pcap\")\nanalyze_capinfos(\"https://example.com/network-metadata.pcap\")\n```\n\n**Features:**\n- Automatic temporary download and cleanup\n- Support for `.pcap`, `.pcapng`, and `.cap` files\n- HTTP/HTTPS protocols supported\n\n## Security Considerations\n\nWhen analyzing PCAP files:\n- Files may contain sensitive network information\n- Remote downloads are performed over HTTPS when possible\n- Temporary files are cleaned up automatically\n- Consider the source and trustworthiness of remote files\n\n## Contributing\n\nContributions welcome! Areas for contribution:\n\n- **New Protocol Modules**: Add support for HTTP, BGP, TCP, etc.",
        "start_pos": 9079,
        "end_pos": 10895,
        "token_count_estimate": 454,
        "source_type": "readme",
        "agent_id": "b78965c3d99cb239"
      },
      {
        "chunk_id": 6,
        "text": "e existing DNS/DHCP analysis\n- **Security Features**: Add more threat detection capabilities\n- **Performance**: Optimize analysis for large PCAP files\n\n## License\n\nMIT\n\n## Requirements\n\n- Python 3.10+\n- scapy (packet parsing and analysis)\n- requests (remote file access)\n- fastmcp (MCP server framework)\n\n## Documentation\n\n- **GitHub**: [github.com/mcpcap/mcpcap](https://github.com/mcpcap/mcpcap)\n- **Documentation**: [docs.mcpcap.ai](https://docs.mcpcap.ai) \n- **Website**: [mcpcap.ai](https://mcpcap.ai)\n\n## Support\n\nFor questions, issues, or feature requests, please open an issue on GitHub.",
        "start_pos": 10927,
        "end_pos": 11522,
        "token_count_estimate": 148,
        "source_type": "readme",
        "agent_id": "b78965c3d99cb239"
      }
    ]
  },
  {
    "agent_id": "223522821e98ff44",
    "name": "ai.meetlark/mcp-server",
    "source": "mcp",
    "source_url": "https://meetlark.ai/mcp",
    "description": "Agent-first meeting schedule polls for humans and agents. Create polls, vote, find times.",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-07T14:09:42.691681Z",
    "indexed_at": "2026-02-18T04:02:13.905439",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create meeting schedule polls",
        "Allow humans and agents to vote on polls",
        "Find available meeting times based on poll results"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of core functionalities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "dc0eb7d86169e77a",
    "name": "ai.meetsquad/squad",
    "source": "mcp",
    "source_url": "https://github.com/the-basilisk-ai/squad-mcp",
    "description": "Your AI Product Manager. Surface insights, build roadmaps, and plan strategy with 30+ tools.",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-12T21:01:17.340634Z",
    "indexed_at": "2026-02-18T04:02:14.377348",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Squad MCP Server\n\nA remote MCP server that brings [Squad](https://meetsquad.ai) ‚Äî the AI-powered product discovery and strategy platform ‚Äî directly into your AI workflows. Connect Squad to Claude, ChatGPT, or any MCP-compatible AI assistant to research, ideate, and plan products without context switching.\n\n## üöÄ Quick Start\n\n### For Users\n\nConnect Squad to your AI assistant in seconds:\n\n**Claude Code:**\n\n```bash\nclaude mcp add --transport http squad https://mcp.meetsquad.ai/mcp\n```\n\nOn first use, you'll be prompted to authenticate via OAuth in your browser.\n\n**Claude Connectors:**\n\n- Coming soon to the Claude MCP directory\n\n**ChatGPT:**\n\n- Coming soon to the ChatGPT plugin store\n\n**Other MCP Clients:**\n\nConnect using `https://mcp.meetsquad.ai/mcp` - OAuth configuration is automatically discovered via the server's `.well-known/oauth-authorization-server` endpoint.\n\n## üìñ Usage Examples\n\nSee **[USAGE_EXAMPLES.md](./USAGE_EXAMPLES.md)** for detailed real-world examples including:\n\n- **Discover opportunities** - \"What opportunities are in my workspace?\"\n- **Explore solutions** - \"Show me solutions for [opportunity] with pros/cons\"\n- **Strategic alignment** - \"How do my solutions map to business goals?\" (OST view)\n- **Generate ideas** - \"Generate solution ideas for [opportunity]\"\n- **Search everything** - \"Find all content related to compliance\"\n- **Create opportunities** - \"Create a new opportunity for [customer pain point]\"\n\nEach example shows the actual user prompt, which tools get called behind the scenes, and the expected output based on real Squad data.\n\n## ‚ú® Available Tools\n\nThe Squad MCP server provides 30+ tools across 6 categories:\n\n| Category          | Tools                                                                                                     | Purpose                                    |\n| ----------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------ |\n| **Opportunities** | `list_opportunities`, `get_opportunity`, `create_opportunity`, `update_opportunity`, `delete_opportunity` | Discover and refine product opportunities  |\n| **Solutions**     | `list_solutions`, `get_solution`, `create_solution`, `update_solution`, `generate_solutions`              | Generate and iterate on solution ideas     |\n| **Outcomes**      | `list_outcomes`, `get_outcome`, `create_outcome`, `update_outcome`                                        | Define and track desired business outcomes |\n| **Knowledge**     | `list_knowledge`, `get_knowledge`, `create_knowledge`, `delete_knowledge`                                 | Store research, references, and insights   |\n| **Feedback**      | `list_feedback`, `get_feedback`, `create_feedback`, `delete_feedback`                                     | Manage customer and stakeholder feedback   |\n| **Workspace**     | `get_workspace`, `update_workspace`                                                                       | Configure workspace settings               |\n\n### Tool Capabilities\n\nAll tools include:\n\n- ‚úÖ Safety annotations (`readOnlyHint` / `destructiveHint`)\n- ‚úÖ Structured JSON schemas for inputs/outputs\n- ‚úÖ User-isolated data access via OAuth\n- ‚úÖ Relationship management between entities\n\n## üèóÔ∏è Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         OAuth          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Claude /   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  PropelAuth  ‚îÇ\n‚îÇ  ChatGPT    ‚îÇ    (Authentication)     ‚îÇ   (IdP)      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚îÇ HTTPS + Bearer Token\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Squad MCP Server                            ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ  OAuth Middleware ‚Üí Validate Token     ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Session Store ‚Üí Manage State          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  MCP Handler ‚Üí Execute Tools           ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚îÇ Squad API Calls\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Squad API   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## üì¶ NPM Package\n\nFor programmatic access to Squad tools in your Node.js applications:\n\n```bash\nnpm install @squadai/tools\n```\n\n```typescript\nimport { tools as squadTools } from \"@squadai/tools\";\n\n// Use with Vercel AI SDK\nconst result = await generateText({\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n  tools: squadTools({\n    jwt: \"YOUR_JWT_TOKEN\",\n    orgId: \"org-123\",\n    workspaceId: \"ws-456\",\n  }),\n  prompt: \"List my current product opportunities\",\n});\n```\n\n## üõ†Ô∏è Development\n\nThis repository contains the source code for the Squad MCP remote server.\n\n### Prerequisites\n\n- Node.js 18+\n- Yarn\n- PropelAuth account (for OAuth2)\n- Squad API credentials\n\n### Local Setup\n\n```bash\n# Clone repository\ngit clone https://github.com/the-basilisk-ai/squad-mcp.git\ncd squad-mcp\n\n# Install dependencies\nyarn install\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your PropelAuth credentials\n\n# Start development server with hot reload\nyarn dev\n\n# Server available at http://localhost:3232\n```\n\n### Available Commands\n\n```bash\nyarn build              # Compile TypeScript\nyarn dev                # Start dev server with hot reload\nyarn start              # Start production server\nyarn openapi:squad      # Regenerate API client from OpenAPI spec\nyarn test               # Run test suite\n```\n\n### Testing the Server\n\n```bash\n# Check health\ncurl http://localhost:3232/health\n\n# Check OAuth discovery\ncurl http://localhost:3232/.well-known/oauth-authorization-server\n\n# Test with MCP Inspector\nnpx @modelcontextprotocol/inspector\n# Then connect to http://localhost:3232/mcp\n```\n\n### Running Tests\n\n```bash\n# Setup environment (if not already done)\ncp .env.example .env\n# Edit .env with your PropelAuth credentials and Squad API key\n\n# Run tests\nyarn test\n```\n\n**Note:** Tests use `SQUAD_API_KEY` from `.env` for authentication (not OAuth).\n\n### Project Structure\n\n```\nsquad-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ server.ts               # MCP server with OAuth\n‚îÇ   ‚îú‚îÄ‚îÄ middleware/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ csp.ts              # Content Security Policy\n‚îÇ   ‚îú‚îÄ‚îÄ helpers/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.ts           # Environment configuration\n‚îÇ   ‚îú‚îÄ‚îÄ lib/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clients/            # Squad API client\n‚îÇ   ‚îî‚îÄ‚îÄ tools/                  # Tool implementations\n‚îÇ       ‚îú‚îÄ‚îÄ opportunity.ts\n‚îÇ       ‚îú‚îÄ‚îÄ solution.ts\n‚îÇ       ‚îú‚îÄ‚îÄ goal.ts\n‚îÇ       ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ railway.toml                # Railway deployment config\n‚îî‚îÄ‚îÄ .env.example                # Environment template\n```\n\n## üè≠ Production Deployment\n\nThis is a hosted service maintained by Squad. Users connect via OAuth - no self-hosting required.\n\n**Architecture Notes (for contributors):**\n\n- Single-instance deployment on Railway\n- Follows [MCP specification](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports) for stateful HTTP sessions\n- In-memory transport storage (standard per MCP spec)\n\n## üí¨ Support\n\nNeed help with the Squad MCP server?\n\n- **Email:** support@meetsquad.ai\n- **Documentation:**\n  - [Squad MCP Guide](https://docs.meetsquad.ai/guides/squad-mcp) - Complete setup and integration guide\n  - [USAGE_EXAMPLES.md](./USAGE_EXAMPLES.md) - Real-world usage examples\n- **Issues:** [GitHub Issues](https://github.com/the-basilisk-ai/squad-mcp/issues) - Bug reports and feature requests\n- **Privacy Policy:** [meetsquad.ai/privacy-policy](https://meetsquad.ai/privacy-policy)\n- **Squad Platform:** [meetsquad.ai](https://meetsquad.ai) - Learn about Opportunity Solution Trees\n\n## ü§ù Contributing\n\nContributions welcome! Please ensure:\n\n- TypeScript builds without errors (`yarn build`)\n- All tools include safety annotations\n- OAuth context properly propagated\n- Tests pass (when test suite is implemented)\n\n## üìÑ License\n\nMIT\n\n## üîó Links\n\n- [Squad MCP Documentation](https://docs.meetsquad.ai/guides/squad-mcp) - Complete setup and integration guide\n- [Squad Platform](https://meetsquad.ai)\n- [MCP Specification](https://modelcontextprotocol.io)\n- [Claude Desktop](https://claude.ai/download)\n- [Issue Tracker](https://github.com/the-basilisk-ai/squad-mcp/issues)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect Squad AI-powered product discovery platform to MCP-compatible AI assistants",
        "Discover and refine product opportunities using opportunity management tools",
        "Generate and iterate on solution ideas with solution management tools",
        "Define and track desired business outcomes",
        "Store and manage research, references, and insights in knowledge base",
        "Manage customer and stakeholder feedback",
        "Configure workspace settings",
        "Authenticate users via OAuth with PropelAuth integration",
        "Provide structured JSON schemas and safety annotations for all tools",
        "Support programmatic access via an NPM package for Node.js applications"
      ],
      "limitations": [
        "No self-hosting required or supported; hosted service maintained by Squad",
        "OAuth authentication required for user-isolated data access",
        "Claude Connectors and ChatGPT plugin support are coming soon, not currently available",
        "Tests require Squad API key authentication, not OAuth",
        "In-memory transport storage limits state persistence to server runtime"
      ],
      "requirements": [
        "OAuth authentication via PropelAuth account",
        "Squad API credentials for API access",
        "Node.js version 18 or higher for local development",
        "Yarn package manager for dependency management",
        "Valid JWT token, organization ID, and workspace ID for programmatic tool access"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, a full tool list with capabilities, architecture overview, development setup, testing instructions, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Squad MCP Server\n\nA remote MCP server that brings [Squad](https://meetsquad.ai) ‚Äî the AI-powered product discovery and strategy platform ‚Äî directly into your AI workflows. Connect Squad to Claude, ChatGPT, or any MCP-compatible AI assistant to research, ideate, and plan products without context switching.\n\n## üöÄ Quick Start\n\n### For Users\n\nConnect Squad to your AI assistant in seconds:\n\n**Claude Code:**\n\n```bash\nclaude mcp add --transport http squad https://mcp.meetsquad.ai/mcp\n```\n\nOn first use, you'll be prompted to authenticate via OAuth in your browser.\n\n**Claude Connectors:**\n\n- Coming soon to the Claude MCP directory\n\n**ChatGPT:**\n\n- Coming soon to the ChatGPT plugin store\n\n**Other MCP Clients:**\n\nConnect using `https://mcp.meetsquad.ai/mcp` - OAuth configuration is automatically discovered via the server's `.well-known/oauth-authorization-server` endpoint.\n\n## üìñ Usage Examples\n\nSee **[USAGE_EXAMPLES.md](./USAGE_EXAMPLES.md)** for detailed real-world examples including:\n\n- **Discover opportunities** - \"What opportunities are in my workspace?\"\n- **Explore solutions** - \"Show me solutions for [opportunity] with pros/cons\"\n- **Strategic alignment** - \"How do my solutions map to business goals?\" (OST view)\n- **Generate ideas** - \"Generate solution ideas for [opportunity]\"\n- **Search everything** - \"Find all content related to compliance\"\n- **Create opportunities** - \"Create a new opportunity for [customer pain point]\"\n\nEach example shows the actual user prompt, which tools get called behind the scenes, and the expected output based on real Squad data.",
        "start_pos": 0,
        "end_pos": 1580,
        "token_count_estimate": 395,
        "source_type": "readme",
        "agent_id": "dc0eb7d86169e77a"
      },
      {
        "chunk_id": 1,
        "text": "------------ | --------------------------------------------------------------------------------------------------------- | ------------------------------------------ |\n| **Opportunities** | `list_opportunities`, `get_opportunity`, `create_opportunity`, `update_opportunity`, `delete_opportunity` | Discover and refine product opportunities  |\n| **Solutions**     | `list_solutions`, `get_solution`, `create_solution`, `update_solution`, `generate_solutions`              | Generate and iterate on solution ideas     |\n| **Outcomes**      | `list_outcomes`, `get_outcome`, `create_outcome`, `update_outcome`                                        | Define and track desired business outcomes |\n| **Knowledge**     | `list_knowledge`, `get_knowledge`, `create_knowledge`, `delete_knowledge`                                 | Store research, references, and insights   |\n| **Feedback**      | `list_feedback`, `get_feedback`, `create_feedback`, `delete_feedback`                                     | Manage customer and stakeholder feedback   |\n| **Workspace**     | `get_workspace`, `update_workspace`                                                                       | Configure workspace settings               |\n\n### Tool Capabilities\n\nAll tools include:\n\n- ‚úÖ Safety annotations (`readOnlyHint` / `destructiveHint`)\n- ‚úÖ Structured JSON schemas for inputs/outputs\n- ‚úÖ User-isolated data access via OAuth\n- ‚úÖ Relationship management between entities\n\n## üèóÔ∏è Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         OAuth          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Claude /   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ  PropelAuth  ‚îÇ\n‚îÇ  ChatGPT    ‚îÇ    (Authentication)     ‚îÇ   (IdP)      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚îÇ HTTPS + Bearer Token\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Squad MCP Server                            ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ  OAuth Middleware ‚Üí Validate Token     ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Session Store ‚Üí Manage State          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  MCP Handler ‚Üí Execute Tools           ‚îÇ  ‚îÇ\n‚îÇ",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "dc0eb7d86169e77a"
      },
      {
        "chunk_id": 2,
        "text": "‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ  OAuth Middleware ‚Üí Validate Token     ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Session Store ‚Üí Manage State          ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  MCP Handler ‚Üí Execute Tools           ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚îÇ Squad API Calls\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Squad API   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## üì¶ NPM Package\n\nFor programmatic access to Squad tools in your Node.js applications:\n\n```bash\nnpm install @squadai/tools\n```\n\n```typescript\nimport { tools as squadTools } from \"@squadai/tools\";\n\n// Use with Vercel AI SDK\nconst result = await generateText({\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n  tools: squadTools({\n    jwt: \"YOUR_JWT_TOKEN\",\n    orgId: \"org-123\",\n    workspaceId: \"ws-456\",\n  }),\n  prompt: \"List my current product opportunities\",\n});\n```\n\n## üõ†Ô∏è Development\n\nThis repository contains the source code for the Squad MCP remote server.",
        "start_pos": 3696,
        "end_pos": 4657,
        "token_count_estimate": 240,
        "source_type": "readme",
        "agent_id": "dc0eb7d86169e77a"
      },
      {
        "chunk_id": 3,
        "text": "own/oauth-authorization-server\n\n# Test with MCP Inspector\nnpx @modelcontextprotocol/inspector\n# Then connect to http://localhost:3232/mcp\n```\n\n### Running Tests\n\n```bash\n# Setup environment (if not already done)\ncp .env.example .env\n# Edit .env with your PropelAuth credentials and Squad API key\n\n# Run tests\nyarn test\n```\n\n**Note:** Tests use `SQUAD_API_KEY` from `.env` for authentication (not OAuth).\n\n### Project Structure\n\n```\nsquad-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ server.ts               # MCP server with OAuth\n‚îÇ   ‚îú‚îÄ‚îÄ middleware/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ csp.ts              # Content Security Policy\n‚îÇ   ‚îú‚îÄ‚îÄ helpers/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.ts           # Environment configuration\n‚îÇ   ‚îú‚îÄ‚îÄ lib/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clients/            # Squad API client\n‚îÇ   ‚îî‚îÄ‚îÄ tools/                  # Tool implementations\n‚îÇ       ‚îú‚îÄ‚îÄ opportunity.ts\n‚îÇ       ‚îú‚îÄ‚îÄ solution.ts\n‚îÇ       ‚îú‚îÄ‚îÄ goal.ts\n‚îÇ       ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ railway.toml                # Railway deployment config\n‚îî‚îÄ‚îÄ .env.example                # Environment template\n```\n\n## üè≠ Production Deployment\n\nThis is a hosted service maintained by Squad. Users connect via OAuth - no self-hosting required.\n\n**Architecture Notes (for contributors):**\n\n- Single-instance deployment on Railway\n- Follows [MCP specification](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports) for stateful HTTP sessions\n- In-memory transport storage (standard per MCP spec)\n\n## üí¨ Support\n\nNeed help with the Squad MCP server?",
        "start_pos": 5544,
        "end_pos": 6981,
        "token_count_estimate": 359,
        "source_type": "readme",
        "agent_id": "dc0eb7d86169e77a"
      },
      {
        "chunk_id": 4,
        "text": "//meetsquad.ai/privacy-policy)\n- **Squad Platform:** [meetsquad.ai](https://meetsquad.ai) - Learn about Opportunity Solution Trees\n\n## ü§ù Contributing\n\nContributions welcome! Please ensure:\n\n- TypeScript builds without errors (`yarn build`)\n- All tools include safety annotations\n- OAuth context properly propagated\n- Tests pass (when test suite is implemented)\n\n## üìÑ License\n\nMIT\n\n## üîó Links\n\n- [Squad MCP Documentation](https://docs.meetsquad.ai/guides/squad-mcp) - Complete setup and integration guide\n- [Squad Platform](https://meetsquad.ai)\n- [MCP Specification](https://modelcontextprotocol.io)\n- [Claude Desktop](https://claude.ai/download)\n- [Issue Tracker](https://github.com/the-basilisk-ai/squad-mcp/issues)",
        "start_pos": 7392,
        "end_pos": 8110,
        "token_count_estimate": 179,
        "source_type": "readme",
        "agent_id": "dc0eb7d86169e77a"
      }
    ]
  },
  {
    "agent_id": "14601397e23cf8bc",
    "name": "ai.meminal/meminal",
    "source": "mcp",
    "source_url": "https://meminal.ai/mcp",
    "description": "Memory for deep conversational context across any platform",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-12T23:40:10.473855Z",
    "indexed_at": "2026-02-18T04:02:15.607341",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide memory for deep conversational context",
        "Support conversational context across any platform"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence that gives a basic idea of the server's purpose but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "0888283e538134dd",
    "name": "ai.mino/web-agent",
    "source": "mcp",
    "source_url": "https://mino.ai/mcp",
    "description": "AI-powered web automation. Navigate websites using AI agents for one page or a thousand",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-28T02:20:51.640868Z",
    "indexed_at": "2026-02-18T04:02:15.961864",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Navigate websites using AI agents",
        "Automate web interactions across multiple pages"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of AI-powered web automation and navigation capabilities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "aca699962ed18fa4",
    "name": "ai.packmind/mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/PackmindHub/packmind",
    "description": "Packmind captures, scales, and enforces your organization's technical decisions.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-11-24T15:23:52.365339Z",
    "indexed_at": "2026-02-18T04:02:17.965439",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# One Engineering Playbook. Synced Everywhere. For Every AI Coding Agent.\n\n![License](https://img.shields.io/github/license/PackmindHub/packmind)\n![Stars](https://img.shields.io/github/stars/PackmindHub/packmind)\n[![Main OSS CI/CD Pipeline](https://github.com/PackmindHub/packmind/actions/workflows/main.yml/badge.svg)](https://github.com/PackmindHub/packmind/actions/workflows/main-oss.yml)\n![Works with GitHub Copilot](https://img.shields.io/badge/works%20with-GitHub%20Copilot-blue?logo=githubcopilot&logoColor=white)\n![Works with Cursor](https://img.shields.io/badge/works%20with-Cursor-blueviolet?logo=cursor&logoColor=white)\n![Works with Claude Code](https://img.shields.io/badge/works%20with-Claude%20Code-purple?logo=anthropic&logoColor=white)\n\n**‚ùó The 2 big problems every AI-native engineer runs into**\n\n### **1Ô∏è‚É£ ‚ÄúWhat do I even put in these AI instructions?‚Äù**\n\nEvery tool expects its own inputs:\n\n- **Copilot** ‚Üí `.github/copilot-instructions.md`, chat modes, reusable prompts\n- **Claude** ‚Üí `CLAUDE.md`, commands, skills\n- **Cursor** ‚Üí `.cursor/rules/*.mdc`, commands, skills\n- **AGENTS.md** ‚Üí `AGENTS.md`\n- _(with more formats appearing every month‚Ä¶)_\n\nBut your team‚Äôs **actual standards aren‚Äôt stored anywhere**:\n\n- architecture rules ‚Üí buried in Slack or Notion\n- naming conventions ‚Üí stuck in your head\n- patterns ‚Üí hiding in PR comments\n- best practices ‚Üí scattered across repos\n\nüëâ **Packmind helps you turn all of this into a real engineering playbook**\n(standards, commands, skills) so **AI agents finally code _your way_.**\n\n### **2Ô∏è‚É£ ‚ÄúWhy am I copy-pasting this across every repo and every agent?‚Äù**\n\nEvery repo.\nEvery assistant.\nDifferent files, different folders, different formats.\n\nKeeping everything in sync is impossible.\n\nüëâ **Packmind centralizes your playbook once ‚Äî and distributes it everywhere**,\ngenerating the exact instruction files each AI tool needs, optimized for context.\n\n# Get started\n\nChoose your preferred setup option:\n\n- **Cloud version**: Get started at [https://app.packmind.ai](https://app.packmind.ai/sign-up?utm_source=oss) (free account)\n- **Self-hosted**: Deploy on your own infrastructure using [Docker Compose or Kubernetes](https://docs.packmind.com/getting-started/gs-install-self-hosted)\n\n## Option 1: Install the CLI (recommended)\n\nFollow the instructions during the onboarding to connect to your Packmind organization\nYou can find them at anytime in the **Settings** menu.\n\nOnce authenticated, run in your project:\n\n```bash\n$> packmind-cli init\n```\n\nThen, in your favorite ai coding agent, run:\n\n```\n/packmind-onboard\n```\n\nTo create your first standards and commands from your codebase.\n\n## Option 2: Connect MCP server\n\nThe MCP server allows you to create and manage standards and commands directly from your AI agent (GitHub Copilot, Claude Code, Cursor, etc.).\n\n1. Go to **Account Settings** in Packmind\n2. Copy your MCP Access token\n3. Configure your AI agent with:\n   - MCP server URL: `{PACKMIND_URL}/mcp`\n   - Your MCP access token\n\nOnce set up, open your AI agent and use this prompt:\n\n```\nStart packmind onboarding\n```\n\nYour AI agent will guide you through creating your first coding standard interactively.\n\n# Documentation\n\nAvailable here: [https://docs.packmind.com](https://docs.packmind.com).\n\n# :compass: Key Links\n\n- [**Docs ‚Üí**](https://docs.packmind.com)\n- [**Packmind Cloud ‚Üí**](https://app.packmind.ai/sign-up)\n- [**Join the Slack Community ‚Üí**](https://join.slack.com/t/promyze/shared_invite/zt-vf6asxsj-aH1RbzuoOR5DNFexeaATVQ)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Centralize engineering playbooks including standards, commands, and skills",
        "Generate instruction files optimized for various AI coding agents",
        "Create and manage coding standards and commands interactively via AI agents",
        "Distribute synchronized playbook content across multiple repositories and AI tools",
        "Support integration with AI coding agents like GitHub Copilot, Claude Code, and Cursor",
        "Provide a CLI tool for initializing and onboarding projects with Packmind",
        "Offer both cloud-hosted and self-hosted deployment options"
      ],
      "limitations": [
        "Does not explicitly mention support for AI agents beyond GitHub Copilot, Claude Code, and Cursor",
        "Requires user to have an existing Packmind organization and access token for MCP server usage",
        "No detailed mention of rate limits or performance constraints"
      ],
      "requirements": [
        "Packmind account with access token for MCP server authentication",
        "Installation of packmind-cli for CLI usage",
        "Supported AI coding agents configured with MCP server URL and access token",
        "Docker Compose or Kubernetes for self-hosted deployment option"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation options, usage examples, integration details, and requirements, but lacks explicit limitations and detailed technical references.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# One Engineering Playbook. Synced Everywhere. For Every AI Coding Agent.\n\n![License](https://img.shields.io/github/license/PackmindHub/packmind)\n![Stars](https://img.shields.io/github/stars/PackmindHub/packmind)\n[![Main OSS CI/CD Pipeline](https://github.com/PackmindHub/packmind/actions/workflows/main.yml/badge.svg)](https://github.com/PackmindHub/packmind/actions/workflows/main-oss.yml)\n![Works with GitHub Copilot](https://img.shields.io/badge/works%20with-GitHub%20Copilot-blue?logo=githubcopilot&logoColor=white)\n![Works with Cursor](https://img.shields.io/badge/works%20with-Cursor-blueviolet?logo=cursor&logoColor=white)\n![Works with Claude Code](https://img.shields.io/badge/works%20with-Claude%20Code-purple?logo=anthropic&logoColor=white)\n\n**‚ùó The 2 big problems every AI-native engineer runs into**\n\n### **1Ô∏è‚É£ ‚ÄúWhat do I even put in these AI instructions?‚Äù**\n\nEvery tool expects its own inputs:\n\n- **Copilot** ‚Üí `.github/copilot-instructions.md`, chat modes, reusable prompts\n- **Claude** ‚Üí `CLAUDE.md`, commands, skills\n- **Cursor** ‚Üí `.cursor/rules/*.mdc`, commands, skills\n- **AGENTS.md** ‚Üí `AGENTS.md`\n- _(with more formats appearing every month‚Ä¶)_\n\nBut your team‚Äôs **actual standards aren‚Äôt stored anywhere**:\n\n- architecture rules ‚Üí buried in Slack or Notion\n- naming conventions ‚Üí stuck in your head\n- patterns ‚Üí hiding in PR comments\n- best practices ‚Üí scattered across repos\n\nüëâ **Packmind helps you turn all of this into a real engineering playbook**\n(standards, commands, skills) so **AI agents finally code _your way_.**\n\n### **2Ô∏è‚É£ ‚ÄúWhy am I copy-pasting this across every repo and every agent?‚Äù**\n\nEvery repo.\nEvery assistant.\nDifferent files, different folders, different formats.\n\nKeeping everything in sync is impossible.\n\nüëâ **Packmind centralizes your playbook once ‚Äî and distributes it everywhere**,\ngenerating the exact instruction files each AI tool needs, optimized for context.",
        "start_pos": 0,
        "end_pos": 1912,
        "token_count_estimate": 478,
        "source_type": "readme",
        "agent_id": "aca699962ed18fa4"
      },
      {
        "chunk_id": 1,
        "text": "ping everything in sync is impossible.\n\nüëâ **Packmind centralizes your playbook once ‚Äî and distributes it everywhere**,\ngenerating the exact instruction files each AI tool needs, optimized for context.\n\n# Get started\n\nChoose your preferred setup option:\n\n- **Cloud version**: Get started at [https://app.packmind.ai](https://app.packmind.ai/sign-up?utm_source=oss) (free account)\n- **Self-hosted**: Deploy on your own infrastructure using [Docker Compose or Kubernetes](https://docs.packmind.com/getting-started/gs-install-self-hosted)\n\n## Option 1: Install the CLI (recommended)\n\nFollow the instructions during the onboarding to connect to your Packmind organization\nYou can find them at anytime in the **Settings** menu.\n\nOnce authenticated, run in your project:\n\n```bash\n$> packmind-cli init\n```\n\nThen, in your favorite ai coding agent, run:\n\n```\n/packmind-onboard\n```\n\nTo create your first standards and commands from your codebase.\n\n## Option 2: Connect MCP server\n\nThe MCP server allows you to create and manage standards and commands directly from your AI agent (GitHub Copilot, Claude Code, Cursor, etc.).\n\n1. Go to **Account Settings** in Packmind\n2. Copy your MCP Access token\n3. Configure your AI agent with:\n   - MCP server URL: `{PACKMIND_URL}/mcp`\n   - Your MCP access token\n\nOnce set up, open your AI agent and use this prompt:\n\n```\nStart packmind onboarding\n```\n\nYour AI agent will guide you through creating your first coding standard interactively.\n\n# Documentation\n\nAvailable here: [https://docs.packmind.com](https://docs.packmind.com).\n\n# :compass: Key Links\n\n- [**Docs ‚Üí**](https://docs.packmind.com)\n- [**Packmind Cloud ‚Üí**](https://app.packmind.ai/sign-up)\n- [**Join the Slack Community ‚Üí**](https://join.slack.com/t/promyze/shared_invite/zt-vf6asxsj-aH1RbzuoOR5DNFexeaATVQ)",
        "start_pos": 1712,
        "end_pos": 3510,
        "token_count_estimate": 449,
        "source_type": "readme",
        "agent_id": "aca699962ed18fa4"
      }
    ]
  },
  {
    "agent_id": "678552f12a35f96b",
    "name": "ai.parallel/search-mcp",
    "source": "mcp",
    "source_url": "https://github.com/parallel-web/search-mcp",
    "description": "The best web search for your AI Agent",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-09T11:15:39.72942Z",
    "indexed_at": "2026-02-18T04:02:19.232055",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Parallel Search MCP\n\nThe **Parallel Search MCP** allows using Parallel Search API from within any MCP-compatible LLM client. It is meant for daily use for everyday smaller web-search tasks. Please read [our MCP docs here](https://docs.parallel.ai/integrations/mcp/getting-started) for more details.\n\n## Installation\n\nThe official installation instructions can be found [here](https://docs.parallel.ai/integrations/mcp/installation).\n\n```json Search MCP\n{\n  \"mcpServers\": {\n    \"Parallel Search MCP\": {\n      \"url\": \"https://search-mcp.parallel.ai/mcp\"\n    }\n  }\n}\n```\n\n## Running locally\n\n<details><summary>Running locally</summary>\n\nThis is a Search MCP proxy server (https://search-mcp.parallel.ai) that proxies `/mcp` to https://mcp.parallel.ai/v1beta/search_mcp and adds minimally needed additions to make it work with oauth.\n\nMCP address: https://search-mcp.parallel.ai/mcp\n\n[![Install Parallel Search MCP](https://img.shields.io/badge/Install_MCP-Parallel%20Search%20MCP-black?style=for-the-badge)](https://installthismcp.com/Parallel%20Search%20MCP?url=https%3A%2F%2Fsearch-mcp.parallel.ai%2Fmcp)\n\n</details>\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Use the Parallel Search API within any MCP-compatible LLM client",
        "Perform daily smaller web-search tasks",
        "Proxy MCP requests to the official Parallel Search MCP endpoint",
        "Add minimal OAuth support for authentication in MCP requests"
      ],
      "limitations": [
        "Intended for smaller, everyday web-search tasks only",
        "No detailed information on rate limits or advanced search features"
      ],
      "requirements": [
        "An MCP-compatible LLM client to interact with the server",
        "OAuth credentials for authentication when running locally"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation instructions, usage context, and proxy details but lacks detailed usage examples and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Parallel Search MCP\n\nThe **Parallel Search MCP** allows using Parallel Search API from within any MCP-compatible LLM client. It is meant for daily use for everyday smaller web-search tasks. Please read [our MCP docs here](https://docs.parallel.ai/integrations/mcp/getting-started) for more details.\n\n## Installation\n\nThe official installation instructions can be found [here](https://docs.parallel.ai/integrations/mcp/installation).\n\n```json Search MCP\n{\n  \"mcpServers\": {\n    \"Parallel Search MCP\": {\n      \"url\": \"https://search-mcp.parallel.ai/mcp\"\n    }\n  }\n}\n```\n\n## Running locally\n\n<details><summary>Running locally</summary>\n\nThis is a Search MCP proxy server (https://search-mcp.parallel.ai) that proxies `/mcp` to https://mcp.parallel.ai/v1beta/search_mcp and adds minimally needed additions to make it work with oauth.\n\nMCP address: https://search-mcp.parallel.ai/mcp\n\n[![Install Parallel Search MCP](https://img.shields.io/badge/Install_MCP-Parallel%20Search%20MCP-black?style=for-the-badge)](https://installthismcp.com/Parallel%20Search%20MCP?url=https%3A%2F%2Fsearch-mcp.parallel.ai%2Fmcp)\n\n</details>",
        "start_pos": 0,
        "end_pos": 1118,
        "token_count_estimate": 279,
        "source_type": "readme",
        "agent_id": "678552f12a35f96b"
      }
    ]
  },
  {
    "agent_id": "89e56cbbca18ee84",
    "name": "ai.parallel/task-mcp",
    "source": "mcp",
    "source_url": "https://github.com/parallel-web/task-mcp",
    "description": "An MCP server for deep research or task groups",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-09T11:11:31.316037Z",
    "indexed_at": "2026-02-18T04:02:23.212117",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Parallel Task MCP\n\nThe **Parallel Task MCP** allows initiating deep research or task groups directly from your favorite LLM client. It can be a great way to get to know Parallel‚Äôs different APIs by exploring their capabilities, but can also be used as a way to easily do small experiments while developing production systems using Parallel APIs. Please read [our MCP docs here](https://docs.parallel.ai/integrations/mcp/getting-started) for more details.\n\n## Installation\n\nThe official installation instructions can be found [here](https://docs.parallel.ai/integrations/mcp/installation).\n\n```json\n{\n  \"mcpServers\": {\n    \"Parallel Task MCP\": {\n      \"url\": \"https://task-mcp.parallel.ai/mcp\"\n    }\n  }\n}\n```\n\n## Running locally\n\n<details><summary>Running locally</summary>\n\nThis repo contains a proxy to the mcp which is hosted at: https://task-mcp.parallel.ai/mcp\n\nHow to run and test locally:\n\n1. `wrangler dev`\n2. `npx @modelcontextprotocol/inspector`\n3. Connect to server: http://localhost:8787/mcp\n\n</details>\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Initiate deep research or task groups from an LLM client",
        "Explore capabilities of Parallel‚Äôs different APIs",
        "Conduct small experiments while developing production systems using Parallel APIs"
      ],
      "limitations": [],
      "requirements": [
        "Access to an LLM client to initiate tasks",
        "Ability to connect to the MCP server at https://task-mcp.parallel.ai/mcp",
        "For local testing: wrangler dev and npx @modelcontextprotocol/inspector tools"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation instructions, usage context, and local running steps but lacks detailed usage examples and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Parallel Task MCP\n\nThe **Parallel Task MCP** allows initiating deep research or task groups directly from your favorite LLM client. It can be a great way to get to know Parallel‚Äôs different APIs by exploring their capabilities, but can also be used as a way to easily do small experiments while developing production systems using Parallel APIs. Please read [our MCP docs here](https://docs.parallel.ai/integrations/mcp/getting-started) for more details.\n\n## Installation\n\nThe official installation instructions can be found [here](https://docs.parallel.ai/integrations/mcp/installation).\n\n```json\n{\n  \"mcpServers\": {\n    \"Parallel Task MCP\": {\n      \"url\": \"https://task-mcp.parallel.ai/mcp\"\n    }\n  }\n}\n```\n\n## Running locally\n\n<details><summary>Running locally</summary>\n\nThis repo contains a proxy to the mcp which is hosted at: https://task-mcp.parallel.ai/mcp\n\nHow to run and test locally:\n\n1. `wrangler dev`\n2. `npx @modelcontextprotocol/inspector`\n3. Connect to server: http://localhost:8787/mcp\n\n</details>",
        "start_pos": 0,
        "end_pos": 1018,
        "token_count_estimate": 254,
        "source_type": "readme",
        "agent_id": "89e56cbbca18ee84"
      }
    ]
  },
  {
    "agent_id": "f780a7d3b5688e31",
    "name": "ai.perplexity/mcp-server",
    "source": "mcp",
    "source_url": "",
    "description": "Real-time web search, reasoning, and research through Perplexity's API",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-16T14:36:53.575623Z",
    "indexed_at": "2026-02-18T04:02:27.357450",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Perform real-time web searches",
        "Conduct reasoning tasks",
        "Facilitate research using Perplexity's API"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of capabilities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "c1fd22339934922c",
    "name": "ai.rapay/mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/Ra-Pay-AI/rapay",
    "description": "Send fiat payments via MCP with two-step confirmation and Stripe Connect.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-09T18:54:17.964435Z",
    "indexed_at": "2026-02-18T04:02:27.384934",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Send fiat payments via MCP",
        "Perform two-step confirmation for payments",
        "Integrate with Stripe Connect for payment processing"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of payment capabilities and integration but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "ceccd5ede78177ca",
    "name": "ai.seltz/seltz-ai-seltz-mcp",
    "source": "mcp",
    "source_url": "https://mcp.seltz.ai/mcp",
    "description": "Provides AI assistants with access to Seltz's powerful Web Search capabilities.\n",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-12-05T09:24:21.447908Z",
    "indexed_at": "2026-02-18T04:02:31.371770",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide AI assistants with access to web search capabilities"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence providing only a basic overview without details on usage, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "8676f457ad3ca4f6",
    "name": "ai.shawndurrani/mcp-merchant",
    "source": "mcp",
    "source_url": "",
    "description": "Search-only commerce MCP server backed by Stripe (test)",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-14T02:22:00.597355Z",
    "indexed_at": "2026-02-18T04:02:33.134149",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search commerce data",
        "Integrate with Stripe for commerce operations"
      ],
      "limitations": [
        "Supports search functionality only",
        "Operates in Stripe test mode"
      ],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of functionality and backend but lacks detailed features, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "8676f457ad3ca4f6",
    "name": "ai.shawndurrani/mcp-merchant",
    "source": "mcp",
    "source_url": "https://mcp.shawndurrani.ai/sse",
    "description": "Search-only commerce MCP server backed by Stripe (test)",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-16T22:54:28.465114Z",
    "indexed_at": "2026-02-18T04:02:33.157816",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search commerce-related data",
        "Integrate with Stripe for commerce operations"
      ],
      "limitations": [
        "Supports search functionality only",
        "Operates in Stripe test mode"
      ],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of functionality and backend but lacks detailed features, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "6d92aafb4e5a7eb9",
    "name": "ai.shawndurrani/mcp-registry",
    "source": "mcp",
    "source_url": "https://mcp-registry.shawndurrani.ai/sse",
    "description": "Search the public MCP Registry; discover servers and copy SSE URLs.",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-15T04:01:11.068068Z",
    "indexed_at": "2026-02-18T04:02:49.279002",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search the public MCP Registry",
        "Discover MCP servers",
        "Copy SSE URLs"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.25,
    "quality_rationale": "The description provides a basic overview of the server's functions but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "160b715368fcd1b9",
    "name": "ai.smithery/222wcnm-bilistalkermcp",
    "source": "mcp",
    "source_url": "https://github.com/222wcnm/BiliStalkerMCP",
    "description": "Track Bilibili creators and get the latest updates on videos, dynamics, and articles. Fetch user p‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T06:57:33.936879Z",
    "indexed_at": "2026-02-18T04:03:03.803284",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# BiliStalkerMCP\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)](https://www.python.org/)\n[![MCP](https://img.shields.io/badge/MCP-Compatible-orange)](https://github.com/jlowin/fastmcp)\n[![Version](https://img.shields.io/badge/Version-2.6.1-green)](https://pypi.org/project/bili-stalker-mcp/)\n\nBiliStalkerMCP is a [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server for Bilibili, specifically designed for AI assistants (like Claude, ChatGPT).\n\n**English | [‰∏≠ÊñáËØ¥Êòé](README_zh.md)**\n\n### Installation\n\n```bash\nuvx bili-stalker-mcp\n# or\npip install bili-stalker-mcp\n```\n\n### Configuration\n\nAdd to your MCP client settings (e.g., Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"bilistalker\": {\n      \"command\": \"uvx\",\n      \"args\": [\"bili-stalker-mcp\"],\n      \"env\": {\n        \"SESSDATA\": \"your_sessdata\",\n        \"BILI_JCT\": \"your_bili_jct\",\n        \"BUVID3\": \"your_buvid3\"\n      }\n    }\n  }\n}\n```\n\n> **Tip**: You can find these values in your browser by pressing F12 -> Application -> Cookies on the Bilibili website.\n\n### Environment Variables\n\n| Variable | Required | Description |\n|----------|:--------:|-------------|\n| `SESSDATA` | **Yes** | Authentication token from Bilibili cookies. |\n| `BILI_JCT` | No | CSRF token from cookies. |\n| `BUVID3` | No | Browser fingerprint, helps reduce rate limiting issues. |\n| `BILI_LOG_LEVEL` | No | Log level (`INFO`, `DEBUG`, `WARNING`), default is `WARNING`. |\n\n## Available Tools\n\n| Tool | Description | Parameters |\n|------|-------------|------------|\n| `get_user_info` | User profile and stats | `user_id` or `username` |\n| `get_user_video_updates` | Video publications with subtitles | `user_id`/`username`, `page`, `limit` |\n| `get_user_dynamic_updates` | User dynamics with type filtering | `user_id`/`username`, `offset`, `limit`, `dynamic_type` |\n| `get_user_articles` | Article publications | `user_id`/`username`, `page`, `limit` |\n| `get_user_followings` | Following list | `user_id`/`username`, `page`, `limit` |\n\n### Dynamic Type Filtering\n\nThe `get_user_dynamic_updates` tool supports filtering by type:\n\n| `dynamic_type` | Description |\n|----------------|-------------|\n| `ALL` (default) | TEXT, IMAGE_TEXT, REPOST only (analysis-focused) |\n| `ALL_RAW` | All types including VIDEO, ARTICLE |\n| `VIDEO` | Video dynamics only |\n| `ARTICLE` | Article dynamics only |\n| `DRAW` | Image-text dynamics only |\n| `TEXT` | Text-only dynamics |\n\n## Development\n\n```bash\n# Clone and setup\ngit clone https://github.com/222wcnm/BiliStalkerMCP.git\ncd BiliStalkerMCP\nuv pip install -e .\n\n# Run tests\npython tests/test_suite.py -u <user_id_or_username>\n```\n\n## üê≥ Docker Support\n\nYou can also run the server using Docker:\n\n```bash\ndocker build -t bilistalker-mcp .\ndocker run -e SESSDATA=... -e BILI_JCT=... -e BUVID3=... bilistalker-mcp\n```\n\n## ‚ùì Troubleshooting\n\n**Q: Why am I getting \"412 Precondition Failed\"?**\nA: This usually means you are being rate-limited or blocked by Bilibili. Try to:\n1. Refresh your `SESSDATA`.\n2. Ensure you provide `BUVID3`.\n3. If running on a cloud server, try running locally as cloud IPs are more likely to be blocked.\n\n## License\n\nMIT\n\n---\n\n*This project is built and maintained with the help of AI.*\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Retrieve user profile and statistics from Bilibili",
        "Fetch video publications with subtitles for a user",
        "Obtain user dynamic updates with type filtering",
        "Get article publications by a user",
        "List the followings of a user"
      ],
      "limitations": [
        "May be rate-limited or blocked by Bilibili resulting in '412 Precondition Failed' errors",
        "Requires valid authentication tokens from Bilibili cookies to function",
        "Cloud server IPs are more likely to be blocked by Bilibili"
      ],
      "requirements": [
        "Python 3.10 or higher",
        "Bilibili authentication tokens: SESSDATA (required), BILI_JCT (optional), BUVID3 (optional)",
        "MCP client capable of configuring external MCP servers",
        "Docker (optional, for containerized deployment)"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides clear installation instructions, environment variable requirements, detailed tool descriptions with parameters, usage examples, Docker support, troubleshooting tips, and development guidance.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# BiliStalkerMCP\n\n[![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)](https://www.python.org/)\n[![MCP](https://img.shields.io/badge/MCP-Compatible-orange)](https://github.com/jlowin/fastmcp)\n[![Version](https://img.shields.io/badge/Version-2.6.1-green)](https://pypi.org/project/bili-stalker-mcp/)\n\nBiliStalkerMCP is a [Model Context Protocol (MCP)](https://modelcontextprotocol.io) server for Bilibili, specifically designed for AI assistants (like Claude, ChatGPT).\n\n**English | [‰∏≠ÊñáËØ¥Êòé](README_zh.md)**\n\n### Installation\n\n```bash\nuvx bili-stalker-mcp\n# or\npip install bili-stalker-mcp\n```\n\n### Configuration\n\nAdd to your MCP client settings (e.g., Claude Desktop):\n\n```json\n{\n  \"mcpServers\": {\n    \"bilistalker\": {\n      \"command\": \"uvx\",\n      \"args\": [\"bili-stalker-mcp\"],\n      \"env\": {\n        \"SESSDATA\": \"your_sessdata\",\n        \"BILI_JCT\": \"your_bili_jct\",\n        \"BUVID3\": \"your_buvid3\"\n      }\n    }\n  }\n}\n```\n\n> **Tip**: You can find these values in your browser by pressing F12 -> Application -> Cookies on the Bilibili website.\n\n### Environment Variables\n\n| Variable | Required | Description |\n|----------|:--------:|-------------|\n| `SESSDATA` | **Yes** | Authentication token from Bilibili cookies. |\n| `BILI_JCT` | No | CSRF token from cookies. |\n| `BUVID3` | No | Browser fingerprint, helps reduce rate limiting issues. |\n| `BILI_LOG_LEVEL` | No | Log level (`INFO`, `DEBUG`, `WARNING`), default is `WARNING`.",
        "start_pos": 0,
        "end_pos": 1447,
        "token_count_estimate": 361,
        "source_type": "readme",
        "agent_id": "160b715368fcd1b9"
      },
      {
        "chunk_id": 1,
        "text": "et_user_articles` | Article publications | `user_id`/`username`, `page`, `limit` |\n| `get_user_followings` | Following list | `user_id`/`username`, `page`, `limit` |\n\n### Dynamic Type Filtering\n\nThe `get_user_dynamic_updates` tool supports filtering by type:\n\n| `dynamic_type` | Description |\n|----------------|-------------|\n| `ALL` (default) | TEXT, IMAGE_TEXT, REPOST only (analysis-focused) |\n| `ALL_RAW` | All types including VIDEO, ARTICLE |\n| `VIDEO` | Video dynamics only |\n| `ARTICLE` | Article dynamics only |\n| `DRAW` | Image-text dynamics only |\n| `TEXT` | Text-only dynamics |\n\n## Development\n\n```bash\n# Clone and setup\ngit clone https://github.com/222wcnm/BiliStalkerMCP.git\ncd BiliStalkerMCP\nuv pip install -e .\n\n# Run tests\npython tests/test_suite.py -u <user_id_or_username>\n```\n\n## üê≥ Docker Support\n\nYou can also run the server using Docker:\n\n```bash\ndocker build -t bilistalker-mcp .\ndocker run -e SESSDATA=... -e BILI_JCT=... -e BUVID3=... bilistalker-mcp\n```\n\n## ‚ùì Troubleshooting\n\n**Q: Why am I getting \"412 Precondition Failed\"?**\nA: This usually means you are being rate-limited or blocked by Bilibili. Try to:\n1. Refresh your `SESSDATA`.\n2. Ensure you provide `BUVID3`.\n3. If running on a cloud server, try running locally as cloud IPs are more likely to be blocked.\n\n## License\n\nMIT\n\n---\n\n*This project is built and maintained with the help of AI.*",
        "start_pos": 1848,
        "end_pos": 3224,
        "token_count_estimate": 343,
        "source_type": "readme",
        "agent_id": "160b715368fcd1b9"
      }
    ]
  },
  {
    "agent_id": "253d9186b34d8c34",
    "name": "ai.smithery/Aman-Amith-Shastry-scientific_computation_mcp",
    "source": "mcp",
    "source_url": "https://github.com/Aman-Amith-Shastry/scientific_computation_mcp",
    "description": "This MCP server enables users to perform scientific computations regarding linear algebra and vect‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-12T01:14:07.07827Z",
    "indexed_at": "2026-02-18T04:03:05.348687",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/aman-amith-shastry-scientific-computation-mcp-badge.png)](https://mseep.ai/app/aman-amith-shastry-scientific-computation-mcp)\n\n# Scientific Computation MCP\n\n[![smithery badge](https://smithery.ai/badge/@Aman-Amith-Shastry/scientific_computation_mcp)](https://smithery.ai/server/@Aman-Amith-Shastry/scientific_computation_mcp)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/5927ad38-70f6-4f5b-9778-e61ec902d735)\n\n[![MCP Badge](https://lobehub.com/badge/mcp-full/aman-amith-shastry-scientific_computation_mcp)](https://lobehub.com/mcp/aman-amith-shastry-scientific_computation_mcp)\n\n## Installation Guide\n\n### Claude Desktop\n\nOpen Claude Desktop's configuration file (claude_desktop_config.json) and add the following:\n\n- Mac/Linux: \n```json\n{\n  \"mcpServers\": {\n    \"numpy_mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@Aman-Amith-Shastry/scientific_computation_mcp\",\n        \"--key\",\n        \"<YOUR_SMITHERY_API_KEY>\"\n      ]\n    }\n  }\n}\n```\n\n- Windows:\n```json\n{\n  \"mcpServers\": {\n    \"numpy_mcp\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@Aman-Amith-Shastry/scientific_computation_mcp\",\n        \"--key\",\n        \"<YOUR_SMITHERY_API_KEY>\"\n      ]\n    }\n  }\n}\n```\n\nOr alternatively, run the following command:\n```commandline\nnpx -y @smithery/cli@latest install @Aman-Amith-Shastry/scientific_computation_mcp --client claude --key <YOUR_SMITHERY_API_KEY>\n```\n\nRestart Claude to load the server properly\n\n### Cursor\n\nIf you prefer to access the server through Cursor instead, then run the following command:\n```commandline\nnpx -y @smithery/cli@latest install @Aman-Amith-Shastry/scientific_computation_mcp --client cursor --key <YOUR_SMITHERY_API_KEY>\n```\n\n## Components of the Server\n\n### Tools\n\n#### Tensor storage\n- ```create_tensor```: Creates a new tensor based on a given name, shape, and values, and adds it to the tensor store. For the purposes of this server, tensors are vectors and matrices.\n- ```view_tensor```: Display the contents of a tensor from the store .\n- ```delete_tensor```: Deletes a tensor based on its name in the tensor store.\n\n#### Linear Algebra\n- ```add_matrices```: Adds two matrices with the provided names, if compatible.\n- ```subtract_matrices```: Subtracts two matrices with the provided names, if compatible.\n- ```multiply_matrices```: Multiplies two matrices with the provided names, if compatible.\n- ```scale_matrix```: Scales a matrix of the provided name by a certain factor, in-place by default.\n- ```matrix_inverse```: Computes the inverse of the matrix with the provided name.\n- ```transpose```: Computes the transpose of the inverse of the matrix of the provided name.\n- ```determinant```: Computes the determinant of the matrix of the provided name.\n- ```rank```: Computes the rank (number of pivots) of the matrix of the provided name.\n- ```compute_eigen```: Calculates the eigenvectors and eigenvalues of the matrix of the provided name.\n- ```qr_decompose```: Computes the QR factorization of the matrix of the provided name. The columns of Q are an orthonormal basis for the image of the matrix, and R is upper triangular.\n- ```svd_decompose```: Computes the Singular Value Decomposition of the matrix of the provided name.\n- ```find_orthonormal_basis```: Finds an orthonormal basis for the matrix of the provided name. The vectors returned are all pair-wise orthogonal and are of unit length.\n- ```change_basis```: Computes the matrix of the provided name in the new basis.\n\n#### Vector Calculus\n- ```vector_project```: Projects a vector in the tensor store to the specified vector in the same vector space\n- ```vector_dot_product```: Computes the dot product of two vectors in the tensor stores based on their provided names.\n- ```vector_cross_product```: Computes the cross product of two vectors in the tensor stores based on their provided names.\n- ```gradient```: Computes the gradient of a multivariable function based on the input function. Example call: ```gradient(\"x^2 + 2xyz + zy^3\")```. Do NOT include the function name (like f(x, y, z) = ...`).\n- ```curl```: Computes the curl of a vector field based on the input vector field. The input string must be formatted as a python list. Example call: ```curl(\"[3xy, 2z^4, 2y]\"\")```.\n- ```divergence```Computes the divergence of a vector field based on the input vector field. The input string must be formatted as a python list. Example call: ```divergence(\"[3xy, 2z^4, 2y]\"\")```.\n- ```laplacian```Computes the laplacian of a scalar function (as the divergence of the gradient) or a vector field (where a component-wise laplacian is computed). If a scalar function is the input, it must be input in the same format as in the ```gradient``` tool. If the input is a vector field, it must be input in the same manner as the ```curl/divergence``` tools.\n- ```directional_deriv```: Computes the directional derivative of a function in a given direction ```u``` By default, the tool normalizes ```u``` before computing the directional derivative, as specified by the ```unit``` parameter.\n\n#### Visualization\n- ```plot_vector_field```: Plots a vector field (specified in the same format as in the curl/divergence functions). Currently, only 3d vector fields are supported. A 2d png perspective image of the vector field is returned. By default, the bounds of the graph are from -1 to 1 on each axis.\n- ```plot_function```: Plots a function in 2d or 3d (based on the input variables), specified in the same format as in the ```gradient``` tool. Only the variables x and y can be used.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Create, view, and delete tensors (vectors and matrices) in a tensor store",
        "Perform matrix operations including addition, subtraction, multiplication, scaling, inversion, transposition, determinant, rank, eigen computation, QR and SVD decomposition",
        "Find orthonormal bases and change matrix bases",
        "Compute vector calculus operations such as vector projection, dot product, cross product, gradient, curl, divergence, laplacian, and directional derivatives",
        "Visualize vector fields and mathematical functions in 2D and 3D plots"
      ],
      "limitations": [
        "Visualization supports only 3D vector fields and 2D/3D functions with variables limited to x and y",
        "Input formats for vector calculus functions require specific string or list formatting",
        "Tensor operations are limited to vectors and matrices; no higher-dimensional tensors mentioned",
        "No mention of handling extremely large datasets or performance constraints"
      ],
      "requirements": [
        "Smithery API key for authentication",
        "Node.js environment to run the server via npx and smithery CLI",
        "Claude Desktop or Cursor client configured to connect to the MCP server"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides detailed installation instructions, comprehensive tool descriptions with usage examples, and mentions requirements and some limitations, making it an excellent resource.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/aman-amith-shastry-scientific-computation-mcp-badge.png)](https://mseep.ai/app/aman-amith-shastry-scientific-computation-mcp)\n\n# Scientific Computation MCP\n\n[![smithery badge](https://smithery.ai/badge/@Aman-Amith-Shastry/scientific_computation_mcp)](https://smithery.ai/server/@Aman-Amith-Shastry/scientific_computation_mcp)\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/5927ad38-70f6-4f5b-9778-e61ec902d735)\n\n[![MCP Badge](https://lobehub.com/badge/mcp-full/aman-amith-shastry-scientific_computation_mcp)](https://lobehub.com/mcp/aman-amith-shastry-scientific_computation_mcp)\n\n## Installation Guide\n\n### Claude Desktop\n\nOpen Claude Desktop's configuration file (claude_desktop_config.json) and add the following:\n\n- Mac/Linux: \n```json\n{\n  \"mcpServers\": {\n    \"numpy_mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@Aman-Amith-Shastry/scientific_computation_mcp\",\n        \"--key\",\n        \"<YOUR_SMITHERY_API_KEY>\"\n      ]\n    }\n  }\n}\n```\n\n- Windows:\n```json\n{\n  \"mcpServers\": {\n    \"numpy_mcp\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@Aman-Amith-Shastry/scientific_computation_mcp\",\n        \"--key\",\n        \"<YOUR_SMITHERY_API_KEY>\"\n      ]\n    }\n  }\n}\n```\n\nOr alternatively, run the following command:\n```commandline\nnpx -y @smithery/cli@latest install @Aman-Amith-Shastry/scientific_computation_mcp --client claude --key <YOUR_SMITHERY_API_KEY>\n```\n\nRestart Claude to load the server properly\n\n### Cursor\n\nIf you prefer to access the server through Cursor instead, then run the following command:\n```commandline\nnpx -y @smithery/cli@latest install @Aman-Amith-Shastry/scientific_computation_mcp --client cursor --key <YOUR_SMITHERY_API_KEY>\n```\n\n## Components of the Server\n\n### Tools\n\n#### Tensor storage\n- ```create_tensor```: Creates a new tensor based on a given name, shape, an",
        "start_pos": 0,
        "end_pos": 2048,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "253d9186b34d8c34"
      },
      {
        "chunk_id": 1,
        "text": "putation_mcp --client cursor --key <YOUR_SMITHERY_API_KEY>\n```\n\n## Components of the Server\n\n### Tools\n\n#### Tensor storage\n- ```create_tensor```: Creates a new tensor based on a given name, shape, and values, and adds it to the tensor store. For the purposes of this server, tensors are vectors and matrices.\n- ```view_tensor```: Display the contents of a tensor from the store .\n- ```delete_tensor```: Deletes a tensor based on its name in the tensor store.\n\n#### Linear Algebra\n- ```add_matrices```: Adds two matrices with the provided names, if compatible.\n- ```subtract_matrices```: Subtracts two matrices with the provided names, if compatible.\n- ```multiply_matrices```: Multiplies two matrices with the provided names, if compatible.\n- ```scale_matrix```: Scales a matrix of the provided name by a certain factor, in-place by default.\n- ```matrix_inverse```: Computes the inverse of the matrix with the provided name.\n- ```transpose```: Computes the transpose of the inverse of the matrix of the provided name.\n- ```determinant```: Computes the determinant of the matrix of the provided name.\n- ```rank```: Computes the rank (number of pivots) of the matrix of the provided name.\n- ```compute_eigen```: Calculates the eigenvectors and eigenvalues of the matrix of the provided name.\n- ```qr_decompose```: Computes the QR factorization of the matrix of the provided name. The columns of Q are an orthonormal basis for the image of the matrix, and R is upper triangular.\n- ```svd_decompose```: Computes the Singular Value Decomposition of the matrix of the provided name.\n- ```find_orthonormal_basis```: Finds an orthonormal basis for the matrix of the provided name. The vectors returned are all pair-wise orthogonal and are of unit length.\n- ```change_basis```: Computes the matrix of the provided name in the new basis.",
        "start_pos": 1848,
        "end_pos": 3676,
        "token_count_estimate": 457,
        "source_type": "readme",
        "agent_id": "253d9186b34d8c34"
      },
      {
        "chunk_id": 2,
        "text": "us\n- ```vector_project```: Projects a vector in the tensor store to the specified vector in the same vector space\n- ```vector_dot_product```: Computes the dot product of two vectors in the tensor stores based on their provided names.\n- ```vector_cross_product```: Computes the cross product of two vectors in the tensor stores based on their provided names.\n- ```gradient```: Computes the gradient of a multivariable function based on the input function. Example call: ```gradient(\"x^2 + 2xyz + zy^3\")```. Do NOT include the function name (like f(x, y, z) = ...`).\n- ```curl```: Computes the curl of a vector field based on the input vector field. The input string must be formatted as a python list. Example call: ```curl(\"[3xy, 2z^4, 2y]\"\")```.\n- ```divergence```Computes the divergence of a vector field based on the input vector field. The input string must be formatted as a python list. Example call: ```divergence(\"[3xy, 2z^4, 2y]\"\")```.\n- ```laplacian```Computes the laplacian of a scalar function (as the divergence of the gradient) or a vector field (where a component-wise laplacian is computed). If a scalar function is the input, it must be input in the same format as in the ```gradient``` tool. If the input is a vector field, it must be input in the same manner as the ```curl/divergence``` tools.\n- ```directional_deriv```: Computes the directional derivative of a function in a given direction ```u``` By default, the tool normalizes ```u``` before computing the directional derivative, as specified by the ```unit``` parameter.\n\n#### Visualization\n- ```plot_vector_field```: Plots a vector field (specified in the same format as in the curl/divergence functions). Currently, only 3d vector fields are supported. A 2d png perspective image of the vector field is returned. By default, the bounds of the graph are from -1 to 1 on each axis.\n- ```plot_function```: Plots a function in 2d or 3d (based on the input variables), specified in the same format as in the ```gradient``` tool. Only the variables x and y can be used.",
        "start_pos": 3696,
        "end_pos": 5738,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "253d9186b34d8c34"
      },
      {
        "chunk_id": 3,
        "text": "1 on each axis.\n- ```plot_function```: Plots a function in 2d or 3d (based on the input variables), specified in the same format as in the ```gradient``` tool. Only the variables x and y can be used.",
        "start_pos": 5538,
        "end_pos": 5738,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "253d9186b34d8c34"
      }
    ]
  },
  {
    "agent_id": "f7566f9ad17f4134",
    "name": "ai.smithery/Artin0123-gemini-image-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/Artin0123/gemini-vision-mcp",
    "description": "Analyze images and videos with Gemini to get fast, reliable visual insights. Handle content from U‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "analyze"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T17:47:25.005282Z",
    "indexed_at": "2026-02-18T04:03:06.939174",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# image-mcp-server-gemini\n\n[![smithery badge](https://smithery.ai/badge/@Artin0123/gemini-image-mcp-server)](https://smithery.ai/server/@Artin0123/gemini-image-mcp-server)\n\n> This is remote server, use [local version](https://github.com/Artin0123/gemini-vision-mcp/tree/local) for local images and videos.\n\n## Features\n\n- Analyze one or more image URLs with a single tool call.\n- Analyze YouTube videos without downloading files locally.\n- Supply an API key and optionally override the Gemini model via environment variables.\n- **File size limit**: Images are limited to 16 MB to ensure fast processing.\n- **YouTube videos**: No size limit as they are streamed directly by Gemini API.\n\n## Installation\n\n### Installing via Smithery\n\nInstall the server in Claude Desktop:\n\n```bash\nnpx -y @smithery/cli install @Artin0123/gemini-image-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Artin0123/gemini-vision-mcp.git\ncd gemini-vision-mcp\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript to dist/\nnpm run build\n```\n\n## Configuration\n\nCreate a Gemini API key in [Google AI Studio](https://aistudio.google.com/app/apikey) and provide `GEMINI_API_KEY` to the server.\n\n```json\n{\n  \"mcpServers\": {\n    \"gemini-media\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/gemini-vision-mcp/dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\",\n        \"GEMINI_MODEL\": \"models/gemini-flash-lite-latest\"\n      }\n    }\n  }\n}\n```\n\nIf no key is supplied, the server can still start (handy for automated scans), but any tool invocation will return a configuration error until a valid API key is configured.\n\n### Model override\n\nThe server defaults to `models/gemini-flash-lite-latest`. Override it by either:\n\n> Setting the `GEMINI_MODEL` environment variable, or Providing `modelName` in the Smithery/SDK configuration schema.\n\n## Available tools\n\n- `analyze_image`: Analyze one or more image URLs. **Maximum file size: 16 MB per image.**\n- `analyze_youtube_video`: Analyze a YouTube video from URL. No size limit.\n\nImage URLs are downloaded and processed with a 16 MB size limit to ensure fast response times. Files exceeding this limit will result in an error message indicating the actual file size.\n\nYouTube videos are streamed directly by Gemini API without downloading, so there is no size restriction.\n\n### Prompt examples\n\n```\nPlease analyze this product photo: https://teimg-bgr.pages.dev/file/mvYT6KeF.webp\n```\n\n```\nExtract the main talking points from this clip: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n```\n\n## Development\n\n```bash\nnpm install\nnpm test\nnpm run build\n```\n\nThe test suite exercises URL forwarding, MIME handling, and configuration fallbacks.\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Analyze one or more image URLs with a single tool call",
        "Analyze YouTube videos without downloading files locally",
        "Stream YouTube videos directly via Gemini API for analysis",
        "Allow API key configuration for authenticated access",
        "Override the Gemini model via environment variables or configuration",
        "Handle image files up to 16 MB in size for fast processing",
        "Provide error messages for images exceeding the size limit"
      ],
      "limitations": [
        "Image files are limited to 16 MB each",
        "Tool invocation returns configuration error if no valid API key is supplied"
      ],
      "requirements": [
        "Gemini API key from Google AI Studio",
        "Node.js environment to run the server",
        "Optional environment variable GEMINI_MODEL to override default model"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, configuration details, usage examples, tool descriptions, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# image-mcp-server-gemini\n\n[![smithery badge](https://smithery.ai/badge/@Artin0123/gemini-image-mcp-server)](https://smithery.ai/server/@Artin0123/gemini-image-mcp-server)\n\n> This is remote server, use [local version](https://github.com/Artin0123/gemini-vision-mcp/tree/local) for local images and videos.\n\n## Features\n\n- Analyze one or more image URLs with a single tool call.\n- Analyze YouTube videos without downloading files locally.\n- Supply an API key and optionally override the Gemini model via environment variables.\n- **File size limit**: Images are limited to 16 MB to ensure fast processing.\n- **YouTube videos**: No size limit as they are streamed directly by Gemini API.\n\n## Installation\n\n### Installing via Smithery\n\nInstall the server in Claude Desktop:\n\n```bash\nnpx -y @smithery/cli install @Artin0123/gemini-image-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Artin0123/gemini-vision-mcp.git\ncd gemini-vision-mcp\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript to dist/\nnpm run build\n```\n\n## Configuration\n\nCreate a Gemini API key in [Google AI Studio](https://aistudio.google.com/app/apikey) and provide `GEMINI_API_KEY` to the server.\n\n```json\n{\n  \"mcpServers\": {\n    \"gemini-media\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/gemini-vision-mcp/dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\",\n        \"GEMINI_MODEL\": \"models/gemini-flash-lite-latest\"\n      }\n    }\n  }\n}\n```\n\nIf no key is supplied, the server can still start (handy for automated scans), but any tool invocation will return a configuration error until a valid API key is configured.\n\n### Model override\n\nThe server defaults to `models/gemini-flash-lite-latest`. Override it by either:\n\n> Setting the `GEMINI_MODEL` environment variable, or Providing `modelName` in the Smithery/SDK configuration schema.\n\n## Available tools\n\n- `analyze_image`: Analyze one or more image URLs.",
        "start_pos": 0,
        "end_pos": 1992,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "f7566f9ad17f4134"
      },
      {
        "chunk_id": 1,
        "text": "y either:\n\n> Setting the `GEMINI_MODEL` environment variable, or Providing `modelName` in the Smithery/SDK configuration schema.\n\n## Available tools\n\n- `analyze_image`: Analyze one or more image URLs. **Maximum file size: 16 MB per image.**\n- `analyze_youtube_video`: Analyze a YouTube video from URL. No size limit.\n\nImage URLs are downloaded and processed with a 16 MB size limit to ensure fast response times. Files exceeding this limit will result in an error message indicating the actual file size.\n\nYouTube videos are streamed directly by Gemini API without downloading, so there is no size restriction.\n\n### Prompt examples\n\n```\nPlease analyze this product photo: https://teimg-bgr.pages.dev/file/mvYT6KeF.webp\n```\n\n```\nExtract the main talking points from this clip: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n```\n\n## Development\n\n```bash\nnpm install\nnpm test\nnpm run build\n```\n\nThe test suite exercises URL forwarding, MIME handling, and configuration fallbacks.\n\n## License\n\nMIT",
        "start_pos": 1792,
        "end_pos": 2783,
        "token_count_estimate": 247,
        "source_type": "readme",
        "agent_id": "f7566f9ad17f4134"
      }
    ]
  },
  {
    "agent_id": "f7566f9ad17f4134",
    "name": "ai.smithery/Artin0123-gemini-image-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/Artin0123/gemini-image-mcp-server",
    "description": "Analyze images and videos with Gemini to get fast, reliable visual insights. Handle content from U‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "analyze"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-06T07:34:28.779387Z",
    "indexed_at": "2026-02-18T04:03:08.104966",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# image-mcp-server-gemini\n\n[![smithery badge](https://smithery.ai/badge/@Artin0123/gemini-image-mcp-server)](https://smithery.ai/server/@Artin0123/gemini-image-mcp-server)\n\n> This is remote server, use [local version](https://github.com/Artin0123/gemini-vision-mcp/tree/local) for local images and videos.\n\n## Features\n\n- Analyze one or more image URLs with a single tool call.\n- Analyze YouTube videos without downloading files locally.\n- Supply an API key and optionally override the Gemini model via environment variables.\n- **File size limit**: Images are limited to 16 MB to ensure fast processing.\n- **YouTube videos**: No size limit as they are streamed directly by Gemini API.\n\n## Installation\n\n### Installing via Smithery\n\nInstall the server in Claude Desktop:\n\n```bash\nnpx -y @smithery/cli install @Artin0123/gemini-image-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Artin0123/gemini-vision-mcp.git\ncd gemini-vision-mcp\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript to dist/\nnpm run build\n```\n\n## Configuration\n\nCreate a Gemini API key in [Google AI Studio](https://aistudio.google.com/app/apikey) and provide `GEMINI_API_KEY` to the server.\n\n```json\n{\n  \"mcpServers\": {\n    \"gemini-media\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/gemini-vision-mcp/dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\",\n        \"GEMINI_MODEL\": \"models/gemini-flash-lite-latest\"\n      }\n    }\n  }\n}\n```\n\nIf no key is supplied, the server can still start (handy for automated scans), but any tool invocation will return a configuration error until a valid API key is configured.\n\n### Model override\n\nThe server defaults to `models/gemini-flash-lite-latest`. Override it by either:\n\n> Setting the `GEMINI_MODEL` environment variable, or Providing `modelName` in the Smithery/SDK configuration schema.\n\n## Available tools\n\n- `analyze_image`: Analyze one or more image URLs. **Maximum file size: 16 MB per image.**\n- `analyze_youtube_video`: Analyze a YouTube video from URL. No size limit.\n\nImage URLs are downloaded and processed with a 16 MB size limit to ensure fast response times. Files exceeding this limit will result in an error message indicating the actual file size.\n\nYouTube videos are streamed directly by Gemini API without downloading, so there is no size restriction.\n\n### Prompt examples\n\n```\nPlease analyze this product photo: https://teimg-bgr.pages.dev/file/mvYT6KeF.webp\n```\n\n```\nExtract the main talking points from this clip: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n```\n\n## Development\n\n```bash\nnpm install\nnpm test\nnpm run build\n```\n\nThe test suite exercises URL forwarding, MIME handling, and configuration fallbacks.\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Analyze one or more image URLs with a single tool call",
        "Analyze YouTube videos without downloading files locally",
        "Stream YouTube videos directly via Gemini API for analysis without size limits",
        "Allow API key configuration for authenticated access to Gemini services",
        "Override the default Gemini model via environment variables or configuration",
        "Provide error messages for images exceeding the 16 MB file size limit"
      ],
      "limitations": [
        "Image files are limited to 16 MB each to ensure fast processing",
        "Tool invocations return configuration errors if no valid API key is supplied"
      ],
      "requirements": [
        "A Gemini API key from Google AI Studio is required for tool usage",
        "Node.js environment to run the server",
        "Dependencies installed via npm",
        "Optional environment variable GEMINI_MODEL to override the default model"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, configuration details, usage examples, tool descriptions, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# image-mcp-server-gemini\n\n[![smithery badge](https://smithery.ai/badge/@Artin0123/gemini-image-mcp-server)](https://smithery.ai/server/@Artin0123/gemini-image-mcp-server)\n\n> This is remote server, use [local version](https://github.com/Artin0123/gemini-vision-mcp/tree/local) for local images and videos.\n\n## Features\n\n- Analyze one or more image URLs with a single tool call.\n- Analyze YouTube videos without downloading files locally.\n- Supply an API key and optionally override the Gemini model via environment variables.\n- **File size limit**: Images are limited to 16 MB to ensure fast processing.\n- **YouTube videos**: No size limit as they are streamed directly by Gemini API.\n\n## Installation\n\n### Installing via Smithery\n\nInstall the server in Claude Desktop:\n\n```bash\nnpx -y @smithery/cli install @Artin0123/gemini-image-mcp-server --client claude\n```\n\n### Manual Installation\n\n```bash\n# Clone the repository\ngit clone https://github.com/Artin0123/gemini-vision-mcp.git\ncd gemini-vision-mcp\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript to dist/\nnpm run build\n```\n\n## Configuration\n\nCreate a Gemini API key in [Google AI Studio](https://aistudio.google.com/app/apikey) and provide `GEMINI_API_KEY` to the server.\n\n```json\n{\n  \"mcpServers\": {\n    \"gemini-media\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/gemini-vision-mcp/dist/index.js\"],\n      \"env\": {\n        \"GEMINI_API_KEY\": \"your_api_key_here\",\n        \"GEMINI_MODEL\": \"models/gemini-flash-lite-latest\"\n      }\n    }\n  }\n}\n```\n\nIf no key is supplied, the server can still start (handy for automated scans), but any tool invocation will return a configuration error until a valid API key is configured.\n\n### Model override\n\nThe server defaults to `models/gemini-flash-lite-latest`. Override it by either:\n\n> Setting the `GEMINI_MODEL` environment variable, or Providing `modelName` in the Smithery/SDK configuration schema.\n\n## Available tools\n\n- `analyze_image`: Analyze one or more image URLs.",
        "start_pos": 0,
        "end_pos": 1992,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "f7566f9ad17f4134"
      },
      {
        "chunk_id": 1,
        "text": "y either:\n\n> Setting the `GEMINI_MODEL` environment variable, or Providing `modelName` in the Smithery/SDK configuration schema.\n\n## Available tools\n\n- `analyze_image`: Analyze one or more image URLs. **Maximum file size: 16 MB per image.**\n- `analyze_youtube_video`: Analyze a YouTube video from URL. No size limit.\n\nImage URLs are downloaded and processed with a 16 MB size limit to ensure fast response times. Files exceeding this limit will result in an error message indicating the actual file size.\n\nYouTube videos are streamed directly by Gemini API without downloading, so there is no size restriction.\n\n### Prompt examples\n\n```\nPlease analyze this product photo: https://teimg-bgr.pages.dev/file/mvYT6KeF.webp\n```\n\n```\nExtract the main talking points from this clip: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n```\n\n## Development\n\n```bash\nnpm install\nnpm test\nnpm run build\n```\n\nThe test suite exercises URL forwarding, MIME handling, and configuration fallbacks.\n\n## License\n\nMIT",
        "start_pos": 1792,
        "end_pos": 2783,
        "token_count_estimate": 247,
        "source_type": "readme",
        "agent_id": "f7566f9ad17f4134"
      }
    ]
  },
  {
    "agent_id": "b0f699b170ca7c81",
    "name": "ai.smithery/BadRooBot-my_test_mcp",
    "source": "mcp",
    "source_url": "https://github.com/BadRooBot/python_mcp",
    "description": "Get current weather for any city and create images from your prompts. Streamline planning, reports‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-14T14:25:46.094496Z",
    "indexed_at": "2026-02-18T04:03:09.549433",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Get current weather for any city",
        "Create images from user prompts",
        "Streamline planning",
        "Generate reports"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "0cda653c2ec7469d",
    "name": "ai.smithery/BadRooBot-test_m",
    "source": "mcp",
    "source_url": "https://github.com/BadRooBot/test_m",
    "description": "Send quick greetings, scrape website content, and generate text or images on demand. Perform web s‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T14:41:53.772797Z",
    "indexed_at": "2026-02-18T04:03:14.230539",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# test\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Run with Docker\n\nBuild the image (from the project root):\n\n```bash\ndocker build -t hello-server .\n```\n\nRun the container:\n\n```bash\ndocker run --rm -p 8081:8081 --shm-size=1g hello-server\n```\n\nNotes:\n\n- The server listens on `0.0.0.0:8081` and serves the MCP endpoint at `/mcp`.\n- `--shm-size=1g` improves browser stability for Playwright/Firefox-based scraping.\n\nYou can now connect an MCP client to:\n\n```\nhttp://localhost:8081/mcp\n```\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server built with Smithery",
        "Serve the MCP endpoint at /mcp",
        "Provide an example tool that responds to commands like 'Say hello to John'",
        "Support interactive testing via a playground",
        "Run the server locally or within a Docker container"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key",
        "Docker for containerized deployment (optional)",
        "GitHub account for deployment"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "The documentation includes installation instructions, usage examples, and deployment steps but lacks detailed descriptions of server capabilities and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# test\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Run with Docker\n\nBuild the image (from the project root):\n\n```bash\ndocker build -t hello-server .\n```\n\nRun the container:\n\n```bash\ndocker run --rm -p 8081:8081 --shm-size=1g hello-server\n```\n\nNotes:\n\n- The server listens on `0.0.0.0:8081` and serves the MCP endpoint at `/mcp`.\n- `--shm-size=1g` improves browser stability for Playwright/Firefox-based scraping.\n\nYou can now connect an MCP client to:\n\n```\nhttp://localhost:8081/mcp\n```\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 1351,
        "token_count_estimate": 337,
        "source_type": "readme",
        "agent_id": "0cda653c2ec7469d"
      }
    ]
  },
  {
    "agent_id": "25d85509b50e052f",
    "name": "ai.smithery/BigVik193-reddit-ads-mcp",
    "source": "mcp",
    "source_url": "https://github.com/BigVik193/reddit-ads-mcp",
    "description": "Manage Reddit advertising across accounts, campaigns, ad groups, posts, and ads. List accounts, fu‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T22:00:51.726311Z",
    "indexed_at": "2026-02-18T04:03:15.446056",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage Reddit advertising across accounts",
        "Manage Reddit advertising across campaigns",
        "Manage Reddit advertising across ad groups",
        "Manage Reddit advertising across posts",
        "Manage Reddit advertising across ads",
        "List Reddit advertising accounts"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of managing Reddit advertising entities but lacks detailed examples, structure, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "df8ffb11c18019d0",
    "name": "ai.smithery/BigVik193-reddit-ads-mcp-api",
    "source": "mcp",
    "source_url": "https://github.com/BigVik193/reddit-ads-mcp-api",
    "description": "Manage Reddit advertising end to end across accounts, funding methods, campaigns, ad groups, and a‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T22:40:30.852738Z",
    "indexed_at": "2026-02-18T04:03:17.492906",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage Reddit advertising across accounts",
        "Manage funding methods for Reddit ads",
        "Manage campaigns for Reddit ads",
        "Manage ad groups within Reddit campaigns"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of managing Reddit ads across multiple entities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "a39b47d2ff697794",
    "name": "ai.smithery/BigVik193-reddit-user-mcp",
    "source": "mcp",
    "source_url": "https://github.com/BigVik193/reddit-user-mcp",
    "description": "Browse and manage Reddit posts, comments, and threads. Fetch user activity, explore hot/new/rising‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T21:19:18.64908Z",
    "indexed_at": "2026-02-18T04:03:19.783524",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Reddit User MCP Server\n\nA Model Context Protocol (MCP) server that provides access to Reddit posts, comments, and interactions through your Reddable account.\n\n## Features\n\n- ‚úÖ Fetch user Reddit posts and comments\n- ‚úÖ Get comments for specific posts\n- ‚úÖ Hide Reddit comments\n- ‚úÖ Reply to comments and posts\n- ‚úÖ Secure API key authentication\n- ‚úÖ Built with TypeScript and Smithery\n\n## Installation\n\n### For Claude Desktop\n\nAdd this configuration to your MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"reddit-user\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@reddable/reddit-user-mcp\"],\n      \"env\": {\n        \"REDDABLE_API_KEY\": \"your_reddable_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n## Getting Your API Key\n\n1. Go to [Reddable Dashboard](https://reddable.com/dashboard)\n2. Connect your Reddit account\n3. Scroll to \"MCP Integration\" section\n4. Copy your API key from the setup instructions\n\n## Available Tools\n\n- `get_user_posts` - Fetch Reddit posts from a user\n- `get_user_comments` - Fetch Reddit comments from a user  \n- `get_post_comments` - Get all comments on a specific post\n- `hide_comment` - Hide a Reddit comment from your view\n- `reply_to_comment` - Reply to a specific comment\n- `post_comment` - Post a new comment on a post\n\n## Development\n\n```bash\nnpm install\nnpm run dev\n```\n\nThis will start the development server with ngrok tunneling for testing.\n\n## Security\n\n- Uses long-term API keys (no password exposure)\n- All Reddit credentials stored securely in Reddable\n- API requests proxied through Reddable servers\n- No direct Reddit app credentials needed\n\n## Support\n\nFor issues or questions, visit [Reddable Support](https://reddable.com/support)."
    },
    "llm_extracted": {
      "capabilities": [
        "Fetch user Reddit posts",
        "Fetch user Reddit comments",
        "Get comments for specific Reddit posts",
        "Hide Reddit comments",
        "Reply to Reddit comments",
        "Post new comments on Reddit posts",
        "Authenticate securely using API keys"
      ],
      "limitations": [],
      "requirements": [
        "Reddable API key for authentication",
        "Connected Reddit account via Reddable",
        "Node.js environment to run with npx",
        "Configuration in MCP settings for Claude Desktop"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, usage examples, detailed tool descriptions, security notes, and requirements, providing a comprehensive overview.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Reddit User MCP Server\n\nA Model Context Protocol (MCP) server that provides access to Reddit posts, comments, and interactions through your Reddable account.\n\n## Features\n\n- ‚úÖ Fetch user Reddit posts and comments\n- ‚úÖ Get comments for specific posts\n- ‚úÖ Hide Reddit comments\n- ‚úÖ Reply to comments and posts\n- ‚úÖ Secure API key authentication\n- ‚úÖ Built with TypeScript and Smithery\n\n## Installation\n\n### For Claude Desktop\n\nAdd this configuration to your MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"reddit-user\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@reddable/reddit-user-mcp\"],\n      \"env\": {\n        \"REDDABLE_API_KEY\": \"your_reddable_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n## Getting Your API Key\n\n1. Go to [Reddable Dashboard](https://reddable.com/dashboard)\n2. Connect your Reddit account\n3. Scroll to \"MCP Integration\" section\n4. Copy your API key from the setup instructions\n\n## Available Tools\n\n- `get_user_posts` - Fetch Reddit posts from a user\n- `get_user_comments` - Fetch Reddit comments from a user  \n- `get_post_comments` - Get all comments on a specific post\n- `hide_comment` - Hide a Reddit comment from your view\n- `reply_to_comment` - Reply to a specific comment\n- `post_comment` - Post a new comment on a post\n\n## Development\n\n```bash\nnpm install\nnpm run dev\n```\n\nThis will start the development server with ngrok tunneling for testing.\n\n## Security\n\n- Uses long-term API keys (no password exposure)\n- All Reddit credentials stored securely in Reddable\n- API requests proxied through Reddable servers\n- No direct Reddit app credentials needed\n\n## Support\n\nFor issues or questions, visit [Reddable Support](https://reddable.com/support).",
        "start_pos": 0,
        "end_pos": 1670,
        "token_count_estimate": 417,
        "source_type": "readme",
        "agent_id": "a39b47d2ff697794"
      }
    ]
  },
  {
    "agent_id": "2813f2bb66869f2c",
    "name": "ai.smithery/BowenXU0126-aistudio_hw3",
    "source": "mcp",
    "source_url": "https://github.com/BowenXU0126/aistudio_hw3",
    "description": "Send personalized greetings with optional pirate flair. Compose friendly salutations for any name‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T00:54:01.690641Z",
    "indexed_at": "2026-02-18T04:03:21.131874",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# hw3\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server using Smithery CLI",
        "Test server interactions interactively via playground",
        "Add or update server capabilities in Python code"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key",
        "Python environment with Smithery CLI installed",
        "GitHub account for deployment"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "The documentation provides installation steps, usage examples, and deployment instructions but lacks detailed descriptions of server capabilities and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# hw3\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 981,
        "token_count_estimate": 245,
        "source_type": "readme",
        "agent_id": "2813f2bb66869f2c"
      }
    ]
  },
  {
    "agent_id": "4ccadffa2e258dbc",
    "name": "ai.smithery/ChiR24-unreal_mcp",
    "source": "mcp",
    "source_url": "https://github.com/ChiR24/Unreal_mcp",
    "description": "Control Unreal Engine to browse assets, import content, and manage levels and sequences. Automate‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-10-03T07:10:16.958159Z",
    "indexed_at": "2026-02-18T04:03:22.700957",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Unreal Engine MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![NPM Package](https://img.shields.io/npm/v/unreal-engine-mcp-server)](https://www.npmjs.com/package/unreal-engine-mcp-server)\n[![MCP SDK](https://img.shields.io/badge/MCP%20SDK-TypeScript-blue)](https://github.com/modelcontextprotocol/sdk)\n[![Unreal Engine](https://img.shields.io/badge/Unreal%20Engine-5.0--5.7-orange)](https://www.unrealengine.com/)\n[![MCP Registry](https://img.shields.io/badge/MCP%20Registry-Published-green)](https://registry.modelcontextprotocol.io/)\n[![Project Board](https://img.shields.io/badge/Project-Roadmap-blueviolet?logo=github)](https://github.com/users/ChiR24/projects/3)\n[![Discussions](https://img.shields.io/badge/Discussions-Join-brightgreen?logo=github)](https://github.com/ChiR24/Unreal_mcp/discussions)\n\nA comprehensive Model Context Protocol (MCP) server that enables AI assistants to control Unreal Engine through a native C++ Automation Bridge plugin. Built with TypeScript, C++, and Rust (WebAssembly).\n\n---\n\n## Table of Contents\n\n- [Features](#features)\n- [Getting Started](#getting-started)\n- [Configuration](#configuration)\n- [Available Tools](#available-tools)\n- [WebAssembly Acceleration](#webassembly-acceleration)\n- [GraphQL API](#graphql-api)\n- [Docker](#docker)\n- [Documentation](#documentation)\n- [Community](#community)\n- [Development](#development)\n- [Contributing](#contributing)\n\n---\n\n## Features\n\n| Category | Capabilities |\n|----------|-------------|\n| **Asset Management** | Browse, import, duplicate, rename, delete assets; create materials |\n| **Actor Control** | Spawn, delete, transform, physics, tags, components |\n| **Editor Control** | PIE sessions, camera, viewport, screenshots, bookmarks |\n| **Level Management** | Load/save levels, streaming, World Partition, data layers |\n| **Animation & Physics** | Animation BPs, state machines, ragdolls, vehicles, constraints |\n| **Visual Effects** | Niagara particles, GPU simulations, procedural effects, debug shapes |\n| **Sequencer** | Cinematics, timeline control, camera animations, keyframes |\n| **Graph Editing** | Blueprint, Niagara, Material, and Behavior Tree graph manipulation |\n| **Audio** | Sound cues, audio components, sound mixes, ambient sounds |\n| **System** | Console commands, UBT, tests, logs, project settings, CVars |\n\n### Architecture\n\n- **Native C++ Automation** ‚Äî All operations route through the MCP Automation Bridge plugin\n- **Dynamic Type Discovery** ‚Äî Runtime introspection for lights, debug shapes, and sequencer tracks\n- **Graceful Degradation** ‚Äî Server starts even without an active Unreal connection\n- **On-Demand Connection** ‚Äî Retries automation handshakes with exponential backoff\n- **Command Safety** ‚Äî Blocks dangerous console commands with pattern-based validation\n- **Asset Caching** ‚Äî 10-second TTL for improved performance\n- **Metrics Rate Limiting** ‚Äî Per-IP rate limiting (60 req/min) on Prometheus endpoint\n- **Centralized Configuration** ‚Äî Unified class aliases and type definitions\n\n---\n\n## Getting Started\n\n### Prerequisites\n\n- **Node.js** 18+\n- **Unreal Engine** 5.0‚Äì5.7\n\n### Step 1: Install MCP Server\n\n**Option A: NPX (Recommended)**\n```bash\nnpx unreal-engine-mcp-server\n```\n\n**Option B: Clone & Build**\n```bash\ngit clone https://github.com/ChiR24/Unreal_mcp.git\ncd Unreal_mcp\nnpm install\nnpm run build\nnode dist/cli.js\n```\n\n### Step 2: Install Unreal Plugin\n\nThe MCP Automation Bridge plugin is included at `Unreal_mcp/plugins/McpAutomationBridge`.\n\n**Method 1: Copy Folder**\n```\nCopy:  Unreal_mcp/plugins/McpAutomationBridge/\nTo:    YourUnrealProject/Plugins/McpAutomationBridge/\n```\nRegenerate project files after copying.\n\n**Method 2: Add in Editor**\n1. Open Unreal Editor ‚Üí **Edit ‚Üí Plugins**\n2. Click **\"Add\"** ‚Üí Browse to `Unreal_mcp/plugins/`\n3. Select the `McpAutomationBridge` folder\n\n**Video Guide:**\n\nhttps://github.com/user-attachments/assets/d8b86ebc-4364-48c9-9781-de854bf3ef7d\n\n### Step 3: Enable Required Plugins\n\nEnable via **Edit ‚Üí Plugins**, then restart the editor:\n\n| Plugin | Required For |\n|--------|--------------|\n| **MCP Automation Bridge** | All automation operations |\n| **Editor Scripting Utilities** | Asset/Actor subsystem operations |\n| **Sequencer** | Sequencer tools |\n| **Level Sequence Editor** | `manage_sequence` operations |\n| **Control Rig** | `animation_physics` operations |\n| **Subobject Data Interface** | Blueprint components (UE 5.7+) |\n| **Geometry Script** | `manage_geometry` operations (procedural mesh) |\n\n### Step 4: Configure MCP Client\n\nAdd to your Claude Desktop / Cursor config file:\n\n**Using Clone/Build:**\n```json\n{\n  \"mcpServers\": {\n    \"unreal-engine\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/Unreal_mcp/dist/cli.js\"],\n      \"env\": {\n        \"UE_PROJECT_PATH\": \"C:/Path/To/YourProject\",\n        \"MCP_AUTOMATION_PORT\": \"8091\"\n      }\n    }\n  }\n}\n```\n\n**Using NPX:**\n```json\n{\n  \"mcpServers\": {\n    \"unreal-engine\": {\n      \"command\": \"npx\",\n      \"args\": [\"unreal-engine-mcp-server\"],\n      \"env\": {\n        \"UE_PROJECT_PATH\": \"C:/Path/To/YourProject\"\n      }\n    }\n  }\n}\n```\n\n---\n\n## Configuration\n\n### Environment Variables\n\n```env\n# Required\nUE_PROJECT_PATH=\"C:/Path/To/YourProject\"\n\n# Automation Bridge\nMCP_AUTOMATION_HOST=127.0.0.1\nMCP_AUTOMATION_PORT=8091\n\n# LAN Access (optional)\n# SECURITY: Set to true to allow binding to non-loopback addresses (e.g., 0.0.0.0)\n# Only enable if you understand the security implications.\nMCP_AUTOMATION_ALLOW_NON_LOOPBACK=false\n\n# Logging\nLOG_LEVEL=info  # debug | info | warn | error\n\n# Optional\nWASM_ENABLED=true\nMCP_AUTOMATION_REQUEST_TIMEOUT_MS=120000\nASSET_LIST_TTL_MS=10000\n```\n\n### LAN Access Configuration\n\nBy default, the automation bridge only binds to loopback addresses (127.0.0.1) for security. To enable access from other machines on your network:\n\n**TypeScript (MCP Server):**\n```env\nMCP_AUTOMATION_ALLOW_NON_LOOPBACK=true\nMCP_AUTOMATION_HOST=0.0.0.0\n```\n\n**Unreal Engine Plugin:**\n1. Go to **Edit ‚Üí Project Settings ‚Üí Plugins ‚Üí MCP Automation Bridge**\n2. Under **Security**, enable **\"Allow Non Loopback\"**\n3. Under **Connection**, set **\"Listen Host\"** to `0.0.0.0`\n4. Restart the editor\n\n‚ö†Ô∏è **Security Warning:** Enabling LAN access exposes the automation bridge to your local network. Only use on trusted networks with appropriate firewall rules.\n\n---\n\n## Available Tools\n\n| Tool | Description |\n|------|-------------|\n| `manage_asset` | Assets, Materials, Render Targets, Behavior Trees |\n| `control_actor` | Spawn, delete, transform, physics, tags |\n| `control_editor` | PIE, Camera, viewport, screenshots |\n| `manage_level` | Load/Save, World Partition, streaming |\n| `manage_lighting` | Spawn lights, GI, shadows, build lighting, **list_light_types** |\n| `manage_performance` | Profiling, optimization, scalability |\n| `animation_physics` | Animation BPs, Vehicles, Ragdolls, Control Rig, IK, Blend Spaces |\n| `manage_effect` | Niagara, Particles, Debug Shapes, Niagara authoring, GPU sim |\n| `manage_blueprint` | Create, SCS, Graph Editing, Node manipulation |\n| `build_environment` | Landscape, Foliage, Procedural |\n| `system_control` | UBT, Tests, Logs, Project Settings, CVars |\n| `manage_sequence` | Sequencer / Cinematics, **list_track_types** |\n| `inspect` | Object Introspection |\n| `manage_audio` | Audio Assets, Components, Sound Cues, MetaSounds, Attenuation |\n| `manage_behavior_tree` | Behavior Tree Graph Editing |\n| `manage_input` | Enhanced Input Actions & Contexts |\n| `manage_geometry` | Procedural mesh creation (Geometry Script) |\n| `manage_skeleton` | Skeleton, sockets, physics assets, cloth binding |\n| `manage_material_authoring` | Material creation, expressions, landscape layers |\n| `manage_texture` | Texture creation, modification, compression settings |\n| `manage_gas` | Gameplay Ability System: abilities, effects, attributes |\n| `manage_character` | Character creation, movement, advanced locomotion |\n| `manage_combat` | Weapons, projectiles, damage, melee combat |\n| `manage_ai` | AI controllers, EQS, perception, State Trees, Smart Objects |\n| `manage_inventory` | Items, equipment, loot tables, crafting |\n| `manage_interaction` | Interactables, destructibles, triggers |\n| `manage_widget_authoring` | UMG widget creation, layout, styling, animations |\n| `manage_networking` | Replication, RPCs, network prediction |\n| `manage_game_framework` | Game modes, game states, player controllers, match flow |\n| `manage_sessions` | Sessions, split-screen, LAN, voice chat |\n| `manage_level_structure` | Level creation, sublevels, World Partition, data layers, HLOD |\n| `manage_volumes` | Trigger volumes, blocking, physics, audio, navigation volumes |\n| `manage_navigation` | NavMesh settings, nav modifiers, nav links, smart links, pathfinding |\n\n### Supported Asset Types\n\nBlueprints ‚Ä¢ Materials ‚Ä¢ Textures ‚Ä¢ Static Meshes ‚Ä¢ Skeletal Meshes ‚Ä¢ Levels ‚Ä¢ Sounds ‚Ä¢ Particles ‚Ä¢ Niagara Systems ‚Ä¢ Behavior Trees\n\n---\n\n## WebAssembly Acceleration\n\nOptional WASM acceleration for computationally intensive operations. **Enabled by default** when available, falls back to TypeScript automatically.\n\n| Operation | Speedup |\n|-----------|---------|\n| JSON parsing | 5‚Äì8x |\n| Transform calculations | 5‚Äì10x |\n| Vector/matrix math | 5x |\n| Dependency resolution | 3‚Äì5x |\n\n### Building WASM (Optional)\n\n```bash\ncargo install wasm-pack  # Once per machine\nnpm run build:wasm       # Builds  WASM\n```\n\nTo disable: `WASM_ENABLED=false`\n\n---\n\n## GraphQL API\n\nOptional GraphQL endpoint for complex queries. **Disabled by default.**\n\n```env\nGRAPHQL_ENABLED=true\nGRAPHQL_PORT=4000\n```\n\nSee [GraphQL API Documentation](docs/GraphQL-API.md).\n\n---\n\n## Docker\n\n```bash\ndocker build -t unreal-mcp .\ndocker run -it --rm -e UE_PROJECT_PATH=/project unreal-mcp\n```\n\n---\n\n## Documentation\n\n| Document | Description |\n|----------|-------------|\n| [Handler Mappings](docs/handler-mapping.md) | TypeScript to C++ routing |\n| [GraphQL API](docs/GraphQL-API.md) | Query and mutation reference |\n| [WebAssembly Integration](docs/WebAssembly-Integration.md) | WASM performance guide |\n| [Plugin Extension](docs/editor-plugin-extension.md) | C++ plugin architecture |\n| [Testing Guide](docs/testing-guide.md) | How to run and write tests |\n| [Migration Guide v0.5.0](docs/Migration-Guide-v0.5.0.md) | Upgrade to v0.5.0 |\n| [Roadmap](docs/Roadmap.md) | Development phases |\n| [Automation Progress](docs/native-automation-progress.md) | Implementation status |\n\n---\n\n## Development\n\n```bash\nnpm run build       # Build TypeScript + WASM\nnpm run lint        # Run ESLint\nnpm run test:unit   # Run unit tests\nnpm run test:all    # Run all tests\n```\n\n---\n\n## Community\n\n| Resource | Description |\n|----------|-------------|\n| [Project Roadmap](https://github.com/users/ChiR24/projects/3) | Track development progress across 47 phases |\n| [Discussions](https://github.com/ChiR24/Unreal_mcp/discussions) | Ask questions, share ideas, get help |\n| [Issues](https://github.com/ChiR24/Unreal_mcp/issues) | Report bugs and request features |\n\n---\n\n## Contributing\n\nContributions welcome! Please:\n- Include reproduction steps for bugs\n- Keep PRs focused and small\n- Follow existing code style\n\n---\n\n## License\n\nMIT ‚Äî See [LICENSE](LICENSE)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Browse, import, duplicate, rename, and delete assets; create materials",
        "Spawn, delete, transform actors and control physics, tags, and components",
        "Control editor features including PIE sessions, camera, viewport, screenshots, and bookmarks",
        "Load and save levels with support for streaming, World Partition, and data layers",
        "Manage animation blueprints, state machines, ragdolls, vehicles, and constraints",
        "Create and manipulate visual effects such as Niagara particles, GPU simulations, and debug shapes",
        "Control sequencer for cinematics, timeline, camera animations, and keyframes",
        "Edit graphs including Blueprint, Niagara, Material, and Behavior Tree graphs",
        "Manage audio assets, components, sound cues, MetaSounds, and attenuation",
        "Execute system commands including console commands, UBT, tests, logs, project settings, and CVars"
      ],
      "limitations": [
        "Automation bridge binds to loopback addresses by default, restricting remote access unless explicitly enabled",
        "Security risks when enabling LAN access due to exposure of automation bridge to local network",
        "GraphQL API is disabled by default and requires manual enabling",
        "WASM acceleration is optional and falls back to TypeScript if unavailable",
        "Rate limiting applies to Prometheus metrics endpoint at 60 requests per minute per IP"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "Unreal Engine version between 5.0 and 5.7",
        "Installation and activation of MCP Automation Bridge Unreal plugin",
        "Enable required Unreal Engine plugins such as Editor Scripting Utilities, Sequencer, Control Rig, and others depending on features used",
        "Set environment variable UE_PROJECT_PATH pointing to the Unreal project directory",
        "Optional environment variables for configuring host, port, logging, WASM, and timeouts"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed feature and tool descriptions, configuration options, usage examples, limitations, and requirements, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Unreal Engine MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![NPM Package](https://img.shields.io/npm/v/unreal-engine-mcp-server)](https://www.npmjs.com/package/unreal-engine-mcp-server)\n[![MCP SDK](https://img.shields.io/badge/MCP%20SDK-TypeScript-blue)](https://github.com/modelcontextprotocol/sdk)\n[![Unreal Engine](https://img.shields.io/badge/Unreal%20Engine-5.0--5.7-orange)](https://www.unrealengine.com/)\n[![MCP Registry](https://img.shields.io/badge/MCP%20Registry-Published-green)](https://registry.modelcontextprotocol.io/)\n[![Project Board](https://img.shields.io/badge/Project-Roadmap-blueviolet?logo=github)](https://github.com/users/ChiR24/projects/3)\n[![Discussions](https://img.shields.io/badge/Discussions-Join-brightgreen?logo=github)](https://github.com/ChiR24/Unreal_mcp/discussions)\n\nA comprehensive Model Context Protocol (MCP) server that enables AI assistants to control Unreal Engine through a native C++ Automation Bridge plugin. Built with TypeScript, C++, and Rust (WebAssembly).",
        "start_pos": 0,
        "end_pos": 1085,
        "token_count_estimate": 271,
        "source_type": "readme",
        "agent_id": "4ccadffa2e258dbc"
      },
      {
        "chunk_id": 1,
        "text": "ming, World Partition, data layers |\n| **Animation & Physics** | Animation BPs, state machines, ragdolls, vehicles, constraints |\n| **Visual Effects** | Niagara particles, GPU simulations, procedural effects, debug shapes |\n| **Sequencer** | Cinematics, timeline control, camera animations, keyframes |\n| **Graph Editing** | Blueprint, Niagara, Material, and Behavior Tree graph manipulation |\n| **Audio** | Sound cues, audio components, sound mixes, ambient sounds |\n| **System** | Console commands, UBT, tests, logs, project settings, CVars |\n\n### Architecture\n\n- **Native C++ Automation** ‚Äî All operations route through the MCP Automation Bridge plugin\n- **Dynamic Type Discovery** ‚Äî Runtime introspection for lights, debug shapes, and sequencer tracks\n- **Graceful Degradation** ‚Äî Server starts even without an active Unreal connection\n- **On-Demand Connection** ‚Äî Retries automation handshakes with exponential backoff\n- **Command Safety** ‚Äî Blocks dangerous console commands with pattern-based validation\n- **Asset Caching** ‚Äî 10-second TTL for improved performance\n- **Metrics Rate Limiting** ‚Äî Per-IP rate limiting (60 req/min) on Prometheus endpoint\n- **Centralized Configuration** ‚Äî Unified class aliases and type definitions\n\n---\n\n## Getting Started\n\n### Prerequisites\n\n- **Node.js** 18+\n- **Unreal Engine** 5.0‚Äì5.7\n\n### Step 1: Install MCP Server\n\n**Option A: NPX (Recommended)**\n```bash\nnpx unreal-engine-mcp-server\n```\n\n**Option B: Clone & Build**\n```bash\ngit clone https://github.com/ChiR24/Unreal_mcp.git\ncd Unreal_mcp\nnpm install\nnpm run build\nnode dist/cli.js\n```\n\n### Step 2: Install Unreal Plugin\n\nThe MCP Automation Bridge plugin is included at `Unreal_mcp/plugins/McpAutomationBridge`.\n\n**Method 1: Copy Folder**\n```\nCopy:  Unreal_mcp/plugins/McpAutomationBridge/\nTo:    YourUnrealProject/Plugins/McpAutomationBridge/\n```\nRegenerate project files after copying.\n\n**Method 2: Add in Editor**\n1. Open Unreal Editor ‚Üí **Edit ‚Üí Plugins**\n2. Click **\"Add\"** ‚Üí Browse to `Unreal_mcp/plugins/`\n3.",
        "start_pos": 1848,
        "end_pos": 3859,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "4ccadffa2e258dbc"
      },
      {
        "chunk_id": 2,
        "text": "Plugins/McpAutomationBridge/\n```\nRegenerate project files after copying.\n\n**Method 2: Add in Editor**\n1. Open Unreal Editor ‚Üí **Edit ‚Üí Plugins**\n2. Click **\"Add\"** ‚Üí Browse to `Unreal_mcp/plugins/`\n3. Select the `McpAutomationBridge` folder\n\n**Video Guide:**\n\nhttps://github.com/user-attachments/assets/d8b86ebc-4364-48c9-9781-de854bf3ef7d\n\n### Step 3: Enable Required Plugins\n\nEnable via **Edit ‚Üí Plugins**, then restart the editor:\n\n| Plugin | Required For |\n|--------|--------------|\n| **MCP Automation Bridge** | All automation operations |\n| **Editor Scripting Utilities** | Asset/Actor subsystem operations |\n| **Sequencer** | Sequencer tools |\n| **Level Sequence Editor** | `manage_sequence` operations |\n| **Control Rig** | `animation_physics` operations |\n| **Subobject Data Interface** | Blueprint components (UE 5.7+) |\n| **Geometry Script** | `manage_geometry` operations (procedural mesh) |\n\n### Step 4: Configure MCP Client\n\nAdd to your Claude Desktop / Cursor config file:\n\n**Using Clone/Build:**\n```json\n{\n  \"mcpServers\": {\n    \"unreal-engine\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/Unreal_mcp/dist/cli.js\"],\n      \"env\": {\n        \"UE_PROJECT_PATH\": \"C:/Path/To/YourProject\",\n        \"MCP_AUTOMATION_PORT\": \"8091\"\n      }\n    }\n  }\n}\n```\n\n**Using NPX:**\n```json\n{\n  \"mcpServers\": {\n    \"unreal-engine\": {\n      \"command\": \"npx\",\n      \"args\": [\"unreal-engine-mcp-server\"],\n      \"env\": {\n        \"UE_PROJECT_PATH\": \"C:/Path/To/YourProject\"\n      }\n    }\n  }\n}\n```\n\n---\n\n## Configuration\n\n### Environment Variables\n\n```env\n# Required\nUE_PROJECT_PATH=\"C:/Path/To/YourProject\"\n\n# Automation Bridge\nMCP_AUTOMATION_HOST=127.0.0.1\nMCP_AUTOMATION_PORT=8091\n\n# LAN Access (optional)\n# SECURITY: Set to true to allow binding to non-loopback addresses (e.g., 0.0.0.0)\n# Only enable if you understand the security implications.",
        "start_pos": 3659,
        "end_pos": 5505,
        "token_count_estimate": 461,
        "source_type": "readme",
        "agent_id": "4ccadffa2e258dbc"
      },
      {
        "chunk_id": 3,
        "text": "CP_AUTOMATION_ALLOW_NON_LOOPBACK=false\n\n# Logging\nLOG_LEVEL=info  # debug | info | warn | error\n\n# Optional\nWASM_ENABLED=true\nMCP_AUTOMATION_REQUEST_TIMEOUT_MS=120000\nASSET_LIST_TTL_MS=10000\n```\n\n### LAN Access Configuration\n\nBy default, the automation bridge only binds to loopback addresses (127.0.0.1) for security. To enable access from other machines on your network:\n\n**TypeScript (MCP Server):**\n```env\nMCP_AUTOMATION_ALLOW_NON_LOOPBACK=true\nMCP_AUTOMATION_HOST=0.0.0.0\n```\n\n**Unreal Engine Plugin:**\n1. Go to **Edit ‚Üí Project Settings ‚Üí Plugins ‚Üí MCP Automation Bridge**\n2. Under **Security**, enable **\"Allow Non Loopback\"**\n3. Under **Connection**, set **\"Listen Host\"** to `0.0.0.0`\n4. Restart the editor\n\n‚ö†Ô∏è **Security Warning:** Enabling LAN access exposes the automation bridge to your local network. Only use on trusted networks with appropriate firewall rules.",
        "start_pos": 5507,
        "end_pos": 6383,
        "token_count_estimate": 219,
        "source_type": "readme",
        "agent_id": "4ccadffa2e258dbc"
      },
      {
        "chunk_id": 4,
        "text": "Assets, Components, Sound Cues, MetaSounds, Attenuation |\n| `manage_behavior_tree` | Behavior Tree Graph Editing |\n| `manage_input` | Enhanced Input Actions & Contexts |\n| `manage_geometry` | Procedural mesh creation (Geometry Script) |\n| `manage_skeleton` | Skeleton, sockets, physics assets, cloth binding |\n| `manage_material_authoring` | Material creation, expressions, landscape layers |\n| `manage_texture` | Texture creation, modification, compression settings |\n| `manage_gas` | Gameplay Ability System: abilities, effects, attributes |\n| `manage_character` | Character creation, movement, advanced locomotion |\n| `manage_combat` | Weapons, projectiles, damage, melee combat |\n| `manage_ai` | AI controllers, EQS, perception, State Trees, Smart Objects |\n| `manage_inventory` | Items, equipment, loot tables, crafting |\n| `manage_interaction` | Interactables, destructibles, triggers |\n| `manage_widget_authoring` | UMG widget creation, layout, styling, animations |\n| `manage_networking` | Replication, RPCs, network prediction |\n| `manage_game_framework` | Game modes, game states, player controllers, match flow |\n| `manage_sessions` | Sessions, split-screen, LAN, voice chat |\n| `manage_level_structure` | Level creation, sublevels, World Partition, data layers, HLOD |\n| `manage_volumes` | Trigger volumes, blocking, physics, audio, navigation volumes |\n| `manage_navigation` | NavMesh settings, nav modifiers, nav links, smart links, pathfinding |\n\n### Supported Asset Types\n\nBlueprints ‚Ä¢ Materials ‚Ä¢ Textures ‚Ä¢ Static Meshes ‚Ä¢ Skeletal Meshes ‚Ä¢ Levels ‚Ä¢ Sounds ‚Ä¢ Particles ‚Ä¢ Niagara Systems ‚Ä¢ Behavior Trees\n\n---\n\n## WebAssembly Acceleration\n\nOptional WASM acceleration for computationally intensive operations. **Enabled by default** when available, falls back to TypeScript automatically.",
        "start_pos": 7355,
        "end_pos": 9160,
        "token_count_estimate": 451,
        "source_type": "readme",
        "agent_id": "4ccadffa2e258dbc"
      },
      {
        "chunk_id": 5,
        "text": "-----|\n| JSON parsing | 5‚Äì8x |\n| Transform calculations | 5‚Äì10x |\n| Vector/matrix math | 5x |\n| Dependency resolution | 3‚Äì5x |\n\n### Building WASM (Optional)\n\n```bash\ncargo install wasm-pack  # Once per machine\nnpm run build:wasm       # Builds  WASM\n```\n\nTo disable: `WASM_ENABLED=false`\n\n---\n\n## GraphQL API\n\nOptional GraphQL endpoint for complex queries. **Disabled by default.**\n\n```env\nGRAPHQL_ENABLED=true\nGRAPHQL_PORT=4000\n```\n\nSee [GraphQL API Documentation](docs/GraphQL-API.md).\n\n---\n\n## Docker\n\n```bash\ndocker build -t unreal-mcp .",
        "start_pos": 9203,
        "end_pos": 9744,
        "token_count_estimate": 135,
        "source_type": "readme",
        "agent_id": "4ccadffa2e258dbc"
      },
      {
        "chunk_id": 6,
        "text": "st features |\n\n---\n\n## Contributing\n\nContributions welcome! Please:\n- Include reproduction steps for bugs\n- Keep PRs focused and small\n- Follow existing code style\n\n---\n\n## License\n\nMIT ‚Äî See [LICENSE](LICENSE)",
        "start_pos": 11051,
        "end_pos": 11262,
        "token_count_estimate": 52,
        "source_type": "readme",
        "agent_id": "4ccadffa2e258dbc"
      }
    ]
  },
  {
    "agent_id": "f830c6a3f695b70d",
    "name": "ai.smithery/CollectiveSpend-collectivespend-smithery-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@CollectiveSpend/collectivespend-smithery-mcp/mcp",
    "description": "Connect CollectiveSpend with Xero to manage contacts. Retrieve, create, and update contact records‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage",
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-10T17:03:02.996325Z",
    "indexed_at": "2026-02-18T04:03:24.026747",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Connect CollectiveSpend with Xero",
        "Manage contacts",
        "Retrieve contact records",
        "Create contact records",
        "Update contact records"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of functionality but lacks detailed examples, structure, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "01a32576e3bf5f91",
    "name": "ai.smithery/CryptoCultCurt-appfolio-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/CryptoCultCurt/appfolio-mcp-server",
    "description": "Provide seamless access to Appfolio Property Manager Reporting API through a standardized MCP serv‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T00:42:54.555978Z",
    "indexed_at": "2026-02-18T04:03:25.167044",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "Appfolio MCP Server (@fluegeldao/appfolio-mcp-server)\n\\n\n\\n\nA Model Context Protocol (MCP) server providing tools to interact with the Appfolio Property Manager Reporting API.\n\\n\nInstallation\n\\n\nInstalling via Smithery\n\\n\nTo install appfolio-mcp-server for Claude Desktop automatically via\nSmithery\n:\n\\n\nnpx -y @smithery/cli install @CryptoCultCurt/appfolio-mcp-server --client claude\n\\n\nManual Installation\n\\n\nInstall the package using npm:\n\\n\nnpm install @fluegeldao/appfolio-mcp-server\n\\n\nUsage\n\\n\nConfiguration as an MCP Server\n\\n\n{\\n  // ... other server configurations\\n\n\\\"\nappfolio\n\\\"\n: {\\n\n\\\"\ncommand\n\\\"\n:\n\\\"\nnpx\n\\\"\n,\\n\n\\\"\nargs\n\\\"\n: [\n\\\"\n@fluegeldao/appfolio-mcp-server\n\\\"\n],\\n\n\\\"\nenv\n\\\"\n: {\\n\n\\\"\nNODE_OPTIONS\n\\\"\n:\n\\\"\n--experimental-vm-modules\n\\\"\n, // Optional, may depend on your Node version/setup\\n\n\\\"\nVHOST\n\\\"\n:\n\\\"\nYOUR_APPFOLIO_HOSTNAME\n\\\"\n, // e.g.,\n\\\"\nyourcompany\n\\\"\n\\n\n\\\"\nUSERNAME\n\\\"\n:\n\\\"\nYOUR_APPFOLIO_API_USERNAME\n\\\"\n,\\n\n\\\"\nPASSWORD\n\\\"\n:\n\\\"\nYOUR_APPFOLIO_API_PASSWORD\n\\\"\n\\n    },\\n\n\\\"\nrestart\n\\\"\n:\ntrue\n// Optional: Restart the server\nif\nit crashes\\n  }\\n  // ... other server configurations\\n}\n\\n\nAs a Tool\n\\n\nnpx @fluegeldao/appfolio-mcp-server\n\\n\n\\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Interact with the Appfolio Property Manager Reporting API",
        "Provide tools for querying and managing property management data via Appfolio",
        "Run as an MCP server compatible with Claude Desktop",
        "Support configuration with environment variables for hostname, username, and password",
        "Restart automatically if the server crashes"
      ],
      "limitations": [],
      "requirements": [
        "Node.js environment supporting --experimental-vm-modules",
        "Appfolio API username and password credentials",
        "Appfolio hostname (VHOST) configuration",
        "npm or npx for installation and execution"
      ]
    },
    "documentation_quality": 0.55,
    "quality_rationale": "The documentation includes installation instructions and configuration details but lacks usage examples and explicit limitations.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "Appfolio MCP Server (@fluegeldao/appfolio-mcp-server)\n\\n\n\\n\nA Model Context Protocol (MCP) server providing tools to interact with the Appfolio Property Manager Reporting API.\n\\n\nInstallation\n\\n\nInstalling via Smithery\n\\n\nTo install appfolio-mcp-server for Claude Desktop automatically via\nSmithery\n:\n\\n\nnpx -y @smithery/cli install @CryptoCultCurt/appfolio-mcp-server --client claude\n\\n\nManual Installation\n\\n\nInstall the package using npm:\n\\n\nnpm install @fluegeldao/appfolio-mcp-server\n\\n\nUsage\n\\n\nConfiguration as an MCP Server\n\\n\n{\\n  // ... other server configurations\\n\n\\\"\nappfolio\n\\\"\n: {\\n\n\\\"\ncommand\n\\\"\n:\n\\\"\nnpx\n\\\"\n,\\n\n\\\"\nargs\n\\\"\n: [\n\\\"\n@fluegeldao/appfolio-mcp-server\n\\\"\n],\\n\n\\\"\nenv\n\\\"\n: {\\n\n\\\"\nNODE_OPTIONS\n\\\"\n:\n\\\"\n--experimental-vm-modules\n\\\"\n, // Optional, may depend on your Node version/setup\\n\n\\\"\nVHOST\n\\\"\n:\n\\\"\nYOUR_APPFOLIO_HOSTNAME\n\\\"\n, // e.g.,\n\\\"\nyourcompany\n\\\"\n\\n\n\\\"\nUSERNAME\n\\\"\n:\n\\\"\nYOUR_APPFOLIO_API_USERNAME\n\\\"\n,\\n\n\\\"\nPASSWORD\n\\\"\n:\n\\\"\nYOUR_APPFOLIO_API_PASSWORD\n\\\"\n\\n    },\\n\n\\\"\nrestart\n\\\"\n:\ntrue\n// Optional: Restart the server\nif\nit crashes\\n  }\\n  // ... other server configurations\\n}\n\\n\nAs a Tool\n\\n\nnpx @fluegeldao/appfolio-mcp-server\n\\n\n\\n",
        "start_pos": 0,
        "end_pos": 1170,
        "token_count_estimate": 292,
        "source_type": "detail_page",
        "agent_id": "01a32576e3bf5f91"
      }
    ]
  },
  {
    "agent_id": "099d0236b2524f48",
    "name": "ai.smithery/Danushkumar-V-mcp-discord",
    "source": "mcp",
    "source_url": "https://github.com/Danushkumar-V/mcp-discord",
    "description": "An MCP server that integrates with Discord to provide AI-powered features.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-13T22:54:39.805525Z",
    "indexed_at": "2026-02-18T04:03:27.311307",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# MCP-Discord\n[![smithery badge](https://smithery.ai/badge/@barryyip0625/mcp-discord)](https://smithery.ai/server/@barryyip0625/mcp-discord) ![](https://badge.mcpx.dev?type=server 'MCP Server') [![Docker Hub](https://img.shields.io/docker/v/barryy625/mcp-discord?logo=docker&label=Docker%20Hub)](https://hub.docker.com/r/barryy625/mcp-discord)\n\nA Discord MCP (Model Context Protocol) server that enables AI assistants to interact with the Discord platform.\n\n<a href=\"https://glama.ai/mcp/servers/@barryyip0625/mcp-discord\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@barryyip0625/mcp-discord/badge\" alt=\"MCP-Discord MCP server\" />\n</a>\n\n## Overview\n\nMCP-Discord provides the following Discord-related functionalities:\n\n- Login to Discord bot\n- Get server information\n- Read/delete channel messages\n- Send messages to specified channels (using either channel IDs or channel names)\n- Retrieve forum channel lists\n- Create/delete/reply to forum posts\n- Create/delete text channels\n- Add/remove message reactions\n- Create/edit/delete/use webhooks\n\n## Table of Contents\n\n- [Prerequisites](#prerequisites)\n- [Installation](#installation)\n- [Configuration](#configuration)\n- [Tools Documentation](#tools-documentation)\n  - [Basic Functions](#basic-functions)\n  - [Channel Management](#channel-management)\n  - [Forum Functions](#forum-functions)\n  - [Messages and Reactions](#messages-and-reactions)\n  - [Webhook Management](#webhook-management)\n- [Development](#development)\n- [License](#license)\n\n## Prerequisites\n\n- Node.js (v16.0.0 or higher)\n- npm (v7.0.0 or higher)\n- A Discord bot with appropriate permissions\n  - Bot token (obtainable from the [Discord Developer Portal](https://discord.com/developers/applications))\n  - Message Content Intent enabled\n  - Server Members Intent enabled\n  - Presence Intent enabled\n- Permissions required in your Discord server:\n\n  #### Easiest Setup\n  - Administrator (Recommended for quick setup and full functionality)\n\n  #### Or, select only the required permissions:\n  - Send Messages\n  - Create Public Threads\n  - Send Messages in Threads\n  - Manage Messages\n  - Manage Threads\n  - Manage Channels\n  - Manage Webhooks\n  - Add Reactions\n  - View Channel\n\n- Add your Discord bot to your server\n  - To add your Discord bot to your server, use one of the following invite links (replace `INSERT_CLIENT_ID_HERE` with your bot's client ID):\n    - **Administrator (full access):**\n        https://discord.com/oauth2/authorize?client_id=INSERT_CLIENT_ID_HERE&scope=bot&permissions=8\n    - **Custom permissions (minimum required):**\n        https://discord.com/oauth2/authorize?client_id=INSERT_CLIENT_ID_HERE&scope=bot&permissions=52076489808\n\n> **Note:**  \n> According to Discord's security model, a bot can only access information from servers it has been explicitly added to.  \n> If you want to use this MCP server to access a specific Discord server, you must add the bot to that server first.  \n> Use the invite link below to add the bot to your target server.\n\n## Installation\n\n### Installing via NPM\n\nYou can use it with the following command:\n```bash\nnpx mcp-discord --config ${DISCORD_TOKEN}\n```\n\nFor more details, you can check out the [NPM Package](https://www.npmjs.com/package/mcp-discord).\n\n### Installing via Smithery\n\nTo install mcp-discord automatically via [Smithery](https://smithery.ai/server/@barryyip0625/mcp-discord)\n\n### Installing via Docker\n\nYou can run mcp-discord using Docker. The Docker images are automatically built and published to Docker Hub.\n\n**Docker Hub Repository**: [barryy625/mcp-discord](https://hub.docker.com/r/barryy625/mcp-discord)\n\n```bash\n# Pull the latest image\ndocker pull barryy625/mcp-discord:latest\n\n# Run with environment variable\ndocker run -e DISCORD_TOKEN=your_discord_bot_token -p 8080:8080 barryy625/mcp-discord:latest\n\n# Or run with command line config\ndocker run -p 8080:8080 barryy625/mcp-discord:latest --config \"your_discord_bot_token\"\n```\n\n**Available Tags:**\n- `latest` - Latest stable version from main branch\n- `v1.3.3`, etc. - Specific version releases\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/barryyip0625/mcp-discord.git\ncd mcp-discord\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript\nnpm run build\n```\n\n## Configuration\n\nA Discord bot token is required for proper operation. The server supports two transport methods: stdio and streamable HTTP.\n\n### Transport Methods\n\n1. **stdio** (Default)\n   - Traditional stdio transport for basic usage\n   - Suitable for simple integrations\n\n2. **streamable HTTP**\n   - HTTP-based transport for more advanced scenarios\n   - Supports stateless operation\n   - Configurable port number\n\n### Configuration Options\n\nYou can provide configuration in two ways:\n\n1. Environment variables:\n```bash\nDISCORD_TOKEN=your_discord_bot_token\n```\n\n2. Using command line arguments:\n```bash\n# For stdio transport (default)\nnode build/index.js --config \"your_discord_bot_token\"\n\n# For streamable HTTP transport\nnode build/index.js --transport http --port 3000 --config \"your_discord_bot_token\"\n```\n\n## Usage with Claude/Cursor\n\n### Docker\n\nYou can use Docker containers with both Claude and Cursor:\n\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"docker\",\n            \"args\": [\n                \"run\",\n                \"--rm\",\n                \"-e\",\n                \"DISCORD_TOKEN=your_discord_bot_token\",\n                \"-p\",\n                \"8080:8080\",\n                \"barryy625/mcp-discord:latest\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"8080\"\n            ]\n        }\n    }\n}\n```\n\n### Claude\n\n1. Using stdio transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"path/to/mcp-discord/build/index.js\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n2. Using streamable HTTP transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"path/to/mcp-discord/build/index.js\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"3000\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n### Cursor\n\n1. Using stdio transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"cmd\",\n            \"args\": [\n                \"/c\",\n                \"node\",\n                \"path/to/mcp-discord/build/index.js\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n2. Using streamable HTTP transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"cmd\",\n            \"args\": [\n                \"/c\",\n                \"node\",\n                \"path/to/mcp-discord/build/index.js\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"3000\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n## Tools Documentation\n\n### Basic Functions\n\n- `discord_send`: Send a message to a specified channel (supports both channel ID and channel name)\n- `discord_get_server_info`: Get Discord server information\n\n### Channel Management\n\n- `discord_create_text_channel`: Create a text channel\n- `discord_delete_channel`: Delete a channel\n\n### Forum Functions\n\n- `discord_get_forum_channels`: Get a list of forum channels\n- `discord_create_forum_post`: Create a forum post\n- `discord_get_forum_post`: Get a forum post\n- `discord_reply_to_forum`: Reply to a forum post\n- `discord_delete_forum_post`: Delete a forum post\n\n### Messages and Reactions\n\n- `discord_read_messages`: Read channel messages\n- `discord_add_reaction`: Add a reaction to a message\n- `discord_add_multiple_reactions`: Add multiple reactions to a message\n- `discord_remove_reaction`: Remove a reaction from a message\n- `discord_delete_message`: Delete a specific message from a channel\n\n### Webhook Management\n\n- `discord_create_webhook`: Creates a new webhook for a Discord channel\n- `discord_send_webhook_message`: Sends a message to a Discord channel using a webhook\n- `discord_edit_webhook`: Edits an existing webhook for a Discord channel\n- `discord_delete_webhook`: Deletes an existing webhook for a Discord channel\n\n## Development\n\n```bash\n# Development mode\nnpm run dev\n```\n\n## License\n\n[MIT License](https://github.com/barryyip0625/mcp-discord?tab=MIT-1-ov-file)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Login to a Discord bot using a bot token",
        "Retrieve Discord server information",
        "Read and delete messages in Discord channels",
        "Send messages to specified Discord channels by ID or name",
        "Retrieve lists of forum channels",
        "Create, reply to, get, and delete forum posts",
        "Create and delete text channels",
        "Add, add multiple, and remove reactions on messages",
        "Create, edit, send messages with, and delete webhooks"
      ],
      "limitations": [
        "Bot can only access information from servers it has been explicitly added to",
        "Requires appropriate Discord bot permissions and intents enabled",
        "No mention of support for voice channels or direct messages",
        "No stated rate limits or concurrency constraints"
      ],
      "requirements": [
        "Node.js version 16.0.0 or higher",
        "npm version 7.0.0 or higher",
        "A Discord bot with a valid bot token",
        "Discord bot must have Message Content Intent, Server Members Intent, and Presence Intent enabled",
        "Bot must have required permissions in the Discord server (Administrator recommended or specific permissions listed)",
        "Bot must be added to the target Discord server"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, a full list of supported tools, configuration options, prerequisites, and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# MCP-Discord\n[![smithery badge](https://smithery.ai/badge/@barryyip0625/mcp-discord)](https://smithery.ai/server/@barryyip0625/mcp-discord) ![](https://badge.mcpx.dev?type=server 'MCP Server') [![Docker Hub](https://img.shields.io/docker/v/barryy625/mcp-discord?logo=docker&label=Docker%20Hub)](https://hub.docker.com/r/barryy625/mcp-discord)\n\nA Discord MCP (Model Context Protocol) server that enables AI assistants to interact with the Discord platform.",
        "start_pos": 0,
        "end_pos": 456,
        "token_count_estimate": 114,
        "source_type": "readme",
        "agent_id": "099d0236b2524f48"
      },
      {
        "chunk_id": 1,
        "text": "ons required in your Discord server:\n\n  #### Easiest Setup\n  - Administrator (Recommended for quick setup and full functionality)\n\n  #### Or, select only the required permissions:\n  - Send Messages\n  - Create Public Threads\n  - Send Messages in Threads\n  - Manage Messages\n  - Manage Threads\n  - Manage Channels\n  - Manage Webhooks\n  - Add Reactions\n  - View Channel\n\n- Add your Discord bot to your server\n  - To add your Discord bot to your server, use one of the following invite links (replace `INSERT_CLIENT_ID_HERE` with your bot's client ID):\n    - **Administrator (full access):**\n        https://discord.com/oauth2/authorize?client_id=INSERT_CLIENT_ID_HERE&scope=bot&permissions=8\n    - **Custom permissions (minimum required):**\n        https://discord.com/oauth2/authorize?client_id=INSERT_CLIENT_ID_HERE&scope=bot&permissions=52076489808\n\n> **Note:**  \n> According to Discord's security model, a bot can only access information from servers it has been explicitly added to.  \n> If you want to use this MCP server to access a specific Discord server, you must add the bot to that server first.  \n> Use the invite link below to add the bot to your target server.\n\n## Installation\n\n### Installing via NPM\n\nYou can use it with the following command:\n```bash\nnpx mcp-discord --config ${DISCORD_TOKEN}\n```\n\nFor more details, you can check out the [NPM Package](https://www.npmjs.com/package/mcp-discord).\n\n### Installing via Smithery\n\nTo install mcp-discord automatically via [Smithery](https://smithery.ai/server/@barryyip0625/mcp-discord)\n\n### Installing via Docker\n\nYou can run mcp-discord using Docker. The Docker images are automatically built and published to Docker Hub.",
        "start_pos": 1848,
        "end_pos": 3530,
        "token_count_estimate": 420,
        "source_type": "readme",
        "agent_id": "099d0236b2524f48"
      },
      {
        "chunk_id": 2,
        "text": "d:latest\n\n# Run with environment variable\ndocker run -e DISCORD_TOKEN=your_discord_bot_token -p 8080:8080 barryy625/mcp-discord:latest\n\n# Or run with command line config\ndocker run -p 8080:8080 barryy625/mcp-discord:latest --config \"your_discord_bot_token\"\n```\n\n**Available Tags:**\n- `latest` - Latest stable version from main branch\n- `v1.3.3`, etc. - Specific version releases\n\n### Manual Installation\n```bash\n# Clone the repository\ngit clone https://github.com/barryyip0625/mcp-discord.git\ncd mcp-discord\n\n# Install dependencies\nnpm install\n\n# Compile TypeScript\nnpm run build\n```\n\n## Configuration\n\nA Discord bot token is required for proper operation. The server supports two transport methods: stdio and streamable HTTP.\n\n### Transport Methods\n\n1. **stdio** (Default)\n   - Traditional stdio transport for basic usage\n   - Suitable for simple integrations\n\n2. **streamable HTTP**\n   - HTTP-based transport for more advanced scenarios\n   - Supports stateless operation\n   - Configurable port number\n\n### Configuration Options\n\nYou can provide configuration in two ways:\n\n1. Environment variables:\n```bash\nDISCORD_TOKEN=your_discord_bot_token\n```\n\n2. Using command line arguments:\n```bash\n# For stdio transport (default)\nnode build/index.js --config \"your_discord_bot_token\"\n\n# For streamable HTTP transport\nnode build/index.js --transport http --port 3000 --config \"your_discord_bot_token\"\n```\n\n## Usage with Claude/Cursor\n\n### Docker\n\nYou can use Docker containers with both Claude and Cursor:\n\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"docker\",\n            \"args\": [\n                \"run\",\n                \"--rm\",\n                \"-e\",\n                \"DISCORD_TOKEN=your_discord_bot_token\",\n                \"-p\",\n                \"8080:8080\",\n                \"barryy625/mcp-discord:latest\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"8080\"\n            ]\n        }\n    }\n}\n```\n\n### Claude\n\n1.",
        "start_pos": 3696,
        "end_pos": 5681,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "099d0236b2524f48"
      },
      {
        "chunk_id": 3,
        "text": "\"barryy625/mcp-discord:latest\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"8080\"\n            ]\n        }\n    }\n}\n```\n\n### Claude\n\n1. Using stdio transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"path/to/mcp-discord/build/index.js\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n2. Using streamable HTTP transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"node\",\n            \"args\": [\n                \"path/to/mcp-discord/build/index.js\",\n                \"--transport\",\n                \"http\",\n                \"--port\",\n                \"3000\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n### Cursor\n\n1. Using stdio transport:\n```json\n{\n    \"mcpServers\": {\n        \"discord\": {\n            \"command\": \"cmd\",\n            \"args\": [\n                \"/c\",\n                \"node\",\n                \"path/to/mcp-discord/build/index.js\",\n                \"--config\",\n                \"your_discord_bot_token\"\n            ]\n        }\n    }\n}\n```\n\n2.",
        "start_pos": 5481,
        "end_pos": 6723,
        "token_count_estimate": 307,
        "source_type": "readme",
        "agent_id": "099d0236b2524f48"
      },
      {
        "chunk_id": 4,
        "text": "t_server_info`: Get Discord server information\n\n### Channel Management\n\n- `discord_create_text_channel`: Create a text channel\n- `discord_delete_channel`: Delete a channel\n\n### Forum Functions\n\n- `discord_get_forum_channels`: Get a list of forum channels\n- `discord_create_forum_post`: Create a forum post\n- `discord_get_forum_post`: Get a forum post\n- `discord_reply_to_forum`: Reply to a forum post\n- `discord_delete_forum_post`: Delete a forum post\n\n### Messages and Reactions\n\n- `discord_read_messages`: Read channel messages\n- `discord_add_reaction`: Add a reaction to a message\n- `discord_add_multiple_reactions`: Add multiple reactions to a message\n- `discord_remove_reaction`: Remove a reaction from a message\n- `discord_delete_message`: Delete a specific message from a channel\n\n### Webhook Management\n\n- `discord_create_webhook`: Creates a new webhook for a Discord channel\n- `discord_send_webhook_message`: Sends a message to a Discord channel using a webhook\n- `discord_edit_webhook`: Edits an existing webhook for a Discord channel\n- `discord_delete_webhook`: Deletes an existing webhook for a Discord channel\n\n## Development\n\n```bash\n# Development mode\nnpm run dev\n```\n\n## License\n\n[MIT License](https://github.com/barryyip0625/mcp-discord?tab=MIT-1-ov-file)",
        "start_pos": 7329,
        "end_pos": 8602,
        "token_count_estimate": 318,
        "source_type": "readme",
        "agent_id": "099d0236b2524f48"
      }
    ]
  },
  {
    "agent_id": "c1f2d1e740127354",
    "name": "ai.smithery/DynamicEndpoints-autogen_mcp",
    "source": "mcp",
    "source_url": "https://github.com/DynamicEndpoints/Autogen_MCP",
    "description": "Create and manage AI agents that collaborate and solve problems through natural language interacti‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage",
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T13:57:38.18553Z",
    "indexed_at": "2026-02-18T04:03:28.649050",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Enhanced AutoGen MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@DynamicEndpoints/autogen_mcp)](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp)\n\nA comprehensive MCP server that provides deep integration with Microsoft's AutoGen framework v0.9+, featuring the latest capabilities including prompts, resources, advanced workflows, and enhanced agent types. This server enables sophisticated multi-agent conversations through a standardized Model Context Protocol interface.\n\n## üöÄ Latest Features (v0.2.0)\n\n### ‚ú® **Enhanced MCP Support**\n- **Prompts**: Pre-built templates for common workflows (code review, research, creative writing)\n- **Resources**: Real-time access to agent status, chat history, and configurations\n- **Dynamic Content**: Template-based prompts with arguments and embedded resources\n- **Latest MCP SDK**: Version 1.12.3 with full feature support\n\n### ü§ñ **Advanced Agent Types**\n- **Assistant Agents**: Enhanced with latest LLM capabilities\n- **Conversable Agents**: Flexible conversation patterns\n- **Teachable Agents**: Learning and memory persistence\n- **Retrievable Agents**: Knowledge base integration\n- **Multimodal Agents**: Image and document processing (when available)\n\n### üîÑ **Sophisticated Workflows**\n- **Code Generation**: Architect ‚Üí Developer ‚Üí Reviewer ‚Üí Executor pipeline\n- **Research Analysis**: Researcher ‚Üí Analyst ‚Üí Critic ‚Üí Synthesizer workflow\n- **Creative Writing**: Multi-stage creative collaboration\n- **Problem Solving**: Structured approach to complex problems\n- **Code Review**: Security ‚Üí Performance ‚Üí Style review teams\n- **Custom Workflows**: Build your own agent collaboration patterns\n\n### üéØ **Enhanced Chat Capabilities**\n- **Smart Speaker Selection**: Auto, manual, random, round-robin modes\n- **Nested Conversations**: Hierarchical agent interactions\n- **Swarm Intelligence**: Coordinated multi-agent problem solving\n- **Memory Management**: Persistent agent knowledge and preferences\n- **Quality Checks**: Built-in validation and improvement loops\n\n## üõ†Ô∏è Available Tools\n\n### Core Agent Management\n- `create_agent` - Create agents with advanced configurations\n- `create_workflow` - Build complete multi-agent workflows\n- `get_agent_status` - Detailed agent metrics and health monitoring\n\n### Conversation Execution\n- `execute_chat` - Enhanced two-agent conversations\n- `execute_group_chat` - Multi-agent group discussions\n- `execute_nested_chat` - Hierarchical conversation structures\n- `execute_swarm` - Swarm-based collaborative problem solving\n\n### Workflow Orchestration\n- `execute_workflow` - Run predefined workflow templates\n- `manage_agent_memory` - Handle agent learning and persistence\n- `configure_teachability` - Enable/configure agent learning capabilities\n\n## üìù Available Prompts\n\n### `autogen-workflow`\nCreate sophisticated multi-agent workflows with customizable parameters:\n- **Arguments**: `task_description`, `agent_count`, `workflow_type`\n- **Use case**: Rapid workflow prototyping and deployment\n\n### `code-review`\nSet up collaborative code review with specialized agents:\n- **Arguments**: `code`, `language`, `focus_areas`\n- **Use case**: Comprehensive code quality assessment\n\n### `research-analysis`\nDeploy research teams for in-depth topic analysis:\n- **Arguments**: `topic`, `depth`\n- **Use case**: Academic research, market analysis, technical investigation\n\n## üìä Available Resources\n\n### `autogen://agents/list`\nLive list of active agents with status and capabilities\n\n### `autogen://workflows/templates`\nAvailable workflow templates and configurations\n\n### `autogen://chat/history`\nRecent conversation history and interaction logs\n\n### `autogen://config/current`\nCurrent server configuration and settings\n\n## Installation\n\n### Installing via Smithery\n\nTo install AutoGen Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/autogen_mcp --client claude\n```\n\n### Manual Installation\n\n1. **Clone the repository:**\n```bash\ngit clone https://github.com/yourusername/autogen-mcp.git\ncd autogen-mcp\n```\n\n2. **Install Node.js dependencies:**\n```bash\nnpm install\n```\n\n3. **Install Python dependencies:**\n```bash\npip install -r requirements.txt --user\n```\n\n4. **Build the TypeScript project:**\n```bash\nnpm run build\n```\n\n5. **Set up configuration:**\n```bash\ncp .env.example .env\ncp config.json.example config.json\n# Edit .env and config.json with your settings\n```\n\n## Configuration\n\n### Environment Variables\n\nCreate a `.env` file from the template:\n\n```bash\n# Required\nOPENAI_API_KEY=your-openai-api-key-here\n\n# Optional - Path to configuration file\nAUTOGEN_MCP_CONFIG=config.json\n\n# Enhanced Features\nENABLE_PROMPTS=true\nENABLE_RESOURCES=true\nENABLE_WORKFLOWS=true\nENABLE_TEACHABILITY=true\n\n# Performance Settings\nMAX_CHAT_TURNS=10\nDEFAULT_OUTPUT_FORMAT=json\n```\n\n### Configuration File\n\nUpdate `config.json` with your preferences:\n\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4o\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0.7\n  },\n  \"enhanced_features\": {\n    \"prompts\": { \"enabled\": true },\n    \"resources\": { \"enabled\": true },\n    \"workflows\": { \"enabled\": true }\n  }\n}\n```\n\n## Usage Examples\n\n### Using with Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"autogen\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/autogen-mcp/build/index.js\"],\n      \"env\": {\n        \"OPENAI_API_KEY\": \"your-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Command Line Testing\n\nTest the server functionality:\n\n```bash\n# Run comprehensive tests\npython test_server.py\n\n# Test CLI interface\npython cli_example.py create_agent \"researcher\" \"assistant\" \"You are a research specialist\"\npython cli_example.py execute_workflow \"code_generation\" '{\"task\":\"Hello world\",\"language\":\"python\"}'\n```\n\n### Using Prompts\n\nThe server provides several built-in prompts:\n\n1. **autogen-workflow** - Create multi-agent workflows\n2. **code-review** - Set up collaborative code review\n3. **research-analysis** - Deploy research teams\n\n### Accessing Resources\n\nAvailable resources provide real-time data:\n\n- `autogen://agents/list` - Current active agents\n- `autogen://workflows/templates` - Available workflow templates  \n- `autogen://chat/history` - Recent conversation history\n- `autogen://config/current` - Server configuration\n\n## Workflow Examples\n\n### Code Generation Workflow\n\n```json\n{\n  \"workflow_name\": \"code_generation\",\n  \"input_data\": {\n    \"task\": \"Create a REST API endpoint\",\n    \"language\": \"python\",\n    \"requirements\": [\"FastAPI\", \"Pydantic\", \"Error handling\"]\n  },\n  \"quality_checks\": true\n}\n```\n\n### Research Workflow\n\n```json\n{\n  \"workflow_name\": \"research\", \n  \"input_data\": {\n    \"topic\": \"AI Ethics in 2025\",\n    \"depth\": \"comprehensive\"\n  },\n  \"output_format\": \"markdown\"\n}\n```\n\n## Advanced Features\n\n### Agent Types\n\n- **Assistant Agents**: LLM-powered conversational agents\n- **User Proxy Agents**: Code execution and human interaction\n- **Conversable Agents**: Flexible conversation patterns\n- **Teachable Agents**: Learning and memory persistence (when available)\n- **Retrievable Agents**: Knowledge base integration (when available)\n\n### Chat Modes\n\n- **Two-Agent Chat**: Direct conversation between agents\n- **Group Chat**: Multi-agent discussions with smart speaker selection\n- **Nested Chat**: Hierarchical conversation structures  \n- **Swarm Intelligence**: Coordinated problem solving (experimental)\n\n### Memory Management\n\n- Persistent agent memory across sessions\n- Conversation history tracking\n- Learning from interactions (teachable agents)\n- Memory cleanup and optimization\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Errors**: Ensure your OpenAI API key is valid and has sufficient credits\n2. **Import Errors**: Install all dependencies with `pip install -r requirements.txt --user`\n3. **Build Failures**: Check Node.js version (>= 18) and run `npm install`\n4. **Chat Failures**: Verify agent creation succeeded before attempting conversations\n\n### Debug Mode\n\nEnable detailed logging:\n\n```bash\nexport LOG_LEVEL=DEBUG\npython test_server.py\n```\n\n### Performance Tips\n\n- Use `gpt-4o-mini` for faster, cost-effective operations\n- Enable caching for repeated operations\n- Set appropriate timeout values for long-running workflows\n- Use quality checks only when needed (increases execution time)\n\n## Development\n\n### Running Tests\n\n```bash\n# Full test suite\npython test_server.py\n\n# Individual workflow tests  \npython -c \"\nimport asyncio\nfrom src.autogen_mcp.workflows import WorkflowManager\nwm = WorkflowManager()\nprint(asyncio.run(wm.execute_workflow('code_generation', {'task': 'test'})))\n\"\n```\n\n### Building\n\n```bash\nnpm run build\nnpm run lint\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests for new functionality\n5. Submit a pull request\n\n## Version History\n\n### v0.2.0 (Latest)\n- ‚ú® Enhanced MCP support with prompts and resources\n- ü§ñ Advanced agent types (teachable, retrievable)\n- üîÑ Sophisticated workflows with quality checks\n- üéØ Smart speaker selection and nested conversations\n- üìä Real-time resource monitoring\n- üß† Memory management and persistence\n\n### v0.1.0\n- Basic AutoGen integration\n- Simple agent creation and chat execution\n- MCP tool interface\n\n## Support\n\nFor issues and questions:\n- Check the troubleshooting section above\n- Review the test examples in `test_server.py`\n- Open an issue on GitHub with detailed reproduction steps\n\n## License\n\nMIT License - see LICENSE file for details.\n\n# OpenAI API Key (optional, can also be set in config.json)\nOPENAI_API_KEY=your-openai-api-key\n```\n\n### Server Configuration\n\n1. Copy `config.json.example` to `config.json`:\n```bash\ncp config.json.example config.json\n```\n\n2. Configure the server settings:\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0\n  },\n  \"code_execution_config\": {\n    \"work_dir\": \"workspace\",\n    \"use_docker\": false\n  }\n}\n```\n\n## Available Operations\n\nThe server supports three main operations:\n\n### 1. Creating Agents\n\n```json\n{\n  \"name\": \"create_agent\",\n  \"arguments\": {\n    \"name\": \"tech_lead\",\n    \"type\": \"assistant\",\n    \"system_message\": \"You are a technical lead with expertise in software architecture and design patterns.\"\n  }\n}\n```\n\n### 2. One-on-One Chat\n\n```json\n{\n  \"name\": \"execute_chat\",\n  \"arguments\": {\n    \"initiator\": \"agent1\",\n    \"responder\": \"agent2\",\n    \"message\": \"Let's discuss the system architecture.\"\n  }\n}\n```\n\n### 3. Group Chat\n\n```json\n{\n  \"name\": \"execute_group_chat\",\n  \"arguments\": {\n    \"agents\": [\"agent1\", \"agent2\", \"agent3\"],\n    \"message\": \"Let's review the proposed solution.\"\n  }\n}\n```\n\n## Error Handling\n\nCommon error scenarios include:\n\n1. Agent Creation Errors\n```json\n{\n  \"error\": \"Agent already exists\"\n}\n```\n\n2. Execution Errors\n```json\n{\n  \"error\": \"Agent not found\"\n}\n```\n\n3. Configuration Errors\n```json\n{\n  \"error\": \"AUTOGEN_MCP_CONFIG environment variable not set\"\n}\n```\n\n## Architecture\n\nThe server follows a modular architecture:\n\n```\nsrc/\n‚îú‚îÄ‚îÄ autogen_mcp/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ agents.py      # Agent management and configuration\n‚îÇ   ‚îú‚îÄ‚îÄ config.py      # Configuration handling and validation\n‚îÇ   ‚îú‚îÄ‚îÄ server.py      # MCP server implementation\n‚îÇ   ‚îî‚îÄ‚îÄ workflows.py   # Conversation workflow management\n```\n\n## License\n\nMIT License - See LICENSE file for details\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Create agents with advanced configurations",
        "Build and execute multi-agent workflows",
        "Perform enhanced two-agent and multi-agent group conversations",
        "Support hierarchical nested conversations and swarm intelligence problem solving",
        "Manage persistent agent memory and learning capabilities",
        "Provide real-time access to agent status, chat history, and server configurations",
        "Use pre-built prompts for workflows like code review, research analysis, and creative writing",
        "Enable quality checks and validation loops within workflows",
        "Configure and customize agent teachability and memory persistence"
      ],
      "limitations": [
        "Swarm intelligence and teachable agents features are experimental or available only when supported",
        "Requires valid OpenAI API key with sufficient credits for LLM operations",
        "Performance may degrade with quality checks enabled due to increased execution time",
        "Requires Node.js version 18 or higher for build and runtime",
        "Some advanced features depend on proper configuration and environment setup"
      ],
      "requirements": [
        "OpenAI API key for LLM access (set via environment variable or config file)",
        "Node.js version 18 or higher",
        "Python with dependencies installed via requirements.txt",
        "Configuration files (.env and config.json) properly set up",
        "Permissions to install and run Node.js and Python packages",
        "Optional Smithery CLI for automated installation"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, tool descriptions, configuration guidance, limitations, troubleshooting, and advanced feature explanations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Enhanced AutoGen MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@DynamicEndpoints/autogen_mcp)](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp)\n\nA comprehensive MCP server that provides deep integration with Microsoft's AutoGen framework v0.9+, featuring the latest capabilities including prompts, resources, advanced workflows, and enhanced agent types. This server enables sophisticated multi-agent conversations through a standardized Model Context Protocol interface.",
        "start_pos": 0,
        "end_pos": 493,
        "token_count_estimate": 123,
        "source_type": "readme",
        "agent_id": "c1f2d1e740127354"
      },
      {
        "chunk_id": 1,
        "text": "nce**: Coordinated multi-agent problem solving\n- **Memory Management**: Persistent agent knowledge and preferences\n- **Quality Checks**: Built-in validation and improvement loops\n\n## üõ†Ô∏è Available Tools\n\n### Core Agent Management\n- `create_agent` - Create agents with advanced configurations\n- `create_workflow` - Build complete multi-agent workflows\n- `get_agent_status` - Detailed agent metrics and health monitoring\n\n### Conversation Execution\n- `execute_chat` - Enhanced two-agent conversations\n- `execute_group_chat` - Multi-agent group discussions\n- `execute_nested_chat` - Hierarchical conversation structures\n- `execute_swarm` - Swarm-based collaborative problem solving\n\n### Workflow Orchestration\n- `execute_workflow` - Run predefined workflow templates\n- `manage_agent_memory` - Handle agent learning and persistence\n- `configure_teachability` - Enable/configure agent learning capabilities\n\n## üìù Available Prompts\n\n### `autogen-workflow`\nCreate sophisticated multi-agent workflows with customizable parameters:\n- **Arguments**: `task_description`, `agent_count`, `workflow_type`\n- **Use case**: Rapid workflow prototyping and deployment\n\n### `code-review`\nSet up collaborative code review with specialized agents:\n- **Arguments**: `code`, `language`, `focus_areas`\n- **Use case**: Comprehensive code quality assessment\n\n### `research-analysis`\nDeploy research teams for in-depth topic analysis:\n- **Arguments**: `topic`, `depth`\n- **Use case**: Academic research, market analysis, technical investigation\n\n## üìä Available Resources\n\n### `autogen://agents/list`\nLive list of active agents with status and capabilities\n\n### `autogen://workflows/templates`\nAvailable workflow templates and configurations\n\n### `autogen://chat/history`\nRecent conversation history and interaction logs\n\n### `autogen://config/current`\nCurrent server configuration and settings\n\n## Installation\n\n### Installing via Smithery\n\nTo install AutoGen Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp):\n\n``",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "c1f2d1e740127354"
      },
      {
        "chunk_id": 2,
        "text": "ion and settings\n\n## Installation\n\n### Installing via Smithery\n\nTo install AutoGen Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/autogen_mcp):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/autogen_mcp --client claude\n```\n\n### Manual Installation\n\n1. **Clone the repository:**\n```bash\ngit clone https://github.com/yourusername/autogen-mcp.git\ncd autogen-mcp\n```\n\n2. **Install Node.js dependencies:**\n```bash\nnpm install\n```\n\n3. **Install Python dependencies:**\n```bash\npip install -r requirements.txt --user\n```\n\n4. **Build the TypeScript project:**\n```bash\nnpm run build\n```\n\n5.",
        "start_pos": 3696,
        "end_pos": 4338,
        "token_count_estimate": 160,
        "source_type": "readme",
        "agent_id": "c1f2d1e740127354"
      },
      {
        "chunk_id": 3,
        "text": "}\n    }\n  }\n}\n```\n\n### Command Line Testing\n\nTest the server functionality:\n\n```bash\n# Run comprehensive tests\npython test_server.py\n\n# Test CLI interface\npython cli_example.py create_agent \"researcher\" \"assistant\" \"You are a research specialist\"\npython cli_example.py execute_workflow \"code_generation\" '{\"task\":\"Hello world\",\"language\":\"python\"}'\n```\n\n### Using Prompts\n\nThe server provides several built-in prompts:\n\n1. **autogen-workflow** - Create multi-agent workflows\n2. **code-review** - Set up collaborative code review\n3.",
        "start_pos": 5544,
        "end_pos": 6077,
        "token_count_estimate": 132,
        "source_type": "readme",
        "agent_id": "c1f2d1e740127354"
      },
      {
        "chunk_id": 4,
        "text": "ection\n- **Nested Chat**: Hierarchical conversation structures  \n- **Swarm Intelligence**: Coordinated problem solving (experimental)\n\n### Memory Management\n\n- Persistent agent memory across sessions\n- Conversation history tracking\n- Learning from interactions (teachable agents)\n- Memory cleanup and optimization\n\n## Troubleshooting\n\n### Common Issues\n\n1. **API Key Errors**: Ensure your OpenAI API key is valid and has sufficient credits\n2. **Import Errors**: Install all dependencies with `pip install -r requirements.txt --user`\n3. **Build Failures**: Check Node.js version (>= 18) and run `npm install`\n4. **Chat Failures**: Verify agent creation succeeded before attempting conversations\n\n### Debug Mode\n\nEnable detailed logging:\n\n```bash\nexport LOG_LEVEL=DEBUG\npython test_server.py\n```\n\n### Performance Tips\n\n- Use `gpt-4o-mini` for faster, cost-effective operations\n- Enable caching for repeated operations\n- Set appropriate timeout values for long-running workflows\n- Use quality checks only when needed (increases execution time)\n\n## Development\n\n### Running Tests\n\n```bash\n# Full test suite\npython test_server.py\n\n# Individual workflow tests  \npython -c \"\nimport asyncio\nfrom src.autogen_mcp.workflows import WorkflowManager\nwm = WorkflowManager()\nprint(asyncio.run(wm.execute_workflow('code_generation', {'task': 'test'})))\n\"\n```\n\n### Building\n\n```bash\nnpm run build\nnpm run lint\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests for new functionality\n5.",
        "start_pos": 7392,
        "end_pos": 8916,
        "token_count_estimate": 381,
        "source_type": "readme",
        "agent_id": "c1f2d1e740127354"
      },
      {
        "chunk_id": 5,
        "text": "nd persistence\n\n### v0.1.0\n- Basic AutoGen integration\n- Simple agent creation and chat execution\n- MCP tool interface\n\n## Support\n\nFor issues and questions:\n- Check the troubleshooting section above\n- Review the test examples in `test_server.py`\n- Open an issue on GitHub with detailed reproduction steps\n\n## License\n\nMIT License - see LICENSE file for details.\n\n# OpenAI API Key (optional, can also be set in config.json)\nOPENAI_API_KEY=your-openai-api-key\n```\n\n### Server Configuration\n\n1. Copy `config.json.example` to `config.json`:\n```bash\ncp config.json.example config.json\n```\n\n2. Configure the server settings:\n```json\n{\n  \"llm_config\": {\n    \"config_list\": [\n      {\n        \"model\": \"gpt-4\",\n        \"api_key\": \"your-openai-api-key\"\n      }\n    ],\n    \"temperature\": 0\n  },\n  \"code_execution_config\": {\n    \"work_dir\": \"workspace\",\n    \"use_docker\": false\n  }\n}\n```\n\n## Available Operations\n\nThe server supports three main operations:\n\n### 1. Creating Agents\n\n```json\n{\n  \"name\": \"create_agent\",\n  \"arguments\": {\n    \"name\": \"tech_lead\",\n    \"type\": \"assistant\",\n    \"system_message\": \"You are a technical lead with expertise in software architecture and design patterns.\"\n  }\n}\n```\n\n### 2. One-on-One Chat\n\n```json\n{\n  \"name\": \"execute_chat\",\n  \"arguments\": {\n    \"initiator\": \"agent1\",\n    \"responder\": \"agent2\",\n    \"message\": \"Let's discuss the system architecture.\"\n  }\n}\n```\n\n### 3. Group Chat\n\n```json\n{\n  \"name\": \"execute_group_chat\",\n  \"arguments\": {\n    \"agents\": [\"agent1\", \"agent2\", \"agent3\"],\n    \"message\": \"Let's review the proposed solution.\"\n  }\n}\n```\n\n## Error Handling\n\nCommon error scenarios include:\n\n1. Agent Creation Errors\n```json\n{\n  \"error\": \"Agent already exists\"\n}\n```\n\n2. Execution Errors\n```json\n{\n  \"error\": \"Agent not found\"\n}\n```\n\n3.",
        "start_pos": 9240,
        "end_pos": 11017,
        "token_count_estimate": 444,
        "source_type": "readme",
        "agent_id": "c1f2d1e740127354"
      },
      {
        "chunk_id": 6,
        "text": "ent variable not set\"\n}\n```\n\n## Architecture\n\nThe server follows a modular architecture:\n\n```\nsrc/\n‚îú‚îÄ‚îÄ autogen_mcp/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ agents.py      # Agent management and configuration\n‚îÇ   ‚îú‚îÄ‚îÄ config.py      # Configuration handling and validation\n‚îÇ   ‚îú‚îÄ‚îÄ server.py      # MCP server implementation\n‚îÇ   ‚îî‚îÄ‚îÄ workflows.py   # Conversation workflow management\n```\n\n## License\n\nMIT License - See LICENSE file for details",
        "start_pos": 11088,
        "end_pos": 11516,
        "token_count_estimate": 106,
        "source_type": "readme",
        "agent_id": "c1f2d1e740127354"
      }
    ]
  },
  {
    "agent_id": "619bfda018cea9dd",
    "name": "ai.smithery/DynamicEndpoints-m365-core-mcp",
    "source": "mcp",
    "source_url": "https://github.com/DynamicEndpoints/m365-core-mcp",
    "description": "*Updated September 25th 2025** Manage your Microsoft 365 services effortlessly. Create and manage‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage",
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-08T04:00:42.356429Z",
    "indexed_at": "2026-02-18T04:03:30.278768",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "## Latest Enhancements (January 2026)\n\n**New Identity & Security Tools:**\n- **`backup_policies`** - Export Microsoft 365 policies to JSON for backup, disaster recovery, and migration\n  - Conditional Access Policies, Named Locations, Authentication Strengths\n  - Intune Device Compliance & Configuration Policies\n  - App Protection Policies, Sensitivity Labels\n  - Full JSON backup with metadata and tenant information\n- **`manage_named_locations`** - Manage Conditional Access named locations\n  - IP address ranges (CIDR notation) with trust settings\n  - Country/region-based locations (ISO 3166-1 alpha-2 codes)\n  - Create, update, delete, and list operations\n- **`manage_authentication_strengths`** - View authentication strength policies\n  - Built-in and custom MFA strength policies\n  - Available authentication method combinations\n  - Authentication method configurations\n- **`manage_cross_tenant_access`** - Manage B2B collaboration settings\n  - Default and partner-specific access policies\n  - Inbound trust settings (MFA, compliant devices, hybrid joined)\n  - B2B collaboration inbound/outbound controls\n- **`manage_identity_protection`** - Monitor identity risks\n  - Risk detections and risky users\n  - Dismiss or confirm compromised users\n  - Filter by risk level and state\n\n**MCP SDK & Smithery Best Practices Update:**\n- **Upgraded to Zod v4** for schema validation (required by @smithery/sdk@3.0.1)\n- **Added OAuth 2.0 authentication infrastructure** with Azure AD/Microsoft Entra ID integration\n- **Implemented DNS rebinding protection** for HTTP transport security\n- **Added Smithery SDK integration** with proper module exports and configuration\n\n**Authentication Improvements:**\n- New `src/auth/` module with OAuth provider and middleware\n- Bearer token extraction and validation\n- Support for both OAuth and API key authentication\n- Token caching and automatic refresh\n- OAuth endpoints (`/oauth/authorize`, `/oauth/callback`, `/oauth/token`)\n\n**Build & Deployment Fixes:**\n- Fixed TypeScript compilation errors for Smithery deployment\n- Added type declarations for `csv-writer`, `xlsx`, `handlebars` modules\n- Excluded broken/backup files from TypeScript compilation\n- Updated `@types/express` to v5.0.0 for Express v5 compatibility\n- Fixed host header validation for production deployments\n\n**Configuration Updates:**\n- Added `smithery.config.js` for esbuild configuration\n- Updated `smithery.yaml` with OAuth config section\n- Added `module` field to `package.json` for Smithery compatibility\n- Updated dependencies: `@smithery/sdk@^3.0.1`, `@smithery/cli@^1.6.7`\n\n## Previous Enhancements (December 2024)\n\n**Comprehensive Microsoft 365 Policy Management Expansion:**\n- **Added 10 new policy management tools** covering all major Microsoft 365 products and services\n- **30+ policy types supported** across security, compliance, governance, and productivity\n- **Full lifecycle management** with create, read, update, delete, enable/disable operations\n- **Enterprise-ready features** including policy assignment, targeting, and multi-location support\n\n**New Policy Management Tools:**\n- `manage_retention_policies` - Data retention across SharePoint, Exchange, Teams, OneDrive\n- `manage_sensitivity_labels` - Information protection with encryption and content marking\n- `manage_information_protection_policies` - Label policies and organization-wide settings\n- `manage_conditional_access_policies` - Identity and access security with MFA, device compliance\n- `manage_defender_policies` - Advanced threat protection (Safe Attachments, Safe Links, Anti-Phishing)\n- `manage_teams_policies` - Teams governance (messaging, meetings, calling, apps)\n- `manage_exchange_policies` - Email security (OWA, ActiveSync, address book policies)\n- `manage_sharepoint_governance_policies` - Content and sharing governance\n- `manage_security_alert_policies` - Security event monitoring and automated responses\n\n**Policy Types Covered:**\n- **Security**: Conditional Access, Defender for Office 365 (Safe Attachments/Links, Anti-Phishing/Malware/Spam)\n- **Compliance**: DLP, Retention Policies, Sensitivity Labels, Information Protection\n- **Governance**: SharePoint Sharing/Access Policies, Information Barriers, Retention Labels\n- **Productivity**: Teams (Messaging/Meeting/Calling/App Setup), Exchange (OWA/ActiveSync/Address Book)\n- **Monitoring**: Security and Compliance Alert Policies with automated notifications\n\n**Key Features:**\n- Granular control with complex conditions and rules\n- Multi-location and multi-target support\n- Policy assignment to users, groups, and roles\n- Enable/disable functionality for testing\n- Comprehensive validation with Zod schemas\n- Type-safe implementations with full TypeScript support\n\nFor complete documentation, examples, and best practices, see:\n- [Policy Management Implementation Guide](./POLICY_MANAGEMENT_EXPANSION_COMPLETE.md)\n- [Quick Reference Guide](./POLICY_MANAGEMENT_QUICK_REFERENCE.md)\n\n## Previous Enhancements (September 25, 2025)\n\n**Universal Microsoft Graph API Framework - Complete Transformation:**\n- **Transformed from specialized tool to universal Graph API gateway** with access to 1000+ Microsoft Graph endpoints\n- **Dynamic Tool Generation System**: Automatically discovers and creates tools for all Graph API endpoints at runtime\n- **Advanced Graph API Features**: Batch operations, delta queries, webhook subscriptions, and advanced search\n- **Comprehensive Service Coverage**: Teams, OneNote, Planner, To Do, Bookings, Security, Analytics, and more\n- **Enhanced Authentication**: Multi-scope token caching with automatic scope detection for all Graph categories\n- **Real-time Capabilities**: Webhook subscriptions for live change notifications across all Microsoft 365 services\n\n**New Advanced Graph API Tools:**\n- `execute_graph_batch` - Execute up to 20 Graph requests in a single high-performance batch operation\n- `execute_delta_query` - Efficiently track changes to any Graph resource using delta queries\n- `manage_graph_subscriptions` - Create, update, delete, and list webhook subscriptions for real-time notifications\n- `execute_graph_search` - Advanced search across Microsoft 365 content with aggregations and filtering\n\n**Dynamic Category Tools (Generated at Runtime):**\n- `manage_teams_resources` - Complete Microsoft Teams management (teams, channels, messages, meetings, chat)\n- `manage_productivity_resources` - OneNote notebooks/pages, Planner plans/tasks, To Do lists, Bookings appointments\n- `manage_security_resources` - Security incidents, threat intelligence, advanced alerts, Defender integration\n- `manage_analytics_resources` - Usage reports, activity insights, trending documents, user analytics\n\n**Enhanced Windows Device Management:**\n- `manage_intune_windows_devices` - Complete Windows device lifecycle management in Intune\n- `manage_intune_windows_policies` - Windows configuration and compliance policy management\n- `manage_intune_windows_apps` - Windows application deployment and management\n- `manage_intune_windows_compliance` - Windows device compliance assessment and reporting\n\n**Technical Architecture Improvements:**\n- **GraphMetadataService**: Auto-discovers Graph endpoints and generates schemas dynamically\n- **DynamicToolGenerator**: Creates tools at runtime based on Graph API metadata\n- **GraphAdvancedFeatures**: Implements batch operations, webhooks, delta queries, and search\n- **Enhanced Error Handling**: Intelligent troubleshooting with Graph-specific error interpretation\n- **Performance Optimizations**: Token caching, batch operations, pagination, and retry logic\n- **Smithery Integration**: All 40+ tools properly configured for Smithery discovery\n\n**Scope Coverage Expansion:**\n- **Microsoft Teams**: Team.ReadBasic.All, Channel.Create, ChannelMessage.Send, OnlineMeetings.ReadWrite\n- **Productivity Apps**: Notes.ReadWrite, Tasks.ReadWrite, Bookings.ReadWrite.All\n- **Advanced Security**: SecurityIncident.ReadWrite.All, ThreatIntelligence.Read.All\n- **Analytics & Reports**: Reports.Read.All, Sites.Read.All for insights and trending data\n- **Power Platform**: Power BI API integration for datasets, reports, and dashboards\n\nThis transformation makes the M365 MCP server the **definitive solution for Microsoft 365 automation**, providing unprecedented access to the entire Microsoft Graph API ecosystem with advanced features and optimal performance.\n\n**Previous HTTP Transport Migration (September 25, 2025):**\n- Migrated M365 Core MCP Server from STDIO to HTTP transport\n- Added Express.js HTTP server with `/mcp` endpoint\n- Implemented CORS configuration for browser compatibility\n- Added configuration parsing from HTTP requests (Smithery integration)\n- Updated Dockerfile for HTTP container deployment (port 8081)\n- Updated smithery.yaml to use container runtime with HTTP transport\n- Added HTTP development and testing scripts\n- Created comprehensive HTTP transport test suite\n- Maintained backward compatibility with STDIO transport\n- Added support for both stateless and stateful HTTP modes\n- Added health and capabilities endpoints for monitoring\n\n## Previous Enhancements (June 16, 2025)\n\n**Extended Resources and Prompts (40 Resources + 5 Comprehensive Prompts):**\n- Added 40 additional Microsoft 365 resources covering security, compliance, device management, and collaboration\n- Implemented 5 intelligent prompts for automated analysis and recommendations:\n  - **Security Assessment**: Comprehensive security posture analysis with recommendations\n  - **Compliance Review**: Framework-specific compliance gap analysis (SOC2, ISO27001, NIST, GDPR, HIPAA)\n  - **User Access Review**: Individual and organization-wide access rights analysis\n  - **Device Compliance Analysis**: Intune device management and compliance assessment\n  - **Collaboration Governance**: Teams and SharePoint governance analysis\n- Enhanced resource coverage includes:\n  - Security alerts, incidents, and conditional access policies\n  - Intune device management, apps, and compliance policies\n  - Extended user, group, and team information\n  - Information protection and DLP policies\n  - Audit logs and privileged access data\n\nFor detailed information about all new resources and prompts, see [EXTENDED_FEATURES.md](./EXTENDED_FEATURES.md).\n\n## Recent Enhancements (June 7, 2025)\n\n**TypeScript Error Resolution & Compliance Module Enhancements:**\n- Resolved all TypeScript errors in `src/server.ts` and `src/handlers/compliance-handler.ts` related to incorrect tool registration syntax and type mismatches.\n- Enhanced the compliance module to include comprehensive support for CIS (Center for Internet Security) controls.\n- Updated `ComplianceFrameworkArgs` to recognize 'cis' as a valid framework.\n- Corrected parameter parsing in compliance handler functions to properly handle string-to-number conversions for implementation groups.\n\n**Conditional Access Policy Review & Reporting:**\n- Implemented functionality to retrieve and review Microsoft Entra Conditional Access policies.\n\n\n## Recent Enhancements (May 3, 2025)\n\n**MCP and HTTP Streaming Updates:**\n- Updated MCP SDK to version 1.12.0\n- Enhanced HTTP streaming support with both stateful and stateless modes\n- Added environment variables for configuring HTTP transport options\n\n## Previous Enhancements (April 4, 2025)\n\nAdded several new tools to expand Microsoft Entra ID management and Security & Compliance capabilities:\n\n**Entra ID Management:**\n- `manage_azuread_roles`: Manage Entra ID directory roles and assignments.\n- `manage_azuread_apps`: Manage Entra ID application registrations (list, view, owners).\n- `manage_azuread_devices`: Manage Entra ID device objects (list, view, enable/disable/delete).\n- `manage_service_principals`: Manage Entra ID Service Principals (list, view, owners).\n\n**Generic API Access:**\n- `dynamicendpoints m365 assistant`: Call arbitrary Microsoft Graph (including Entra APIs) or Azure Resource Management API endpoints.\n\n**Security & Compliance:**\n- `search_audit_log`: Search the Entra ID Unified Audit Log.\n- `manage_alerts`: List and view security alerts from Microsoft security products.\n\n**Note:** Ensure the associated Entra ID App Registration has the necessary Graph API permissions and Azure RBAC roles for these tools to function correctly.\n\n---\n\n# Microsoft 365 Core MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@DynamicEndpoints/m365-core-mcp)](https://smithery.ai/server/@DynamicEndpoints/m365-core-mcp)\n\nAn MCP server that provides tools for managing Microsoft 365 core services including:\n- Distribution Lists\n- Security Groups\n- Microsoft 365 Groups\n- Exchange Settings\n- User Management\n- Offboarding Processes\n- SharePoint Sites and Lists\n\n## Features\n\n### Core Microsoft 365 Management\n- **Distribution Lists**: Create, delete, manage membership and settings\n- **Security Groups**: Full lifecycle management with mail-enabled options\n- **Microsoft 365 Groups**: Create, configure, and manage owners/members\n- **Exchange Settings**: Mailbox, transport, organization, and retention policies\n- **User Management**: Get and update user settings and configurations\n- **Offboarding Processes**: Automated user offboarding with configurable options\n\n### SharePoint Management\n- **Site Management**: Create, update, delete sites with template support\n- **List Management**: Create, configure, and manage SharePoint lists\n- **Item Management**: Add, update, and retrieve list items\n- **Permissions**: Manage site users and permissions\n- **Settings**: Configure site-level and organization settings\n\n### Azure AD Management\n- **Role Management**: Assign and manage directory roles and role assignments\n- **Application Management**: Manage app registrations, owners, and settings\n- **Device Management**: Enable, disable, delete Azure AD devices\n- **Service Principals**: Manage service principal objects and ownership\n\n### Security & Compliance\n- **Audit Logging**: Search and analyze Microsoft 365 Unified Audit Log\n- **Security Alerts**: List, view, and manage security alerts across Microsoft products\n- **Data Loss Prevention**: Create, configure, and manage DLP policies and incidents\n- **Sensitivity Labels**: Manage Microsoft Purview sensitivity labels and policies\n- **Compliance Frameworks**: Support for HITRUST, ISO27001, SOC2, CIS Controls\n- **Assessment & Monitoring**: Automated compliance assessments and continuous monitoring\n- **Evidence Collection**: Automated evidence gathering for compliance audits\n- **Gap Analysis**: Cross-framework compliance gap analysis and remediation planning\n\n### Intune Device Management (macOS Focus)\n- **Device Inventory**: List, filter, and manage macOS devices in Intune\n- **Policy Management**: Create, deploy, and monitor macOS configuration policies\n- **Application Management**: Deploy and manage macOS applications via Intune\n- **Compliance Monitoring**: Track and enforce macOS device compliance policies\n\n### Advanced Features\n- **Dynamic API Access**: Call arbitrary Microsoft Graph and Azure Resource Management APIs\n- **Real-time Capabilities**: Server-sent events, progress reporting, streaming responses\n- **Intelligent Prompts**: 5 comprehensive analysis prompts for security, compliance, and governance\n- **Extended Resources**: 44 resources covering security, compliance, device management, and collaboration\n- **Modern MCP Features**: Enhanced error handling, response validation, lazy loading\n\n## Setup\n\n### Installing via Smithery\n\nTo install Microsoft 365 Core Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/m365-core-mcp):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/m365-core-mcp --client claude\n```\n\n### Installing Manually\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Create a `.env` file based on `.env.example`:\n   ```\n   MS_TENANT_ID=your-tenant-id\n   MS_CLIENT_ID=your-client-id\n   MS_CLIENT_SECRET=your-client-secret\n   \n   # Optional Configuration\n   # LOG_LEVEL=info    # debug, info, warn, error\n   # PORT=3000         # Port for HTTP server if needed\n   # USE_HTTP=true     # Set to 'true' to use HTTP transport instead of stdio\n   # STATELESS=false   # Set to 'true' to use stateless HTTP mode (no session management)\n   ```\n4. Register an application in Azure AD:\n   - **Required Microsoft Graph permissions:**\n     - Directory.ReadWrite.All\n     - Group.ReadWrite.All\n     - User.ReadWrite.All\n     - Mail.ReadWrite\n     - MailboxSettings.ReadWrite\n     - Organization.ReadWrite.All\n     - Sites.ReadWrite.All\n     - Sites.Manage.All\n     - SecurityEvents.ReadWrite.All\n     - SecurityActions.ReadWrite.All\n     - Device.ReadWrite.All\n     - DeviceManagementConfiguration.ReadWrite.All\n     - DeviceManagementManagedDevices.ReadWrite.All\n     - DeviceManagementApps.ReadWrite.All\n     - InformationProtectionPolicy.ReadWrite.All\n     - Policy.ReadWrite.ConditionalAccess\n     - RoleManagement.ReadWrite.Directory\n     - AuditLog.Read.All\n     - Reports.Read.All\n     - ThreatIndicators.ReadWrite.OwnedBy\n     - IdentityRiskyUser.ReadWrite.All\n     - IdentityRiskEvent.Read.All\n\n   - **Required Azure RBAC roles** (for Azure Resource Management):\n     - Security Admin (for security-related operations)\n     - Compliance Administrator (for compliance management)\n     - Intune Administrator (for device management)\n     - Reports Reader (for audit and reporting functions)\n\n5. Build the server:\n   ```bash\n   npm run build\n   ```\n\n6. Start the server:\n   ```bash\n   npm start\n   ```\n\n## Transport Options\n\nThe server supports multiple transport options for MCP communication:\n\n### stdio Transport\n\nBy default, the server uses stdio transport, which is ideal for:\n- Command-line tools and direct integrations\n- Local development and testing\n- Integration with Smithery and other MCP clients that support stdio\n\n### HTTP Transport\n\nThe server also supports HTTP transport with two modes:\n\n#### Stateful Mode (With Session Management)\n\nThis is the default HTTP mode when `USE_HTTP=true` and `STATELESS=false`:\n- Maintains session state between requests\n- Supports server-to-client notifications via GET requests\n- Handles session termination via DELETE requests\n- Ideal for long-running sessions and interactive applications\n- Provides better performance for multiple requests in the same session\n\n#### Stateless Mode\n\nEnable this mode by setting `USE_HTTP=true` and `STATELESS=true`:\n- Creates a new server instance for each request\n- No session state is maintained between requests\n- Only supports POST requests (GET and DELETE are not supported)\n- Ideal for RESTful scenarios where each request is independent\n- Better for horizontally scaled deployments without shared session state\n- Simpler API wrappers where session management isn't needed\n\nTo configure the transport options, set the appropriate environment variables in your `.env` file:\n```\nUSE_HTTP=true     # Use HTTP transport instead of stdio\nSTATELESS=false   # Use stateful mode with session management (default)\nPORT=3000         # Port for the HTTP server\n```\n\n## Usage\n\nThe server provides MCP tools and resources that can be used to manage various aspects of Microsoft 365. Each tool accepts specific parameters and returns structured responses.\n\n### Tools\n\nThe server provides **29 comprehensive tools** for Microsoft 365 management:\n\n#### Core Management Tools\n- `manage_distribution_lists` - Create, delete, and manage distribution lists and membership\n- `manage_security_groups` - Create, delete, and manage security groups and membership  \n- `manage_m365_groups` - Create, delete, and manage Microsoft 365 groups and membership\n- `manage_exchange_settings` - Configure mailbox, transport, organization, and retention settings\n- `manage_user_settings` - Get and update user settings and configurations\n- `manage_offboarding` - Automated user offboarding processes with configurable options\n\n#### SharePoint Management Tools\n- `manage_sharepoint_sites` - Create, update, delete SharePoint sites and manage users\n- `manage_sharepoint_lists` - Create, update, delete SharePoint lists and manage items\n\n#### Azure AD Management Tools\n- `manage_azuread_roles` - Manage Azure AD directory roles and role assignments\n- `manage_azuread_apps` - Manage Azure AD application registrations and owners\n- `manage_azuread_devices` - Manage Azure AD device objects (enable, disable, delete)\n- `manage_service_principals` - Manage Azure AD Service Principals and ownership\n\n#### Security & Compliance Tools\n- `search_audit_log` - Search the Microsoft 365 Unified Audit Log\n- `manage_alerts` - List and view security alerts from Microsoft security products\n- `manage_dlp_policies` - Manage Data Loss Prevention policies and configurations\n- `manage_dlp_incidents` - Handle DLP policy violations and incident management\n- `manage_sensitivity_labels` - Manage Microsoft Purview sensitivity labels\n\n#### Intune Device Management Tools\n- `manage_intune_macos_devices` - Manage Intune macOS devices and enrollment\n- `manage_intune_macos_policies` - Configure and deploy macOS device policies\n- `manage_intune_macos_apps` - Deploy and manage macOS applications via Intune\n- `manage_intune_macos_compliance` - Monitor and enforce macOS device compliance\n\n#### Compliance Framework Tools\n- `manage_compliance_frameworks` - Configure compliance frameworks (HITRUST, ISO27001, SOC2)\n- `manage_compliance_assessments` - Run and manage compliance assessments\n- `manage_compliance_monitoring` - Monitor compliance status and configure alerts\n- `manage_evidence_collection` - Collect and manage compliance evidence\n- `manage_gap_analysis` - Perform compliance gap analysis and remediation planning\n- `manage_cis_compliance` - Manage CIS Controls compliance and benchmarks\n\n#### Audit & Reporting Tools\n- `generate_audit_reports` - Generate comprehensive audit reports for various frameworks\n\n#### Dynamic API Access\n- `dynamicendpoints m365 assistant` - Call arbitrary Microsoft Graph or Azure Resource Management API endpoints\n\n### Resources\n\nThe server provides **44 comprehensive resources** covering security, compliance, device management, and collaboration:\n\n#### Core Resources\n- `sharepoint_sites` - SharePoint site information and configuration\n- `sharepoint_lists` - SharePoint list structures and metadata  \n- `sharepoint_list_items` - Items within SharePoint lists\n- `security_incidents` - Microsoft security incidents and details\n\n#### Extended Security Resources (20 resources)\n- Security alerts and incidents from Microsoft Defender\n- Conditional access policies and assignments\n- Privileged access management data\n- Threat intelligence and vulnerability assessments\n- Identity protection risks and policies\n- Authentication methods and security defaults\n- Compliance policies and their status\n- Data governance and retention policies\n- Insider risk management insights\n- Security baselines and configurations\n\n#### Device Management Resources (10 resources)\n- Intune device inventories and compliance status\n- Mobile application management policies\n- Device configuration profiles and assignments\n- Compliance policies for various platforms\n- App protection policies and status\n- Device enrollment configurations\n- Update policies and deployment rings\n- Certificate profiles and management\n- Wi-Fi and VPN configuration profiles\n- Endpoint protection policies\n\n#### Collaboration Resources (10 resources)\n- Microsoft Teams structures and policies\n- Exchange Online configurations and settings\n- Calendar and scheduling information\n- OneDrive storage and sharing policies\n- Planner tasks and project management\n- Viva Engage (Yammer) communities\n- Power Platform environments and apps\n- Booking services and appointments\n- Whiteboard collaboration data\n- Stream video content and policies\n\n#### Extended Dynamic Resources\nAll resources support URI templates for specific object access:\n- `m365://security/alerts/{alertId}` - Specific security alert details\n- `m365://devices/{deviceId}` - Individual device information\n- `m365://users/{userId}/compliance` - User-specific compliance status\n- `m365://teams/{teamId}/governance` - Team governance and policies\n\n### Intelligent Prompts\n\nThe server provides **5 comprehensive prompts** for automated analysis and recommendations:\n\n#### Security Assessment Prompt\n- **Purpose**: Comprehensive security posture analysis with actionable recommendations\n- **Scope**: Security policies, access controls, threat detection, identity protection\n- **Output**: Risk assessment, security gaps, remediation roadmap\n\n#### Compliance Review Prompt  \n- **Purpose**: Framework-specific compliance gap analysis\n- **Frameworks**: SOC2, ISO27001, NIST, GDPR, HIPAA, CIS Controls\n- **Scope**: Control implementation status, evidence collection, audit readiness\n- **Output**: Compliance dashboard, gap analysis, remediation plans\n\n#### User Access Review Prompt\n- **Purpose**: Individual and organization-wide access rights analysis\n- **Scope**: Role assignments, group memberships, application access, privileged accounts\n- **Output**: Access recommendations, risk-based prioritization, cleanup tasks\n\n#### Device Compliance Analysis Prompt\n- **Purpose**: Intune device management and compliance assessment\n- **Scope**: Device policies, compliance status, security configurations, app management\n- **Output**: Compliance reports, policy recommendations, deployment guidance\n\n#### Collaboration Governance Prompt\n- **Purpose**: Teams and SharePoint governance analysis\n- **Scope**: Team structures, sharing policies, external access, data governance\n- **Output**: Governance recommendations, policy suggestions, compliance alignment\n\nEach prompt provides contextual analysis, actionable insights, and integration with the corresponding management tools for immediate remediation.\n\n### Example Tool Usage\n\n```typescript\n// Managing a distribution list\nawait callTool('manage_distribution_lists', {\n  action: 'create',\n  displayName: 'Marketing Team',\n  emailAddress: 'marketing@company.com',\n  members: ['user1@company.com', 'user2@company.com']\n});\n\n// Managing security groups\nawait callTool('manage_security_groups', {\n  action: 'create',\n  displayName: 'IT Admins',\n  description: 'IT Administration Team',\n  members: ['admin1@company.com']\n});\n\n// Managing Azure AD roles (note: using correct tool name)\nawait callTool('manage_azuread_roles', {\n  action: 'assign_role',\n  roleId: 'role-id-here',\n  principalId: 'user-id-here'\n});\n\n// Managing DLP policies\nawait callTool('manage_dlp_policies', {\n  action: 'create',\n  policyName: 'Financial Data Protection',\n  rules: [{\n    name: 'Block Credit Cards',\n    conditions: { contentContainsSensitiveInfo: ['CreditCardNumber'] },\n    actions: { blockAccess: true }\n  }]\n});\n\n// Managing Intune macOS devices\nawait callTool('manage_intune_macos_devices', {\n  action: 'list',\n  filters: { complianceState: 'compliant' }\n});\n\n// Running compliance assessments\nawait callTool('manage_compliance_assessments', {\n  action: 'run_assessment',\n  framework: 'iso27001',\n  scope: ['access_control', 'data_protection'],\n  settings: {\n    automated: true,\n    generateRemediation: true\n  }\n});\n\n// Generating audit reports\nawait callTool('generate_audit_reports', {\n  framework: 'soc2',\n  reportType: 'comprehensive',\n  dateRange: { start: '2025-01-01', end: '2025-06-16' },\n  format: 'pdf',\n  includeEvidence: true\n});\n\n// Managing Exchange settings\nawait callTool('manage_exchange_settings', {\n  action: 'update',\n  settingType: 'mailbox',\n  target: 'user@company.com',\n  settings: {\n    automateProcessing: {\n      autoReplyEnabled: true\n    }\n  }\n});\n\n// Managing SharePoint sites\nawait callTool('manage_sharepoint_sites', {\n  action: 'create',\n  title: 'Marketing Site',\n  description: 'Site for marketing team',\n  template: 'STS#0',\n  url: 'https://contoso.sharepoint.com/sites/marketing',\n  owners: ['user1@company.com'],\n  members: ['user2@company.com', 'user3@company.com']\n});\n\n// Managing SharePoint lists\nawait callTool('manage_sharepoint_lists', {\n  action: 'create',\n  siteId: 'contoso.sharepoint.com,5a14e1cf-e284-4722-8f50-a5e1b2b0a8d6,9528e4bb-7660-4b11-a758-9d8fb3ca295f',\n  title: 'Project Tasks',\n  description: 'List of project tasks',\n  columns: [\n    { name: 'Title', type: 'text', required: true },\n    { name: 'DueDate', type: 'dateTime' },\n    { name: 'Status', type: 'choice', choices: ['Not Started', 'In Progress', 'Completed'] }\n  ]\n});\n\n// Dynamic API calls for custom scenarios\nawait callTool('dynamicendpoints m365 assistant', {\n  apiType: 'graph',\n  path: '/me/messages',\n  method: 'get',\n  queryParams: { '$top': '10', '$filter': 'isRead eq false' }\n});\n```\n\n## Implementation Details\n\n### Schema Validation\n\nThe server uses Zod for schema validation, providing:\n- Runtime type checking for all inputs\n- Detailed validation error messages\n- Type inference for TypeScript\n- Automatic documentation of input schemas\n\n### Error Handling\n\nThe server implements comprehensive error handling:\n- Input validation for all parameters\n- Graph API error handling\n- Token refresh management\n- Detailed error messages with proper error codes\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Export Microsoft 365 policies to JSON for backup, disaster recovery, and migration",
        "Manage Conditional Access named locations including IP ranges and country-based locations",
        "View and manage authentication strength policies including MFA configurations",
        "Manage B2B collaboration settings with inbound and outbound access policies",
        "Monitor identity risks with risk detections and user risk state management",
        "Perform full lifecycle management of over 30 Microsoft 365 policy types with create, read, update, delete, enable/disable operations",
        "Execute batch operations, delta queries, webhook subscriptions, and advanced searches across Microsoft Graph API endpoints",
        "Dynamically generate tools at runtime for comprehensive Microsoft Graph API coverage including Teams, OneNote, Planner, Security, and Analytics",
        "Manage Windows devices and policies in Intune including compliance and application deployment",
        "Support OAuth 2.0 and API key authentication with token caching and automatic refresh",
        "Operate as an HTTP server with CORS support, health and capabilities endpoints, and support for both stateless and stateful HTTP modes"
      ],
      "limitations": [
        "No explicit limitations or rate limits mentioned in the documentation"
      ],
      "requirements": [
        "OAuth 2.0 authentication infrastructure with Azure AD/Microsoft Entra ID integration",
        "Smithery SDK version 3.0.1 or higher for schema validation and integration",
        "Express.js HTTP server environment for HTTP transport mode",
        "TypeScript environment compatible with Express v5 and required type declarations",
        "Configuration files such as smithery.config.js and smithery.yaml with OAuth and runtime settings"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation and configuration details, extensive usage examples with numerous tools and features, clear descriptions of capabilities, authentication methods, and deployment considerations, making it highly complete and well-structured.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "## Latest Enhancements (January 2026)\n\n**New Identity & Security Tools:**\n- **`backup_policies`** - Export Microsoft 365 policies to JSON for backup, disaster recovery, and migration\n  - Conditional Access Policies, Named Locations, Authentication Strengths\n  - Intune Device Compliance & Configuration Policies\n  - App Protection Policies, Sensitivity Labels\n  - Full JSON backup with metadata and tenant information\n- **`manage_named_locations`** - Manage Conditional Access named locations\n  - IP address ranges (CIDR notation) with trust settings\n  - Country/region-based locations (ISO 3166-1 alpha-2 codes)\n  - Create, update, delete, and list operations\n- **`manage_authentication_strengths`** - View authentication strength policies\n  - Built-in and custom MFA strength policies\n  - Available authentication method combinations\n  - Authentication method configurations\n- **`manage_cross_tenant_access`** - Manage B2B collaboration settings\n  - Default and partner-specific access policies\n  - Inbound trust settings (MFA, compliant devices, hybrid joined)\n  - B2B collaboration inbound/outbound controls\n- **`manage_identity_protection`** - Monitor identity risks\n  - Risk detections and risky users\n  - Dismiss or confirm compromised users\n  - Filter by risk level and state\n\n**MCP SDK & Smithery Best Practices Update:**\n- **Upgraded to Zod v4** for schema validation (required by @smithery/sdk@3.0.1)\n- **Added OAuth 2.0 authentication infrastructure** with Azure AD/Microsoft Entra ID integration\n- **Implemented DNS rebinding protection** for HTTP transport security\n- **Added Smithery SDK integration** with proper module exports and configuration\n\n**Authentication Improvements:**\n- New `src/auth/` module with OAuth provider and middleware\n- Bearer token extraction and validation\n- Support for both OAuth and API key authentication\n- Token caching and automatic refresh\n- OAuth endpoints (`/oauth/authorize`, `/oauth/callback`, `/oauth/token`)\n\n**Build & Deployment Fixes:**\n- Fixed TypeScript compilation errors for Smithery deplo",
        "start_pos": 0,
        "end_pos": 2048,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 1,
        "text": "- Token caching and automatic refresh\n- OAuth endpoints (`/oauth/authorize`, `/oauth/callback`, `/oauth/token`)\n\n**Build & Deployment Fixes:**\n- Fixed TypeScript compilation errors for Smithery deployment\n- Added type declarations for `csv-writer`, `xlsx`, `handlebars` modules\n- Excluded broken/backup files from TypeScript compilation\n- Updated `@types/express` to v5.0.0 for Express v5 compatibility\n- Fixed host header validation for production deployments\n\n**Configuration Updates:**\n- Added `smithery.config.js` for esbuild configuration\n- Updated `smithery.yaml` with OAuth config section\n- Added `module` field to `package.json` for Smithery compatibility\n- Updated dependencies: `@smithery/sdk@^3.0.1`, `@smithery/cli@^1.6.7`\n\n## Previous Enhancements (December 2024)\n\n**Comprehensive Microsoft 365 Policy Management Expansion:**\n- **Added 10 new policy management tools** covering all major Microsoft 365 products and services\n- **30+ policy types supported** across security, compliance, governance, and productivity\n- **Full lifecycle management** with create, read, update, delete, enable/disable operations\n- **Enterprise-ready features** including policy assignment, targeting, and multi-location support\n\n**New Policy Management Tools:**\n- `manage_retention_policies` - Data retention across SharePoint, Exchange, Teams, OneDrive\n- `manage_sensitivity_labels` - Information protection with encryption and content marking\n- `manage_information_protection_policies` - Label policies and organization-wide settings\n- `manage_conditional_access_policies` - Identity and access security with MFA, device compliance\n- `manage_defender_policies` - Advanced threat protection (Safe Attachments, Safe Links, Anti-Phishing)\n- `manage_teams_policies` - Teams governance (messaging, meetings, calling, apps)\n- `manage_exchange_policies` - Email security (OWA, ActiveSync, address book policies)\n- `manage_sharepoint_governance_policies` - Content and sharing governance\n- `manage_security_alert_policies` - Security event monitoring and automa",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 2,
        "text": "l security (OWA, ActiveSync, address book policies)\n- `manage_sharepoint_governance_policies` - Content and sharing governance\n- `manage_security_alert_policies` - Security event monitoring and automated responses\n\n**Policy Types Covered:**\n- **Security**: Conditional Access, Defender for Office 365 (Safe Attachments/Links, Anti-Phishing/Malware/Spam)\n- **Compliance**: DLP, Retention Policies, Sensitivity Labels, Information Protection\n- **Governance**: SharePoint Sharing/Access Policies, Information Barriers, Retention Labels\n- **Productivity**: Teams (Messaging/Meeting/Calling/App Setup), Exchange (OWA/ActiveSync/Address Book)\n- **Monitoring**: Security and Compliance Alert Policies with automated notifications\n\n**Key Features:**\n- Granular control with complex conditions and rules\n- Multi-location and multi-target support\n- Policy assignment to users, groups, and roles\n- Enable/disable functionality for testing\n- Comprehensive validation with Zod schemas\n- Type-safe implementations with full TypeScript support\n\nFor complete documentation, examples, and best practices, see:\n- [Policy Management Implementation Guide](./POLICY_MANAGEMENT_EXPANSION_COMPLETE.md)\n- [Quick Reference Guide](./POLICY_MANAGEMENT_QUICK_REFERENCE.md)\n\n## Previous Enhancements (September 25, 2025)\n\n**Universal Microsoft Graph API Framework - Complete Transformation:**\n- **Transformed from specialized tool to universal Graph API gateway** with access to 1000+ Microsoft Graph endpoints\n- **Dynamic Tool Generation System**: Automatically discovers and creates tools for all Graph API endpoints at runtime\n- **Advanced Graph API Features**: Batch operations, delta queries, webhook subscriptions, and advanced search\n- **Comprehensive Service Coverage**: Teams, OneNote, Planner, To Do, Bookings, Security, Analytics, and more\n- **Enhanced Authentication**: Multi-scope token caching with automatic scope detection for all Graph categories\n- **Real-time Capabilities**: Webhook subscriptions for live change notifications across all Microsoft 365 servic",
        "start_pos": 3696,
        "end_pos": 5744,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 3,
        "text": "n**: Multi-scope token caching with automatic scope detection for all Graph categories\n- **Real-time Capabilities**: Webhook subscriptions for live change notifications across all Microsoft 365 services\n\n**New Advanced Graph API Tools:**\n- `execute_graph_batch` - Execute up to 20 Graph requests in a single high-performance batch operation\n- `execute_delta_query` - Efficiently track changes to any Graph resource using delta queries\n- `manage_graph_subscriptions` - Create, update, delete, and list webhook subscriptions for real-time notifications\n- `execute_graph_search` - Advanced search across Microsoft 365 content with aggregations and filtering\n\n**Dynamic Category Tools (Generated at Runtime):**\n- `manage_teams_resources` - Complete Microsoft Teams management (teams, channels, messages, meetings, chat)\n- `manage_productivity_resources` - OneNote notebooks/pages, Planner plans/tasks, To Do lists, Bookings appointments\n- `manage_security_resources` - Security incidents, threat intelligence, advanced alerts, Defender integration\n- `manage_analytics_resources` - Usage reports, activity insights, trending documents, user analytics\n\n**Enhanced Windows Device Management:**\n- `manage_intune_windows_devices` - Complete Windows device lifecycle management in Intune\n- `manage_intune_windows_policies` - Windows configuration and compliance policy management\n- `manage_intune_windows_apps` - Windows application deployment and management\n- `manage_intune_windows_compliance` - Windows device compliance assessment and reporting\n\n**Technical Architecture Improvements:**\n- **GraphMetadataService**: Auto-discovers Graph endpoints and generates schemas dynamically\n- **DynamicToolGenerator**: Creates tools at runtime based on Graph API metadata\n- **GraphAdvancedFeatures**: Implements batch operations, webhooks, delta queries, and search\n- **Enhanced Error Handling**: Intelligent troubleshooting with Graph-specific error interpretation\n- **Performance Optimizations**: Token caching, batch operations, pagination, and retry logic\n- **S",
        "start_pos": 5544,
        "end_pos": 7592,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 4,
        "text": "- **Enhanced Error Handling**: Intelligent troubleshooting with Graph-specific error interpretation\n- **Performance Optimizations**: Token caching, batch operations, pagination, and retry logic\n- **Smithery Integration**: All 40+ tools properly configured for Smithery discovery\n\n**Scope Coverage Expansion:**\n- **Microsoft Teams**: Team.ReadBasic.All, Channel.Create, ChannelMessage.Send, OnlineMeetings.ReadWrite\n- **Productivity Apps**: Notes.ReadWrite, Tasks.ReadWrite, Bookings.ReadWrite.All\n- **Advanced Security**: SecurityIncident.ReadWrite.All, ThreatIntelligence.Read.All\n- **Analytics & Reports**: Reports.Read.All, Sites.Read.All for insights and trending data\n- **Power Platform**: Power BI API integration for datasets, reports, and dashboards\n\nThis transformation makes the M365 MCP server the **definitive solution for Microsoft 365 automation**, providing unprecedented access to the entire Microsoft Graph API ecosystem with advanced features and optimal performance.",
        "start_pos": 7392,
        "end_pos": 8378,
        "token_count_estimate": 246,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 5,
        "text": "resources covering security, compliance, device management, and collaboration\n- Implemented 5 intelligent prompts for automated analysis and recommendations:\n  - **Security Assessment**: Comprehensive security posture analysis with recommendations\n  - **Compliance Review**: Framework-specific compliance gap analysis (SOC2, ISO27001, NIST, GDPR, HIPAA)\n  - **User Access Review**: Individual and organization-wide access rights analysis\n  - **Device Compliance Analysis**: Intune device management and compliance assessment\n  - **Collaboration Governance**: Teams and SharePoint governance analysis\n- Enhanced resource coverage includes:\n  - Security alerts, incidents, and conditional access policies\n  - Intune device management, apps, and compliance policies\n  - Extended user, group, and team information\n  - Information protection and DLP policies\n  - Audit logs and privileged access data\n\nFor detailed information about all new resources and prompts, see [EXTENDED_FEATURES.md](./EXTENDED_FEATURES.md).\n\n## Recent Enhancements (June 7, 2025)\n\n**TypeScript Error Resolution & Compliance Module Enhancements:**\n- Resolved all TypeScript errors in `src/server.ts` and `src/handlers/compliance-handler.ts` related to incorrect tool registration syntax and type mismatches.\n- Enhanced the compliance module to include comprehensive support for CIS (Center for Internet Security) controls.\n- Updated `ComplianceFrameworkArgs` to recognize 'cis' as a valid framework.\n- Corrected parameter parsing in compliance handler functions to properly handle string-to-number conversions for implementation groups.\n\n**Conditional Access Policy Review & Reporting:**\n- Implemented functionality to retrieve and review Microsoft Entra Conditional Access policies.",
        "start_pos": 9240,
        "end_pos": 10992,
        "token_count_estimate": 438,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 6,
        "text": "o version 1.12.0\n- Enhanced HTTP streaming support with both stateful and stateless modes\n- Added environment variables for configuring HTTP transport options\n\n## Previous Enhancements (April 4, 2025)\n\nAdded several new tools to expand Microsoft Entra ID management and Security & Compliance capabilities:\n\n**Entra ID Management:**\n- `manage_azuread_roles`: Manage Entra ID directory roles and assignments.\n- `manage_azuread_apps`: Manage Entra ID application registrations (list, view, owners).\n- `manage_azuread_devices`: Manage Entra ID device objects (list, view, enable/disable/delete).\n- `manage_service_principals`: Manage Entra ID Service Principals (list, view, owners).\n\n**Generic API Access:**\n- `dynamicendpoints m365 assistant`: Call arbitrary Microsoft Graph (including Entra APIs) or Azure Resource Management API endpoints.\n\n**Security & Compliance:**\n- `search_audit_log`: Search the Entra ID Unified Audit Log.\n- `manage_alerts`: List and view security alerts from Microsoft security products.\n\n**Note:** Ensure the associated Entra ID App Registration has the necessary Graph API permissions and Azure RBAC roles for these tools to function correctly.",
        "start_pos": 11088,
        "end_pos": 12258,
        "token_count_estimate": 292,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 7,
        "text": "s/members\n- **Exchange Settings**: Mailbox, transport, organization, and retention policies\n- **User Management**: Get and update user settings and configurations\n- **Offboarding Processes**: Automated user offboarding with configurable options\n\n### SharePoint Management\n- **Site Management**: Create, update, delete sites with template support\n- **List Management**: Create, configure, and manage SharePoint lists\n- **Item Management**: Add, update, and retrieve list items\n- **Permissions**: Manage site users and permissions\n- **Settings**: Configure site-level and organization settings\n\n### Azure AD Management\n- **Role Management**: Assign and manage directory roles and role assignments\n- **Application Management**: Manage app registrations, owners, and settings\n- **Device Management**: Enable, disable, delete Azure AD devices\n- **Service Principals**: Manage service principal objects and ownership\n\n### Security & Compliance\n- **Audit Logging**: Search and analyze Microsoft 365 Unified Audit Log\n- **Security Alerts**: List, view, and manage security alerts across Microsoft products\n- **Data Loss Prevention**: Create, configure, and manage DLP policies and incidents\n- **Sensitivity Labels**: Manage Microsoft Purview sensitivity labels and policies\n- **Compliance Frameworks**: Support for HITRUST, ISO27001, SOC2, CIS Controls\n- **Assessment & Monitoring**: Automated compliance assessments and continuous monitoring\n- **Evidence Collection**: Automated evidence gathering for compliance audits\n- **Gap Analysis**: Cross-framework compliance gap analysis and remediation planning\n\n### Intune Device Management (macOS Focus)\n- **Device Inventory**: List, filter, and manage macOS devices in Intune\n- **Policy Management**: Create, deploy, and monitor macOS configuration policies\n- **Application Management**: Deploy and manage macOS applications via Intune\n- **Compliance Monitoring**: Track and enforce macOS device compliance policies\n\n### Advanced Features\n- **Dynamic API Access**: Call arbitrary Microsoft Graph and Azure Res",
        "start_pos": 12936,
        "end_pos": 14984,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 8,
        "text": "OS applications via Intune\n- **Compliance Monitoring**: Track and enforce macOS device compliance policies\n\n### Advanced Features\n- **Dynamic API Access**: Call arbitrary Microsoft Graph and Azure Resource Management APIs\n- **Real-time Capabilities**: Server-sent events, progress reporting, streaming responses\n- **Intelligent Prompts**: 5 comprehensive analysis prompts for security, compliance, and governance\n- **Extended Resources**: 44 resources covering security, compliance, device management, and collaboration\n- **Modern MCP Features**: Enhanced error handling, response validation, lazy loading\n\n## Setup\n\n### Installing via Smithery\n\nTo install Microsoft 365 Core Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/m365-core-mcp):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/m365-core-mcp --client claude\n```\n\n### Installing Manually\n1. Clone the repository\n2. Install dependencies:\n   ```bash\n   npm install\n   ```\n3. Create a `.env` file based on `.env.example`:\n   ```\n   MS_TENANT_ID=your-tenant-id\n   MS_CLIENT_ID=your-client-id\n   MS_CLIENT_SECRET=your-client-secret\n   \n   # Optional Configuration\n   # LOG_LEVEL=info    # debug, info, warn, error\n   # PORT=3000         # Port for HTTP server if needed\n   # USE_HTTP=true     # Set to 'true' to use HTTP transport instead of stdio\n   # STATELESS=false   # Set to 'true' to use stateless HTTP mode (no session management)\n   ```\n4.",
        "start_pos": 14784,
        "end_pos": 16245,
        "token_count_estimate": 365,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 9,
        "text": "- Device.ReadWrite.All\n     - DeviceManagementConfiguration.ReadWrite.All\n     - DeviceManagementManagedDevices.ReadWrite.All\n     - DeviceManagementApps.ReadWrite.All\n     - InformationProtectionPolicy.ReadWrite.All\n     - Policy.ReadWrite.ConditionalAccess\n     - RoleManagement.ReadWrite.Directory\n     - AuditLog.Read.All\n     - Reports.Read.All\n     - ThreatIndicators.ReadWrite.OwnedBy\n     - IdentityRiskyUser.ReadWrite.All\n     - IdentityRiskEvent.Read.All\n\n   - **Required Azure RBAC roles** (for Azure Resource Management):\n     - Security Admin (for security-related operations)\n     - Compliance Administrator (for compliance management)\n     - Intune Administrator (for device management)\n     - Reports Reader (for audit and reporting functions)\n\n5. Build the server:\n   ```bash\n   npm run build\n   ```\n\n6.",
        "start_pos": 16632,
        "end_pos": 17452,
        "token_count_estimate": 205,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 10,
        "text": "ed between requests\n- Only supports POST requests (GET and DELETE are not supported)\n- Ideal for RESTful scenarios where each request is independent\n- Better for horizontally scaled deployments without shared session state\n- Simpler API wrappers where session management isn't needed\n\nTo configure the transport options, set the appropriate environment variables in your `.env` file:\n```\nUSE_HTTP=true     # Use HTTP transport instead of stdio\nSTATELESS=false   # Use stateful mode with session management (default)\nPORT=3000         # Port for the HTTP server\n```\n\n## Usage\n\nThe server provides MCP tools and resources that can be used to manage various aspects of Microsoft 365. Each tool accepts specific parameters and returns structured responses.",
        "start_pos": 18480,
        "end_pos": 19232,
        "token_count_estimate": 188,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 11,
        "text": "ice objects (enable, disable, delete)\n- `manage_service_principals` - Manage Azure AD Service Principals and ownership\n\n#### Security & Compliance Tools\n- `search_audit_log` - Search the Microsoft 365 Unified Audit Log\n- `manage_alerts` - List and view security alerts from Microsoft security products\n- `manage_dlp_policies` - Manage Data Loss Prevention policies and configurations\n- `manage_dlp_incidents` - Handle DLP policy violations and incident management\n- `manage_sensitivity_labels` - Manage Microsoft Purview sensitivity labels\n\n#### Intune Device Management Tools\n- `manage_intune_macos_devices` - Manage Intune macOS devices and enrollment\n- `manage_intune_macos_policies` - Configure and deploy macOS device policies\n- `manage_intune_macos_apps` - Deploy and manage macOS applications via Intune\n- `manage_intune_macos_compliance` - Monitor and enforce macOS device compliance\n\n#### Compliance Framework Tools\n- `manage_compliance_frameworks` - Configure compliance frameworks (HITRUST, ISO27001, SOC2)\n- `manage_compliance_assessments` - Run and manage compliance assessments\n- `manage_compliance_monitoring` - Monitor compliance status and configure alerts\n- `manage_evidence_collection` - Collect and manage compliance evidence\n- `manage_gap_analysis` - Perform compliance gap analysis and remediation planning\n- `manage_cis_compliance` - Manage CIS Controls compliance and benchmarks\n\n#### Audit & Reporting Tools\n- `generate_audit_reports` - Generate comprehensive audit reports for various frameworks\n\n#### Dynamic API Access\n- `dynamicendpoints m365 assistant` - Call arbitrary Microsoft Graph or Azure Resource Management API endpoints\n\n### Resources\n\nThe server provides **44 comprehensive resources** covering security, compliance, device management, and collaboration:\n\n#### Core Resources\n- `sharepoint_sites` - SharePoint site information and configuration\n- `sharepoint_lists` - SharePoint list structures and metadata  \n- `sharepoint_list_items` - Items within SharePoint lists\n- `security_incidents` - Microsoft secur",
        "start_pos": 20328,
        "end_pos": 22376,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 12,
        "text": "t site information and configuration\n- `sharepoint_lists` - SharePoint list structures and metadata  \n- `sharepoint_list_items` - Items within SharePoint lists\n- `security_incidents` - Microsoft security incidents and details\n\n#### Extended Security Resources (20 resources)\n- Security alerts and incidents from Microsoft Defender\n- Conditional access policies and assignments\n- Privileged access management data\n- Threat intelligence and vulnerability assessments\n- Identity protection risks and policies\n- Authentication methods and security defaults\n- Compliance policies and their status\n- Data governance and retention policies\n- Insider risk management insights\n- Security baselines and configurations\n\n#### Device Management Resources (10 resources)\n- Intune device inventories and compliance status\n- Mobile application management policies\n- Device configuration profiles and assignments\n- Compliance policies for various platforms\n- App protection policies and status\n- Device enrollment configurations\n- Update policies and deployment rings\n- Certificate profiles and management\n- Wi-Fi and VPN configuration profiles\n- Endpoint protection policies\n\n#### Collaboration Resources (10 resources)\n- Microsoft Teams structures and policies\n- Exchange Online configurations and settings\n- Calendar and scheduling information\n- OneDrive storage and sharing policies\n- Planner tasks and project management\n- Viva Engage (Yammer) communities\n- Power Platform environments and apps\n- Booking services and appointments\n- Whiteboard collaboration data\n- Stream video content and policies\n\n#### Extended Dynamic Resources\nAll resources support URI templates for specific object access:\n- `m365://security/alerts/{alertId}` - Specific security alert details\n- `m365://devices/{deviceId}` - Individual device information\n- `m365://users/{userId}/compliance` - User-specific compliance status\n- `m365://teams/{teamId}/governance` - Team governance and policies\n\n### Intelligent Prompts\n\nThe server provides **5 comprehensive prompts** for automated anal",
        "start_pos": 22176,
        "end_pos": 24224,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 13,
        "text": "ance` - User-specific compliance status\n- `m365://teams/{teamId}/governance` - Team governance and policies\n\n### Intelligent Prompts\n\nThe server provides **5 comprehensive prompts** for automated analysis and recommendations:\n\n#### Security Assessment Prompt\n- **Purpose**: Comprehensive security posture analysis with actionable recommendations\n- **Scope**: Security policies, access controls, threat detection, identity protection\n- **Output**: Risk assessment, security gaps, remediation roadmap\n\n#### Compliance Review Prompt  \n- **Purpose**: Framework-specific compliance gap analysis\n- **Frameworks**: SOC2, ISO27001, NIST, GDPR, HIPAA, CIS Controls\n- **Scope**: Control implementation status, evidence collection, audit readiness\n- **Output**: Compliance dashboard, gap analysis, remediation plans\n\n#### User Access Review Prompt\n- **Purpose**: Individual and organization-wide access rights analysis\n- **Scope**: Role assignments, group memberships, application access, privileged accounts\n- **Output**: Access recommendations, risk-based prioritization, cleanup tasks\n\n#### Device Compliance Analysis Prompt\n- **Purpose**: Intune device management and compliance assessment\n- **Scope**: Device policies, compliance status, security configurations, app management\n- **Output**: Compliance reports, policy recommendations, deployment guidance\n\n#### Collaboration Governance Prompt\n- **Purpose**: Teams and SharePoint governance analysis\n- **Scope**: Team structures, sharing policies, external access, data governance\n- **Output**: Governance recommendations, policy suggestions, compliance alignment\n\nEach prompt provides contextual analysis, actionable insights, and integration with the corresponding management tools for immediate remediation.",
        "start_pos": 24024,
        "end_pos": 25778,
        "token_count_estimate": 438,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 14,
        "text": "_distribution_lists', {\n  action: 'create',\n  displayName: 'Marketing Team',\n  emailAddress: 'marketing@company.com',\n  members: ['user1@company.com', 'user2@company.com']\n});\n\n// Managing security groups\nawait callTool('manage_security_groups', {\n  action: 'create',\n  displayName: 'IT Admins',\n  description: 'IT Administration Team',\n  members: ['admin1@company.com']\n});\n\n// Managing Azure AD roles (note: using correct tool name)\nawait callTool('manage_azuread_roles', {\n  action: 'assign_role',\n  roleId: 'role-id-here',\n  principalId: 'user-id-here'\n});\n\n// Managing DLP policies\nawait callTool('manage_dlp_policies', {\n  action: 'create',\n  policyName: 'Financial Data Protection',\n  rules: [{\n    name: 'Block Credit Cards',\n    conditions: { contentContainsSensitiveInfo: ['CreditCardNumber'] },\n    actions: { blockAccess: true }\n  }]\n});\n\n// Managing Intune macOS devices\nawait callTool('manage_intune_macos_devices', {\n  action: 'list',\n  filters: { complianceState: 'compliant' }\n});\n\n// Running compliance assessments\nawait callTool('manage_compliance_assessments', {\n  action: 'run_assessment',\n  framework: 'iso27001',\n  scope: ['access_control', 'data_protection'],\n  settings: {\n    automated: true,\n    generateRemediation: true\n  }\n});\n\n// Generating audit reports\nawait callTool('generate_audit_reports', {\n  framework: 'soc2',\n  reportType: 'comprehensive',\n  dateRange: { start: '2025-01-01', end: '2025-06-16' },\n  format: 'pdf',\n  includeEvidence: true\n});\n\n// Managing Exchange settings\nawait callTool('manage_exchange_settings', {\n  action: 'update',\n  settingType: 'mailbox',\n  target: 'user@company.com',\n  settings: {\n    automateProcessing: {\n      autoReplyEnabled: true\n    }\n  }\n});\n\n// Managing SharePoint sites\nawait callTool('manage_sharepoint_sites', {\n  action: 'create',\n  title: 'Marketing Site',\n  description: 'Site for marketing team',\n  template: 'STS#0',\n  url: 'https://contoso.sharepoint.com/sites/marketing',\n  owners: ['user1@company.com'],\n  members: ['user2@company.com', 'user3@company.com']\n})",
        "start_pos": 25872,
        "end_pos": 27920,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      },
      {
        "chunk_id": 15,
        "text": "tion: 'Site for marketing team',\n  template: 'STS#0',\n  url: 'https://contoso.sharepoint.com/sites/marketing',\n  owners: ['user1@company.com'],\n  members: ['user2@company.com', 'user3@company.com']\n});\n\n// Managing SharePoint lists\nawait callTool('manage_sharepoint_lists', {\n  action: 'create',\n  siteId: 'contoso.sharepoint.com,5a14e1cf-e284-4722-8f50-a5e1b2b0a8d6,9528e4bb-7660-4b11-a758-9d8fb3ca295f',\n  title: 'Project Tasks',\n  description: 'List of project tasks',\n  columns: [\n    { name: 'Title', type: 'text', required: true },\n    { name: 'DueDate', type: 'dateTime' },\n    { name: 'Status', type: 'choice', choices: ['Not Started', 'In Progress', 'Completed'] }\n  ]\n});\n\n// Dynamic API calls for custom scenarios\nawait callTool('dynamicendpoints m365 assistant', {\n  apiType: 'graph',\n  path: '/me/messages',\n  method: 'get',\n  queryParams: { '$top': '10', '$filter': 'isRead eq false' }\n});\n```\n\n## Implementation Details\n\n### Schema Validation\n\nThe server uses Zod for schema validation, providing:\n- Runtime type checking for all inputs\n- Detailed validation error messages\n- Type inference for TypeScript\n- Automatic documentation of input schemas\n\n### Error Handling\n\nThe server implements comprehensive error handling:\n- Input validation for all parameters\n- Graph API error handling\n- Token refresh management\n- Detailed error messages with proper error codes\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Commit your changes\n4. Push to the branch\n5. Create a Pull Request\n\n## License\n\nMIT",
        "start_pos": 27720,
        "end_pos": 29254,
        "token_count_estimate": 383,
        "source_type": "readme",
        "agent_id": "619bfda018cea9dd"
      }
    ]
  },
  {
    "agent_id": "2258d77176049388",
    "name": "ai.smithery/DynamicEndpoints-powershell-exec-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/DynamicEndpoints/PowerShell-Exec-MCP-Server",
    "description": "Execute PowerShell commands securely with controlled timeouts and input validation. Retrieve syste‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T13:54:30.703395Z",
    "indexed_at": "2026-02-18T04:03:32.147565",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# PowerShell Exec MCP Server\n[![smithery badge](https://smithery.ai/badge/@DynamicEndpoints/powershell-exec-mcp-server)](https://smithery.ai/server/@DynamicEndpoints/powershell-exec-mcp-server)\n\nA secure Model Context Protocol (MCP) server that provides controlled PowerShell command execution capabilities through MCP tools. This server includes security features to prevent dangerous commands, provides timeouts for command execution, and specializes in enterprise script generation for **Microsoft Intune** and **IBM BigFix** management platforms.\n\n## Features\n\n- Secure PowerShell command execution\n- JSON-formatted output for structured data\n- System information retrieval\n- Service management and monitoring\n- Process monitoring and analysis\n- Event log access\n- PowerShell script generation\n- Template-based script generation\n- Dynamic script generation\n- Microsoft Intune detection and remediation script generation\n- IBM BigFix relevance and action script generation\n- Command timeout support\n- Blocking of dangerous commands\n- Non-interactive and profile-less execution\n- Async support\n- Type hints and input validation\n\n## Project Structure\n\n```\nmcp-powershell-exec/\n‚îú‚îÄ‚îÄ mcp_powershell_exec/         # Main package directory\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py             # Package initialization\n‚îÇ   ‚îú‚îÄ‚îÄ __main__.py            # Entry point\n‚îÇ   ‚îú‚îÄ‚îÄ server.py              # Server implementation\n‚îÇ   ‚îú‚îÄ‚îÄ templates/             # PowerShell script templates\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basic_script.ps1   # Basic script template\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system_inventory.ps1 # System inventory template\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intune_detection.ps1 # Intune detection script template\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intune_remediation.ps1 # Intune remediation script template\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bigfix_relevance.ps1 # BigFix relevance script template\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bigfix_action.ps1  # BigFix action script template\n‚îÇ   ‚îî‚îÄ‚îÄ py.typed               # Type hints marker\n‚îú‚îÄ‚îÄ pyproject.toml             # Project metadata and dependencies\n‚îú‚îÄ‚îÄ setup.py                   # Package installation\n‚îî‚îÄ‚îÄ README.md                  # Documentation\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install PowerShell Exec Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/powershell-exec-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/powershell-exec-mcp-server --client claude\n```\n\n### Manual Installation\n1. Ensure you have Python 3.7+ installed\n2. Install the package:\n```bash\npip install .\n```\n\nOr install in development mode:\n```bash\npip install -e .\n```\n\n## Usage\n\n### Running the Server\n\nYou can run the server in several ways:\n\n1. Using the MCP CLI:\n```bash\nmcp run mcp_powershell_exec\n```\n\n2. Using Python module:\n```bash\npython -m mcp_powershell_exec\n```\n\n3. Using the console script:\n```bash\nmcp-powershell-exec\n```\n\nFor development and testing:\n```bash\nmcp dev mcp_powershell_exec\n```\n\n### Installing in Claude Desktop\n\nTo install the server in Claude Desktop:\n```bash\nmcp install mcp_powershell_exec\n```\n\n## Available Tools\n\n### run_powershell\n\nThe base tool for executing PowerShell commands securely with timeout support.\n\nParameters:\n- `code` (required): PowerShell code to execute\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await run_powershell(\n    code=\"Get-Process | Select-Object Name, Id, CPU\",\n    timeout=30\n)\n```\n\n### get_system_info\n\nRetrieve system information using Get-ComputerInfo cmdlet.\n\nParameters:\n- `properties` (optional): List of ComputerInfo properties to retrieve\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await get_system_info(\n    properties=[\"OsName\", \"OsVersion\", \"OsArchitecture\"]\n)\n```\n\n### get_running_services\n\nGet information about Windows services.\n\nParameters:\n- `name` (optional): Filter services by name (supports wildcards)\n- `status` (optional): Filter by status (Running, Stopped, etc.)\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await get_running_services(\n    name=\"*sql*\",\n    status=\"Running\"\n)\n```\n\n### get_processes\n\nMonitor running processes with filtering and sorting capabilities.\n\nParameters:\n- `name` (optional): Filter processes by name (supports wildcards)\n- `top` (optional): Limit to top N processes\n- `sort_by` (optional): Property to sort by (e.g., CPU, WorkingSet)\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await get_processes(\n    top=5,\n    sort_by=\"CPU\"\n)\n```\n\n### get_event_logs\n\nAccess Windows event logs with filtering capabilities.\n\nParameters:\n- `logname` (required): Name of the event log (System, Application, Security, etc.)\n- `newest` (optional): Number of most recent events to retrieve (default 10)\n- `level` (optional): Filter by event level (1: Critical, 2: Error, 3: Warning, 4: Information)\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await get_event_logs(\n    logname=\"System\",\n    newest=5,\n    level=2  # Error events only\n)\n```\n\n### generate_script_from_template\n\nGenerate PowerShell scripts using predefined templates.\n\nParameters:\n- `template_name` (required): Name of the template to use (without .ps1 extension)\n- `parameters` (required): Dictionary of parameters to replace in the template\n- `output_path` (optional): Where to save the generated script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_script_from_template(\n    template_name=\"basic_script\",\n    parameters={\n        \"SYNOPSIS\": \"My Test Script\",\n        \"DESCRIPTION\": \"A test script generated from template\",\n        \"PARAM1_DESCRIPTION\": \"First parameter\",\n        \"PARAM2_DESCRIPTION\": \"Second parameter\",\n        \"PARAM1_MANDATORY\": \"true\",\n        \"PARAM2_MANDATORY\": \"false\",\n        \"PARAM1_DEFAULT\": \"\",\n        \"PARAM2_DEFAULT\": \"default_value\",\n        \"MAIN_CODE\": \"Write-Host 'Hello World!'\"\n    },\n    output_path=\"test_script.ps1\"\n)\n```\n\n### generate_custom_script\n\nGenerate custom PowerShell scripts based on description.\n\nParameters:\n- `description` (required): Natural language description of what the script should do\n- `script_type` (required): Type of script to generate (file_ops, service_mgmt, etc.)\n- `parameters` (optional): List of parameters the script should accept\n- `include_logging` (optional): Whether to include logging functions (default: true)\n- `include_error_handling` (optional): Whether to include error handling (default: true)\n- `output_path` (optional): Where to save the generated script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_custom_script(\n    description=\"Script to monitor CPU usage and log high utilization\",\n    script_type=\"monitoring\",\n    parameters=[\n        {\n            \"name\": \"ThresholdPercent\",\n            \"type\": \"int\",\n            \"mandatory\": \"true\",\n            \"default\": \"90\"\n        },\n        {\n            \"name\": \"LogPath\",\n            \"type\": \"string\",\n            \"mandatory\": \"false\",\n            \"default\": \"cpu_usage.log\"\n        }\n    ],\n    output_path=\"monitor_cpu.ps1\"\n)\n```\n\n### generate_intune_detection_script\n\nGenerate Intune detection scripts with proper exit codes and logging.\n\nParameters:\n- `description` (required): What the script should detect\n- `detection_logic` (required): PowerShell code that performs the detection\n- `output_path` (optional): Where to save the script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_intune_detection_script(\n    description=\"Check if Chrome is installed with correct version\",\n    detection_logic=\"\"\"\n    $app = Get-ItemProperty HKLM:\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\App Paths\\\\chrome.exe\n    $version = (Get-Item $app.Path).VersionInfo.FileVersion\n    $compliant = [version]$version -ge [version]\"100.0.0.0\"\n    Complete-Detection -Compliant $compliant -Message \"Chrome version: $version\"\n    \"\"\",\n    output_path=\"detect_chrome.ps1\"\n)\n```\n\n### generate_intune_remediation_script\n\nGenerate Intune remediation scripts with system restore points and error handling.\n\nParameters:\n- `description` (required): What the script should remediate\n- `remediation_logic` (required): PowerShell code that performs the remediation\n- `output_path` (optional): Where to save the script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_intune_remediation_script(\n    description=\"Install or update Chrome browser\",\n    remediation_logic=\"\"\"\n    $installer = \"C:\\\\Windows\\\\Temp\\\\ChromeSetup.exe\"\n    Invoke-WebRequest -Uri \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\" -OutFile $installer\n    Start-Process -FilePath $installer -Args \"/silent /install\" -Wait\n    Remove-Item $installer\n    Complete-Remediation -Success $true -Message \"Chrome installation completed\"\n    \"\"\",\n    output_path=\"remedy_chrome.ps1\"\n)\n```\n\n### generate_intune_script_pair\n\nGenerate both detection and remediation scripts as a matched pair.\n\nParameters:\n- `description` (required): What the scripts should detect and remediate\n- `detection_logic` (required): PowerShell code that performs the detection\n- `remediation_logic` (required): PowerShell code that performs the remediation\n- `output_dir` (optional): Directory to save the scripts\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample Usage:\n\n1. Software Installation Check:\n```python\nresult = await generate_intune_script_pair(\n    description=\"Manage Chrome browser installation and version\",\n    detection_logic=\"\"\"\n    $app = Get-ItemProperty HKLM:\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\App Paths\\\\chrome.exe\n    $version = (Get-Item $app.Path).VersionInfo.FileVersion\n    $compliant = [version]$version -ge [version]\"100.0.0.0\"\n    Complete-Detection -Compliant $compliant -Message \"Chrome version: $version\"\n    \"\"\",\n    remediation_logic=\"\"\"\n    $installer = \"C:\\\\Windows\\\\Temp\\\\ChromeSetup.exe\"\n    Invoke-WebRequest -Uri \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\" -OutFile $installer\n    Start-Process -FilePath $installer -Args \"/silent /install\" -Wait\n    Remove-Item $installer\n    Complete-Remediation -Success $true -Message \"Chrome installation completed\"\n    \"\"\",\n    output_dir=\"chrome_scripts\"\n)\n```\n\n2. BitLocker Encryption:\n```python\nresult = await generate_intune_script_pair(\n    description=\"Check and enable BitLocker encryption on system drive\",\n    detection_logic=\"\"\"\n    $systemDrive = $env:SystemDrive\n    $bitlockerVolume = Get-BitLockerVolume -MountPoint $systemDrive\n    $compliant = $bitlockerVolume.ProtectionStatus -eq 'On'\n    Complete-Detection -Compliant $compliant -Message \"BitLocker status: $($bitlockerVolume.ProtectionStatus)\"\n    \"\"\",\n    remediation_logic=\"\"\"\n    $systemDrive = $env:SystemDrive\n    Enable-BitLocker -MountPoint $systemDrive -TpmProtector -UsedSpaceOnly\n    Backup-BitLockerKeyProtector -MountPoint $systemDrive -KeyProtectorId $bitlockerVolume.KeyProtector[0].KeyProtectorId\n    Complete-Remediation -Success $true -Message \"BitLocker enabled with TPM protection\"\n    \"\"\",\n    output_dir=\"bitlocker_scripts\"\n)\n```\n\n3. Windows Update Configuration:\n```python\nresult = await generate_intune_script_pair(\n    description=\"Check and configure Windows Update settings\",\n    detection_logic=\"\"\"\n    $wu = New-Object -ComObject Microsoft.Update.AutoUpdate\n    $settings = $wu.Settings\n    $compliant = ($settings.NotificationLevel -eq 4) -and ($settings.NoAutoRebootWithLoggedOnUsers -eq $true)\n    Complete-Detection -Compliant $compliant -Message \"Windows Update settings status\"\n    \"\"\",\n    remediation_logic=\"\"\"\n    $wu = New-Object -ComObject Microsoft.Update.AutoUpdate\n    $settings = $wu.Settings\n    $settings.NotificationLevel = 4\n    $settings.NoAutoRebootWithLoggedOnUsers = $true\n    $settings.Save()\n    Complete-Remediation -Success $true -Message \"Windows Update settings configured\"\n    \"\"\",\n    output_dir=\"windows_update_scripts\"\n)\n```\n\n4. Security Settings:\n```python\nresult = await generate_intune_script_pair(\n    description=\"Check and configure basic Windows security settings\",\n    detection_logic=\"\"\"\n    $firewall = Get-NetFirewallProfile\n    $uac = (Get-ItemProperty HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Policies\\\\System).EnableLUA\n    $screenSaver = (Get-ItemProperty 'HKCU:\\\\Control Panel\\\\Desktop').ScreenSaveActive\n    $compliant = ($firewall.Enabled -contains $true) -and ($uac -eq 1) -and ($screenSaver -eq 1)\n    Complete-Detection -Compliant $compliant -Message \"Security settings status\"\n    \"\"\",\n    remediation_logic=\"\"\"\n    Set-NetFirewallProfile -Profile Domain,Public,Private -Enabled True\n    Set-ItemProperty -Path HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Policies\\\\System -Name EnableLUA -Value 1\n    Set-ItemProperty -Path 'HKCU:\\\\Control Panel\\\\Desktop' -Name ScreenSaveActive -Value 1\n    Complete-Remediation -Success $true -Message \"Security settings configured\"\n    \"\"\",\n    output_dir=\"security_scripts\"\n)\n```\n\n## BigFix Script Generation Tools\n\n### generate_bigfix_relevance_script\n\nGenerate BigFix relevance scripts to determine if computers need action.\n\nParameters:\n- `description` (required): What the script should check\n- `relevance_logic` (required): PowerShell code that determines relevance\n- `output_path` (optional): Where to save the script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_bigfix_relevance_script(\n    description=\"Check if Chrome needs updating to version 100.0.0.0\",\n    relevance_logic=\"\"\"\n    try {\n        $app = Get-ItemProperty \"HKLM:\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\App Paths\\\\chrome.exe\" -ErrorAction Stop\n        $version = (Get-Item $app.'(Default)').VersionInfo.FileVersion\n        $needsUpdate = [version]$version -lt [version]\"100.0.0.0\"\n        Complete-Relevance -Relevant $needsUpdate -Message \"Chrome version: $version (Target: 100.0.0.0+)\"\n    } catch {\n        Complete-Relevance -Relevant $true -Message \"Chrome not found - installation needed\"\n    }\n    \"\"\",\n    output_path=\"chrome_relevance.ps1\"\n)\n```\n\n### generate_bigfix_action_script\n\nGenerate BigFix action scripts to perform remediation or configuration changes.\n\nParameters:\n- `description` (required): What the script should accomplish\n- `action_logic` (required): PowerShell code that performs the action\n- `output_path` (optional): Where to save the script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_bigfix_action_script(\n    description=\"Install Chrome browser to latest version\",\n    action_logic=\"\"\"\n    try {\n        $installer = \"$env:TEMP\\\\ChromeSetup.exe\"\n        Write-BigFixLog \"Downloading Chrome installer...\"\n        Invoke-WebRequest -Uri \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\" -OutFile $installer -UseBasicParsing\n        Write-BigFixLog \"Installing Chrome silently...\"\n        Start-Process -FilePath $installer -Args \"/silent /install\" -Wait\n        Remove-Item $installer -Force\n        Complete-Action -Result \"Success\" -Message \"Chrome installation completed successfully\"\n    } catch {\n        Complete-Action -Result \"RetryableFailure\" -Message \"Chrome installation failed: $($_.Exception.Message)\"\n    }\n    \"\"\",\n    output_path=\"chrome_action.ps1\"\n)\n```\n\n### generate_bigfix_script_pair\n\nGenerate both relevance and action scripts as a matched pair for BigFix fixlet deployment.\n\nParameters:\n- `description` (required): What the scripts should accomplish\n- `relevance_logic` (required): PowerShell code that determines relevance\n- `action_logic` (required): PowerShell code that performs the action\n- `output_dir` (optional): Directory to save the scripts\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample Usage:\n\n1. Chrome Browser Management:\n```python\nresult = await generate_bigfix_script_pair(\n    description=\"Manage Chrome browser installation with version 100.0.0.0 or higher\",\n    relevance_logic=\"\"\"\n    try {\n        $app = Get-ItemProperty \"HKLM:\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\App Paths\\\\chrome.exe\" -ErrorAction Stop\n        $version = (Get-Item $app.'(Default)').VersionInfo.FileVersion\n        $needsAction = [version]$version -lt [version]\"100.0.0.0\"\n        Complete-Relevance -Relevant $needsAction -Message \"Chrome version: $version (Target: 100.0.0.0+)\"\n    } catch {\n        Complete-Relevance -Relevant $true -Message \"Chrome not found - installation needed\"\n    }\n    \"\"\",\n    action_logic=\"\"\"\n    try {\n        $installer = \"$env:TEMP\\\\ChromeSetup.exe\"\n        Write-BigFixLog \"Downloading Chrome installer...\"\n        Invoke-WebRequest -Uri \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\" -OutFile $installer -UseBasicParsing\n        Write-BigFixLog \"Installing Chrome silently...\"\n        Start-Process -FilePath $installer -Args \"/silent /install\" -Wait\n        Remove-Item $installer -Force\n        Complete-Action -Result \"Success\" -Message \"Chrome installation completed successfully\"\n    } catch {\n        Complete-Action -Result \"RetryableFailure\" -Message \"Chrome installation failed: $($_.Exception.Message)\"\n    }\n    \"\"\",\n    output_dir=\"chrome_bigfix_scripts\"\n)\n```\n\n2. Windows Update Configuration:\n```python\nresult = await generate_bigfix_script_pair(\n    description=\"Ensure Windows Update service is running and configured properly\",\n    relevance_logic=\"\"\"\n    $service = Get-Service -Name \"wuauserv\" -ErrorAction SilentlyContinue\n    $needsAction = ($service.Status -ne \"Running\") -or ($service.StartType -ne \"Automatic\")\n    Complete-Relevance -Relevant $needsAction -Message \"Windows Update service status: $($service.Status), StartType: $($service.StartType)\"\n    \"\"\",\n    action_logic=\"\"\"\n    try {\n        Set-Service -Name \"wuauserv\" -StartupType Automatic\n        Start-Service -Name \"wuauserv\"\n        Complete-Action -Result \"Success\" -Message \"Windows Update service configured and started\"\n    } catch {\n        Complete-Action -Result \"RetryableFailure\" -Message \"Failed to configure Windows Update service: $($_.Exception.Message)\"\n    }\n    \"\"\",\n    output_dir=\"windows_update_bigfix_scripts\"\n)\n```\n\n3. Security Settings Configuration:\n```python\nresult = await generate_bigfix_script_pair(\n    description=\"Ensure basic Windows security settings are properly configured\",\n    relevance_logic=\"\"\"\n    $firewall = Get-NetFirewallProfile\n    $uac = (Get-ItemProperty HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Policies\\\\System).EnableLUA\n    $firewallOk = ($firewall | Where-Object { $_.Enabled -eq $false }).Count -eq 0\n    $needsAction = (-not $firewallOk) -or ($uac -ne 1)\n    Complete-Relevance -Relevant $needsAction -Message \"Security settings check - Firewall OK: $firewallOk, UAC Enabled: $($uac -eq 1)\"\n    \"\"\",\n    action_logic=\"\"\"\n    try {\n        Set-NetFirewallProfile -Profile Domain,Public,Private -Enabled True\n        Set-ItemProperty -Path HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Policies\\\\System -Name EnableLUA -Value 1\n        Complete-Action -Result \"Success\" -Message \"Security settings configured successfully\"\n    } catch {\n        Complete-Action -Result \"RetryableFailure\" -Message \"Failed to configure security settings: $($_.Exception.Message)\"\n    }\n    \"\"\",\n    output_dir=\"security_bigfix_scripts\"\n)\n```\n\n## Security Features\n\nThe server implements several security measures:\n\n1. Blocks dangerous commands like:\n   - Recursive deletions\n   - Drive formatting\n   - System shutdown/restart\n   - Service manipulation\n   - User account manipulation\n   - Dynamic code execution\n\n2. Command timeout enforcement\n3. Non-interactive mode to prevent hangs\n4. No profile loading to ensure clean execution environment\n5. JSON output formatting for consistent data structures\n6. Input validation for all tool parameters\n\n## Development\n\nThe project uses modern Python packaging tools and includes full type hints support. To set up a development environment:\n\n1. Clone the repository\n2. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n3. Install development dependencies:\n```bash\npip install -e .\n```\n\n## Contributing\n\nContributions are welcome! Please ensure any changes maintain the security standards of the server.\n\n## License\n\nMIT License\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Execute PowerShell commands securely with timeout and dangerous command blocking",
        "Retrieve system information using PowerShell cmdlets",
        "Manage and monitor Windows services",
        "Monitor and analyze running processes with filtering and sorting",
        "Access and filter Windows event logs",
        "Generate PowerShell scripts from predefined templates",
        "Generate custom PowerShell scripts based on natural language descriptions",
        "Generate Microsoft Intune detection and remediation scripts with proper exit codes and logging",
        "Generate IBM BigFix relevance and action scripts",
        "Support asynchronous execution and input validation with type hints"
      ],
      "limitations": [
        "Command execution timeouts limited to 1-300 seconds",
        "Blocking of dangerous PowerShell commands to ensure security",
        "Non-interactive and profile-less execution environment",
        "No support for interactive PowerShell sessions",
        "Script generation limited to supported templates and script types",
        "Documentation does not mention support for platforms other than Windows"
      ],
      "requirements": [
        "Python 3.7 or higher",
        "Installation via pip or Smithery CLI",
        "Permissions to execute PowerShell commands on the host system",
        "Claude Desktop client for integration via Smithery",
        "Network access if scripts require downloading resources (e.g., installers)"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for all tools, clear descriptions of capabilities, and explicit mention of security features and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# PowerShell Exec MCP Server\n[![smithery badge](https://smithery.ai/badge/@DynamicEndpoints/powershell-exec-mcp-server)](https://smithery.ai/server/@DynamicEndpoints/powershell-exec-mcp-server)\n\nA secure Model Context Protocol (MCP) server that provides controlled PowerShell command execution capabilities through MCP tools. This server includes security features to prevent dangerous commands, provides timeouts for command execution, and specializes in enterprise script generation for **Microsoft Intune** and **IBM BigFix** management platforms.",
        "start_pos": 0,
        "end_pos": 550,
        "token_count_estimate": 137,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 1,
        "text": "py.typed               # Type hints marker\n‚îú‚îÄ‚îÄ pyproject.toml             # Project metadata and dependencies\n‚îú‚îÄ‚îÄ setup.py                   # Package installation\n‚îî‚îÄ‚îÄ README.md                  # Documentation\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install PowerShell Exec Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@DynamicEndpoints/powershell-exec-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @DynamicEndpoints/powershell-exec-mcp-server --client claude\n```\n\n### Manual Installation\n1. Ensure you have Python 3.7+ installed\n2. Install the package:\n```bash\npip install .\n```\n\nOr install in development mode:\n```bash\npip install -e .\n```\n\n## Usage\n\n### Running the Server\n\nYou can run the server in several ways:\n\n1. Using the MCP CLI:\n```bash\nmcp run mcp_powershell_exec\n```\n\n2. Using Python module:\n```bash\npython -m mcp_powershell_exec\n```\n\n3. Using the console script:\n```bash\nmcp-powershell-exec\n```\n\nFor development and testing:\n```bash\nmcp dev mcp_powershell_exec\n```\n\n### Installing in Claude Desktop\n\nTo install the server in Claude Desktop:\n```bash\nmcp install mcp_powershell_exec\n```\n\n## Available Tools\n\n### run_powershell\n\nThe base tool for executing PowerShell commands securely with timeout support.\n\nParameters:\n- `code` (required): PowerShell code to execute\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await run_powershell(\n    code=\"Get-Process | Select-Object Name, Id, CPU\",\n    timeout=30\n)\n```\n\n### get_system_info\n\nRetrieve system information using Get-ComputerInfo cmdlet.\n\nParameters:\n- `properties` (optional): List of ComputerInfo properties to retrieve\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await get_system_info(\n    properties=[\"OsName\", \"OsVersion\", \"OsArchitecture\"]\n)\n```\n\n### get_running_services\n\nGet information about Windows services.",
        "start_pos": 1848,
        "end_pos": 3800,
        "token_count_estimate": 487,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 2,
        "text": "1-300, default 60)\n\nExample:\n```python\nresult = await get_system_info(\n    properties=[\"OsName\", \"OsVersion\", \"OsArchitecture\"]\n)\n```\n\n### get_running_services\n\nGet information about Windows services.\n\nParameters:\n- `name` (optional): Filter services by name (supports wildcards)\n- `status` (optional): Filter by status (Running, Stopped, etc.)\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await get_running_services(\n    name=\"*sql*\",\n    status=\"Running\"\n)\n```\n\n### get_processes\n\nMonitor running processes with filtering and sorting capabilities.\n\nParameters:\n- `name` (optional): Filter processes by name (supports wildcards)\n- `top` (optional): Limit to top N processes\n- `sort_by` (optional): Property to sort by (e.g., CPU, WorkingSet)\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await get_processes(\n    top=5,\n    sort_by=\"CPU\"\n)\n```\n\n### get_event_logs\n\nAccess Windows event logs with filtering capabilities.\n\nParameters:\n- `logname` (required): Name of the event log (System, Application, Security, etc.)\n- `newest` (optional): Number of most recent events to retrieve (default 10)\n- `level` (optional): Filter by event level (1: Critical, 2: Error, 3: Warning, 4: Information)\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await get_event_logs(\n    logname=\"System\",\n    newest=5,\n    level=2  # Error events only\n)\n```\n\n### generate_script_from_template\n\nGenerate PowerShell scripts using predefined templates.",
        "start_pos": 3600,
        "end_pos": 5188,
        "token_count_estimate": 397,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 3,
        "text": "): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_script_from_template(\n    template_name=\"basic_script\",\n    parameters={\n        \"SYNOPSIS\": \"My Test Script\",\n        \"DESCRIPTION\": \"A test script generated from template\",\n        \"PARAM1_DESCRIPTION\": \"First parameter\",\n        \"PARAM2_DESCRIPTION\": \"Second parameter\",\n        \"PARAM1_MANDATORY\": \"true\",\n        \"PARAM2_MANDATORY\": \"false\",\n        \"PARAM1_DEFAULT\": \"\",\n        \"PARAM2_DEFAULT\": \"default_value\",\n        \"MAIN_CODE\": \"Write-Host 'Hello World!'\"\n    },\n    output_path=\"test_script.ps1\"\n)\n```\n\n### generate_custom_script\n\nGenerate custom PowerShell scripts based on description.\n\nParameters:\n- `description` (required): Natural language description of what the script should do\n- `script_type` (required): Type of script to generate (file_ops, service_mgmt, etc.)\n- `parameters` (optional): List of parameters the script should accept\n- `include_logging` (optional): Whether to include logging functions (default: true)\n- `include_error_handling` (optional): Whether to include error handling (default: true)\n- `output_path` (optional): Where to save the generated script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_custom_script(\n    description=\"Script to monitor CPU usage and log high utilization\",\n    script_type=\"monitoring\",\n    parameters=[\n        {\n            \"name\": \"ThresholdPercent\",\n            \"type\": \"int\",\n            \"mandatory\": \"true\",\n            \"default\": \"90\"\n        },\n        {\n            \"name\": \"LogPath\",\n            \"type\": \"string\",\n            \"mandatory\": \"false\",\n            \"default\": \"cpu_usage.log\"\n        }\n    ],\n    output_path=\"monitor_cpu.ps1\"\n)\n```\n\n### generate_intune_detection_script\n\nGenerate Intune detection scripts with proper exit codes and logging.",
        "start_pos": 5448,
        "end_pos": 7344,
        "token_count_estimate": 474,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 4,
        "text": "\"default\": \"cpu_usage.log\"\n        }\n    ],\n    output_path=\"monitor_cpu.ps1\"\n)\n```\n\n### generate_intune_detection_script\n\nGenerate Intune detection scripts with proper exit codes and logging.\n\nParameters:\n- `description` (required): What the script should detect\n- `detection_logic` (required): PowerShell code that performs the detection\n- `output_path` (optional): Where to save the script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_intune_detection_script(\n    description=\"Check if Chrome is installed with correct version\",\n    detection_logic=\"\"\"\n    $app = Get-ItemProperty HKLM:\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\App Paths\\\\chrome.exe\n    $version = (Get-Item $app.Path).VersionInfo.FileVersion\n    $compliant = [version]$version -ge [version]\"100.0.0.0\"\n    Complete-Detection -Compliant $compliant -Message \"Chrome version: $version\"\n    \"\"\",\n    output_path=\"detect_chrome.ps1\"\n)\n```\n\n### generate_intune_remediation_script\n\nGenerate Intune remediation scripts with system restore points and error handling.\n\nParameters:\n- `description` (required): What the script should remediate\n- `remediation_logic` (required): PowerShell code that performs the remediation\n- `output_path` (optional): Where to save the script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_intune_remediation_script(\n    description=\"Install or update Chrome browser\",\n    remediation_logic=\"\"\"\n    $installer = \"C:\\\\Windows\\\\Temp\\\\ChromeSetup.exe\"\n    Invoke-WebRequest -Uri \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\" -OutFile $installer\n    Start-Process -FilePath $installer -Args \"/silent /install\" -Wait\n    Remove-Item $installer\n    Complete-Remediation -Success $true -Message \"Chrome installation completed\"\n    \"\"\",\n    output_path=\"remedy_chrome.ps1\"\n)\n```\n\n### generate_intune_script_pair\n\nGenerate both detection and remediation scripts as a matched pair.",
        "start_pos": 7144,
        "end_pos": 9181,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 5,
        "text": "s $true -Message \"Chrome installation completed\"\n    \"\"\",\n    output_path=\"remedy_chrome.ps1\"\n)\n```\n\n### generate_intune_script_pair\n\nGenerate both detection and remediation scripts as a matched pair.\n\nParameters:\n- `description` (required): What the scripts should detect and remediate\n- `detection_logic` (required): PowerShell code that performs the detection\n- `remediation_logic` (required): PowerShell code that performs the remediation\n- `output_dir` (optional): Directory to save the scripts\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample Usage:\n\n1. Software Installation Check:\n```python\nresult = await generate_intune_script_pair(\n    description=\"Manage Chrome browser installation and version\",\n    detection_logic=\"\"\"\n    $app = Get-ItemProperty HKLM:\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\App Paths\\\\chrome.exe\n    $version = (Get-Item $app.Path).VersionInfo.FileVersion\n    $compliant = [version]$version -ge [version]\"100.0.0.0\"\n    Complete-Detection -Compliant $compliant -Message \"Chrome version: $version\"\n    \"\"\",\n    remediation_logic=\"\"\"\n    $installer = \"C:\\\\Windows\\\\Temp\\\\ChromeSetup.exe\"\n    Invoke-WebRequest -Uri \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\" -OutFile $installer\n    Start-Process -FilePath $installer -Args \"/silent /install\" -Wait\n    Remove-Item $installer\n    Complete-Remediation -Success $true -Message \"Chrome installation completed\"\n    \"\"\",\n    output_dir=\"chrome_scripts\"\n)\n```\n\n2.",
        "start_pos": 8981,
        "end_pos": 10480,
        "token_count_estimate": 374,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 6,
        "text": "e-Detection -Compliant $compliant -Message \"BitLocker status: $($bitlockerVolume.ProtectionStatus)\"\n    \"\"\",\n    remediation_logic=\"\"\"\n    $systemDrive = $env:SystemDrive\n    Enable-BitLocker -MountPoint $systemDrive -TpmProtector -UsedSpaceOnly\n    Backup-BitLockerKeyProtector -MountPoint $systemDrive -KeyProtectorId $bitlockerVolume.KeyProtector[0].KeyProtectorId\n    Complete-Remediation -Success $true -Message \"BitLocker enabled with TPM protection\"\n    \"\"\",\n    output_dir=\"bitlocker_scripts\"\n)\n```\n\n3. Windows Update Configuration:\n```python\nresult = await generate_intune_script_pair(\n    description=\"Check and configure Windows Update settings\",\n    detection_logic=\"\"\"\n    $wu = New-Object -ComObject Microsoft.Update.AutoUpdate\n    $settings = $wu.Settings\n    $compliant = ($settings.NotificationLevel -eq 4) -and ($settings.NoAutoRebootWithLoggedOnUsers -eq $true)\n    Complete-Detection -Compliant $compliant -Message \"Windows Update settings status\"\n    \"\"\",\n    remediation_logic=\"\"\"\n    $wu = New-Object -ComObject Microsoft.Update.AutoUpdate\n    $settings = $wu.Settings\n    $settings.NotificationLevel = 4\n    $settings.NoAutoRebootWithLoggedOnUsers = $true\n    $settings.Save()\n    Complete-Remediation -Success $true -Message \"Windows Update settings configured\"\n    \"\"\",\n    output_dir=\"windows_update_scripts\"\n)\n```\n\n4.",
        "start_pos": 10829,
        "end_pos": 12174,
        "token_count_estimate": 336,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 7,
        "text": "Complete-Detection -Compliant $compliant -Message \"Security settings status\"\n    \"\"\",\n    remediation_logic=\"\"\"\n    Set-NetFirewallProfile -Profile Domain,Public,Private -Enabled True\n    Set-ItemProperty -Path HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Policies\\\\System -Name EnableLUA -Value 1\n    Set-ItemProperty -Path 'HKCU:\\\\Control Panel\\\\Desktop' -Name ScreenSaveActive -Value 1\n    Complete-Remediation -Success $true -Message \"Security settings configured\"\n    \"\"\",\n    output_dir=\"security_scripts\"\n)\n```\n\n## BigFix Script Generation Tools\n\n### generate_bigfix_relevance_script\n\nGenerate BigFix relevance scripts to determine if computers need action.\n\nParameters:\n- `description` (required): What the script should check\n- `relevance_logic` (required): PowerShell code that determines relevance\n- `output_path` (optional): Where to save the script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_bigfix_relevance_script(\n    description=\"Check if Chrome needs updating to version 100.0.0.0\",\n    relevance_logic=\"\"\"\n    try {\n        $app = Get-ItemProperty \"HKLM:\\\\Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\App Paths\\\\chrome.exe\" -ErrorAction Stop\n        $version = (Get-Item $app.'(Default)').VersionInfo.FileVersion\n        $needsUpdate = [version]$version -lt [version]\"100.0.0.0\"\n        Complete-Relevance -Relevant $needsUpdate -Message \"Chrome version: $version (Target: 100.0.0.0+)\"\n    } catch {\n        Complete-Relevance -Relevant $true -Message \"Chrome not found - installation needed\"\n    }\n    \"\"\",\n    output_path=\"chrome_relevance.ps1\"\n)\n```\n\n### generate_bigfix_action_script\n\nGenerate BigFix action scripts to perform remediation or configuration changes.",
        "start_pos": 12677,
        "end_pos": 14448,
        "token_count_estimate": 442,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 8,
        "text": "`action_logic` (required): PowerShell code that performs the action\n- `output_path` (optional): Where to save the script\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample:\n```python\nresult = await generate_bigfix_action_script(\n    description=\"Install Chrome browser to latest version\",\n    action_logic=\"\"\"\n    try {\n        $installer = \"$env:TEMP\\\\ChromeSetup.exe\"\n        Write-BigFixLog \"Downloading Chrome installer...\"\n        Invoke-WebRequest -Uri \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\" -OutFile $installer -UseBasicParsing\n        Write-BigFixLog \"Installing Chrome silently...\"\n        Start-Process -FilePath $installer -Args \"/silent /install\" -Wait\n        Remove-Item $installer -Force\n        Complete-Action -Result \"Success\" -Message \"Chrome installation completed successfully\"\n    } catch {\n        Complete-Action -Result \"RetryableFailure\" -Message \"Chrome installation failed: $($_.Exception.Message)\"\n    }\n    \"\"\",\n    output_path=\"chrome_action.ps1\"\n)\n```\n\n### generate_bigfix_script_pair\n\nGenerate both relevance and action scripts as a matched pair for BigFix fixlet deployment.\n\nParameters:\n- `description` (required): What the scripts should accomplish\n- `relevance_logic` (required): PowerShell code that determines relevance\n- `action_logic` (required): PowerShell code that performs the action\n- `output_dir` (optional): Directory to save the scripts\n- `timeout` (optional): Command timeout in seconds (1-300, default 60)\n\nExample Usage:\n\n1.",
        "start_pos": 14525,
        "end_pos": 16059,
        "token_count_estimate": 383,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 9,
        "text": "rrorAction Stop\n        $version = (Get-Item $app.'(Default)').VersionInfo.FileVersion\n        $needsAction = [version]$version -lt [version]\"100.0.0.0\"\n        Complete-Relevance -Relevant $needsAction -Message \"Chrome version: $version (Target: 100.0.0.0+)\"\n    } catch {\n        Complete-Relevance -Relevant $true -Message \"Chrome not found - installation needed\"\n    }\n    \"\"\",\n    action_logic=\"\"\"\n    try {\n        $installer = \"$env:TEMP\\\\ChromeSetup.exe\"\n        Write-BigFixLog \"Downloading Chrome installer...\"\n        Invoke-WebRequest -Uri \"https://dl.google.com/chrome/install/latest/chrome_installer.exe\" -OutFile $installer -UseBasicParsing\n        Write-BigFixLog \"Installing Chrome silently...\"\n        Start-Process -FilePath $installer -Args \"/silent /install\" -Wait\n        Remove-Item $installer -Force\n        Complete-Action -Result \"Success\" -Message \"Chrome installation completed successfully\"\n    } catch {\n        Complete-Action -Result \"RetryableFailure\" -Message \"Chrome installation failed: $($_.Exception.Message)\"\n    }\n    \"\"\",\n    output_dir=\"chrome_bigfix_scripts\"\n)\n```\n\n2.",
        "start_pos": 16373,
        "end_pos": 17484,
        "token_count_estimate": 277,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 10,
        "text": "} catch {\n        Complete-Action -Result \"RetryableFailure\" -Message \"Failed to configure Windows Update service: $($_.Exception.Message)\"\n    }\n    \"\"\",\n    output_dir=\"windows_update_bigfix_scripts\"\n)\n```\n\n3. Security Settings Configuration:\n```python\nresult = await generate_bigfix_script_pair(\n    description=\"Ensure basic Windows security settings are properly configured\",\n    relevance_logic=\"\"\"\n    $firewall = Get-NetFirewallProfile\n    $uac = (Get-ItemProperty HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Policies\\\\System).EnableLUA\n    $firewallOk = ($firewall | Where-Object { $_.Enabled -eq $false }).Count -eq 0\n    $needsAction = (-not $firewallOk) -or ($uac -ne 1)\n    Complete-Relevance -Relevant $needsAction -Message \"Security settings check - Firewall OK: $firewallOk, UAC Enabled: $($uac -eq 1)\"\n    \"\"\",\n    action_logic=\"\"\"\n    try {\n        Set-NetFirewallProfile -Profile Domain,Public,Private -Enabled True\n        Set-ItemProperty -Path HKLM:\\\\SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Policies\\\\System -Name EnableLUA -Value 1\n        Complete-Action -Result \"Success\" -Message \"Security settings configured successfully\"\n    } catch {\n        Complete-Action -Result \"RetryableFailure\" -Message \"Failed to configure security settings: $($_.Exception.Message)\"\n    }\n    \"\"\",\n    output_dir=\"security_bigfix_scripts\"\n)\n```\n\n## Security Features\n\nThe server implements several security measures:\n\n1. Blocks dangerous commands like:\n   - Recursive deletions\n   - Drive formatting\n   - System shutdown/restart\n   - Service manipulation\n   - User account manipulation\n   - Dynamic code execution\n\n2. Command timeout enforcement\n3. Non-interactive mode to prevent hangs\n4. No profile loading to ensure clean execution environment\n5. JSON output formatting for consistent data structures\n6. Input validation for all tool parameters\n\n## Development\n\nThe project uses modern Python packaging tools and includes full type hints support. To set up a development environment:\n\n1. Clone the repository\n2.",
        "start_pos": 18221,
        "end_pos": 20261,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      },
      {
        "chunk_id": 11,
        "text": "alidation for all tool parameters\n\n## Development\n\nThe project uses modern Python packaging tools and includes full type hints support. To set up a development environment:\n\n1. Clone the repository\n2. Create a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n3. Install development dependencies:\n```bash\npip install -e .\n```\n\n## Contributing\n\nContributions are welcome! Please ensure any changes maintain the security standards of the server.\n\n## License\n\nMIT License",
        "start_pos": 20061,
        "end_pos": 20596,
        "token_count_estimate": 133,
        "source_type": "readme",
        "agent_id": "2258d77176049388"
      }
    ]
  },
  {
    "agent_id": "2ce050c9759a38e4",
    "name": "ai.smithery/FelixYifeiWang-felix-mcp-smithery",
    "source": "mcp",
    "source_url": "https://github.com/FelixYifeiWang/felix-mcp-smithery",
    "description": "Streamline your workflow with Felix. Integrate it into your workspace and tailor its behavior to y‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T07:15:10.498525Z",
    "indexed_at": "2026-02-18T04:03:34.108569",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Felix MCP (Smithery)\r\n\r\nA tiny **Model Context Protocol** server with a few useful tools, deployed on **Smithery**, tested in **Claude Desktop**, and indexed in **NANDA**.\r\n\r\n**Tools included**\r\n\r\n* `hello(name)` ‚Äì quick greeting\r\n* `randomNumber(max?)` ‚Äì random integer (default 100)\r\n* `weather(city)` ‚Äì current weather via wttr.in\r\n* `summarize(text, maxSentences?, model?)` ‚Äì OpenAI-powered summary *(requires `OPENAI_API_KEY`)*\r\n\r\n**Public server page**\r\n`https://smithery.ai/server/@FelixYifeiWang/felix-mcp-smithery`\r\n\r\n**MCP endpoint (streamable HTTP)**\r\n`https://server.smithery.ai/@FelixYifeiWang/felix-mcp-smithery/mcp`\r\n*(In Smithery/NANDA, auth is attached via query param `api_key` and optional `profile`, configured in the platform UI; do **not** hardcode secrets here.)*\r\n\r\n---\r\n\r\n## Demo\r\n\r\n### In Claude Desktop (recommended)\r\n\r\n1. Open **Settings ‚Üí Developer ‚Üí mcpServers** and add:\r\n\r\n   ```json\r\n   {\r\n     \"mcpServers\": {\r\n       \"felix-mcp-smithery\": {\r\n         \"command\": \"npx\",\r\n         \"args\": [\r\n           \"-y\",\r\n           \"@smithery/cli@latest\",\r\n           \"run\",\r\n           \"@FelixYifeiWang/felix-mcp-smithery\",\r\n           \"--key\",\r\n           \"YOUR_SMITHERY_API_KEY\",\r\n           \"--profile\",\r\n           \"YOUR_PROFILE_ID\"\r\n         ]\r\n       }\r\n     }\r\n   }\r\n   ```\r\n2. Start a new chat and run:\r\n\r\n   * ‚ÄúList tools from **felix-mcp-smithery**‚Äù\r\n   * ‚ÄúCall **hello** with `{ \"name\": \"Felix\" }`‚Äù\r\n   * ‚ÄúCall **summarize** on this text (2 sentences): ‚Ä¶‚Äù\r\n\r\n\r\n---\r\n\r\n## Features\r\n\r\n* **Streamable HTTP MCP** ‚Äì Express + MCP SDK‚Äôs `StreamableHTTPServerTransport` on `/mcp` (POST/GET/DELETE).\r\n* **Session-aware** ‚Äì proper handling of `Mcp-Session-Id` (no close recursion).\r\n* **OpenAI summarization** ‚Äì tidy summaries via chat completions (model default `gpt-4o-mini`).\r\n* **Zero-friction hosting** ‚Äì packaged as a container and deployed on Smithery.\r\n\r\n---\r\n\r\n## Install (local)\r\n\r\n> Requires **Node 18+** (tested on Node 20).\r\n\r\n```bash\r\ngit clone https://github.com/FelixYifeiWang/felix-mcp-smithery\r\ncd felix-mcp-smithery\r\nnpm install\r\n```\r\n\r\nSet env (only needed if you‚Äôll call `summarize` locally):\r\n\r\n```bash\r\nexport OPENAI_API_KEY=\"sk-...\"\r\n```\r\n\r\nRun:\r\n\r\n```bash\r\nnode index.js\r\n# ‚úÖ MCP Streamable HTTP server on 0.0.0.0:8081 (POST/GET/DELETE /mcp)\r\n```\r\n\r\nLocal curl:\r\n\r\n```bash\r\ncurl -s -X POST \"http://localhost:8081/mcp\" \\\r\n  -H 'Content-Type: application/json' \\\r\n  -H 'Mcp-Protocol-Version: 2025-06-18' \\\r\n  --data '{\"jsonrpc\":\"2.0\",\"id\":0,\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"2025-06-18\"}}'\r\n```\r\n\r\n---\r\n\r\n## Usage (tools)\r\n\r\n**hello**\r\n\r\n```json\r\n{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/call\",\"params\":{\"name\":\"hello\",\"arguments\":{\"name\":\"Felix\"}}}\r\n```\r\n\r\n**randomNumber**\r\n\r\n```json\r\n{\"jsonrpc\":\"2.0\",\"id\":2,\"method\":\"tools/call\",\"params\":{\"name\":\"randomNumber\",\"arguments\":{\"max\":10}}}\r\n```\r\n\r\n**weather**\r\n\r\n```json\r\n{\"jsonrpc\":\"2.0\",\"id\":3,\"method\":\"tools/call\",\"params\":{\"name\":\"weather\",\"arguments\":{\"city\":\"Boston\"}}}\r\n```\r\n\r\n**summarize** *(needs `OPENAI_API_KEY` set on the server)*\r\n\r\n```json\r\n{\"jsonrpc\":\"2.0\",\"id\":4,\"method\":\"tools/call\",\"params\":{\"name\":\"summarize\",\"arguments\":{\"text\":\"(paste long text)\",\"maxSentences\":2}}}\r\n```\r\n\r\n---\r\n\r\n## How it works\r\n\r\n* **Server core:**\r\n  `McpServer` from `@modelcontextprotocol/sdk` with tools registered in `buildServer()`.\r\n  Transport: `StreamableHTTPServerTransport` on `/mcp` handling:\r\n\r\n  * `POST /mcp` ‚Äî JSON-RPC requests (and first-time `initialize`)\r\n  * `GET /mcp` ‚Äî server-to-client notifications (SSE)\r\n  * `DELETE /mcp` ‚Äî end session\r\n* **CORS:** Allows all origins; exposes `Mcp-Session-Id` header (good for hosted clients).\r\n* **OpenAI summarize:** Thin `fetch` wrapper around `/v1/chat/completions` with a short ‚Äúcrisp summarizer‚Äù system prompt.\r\n\r\n---\r\n\r\n## Deployment (Smithery)\r\n\r\n1. GitHub repo with:\r\n\r\n   * `index.js` (Express + MCP)\r\n   * `package.json` (`@modelcontextprotocol/sdk`, `express`, `cors`, `zod`)\r\n   * `Dockerfile`\r\n   * `smithery.yaml`:\r\n\r\n     ```yaml\r\n     kind: server\r\n     name: felix-mcp-smithery\r\n     version: 1.0.0\r\n     runtime: container\r\n\r\n     startCommand:\r\n       type: http\r\n\r\n     transport: streamable-http\r\n     port: 8081\r\n     path: /mcp\r\n     ssePath: /mcp\r\n     health: /\r\n     ```\r\n2. In Smithery:\r\n\r\n   * Create server from the repo.\r\n   * Add **Environment Variables**: `OPENAI_API_KEY` (optional for `summarize`).\r\n   * Deploy ‚Üí confirm logs show:\r\n     `‚úÖ MCP Streamable HTTP server on 0.0.0.0:8081 (POST/GET/DELETE /mcp)`\r\n\r\n---\r\n\r\n## NANDA Index\r\n\r\n* Go to **join39.org ‚Üí Context Agents ‚Üí Add**\r\n\r\n  * **Agent Name:** `Felix MCP (Smithery)`\r\n  * **MCP Endpoint:**\r\n    `https://server.smithery.ai/@FelixYifeiWang/felix-mcp-smithery/mcp?api_key=YOUR_KEY&profile=YOUR_PROFILE`\r\n  * **Description:**\r\n    `Streamable-HTTP MCP hosted on Smithery. Tools: hello, randomNumber, weather, summarize (OpenAI).`\r\n\r\n* Test from NANDA: `initialize` ‚Üí `tools/list` ‚Üí call `hello`.\r\n\r\n---\r\n\r\n## Project structure\r\n\r\n```\r\n.\r\n‚îú‚îÄ index.js            # Express + Streamable HTTP + tools\r\n‚îú‚îÄ package.json        # sdk/express/cors/zod\r\n‚îú‚îÄ Dockerfile          # container build for Smithery\r\n‚îî‚îÄ smithery.yaml       # Smithery project config\r\n```\r\n\r\n---\r\n\r\n## Assignment rubric mapping\r\n\r\n* ‚úÖ **Find/Build:** Custom MCP server with 4 tools\r\n* ‚úÖ **Deploy:** Hosted on Smithery (public server page linked)\r\n* ‚úÖ **Test in a host:** Verified in Claude Desktop (screenshots/recording included)\r\n* ‚úÖ **NANDA Index:** Added as a Context Agent (screenshot included)\r\n* ‚úÖ **Deliverables:** Repo link + working endpoint + host screenshots\r\n\r\n---\r\n\r\n## What worked\r\n\r\n* Streamable HTTP transport with session management is stable once the close-loop gotcha is avoided.\r\n* Smithery makes deployment + auth key distribution straightforward.\r\n* Claude Desktop connects cleanly via `@smithery/cli run ‚Ä¶`.\r\n\r\n---\r\n\r\n## AI Acknowledgement\r\n\r\nParts of this project (tool scaffolding, error fixes, and documentation polish) were produced with AI assistance.\r\nThe final code, deployment, and testing steps were implemented and verified by me.\r\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide a greeting message with the hello tool",
        "Generate a random integer up to a specified maximum with randomNumber",
        "Fetch current weather information for a given city using weather",
        "Summarize text into a concise summary using OpenAI chat completions with summarize",
        "Handle MCP protocol requests over a streamable HTTP server",
        "Manage sessions properly with Mcp-Session-Id header support",
        "Deploy as a containerized server on Smithery platform",
        "Support JSON-RPC requests and server-to-client notifications via SSE"
      ],
      "limitations": [
        "Summarize tool requires a valid OPENAI_API_KEY environment variable",
        "Authentication is managed via query parameters and should not be hardcoded",
        "No mention of support for tools beyond the four provided (hello, randomNumber, weather, summarize)",
        "Rate limits or usage quotas for OpenAI API are not specified",
        "No explicit support for non-HTTP transports or protocols"
      ],
      "requirements": [
        "Node.js version 18 or higher (tested on Node 20) for local installation",
        "OPENAI_API_KEY environment variable set for using the summarize tool",
        "Smithery API key and profile ID for deployment and usage in Smithery platform",
        "Access to Smithery platform for deployment and management",
        "Use of Claude Desktop or compatible client for testing and interaction"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples for all tools, detailed feature descriptions, deployment steps, limitations, and requirements, making it excellent in coverage.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Felix MCP (Smithery)\r\n\r\nA tiny **Model Context Protocol** server with a few useful tools, deployed on **Smithery**, tested in **Claude Desktop**, and indexed in **NANDA**.\r\n\r\n**Tools included**\r\n\r\n* `hello(name)` ‚Äì quick greeting\r\n* `randomNumber(max?)` ‚Äì random integer (default 100)\r\n* `weather(city)` ‚Äì current weather via wttr.in\r\n* `summarize(text, maxSentences?, model?)` ‚Äì OpenAI-powered summary *(requires `OPENAI_API_KEY`)*\r\n\r\n**Public server page**\r\n`https://smithery.ai/server/@FelixYifeiWang/felix-mcp-smithery`\r\n\r\n**MCP endpoint (streamable HTTP)**\r\n`https://server.smithery.ai/@FelixYifeiWang/felix-mcp-smithery/mcp`\r\n*(In Smithery/NANDA, auth is attached via query param `api_key` and optional `profile`, configured in the platform UI; do **not** hardcode secrets here.)*\r\n\r\n---\r\n\r\n## Demo\r\n\r\n### In Claude Desktop (recommended)\r\n\r\n1. Open **Settings ‚Üí Developer ‚Üí mcpServers** and add:\r\n\r\n   ```json\r\n   {\r\n     \"mcpServers\": {\r\n       \"felix-mcp-smithery\": {\r\n         \"command\": \"npx\",\r\n         \"args\": [\r\n           \"-y\",\r\n           \"@smithery/cli@latest\",\r\n           \"run\",\r\n           \"@FelixYifeiWang/felix-mcp-smithery\",\r\n           \"--key\",\r\n           \"YOUR_SMITHERY_API_KEY\",\r\n           \"--profile\",\r\n           \"YOUR_PROFILE_ID\"\r\n         ]\r\n       }\r\n     }\r\n   }\r\n   ```\r\n2.",
        "start_pos": 0,
        "end_pos": 1309,
        "token_count_estimate": 327,
        "source_type": "readme",
        "agent_id": "2ce050c9759a38e4"
      },
      {
        "chunk_id": 1,
        "text": "a container and deployed on Smithery.\r\n\r\n---\r\n\r\n## Install (local)\r\n\r\n> Requires **Node 18+** (tested on Node 20).\r\n\r\n```bash\r\ngit clone https://github.com/FelixYifeiWang/felix-mcp-smithery\r\ncd felix-mcp-smithery\r\nnpm install\r\n```\r\n\r\nSet env (only needed if you‚Äôll call `summarize` locally):\r\n\r\n```bash\r\nexport OPENAI_API_KEY=\"sk-...\"\r\n```\r\n\r\nRun:\r\n\r\n```bash\r\nnode index.js\r\n# ‚úÖ MCP Streamable HTTP server on 0.0.0.0:8081 (POST/GET/DELETE /mcp)\r\n```\r\n\r\nLocal curl:\r\n\r\n```bash\r\ncurl -s -X POST \"http://localhost:8081/mcp\" \\\r\n  -H 'Content-Type: application/json' \\\r\n  -H 'Mcp-Protocol-Version: 2025-06-18' \\\r\n  --data '{\"jsonrpc\":\"2.0\",\"id\":0,\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"2025-06-18\"}}'\r\n```\r\n\r\n---\r\n\r\n## Usage (tools)\r\n\r\n**hello**\r\n\r\n```json\r\n{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/call\",\"params\":{\"name\":\"hello\",\"arguments\":{\"name\":\"Felix\"}}}\r\n```\r\n\r\n**randomNumber**\r\n\r\n```json\r\n{\"jsonrpc\":\"2.0\",\"id\":2,\"method\":\"tools/call\",\"params\":{\"name\":\"randomNumber\",\"arguments\":{\"max\":10}}}\r\n```\r\n\r\n**weather**\r\n\r\n```json\r\n{\"jsonrpc\":\"2.0\",\"id\":3,\"method\":\"tools/call\",\"params\":{\"name\":\"weather\",\"arguments\":{\"city\":\"Boston\"}}}\r\n```\r\n\r\n**summarize** *(needs `OPENAI_API_KEY` set on the server)*\r\n\r\n```json\r\n{\"jsonrpc\":\"2.0\",\"id\":4,\"method\":\"tools/call\",\"params\":{\"name\":\"summarize\",\"arguments\":{\"text\":\"(paste long text)\",\"maxSentences\":2}}}\r\n```\r\n\r\n---\r\n\r\n## How it works\r\n\r\n* **Server core:**\r\n  `McpServer` from `@modelcontextprotocol/sdk` with tools registered in `buildServer()`.\r\n  Transport: `StreamableHTTPServerTransport` on `/mcp` handling:\r\n\r\n  * `POST /mcp` ‚Äî JSON-RPC requests (and first-time `initialize`)\r\n  * `GET /mcp` ‚Äî server-to-client notifications (SSE)\r\n  * `DELETE /mcp` ‚Äî end session\r\n* **CORS:** Allows all origins; exposes `Mcp-Session-Id` header (good for hosted clients).\r\n* **OpenAI summarize:** Thin `fetch` wrapper around `/v1/chat/completions` with a short ‚Äúcrisp summarizer‚Äù system prompt.\r\n\r\n---\r\n\r\n## Deployment (Smithery)\r\n\r\n1.",
        "start_pos": 1848,
        "end_pos": 3828,
        "token_count_estimate": 494,
        "source_type": "readme",
        "agent_id": "2ce050c9759a38e4"
      },
      {
        "chunk_id": 2,
        "text": "d` header (good for hosted clients).\r\n* **OpenAI summarize:** Thin `fetch` wrapper around `/v1/chat/completions` with a short ‚Äúcrisp summarizer‚Äù system prompt.\r\n\r\n---\r\n\r\n## Deployment (Smithery)\r\n\r\n1. GitHub repo with:\r\n\r\n   * `index.js` (Express + MCP)\r\n   * `package.json` (`@modelcontextprotocol/sdk`, `express`, `cors`, `zod`)\r\n   * `Dockerfile`\r\n   * `smithery.yaml`:\r\n\r\n     ```yaml\r\n     kind: server\r\n     name: felix-mcp-smithery\r\n     version: 1.0.0\r\n     runtime: container\r\n\r\n     startCommand:\r\n       type: http\r\n\r\n     transport: streamable-http\r\n     port: 8081\r\n     path: /mcp\r\n     ssePath: /mcp\r\n     health: /\r\n     ```\r\n2. In Smithery:\r\n\r\n   * Create server from the repo.\r\n   * Add **Environment Variables**: `OPENAI_API_KEY` (optional for `summarize`).\r\n   * Deploy ‚Üí confirm logs show:\r\n     `‚úÖ MCP Streamable HTTP server on 0.0.0.0:8081 (POST/GET/DELETE /mcp)`\r\n\r\n---\r\n\r\n## NANDA Index\r\n\r\n* Go to **join39.org ‚Üí Context Agents ‚Üí Add**\r\n\r\n  * **Agent Name:** `Felix MCP (Smithery)`\r\n  * **MCP Endpoint:**\r\n    `https://server.smithery.ai/@FelixYifeiWang/felix-mcp-smithery/mcp?api_key=YOUR_KEY&profile=YOUR_PROFILE`\r\n  * **Description:**\r\n    `Streamable-HTTP MCP hosted on Smithery.",
        "start_pos": 3628,
        "end_pos": 4836,
        "token_count_estimate": 302,
        "source_type": "readme",
        "agent_id": "2ce050c9759a38e4"
      },
      {
        "chunk_id": 3,
        "text": "dex:** Added as a Context Agent (screenshot included)\r\n* ‚úÖ **Deliverables:** Repo link + working endpoint + host screenshots\r\n\r\n---\r\n\r\n## What worked\r\n\r\n* Streamable HTTP transport with session management is stable once the close-loop gotcha is avoided.\r\n* Smithery makes deployment + auth key distribution straightforward.\r\n* Claude Desktop connects cleanly via `@smithery/cli run ‚Ä¶`.\r\n\r\n---\r\n\r\n## AI Acknowledgement\r\n\r\nParts of this project (tool scaffolding, error fixes, and documentation polish) were produced with AI assistance.\r\nThe final code, deployment, and testing steps were implemented and verified by me.",
        "start_pos": 5476,
        "end_pos": 6096,
        "token_count_estimate": 154,
        "source_type": "readme",
        "agent_id": "2ce050c9759a38e4"
      }
    ]
  },
  {
    "agent_id": "ab46ef9628444c50",
    "name": "ai.smithery/Funding-Machine-ghl-mcp-fundingmachine",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@Funding-Machine/ghl-mcp-fundingmachine/mcp",
    "description": "Automate GoHighLevel across CRM, messaging, calendars, marketing, e-commerce, and billing. Manage‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T21:20:15.688859Z",
    "indexed_at": "2026-02-18T04:03:35.584184",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Automate GoHighLevel CRM operations",
        "Automate messaging workflows",
        "Automate calendar management",
        "Automate marketing tasks",
        "Automate e-commerce processes",
        "Automate billing operations"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of automation capabilities across multiple domains but lacks detailed examples, structure, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "605302c7f883d968",
    "name": "ai.smithery/HARJAP-SINGH-3105-splitwise_mcp",
    "source": "mcp",
    "source_url": "https://github.com/HARJAP-SINGH-3105/Splitwise_MCP",
    "description": "Manage Splitwise balances, expenses, and groups from your workspace. Fetch friends and recent acti‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T18:42:59.470988Z",
    "indexed_at": "2026-02-18T04:03:37.125268",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# My_Splitwise_server\r\n\r\nAn MCP server built with Smithery.\r\n\r\n## Prerequisites\r\n\r\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\r\n\r\n## Getting Started\r\n\r\n1. Run the server:\r\n   ```bash\r\n   uv run dev\r\n   ```\r\n\r\n2. Test interactively:\r\n\r\n   ```bash\r\n   uv run playground\r\n   ```\r\n\r\nTry saying \"Say hello to John\" to test the example tool.\r\n\r\n## Development\r\n\r\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\r\n\r\n## Deploy\r\n\r\nReady to deploy? Push your code to GitHub and deploy to Smithery:\r\n\r\n1. Create a new repository at [github.com/new](https://github.com/new)\r\n\r\n2. Initialize git and push to GitHub:\r\n   ```bash\r\n   git add .\r\n   git commit -m \"Hello world üëã\"\r\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\r\n   git push -u origin main\r\n   ```\r\n\r\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\r\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server built with Smithery",
        "Test server interactions interactively via playground",
        "Add or update server capabilities through server.py code"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai/account/api-keys",
        "Python environment capable of running uv commands",
        "GitHub account for deployment"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation steps, usage examples, and deployment instructions but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# My_Splitwise_server\r\n\r\nAn MCP server built with Smithery.\r\n\r\n## Prerequisites\r\n\r\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\r\n\r\n## Getting Started\r\n\r\n1. Run the server:\r\n   ```bash\r\n   uv run dev\r\n   ```\r\n\r\n2. Test interactively:\r\n\r\n   ```bash\r\n   uv run playground\r\n   ```\r\n\r\nTry saying \"Say hello to John\" to test the example tool.\r\n\r\n## Development\r\n\r\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\r\n\r\n## Deploy\r\n\r\nReady to deploy? Push your code to GitHub and deploy to Smithery:\r\n\r\n1. Create a new repository at [github.com/new](https://github.com/new)\r\n\r\n2. Initialize git and push to GitHub:\r\n   ```bash\r\n   git add .\r\n   git commit -m \"Hello world üëã\"\r\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\r\n   git push -u origin main\r\n   ```\r\n\r\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 968,
        "token_count_estimate": 241,
        "source_type": "readme",
        "agent_id": "605302c7f883d968"
      }
    ]
  },
  {
    "agent_id": "e10cdabd78d6fa29",
    "name": "ai.smithery/Hint-Services-obsidian-github-mcp",
    "source": "mcp",
    "source_url": "https://github.com/Hint-Services/obsidian-github-mcp",
    "description": "Connect AI assistants to your GitHub-hosted Obsidian vault to seamlessly access, search, and analy‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T15:20:36.371442Z",
    "indexed_at": "2026-02-18T04:03:38.948691",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![smithery badge](https://smithery.ai/badge/@Hint-Services/obsidian-github-mcp)](https://smithery.ai/server/@Hint-Services/obsidian-github-mcp)\n[![npm version](https://img.shields.io/npm/v/obsidian-github-mcp)](https://www.npmjs.com/package/obsidian-github-mcp)\n\n# Obsidian GitHub MCP\n\nA Model Context Protocol (MCP) server that connects AI assistants to GitHub repositories containing Obsidian vaults. This server enables seamless integration with your knowledge base stored on GitHub, allowing AI assistants to read, search, and analyze your Obsidian notes and documentation.\n\n## Why This Tool?\n\nMany Obsidian users store their vaults in GitHub for backup, versioning, and collaboration. This MCP server bridges the gap between your GitHub-hosted Obsidian vault and AI assistants, enabling:\n\n- **Knowledge Base Access**: Retrieve specific notes and documents from your Obsidian vault\n- **Intelligent Search**: Find relevant content across your entire knowledge base\n- **Evolution Tracking**: See how your ideas and notes have developed over time\n- **Task Integration**: Connect with issues and project management workflows\n\n## Features\n\n- **GitHub Repository Access**: Connect to any GitHub repository containing your Obsidian vault\n- **Type-Safe Implementation**: Written in TypeScript with comprehensive type definitions\n- **Input Validation**: Robust validation for all API inputs using Zod schemas\n- **Error Handling**: Graceful error handling with informative messages\n- **MCP Integration**: Full compatibility with Claude, Cursor, Windsurf, Cline, and other MCP hosts\n\n## Available Tools\n\n### Knowledge Base Access\n\n- **getFileContents**: Retrieve the contents of specific notes, documents, or files from your Obsidian vault\n- **searchFiles**: Enhanced search with multiple modes:\n  - `filename`: Find files by exact filename (perfect for \"OKR 2025\" type searches)\n  - `path`: Search anywhere in file paths \n  - `content`: Search within file contents\n  - `all`: Comprehensive search across filenames, paths, and content\n\n### Project Management Integration\n\n- **searchIssues**: Search for issues and discussions related to your knowledge base projects\n- **getCommitHistory**: Track how your knowledge base has evolved over time with detailed commit history and diffs\n\n## Use Cases\n\n### For Knowledge Workers\n- **Research Assistant**: AI can access your research notes and reference materials\n- **Writing Support**: Pull relevant background information from your knowledge base\n- **Idea Development**: Track how concepts have evolved across your notes\n\n### For Developers\n- **Documentation Access**: Retrieve project documentation and technical notes\n- **Learning Journals**: Access your learning notes and code examples\n- **Project Planning**: Integrate with GitHub issues for comprehensive project management\n\n### For Students & Academics\n- **Study Materials**: Access lecture notes and study guides\n- **Research Papers**: Retrieve research notes and citations\n- **Collaboration**: Work with shared knowledge bases stored in GitHub\n\n## Installation\n\n### Using Smithery (Recommended)\n\nThe easiest way to install Obsidian GitHub MCP is using Smithery:\n\n```bash\n# For Claude Desktop\nnpx -y @smithery/cli install @Hint-Services/obsidian-github-mcp --client claude\n\n# For Cursor\nnpx -y @smithery/cli install @Hint-Services/obsidian-github-mcp --client cursor\n\n# For Windsurf\nnpx -y @smithery/cli install @Hint-Services/obsidian-github-mcp --client windsurf\n\n# For Cline\nnpx -y @smithery/cli install @Hint-Services/obsidian-github-mcp --client cline\n```\n\n### Manual Installation\n\n```bash\nnpm install obsidian-github-mcp\n```\n\n## Configuration\n\nAdd the server to your MCP settings file with the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"obsidianGithub\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"obsidian-github-mcp\"],\n      \"env\": {\n        \"GITHUB_TOKEN\": \"your-github-token\",\n        \"GITHUB_OWNER\": \"your-github-username\",\n        \"GITHUB_REPO\": \"your-obsidian-vault-repo\"\n      }\n    }\n  }\n}\n```\n\n### Required Environment Variables\n\n- `GITHUB_TOKEN`: Your GitHub personal access token ([create one here](https://github.com/settings/tokens))\n- `GITHUB_OWNER`: The owner/organization of the GitHub repository\n- `GITHUB_REPO`: The name of the repository containing your Obsidian vault\n\n### GitHub Token Permissions\n\nYour GitHub token needs the following permissions:\n- `repo` (for private repositories) or `public_repo` (for public repositories)\n- `read:org` (if accessing organization repositories)\n\n## Example Workflows\n\n### Accessing Your Daily Notes\n\n```json\n{\n  \"tool\": \"getFileContents\",\n  \"arguments\": {\n    \"filePath\": \"Daily Notes/2024-01-15.md\"\n  }\n}\n```\n\n### Finding Files by Name (Perfect for \"OKR 2025\" example!)\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"OKR 2025\",\n    \"searchIn\": \"filename\"\n  }\n}\n```\n\n### Finding Files by Path\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"Daily Notes\",\n    \"searchIn\": \"path\"\n  }\n}\n```\n\n### Finding Research on a Topic (Content Search)\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"machine learning algorithms\",\n    \"searchIn\": \"content\",\n    \"perPage\": 10\n  }\n}\n```\n\n### Comprehensive Search (Filename, Path, and Content)\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"project planning\",\n    \"searchIn\": \"all\"\n  }\n}\n```\n\n### Advanced GitHub Search Syntax\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"path:*.md extension:md\",\n    \"searchIn\": \"content\"\n  }\n}\n```\n\n### Tracking Knowledge Evolution\n\n```json\n{\n  \"tool\": \"getCommitHistory\",\n  \"arguments\": {\n    \"days\": 30,\n    \"includeDiffs\": true,\n    \"maxCommits\": 10\n  }\n}\n```\n\n## Search Tips & Troubleshooting\n\n### Can't Find Your File by Name?\n\n**Problem**: Searching for \"OKR 2025\" doesn't find your file named \"OKR 2025.md\"\n\n**Solution**: Use the `searchIn: \"filename\"` parameter:\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"OKR 2025\",\n    \"searchIn\": \"filename\"\n  }\n}\n```\n\n### Advanced Search Techniques\n\n1. **Exact Phrases**: Use quotes for multi-word searches\n   ```json\n   {\n     \"query\": \"\\\"project planning 2025\\\"\",\n     \"searchIn\": \"content\"\n   }\n   ```\n\n2. **File Extensions**: Find specific file types\n   ```json\n   {\n     \"query\": \"path:*.md\",\n     \"searchIn\": \"content\"\n   }\n   ```\n\n3. **Directory Specific**: Search within folders\n   ```json\n   {\n     \"query\": \"path:\\\"Daily Notes/\\\"\",\n     \"searchIn\": \"content\"\n   }\n   ```\n\n## Project Structure\n\n```\nobsidian-github-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ index.ts          # Main MCP server entry point\n‚îÇ   ‚îî‚îÄ‚îÄ github/           # GitHub API integration\n‚îÇ       ‚îú‚îÄ‚îÄ client.ts     # GitHub client implementation\n‚îÇ       ‚îî‚îÄ‚îÄ types.ts      # TypeScript type definitions\n‚îú‚îÄ‚îÄ docs/                 # Documentation\n‚îú‚îÄ‚îÄ package.json          # Project configuration\n‚îî‚îÄ‚îÄ tsconfig.json         # TypeScript configuration\n```\n\n## For Developers\n\nIf you're interested in contributing to this project or developing your own tools with this server, please see the [Development Guide](docs/development.md).\n\n### Development Commands\n\n- `pnpm install` - Install dependencies\n- `pnpm run build` - Build the project\n- `pnpm run dev` - Run in development mode with inspector\n- `pnpm run inspector` - Launch MCP inspector for testing\n\n## Migration from mcp-private-github-search\n\nIf you're migrating from the older `mcp-private-github-search` package:\n\n1. Update your package name in configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"obsidianGithub\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"obsidian-github-mcp\"]\n       }\n     }\n   }\n   ```\n\n2. The functionality remains the same - all existing tools work identically\n3. Consider the new Obsidian-focused use cases and workflows\n\n## Learn More\n\nFor further information on the MCP ecosystem, refer to:\n\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io): Detailed coverage of MCP architecture and design principles\n- [Smithery - MCP Server Registry](https://smithery.ai/docs): Guidelines for publishing MCP servers\n- [MCP TypeScript SDK Documentation](https://modelcontextprotocol.io/typescript): Comprehensive TypeScript SDK documentation\n- [Obsidian](https://obsidian.md): The knowledge management app that inspired this tool\n\n## About Hint Services\n\n> \"The future is already here, it's just unevenly distributed\"\n>\n> ‚Äî William Gibson, Author\n\nHint Services is a boutique consultancy with a mission to develop and expand how user interfaces leverage artificial intelligence technology. We architect ambition at the intersection of AI and User Experience, founded and led by Ben Hofferber.\n\nWe offer specialized AI workshops for design teams looking to embrace AI tools without becoming developers. [Learn more about our training and workshops](https://hint.services/training-workshops)."
    },
    "llm_extracted": {
      "capabilities": [
        "Retrieve contents of specific notes, documents, or files from Obsidian vaults stored in GitHub",
        "Perform intelligent searches across filenames, file paths, and file contents within Obsidian vaults",
        "Track the evolution of knowledge base content through detailed GitHub commit history and diffs",
        "Search GitHub issues and discussions related to knowledge base projects",
        "Integrate with AI assistants via the Model Context Protocol (MCP) for seamless knowledge base access",
        "Validate all API inputs robustly using Zod schemas",
        "Handle errors gracefully with informative messages",
        "Support multiple MCP hosts including Claude, Cursor, Windsurf, and Cline"
      ],
      "limitations": [
        "Requires a GitHub personal access token with appropriate permissions to access repositories",
        "Limited to GitHub repositories containing Obsidian vaults; does not support other storage platforms",
        "Search functionality depends on GitHub's search capabilities and syntax",
        "Requires repository owner and repository name configuration; cannot auto-discover repositories",
        "No mention of support for private vault encryption or non-Obsidian file formats"
      ],
      "requirements": [
        "GitHub personal access token with 'repo' or 'public_repo' and 'read:org' permissions",
        "GitHub repository owner and repository name containing the Obsidian vault",
        "Node.js environment to run the MCP server (installation via npm or Smithery CLI)",
        "MCP host client such as Claude, Cursor, Windsurf, or Cline to interact with the server",
        "Configuration of environment variables: GITHUB_TOKEN, GITHUB_OWNER, GITHUB_REPO"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear descriptions of tools and features, explicit requirements including environment variables and permissions, and outlines limitations, making it excellent in quality.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![smithery badge](https://smithery.ai/badge/@Hint-Services/obsidian-github-mcp)](https://smithery.ai/server/@Hint-Services/obsidian-github-mcp)\n[![npm version](https://img.shields.io/npm/v/obsidian-github-mcp)](https://www.npmjs.com/package/obsidian-github-mcp)\n\n# Obsidian GitHub MCP\n\nA Model Context Protocol (MCP) server that connects AI assistants to GitHub repositories containing Obsidian vaults. This server enables seamless integration with your knowledge base stored on GitHub, allowing AI assistants to read, search, and analyze your Obsidian notes and documentation.\n\n## Why This Tool?\n\nMany Obsidian users store their vaults in GitHub for backup, versioning, and collaboration.",
        "start_pos": 0,
        "end_pos": 690,
        "token_count_estimate": 172,
        "source_type": "readme",
        "agent_id": "e10cdabd78d6fa29"
      },
      {
        "chunk_id": 1,
        "text": "\"OKR 2025\" type searches)\n  - `path`: Search anywhere in file paths \n  - `content`: Search within file contents\n  - `all`: Comprehensive search across filenames, paths, and content\n\n### Project Management Integration\n\n- **searchIssues**: Search for issues and discussions related to your knowledge base projects\n- **getCommitHistory**: Track how your knowledge base has evolved over time with detailed commit history and diffs\n\n## Use Cases\n\n### For Knowledge Workers\n- **Research Assistant**: AI can access your research notes and reference materials\n- **Writing Support**: Pull relevant background information from your knowledge base\n- **Idea Development**: Track how concepts have evolved across your notes\n\n### For Developers\n- **Documentation Access**: Retrieve project documentation and technical notes\n- **Learning Journals**: Access your learning notes and code examples\n- **Project Planning**: Integrate with GitHub issues for comprehensive project management\n\n### For Students & Academics\n- **Study Materials**: Access lecture notes and study guides\n- **Research Papers**: Retrieve research notes and citations\n- **Collaboration**: Work with shared knowledge bases stored in GitHub\n\n## Installation\n\n### Using Smithery (Recommended)\n\nThe easiest way to install Obsidian GitHub MCP is using Smithery:\n\n```bash\n# For Claude Desktop\nnpx -y @smithery/cli install @Hint-Services/obsidian-github-mcp --client claude\n\n# For Cursor\nnpx -y @smithery/cli install @Hint-Services/obsidian-github-mcp --client cursor\n\n# For Windsurf\nnpx -y @smithery/cli install @Hint-Services/obsidian-github-mcp --client windsurf\n\n# For Cline\nnpx -y @smithery/cli install @Hint-Services/obsidian-github-mcp --client cline\n```\n\n### Manual Installation\n\n```bash\nnpm install obsidian-github-mcp\n```\n\n## Configuration\n\nAdd the server to your MCP settings file with the following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"obsidianGithub\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"obsidian-github-mcp\"],\n      \"env\": {\n        \"GITHUB_TOKEN\": \"your-github-t",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "e10cdabd78d6fa29"
      },
      {
        "chunk_id": 2,
        "text": "following configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"obsidianGithub\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"obsidian-github-mcp\"],\n      \"env\": {\n        \"GITHUB_TOKEN\": \"your-github-token\",\n        \"GITHUB_OWNER\": \"your-github-username\",\n        \"GITHUB_REPO\": \"your-obsidian-vault-repo\"\n      }\n    }\n  }\n}\n```\n\n### Required Environment Variables\n\n- `GITHUB_TOKEN`: Your GitHub personal access token ([create one here](https://github.com/settings/tokens))\n- `GITHUB_OWNER`: The owner/organization of the GitHub repository\n- `GITHUB_REPO`: The name of the repository containing your Obsidian vault\n\n### GitHub Token Permissions\n\nYour GitHub token needs the following permissions:\n- `repo` (for private repositories) or `public_repo` (for public repositories)\n- `read:org` (if accessing organization repositories)\n\n## Example Workflows\n\n### Accessing Your Daily Notes\n\n```json\n{\n  \"tool\": \"getFileContents\",\n  \"arguments\": {\n    \"filePath\": \"Daily Notes/2024-01-15.md\"\n  }\n}\n```\n\n### Finding Files by Name (Perfect for \"OKR 2025\" example!)\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"OKR 2025\",\n    \"searchIn\": \"filename\"\n  }\n}\n```\n\n### Finding Files by Path\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"Daily Notes\",\n    \"searchIn\": \"path\"\n  }\n}\n```\n\n### Finding Research on a Topic (Content Search)\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"machine learning algorithms\",\n    \"searchIn\": \"content\",\n    \"perPage\": 10\n  }\n}\n```\n\n### Comprehensive Search (Filename, Path, and Content)\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"project planning\",\n    \"searchIn\": \"all\"\n  }\n}\n```\n\n### Advanced GitHub Search Syntax\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"path:*.md extension:md\",\n    \"searchIn\": \"content\"\n  }\n}\n```\n\n### Tracking Knowledge Evolution\n\n```json\n{\n  \"tool\": \"getCommitHistory\",\n  \"arguments\": {\n    \"days\": 30,\n    \"includeDiffs\": true,\n    \"maxCommits\": 10\n  }\n}\n```\n\n## Search Tips & Troubleshootin",
        "start_pos": 3696,
        "end_pos": 5744,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "e10cdabd78d6fa29"
      },
      {
        "chunk_id": 3,
        "text": "``\n\n### Tracking Knowledge Evolution\n\n```json\n{\n  \"tool\": \"getCommitHistory\",\n  \"arguments\": {\n    \"days\": 30,\n    \"includeDiffs\": true,\n    \"maxCommits\": 10\n  }\n}\n```\n\n## Search Tips & Troubleshooting\n\n### Can't Find Your File by Name?\n\n**Problem**: Searching for \"OKR 2025\" doesn't find your file named \"OKR 2025.md\"\n\n**Solution**: Use the `searchIn: \"filename\"` parameter:\n\n```json\n{\n  \"tool\": \"searchFiles\",\n  \"arguments\": {\n    \"query\": \"OKR 2025\",\n    \"searchIn\": \"filename\"\n  }\n}\n```\n\n### Advanced Search Techniques\n\n1. **Exact Phrases**: Use quotes for multi-word searches\n   ```json\n   {\n     \"query\": \"\\\"project planning 2025\\\"\",\n     \"searchIn\": \"content\"\n   }\n   ```\n\n2. **File Extensions**: Find specific file types\n   ```json\n   {\n     \"query\": \"path:*.md\",\n     \"searchIn\": \"content\"\n   }\n   ```\n\n3. **Directory Specific**: Search within folders\n   ```json\n   {\n     \"query\": \"path:\\\"Daily Notes/\\\"\",\n     \"searchIn\": \"content\"\n   }\n   ```\n\n## Project Structure\n\n```\nobsidian-github-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ index.ts          # Main MCP server entry point\n‚îÇ   ‚îî‚îÄ‚îÄ github/           # GitHub API integration\n‚îÇ       ‚îú‚îÄ‚îÄ client.ts     # GitHub client implementation\n‚îÇ       ‚îî‚îÄ‚îÄ types.ts      # TypeScript type definitions\n‚îú‚îÄ‚îÄ docs/                 # Documentation\n‚îú‚îÄ‚îÄ package.json          # Project configuration\n‚îî‚îÄ‚îÄ tsconfig.json         # TypeScript configuration\n```\n\n## For Developers\n\nIf you're interested in contributing to this project or developing your own tools with this server, please see the [Development Guide](docs/development.md).\n\n### Development Commands\n\n- `pnpm install` - Install dependencies\n- `pnpm run build` - Build the project\n- `pnpm run dev` - Run in development mode with inspector\n- `pnpm run inspector` - Launch MCP inspector for testing\n\n## Migration from mcp-private-github-search\n\nIf you're migrating from the older `mcp-private-github-search` package:\n\n1.",
        "start_pos": 5544,
        "end_pos": 7445,
        "token_count_estimate": 475,
        "source_type": "readme",
        "agent_id": "e10cdabd78d6fa29"
      },
      {
        "chunk_id": 4,
        "text": "mode with inspector\n- `pnpm run inspector` - Launch MCP inspector for testing\n\n## Migration from mcp-private-github-search\n\nIf you're migrating from the older `mcp-private-github-search` package:\n\n1. Update your package name in configuration:\n   ```json\n   {\n     \"mcpServers\": {\n       \"obsidianGithub\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"obsidian-github-mcp\"]\n       }\n     }\n   }\n   ```\n\n2. The functionality remains the same - all existing tools work identically\n3. Consider the new Obsidian-focused use cases and workflows\n\n## Learn More\n\nFor further information on the MCP ecosystem, refer to:\n\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io): Detailed coverage of MCP architecture and design principles\n- [Smithery - MCP Server Registry](https://smithery.ai/docs): Guidelines for publishing MCP servers\n- [MCP TypeScript SDK Documentation](https://modelcontextprotocol.io/typescript): Comprehensive TypeScript SDK documentation\n- [Obsidian](https://obsidian.md): The knowledge management app that inspired this tool\n\n## About Hint Services\n\n> \"The future is already here, it's just unevenly distributed\"\n>\n> ‚Äî William Gibson, Author\n\nHint Services is a boutique consultancy with a mission to develop and expand how user interfaces leverage artificial intelligence technology. We architect ambition at the intersection of AI and User Experience, founded and led by Ben Hofferber.\n\nWe offer specialized AI workshops for design teams looking to embrace AI tools without becoming developers. [Learn more about our training and workshops](https://hint.services/training-workshops).",
        "start_pos": 7245,
        "end_pos": 8874,
        "token_count_estimate": 407,
        "source_type": "readme",
        "agent_id": "e10cdabd78d6fa29"
      }
    ]
  },
  {
    "agent_id": "c533f01be4ff2644",
    "name": "ai.smithery/IlyaGusev-academia_mcp",
    "source": "mcp",
    "source_url": "https://github.com/IlyaGusev/academia_mcp",
    "description": "Search arXiv and ACL Anthology, retrieve citations and references, and browse web sources to accel‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-16T12:14:42.162776Z",
    "indexed_at": "2026-02-18T04:03:40.791559",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Academia MCP\n\n[![PyPI](https://img.shields.io/pypi/v/academia-mcp?label=PyPI%20package)](https://pypi.org/project/academia-mcp/)\n[![CI](https://github.com/IlyaGusev/academia_mcp/actions/workflows/python.yml/badge.svg)](https://github.com/IlyaGusev/academia_mcp/actions/workflows/python.yml)\n[![License](https://img.shields.io/github/license/IlyaGusev/academia_mcp)](LICENSE)\n[![smithery badge](https://smithery.ai/badge/@IlyaGusev/academia_mcp)](https://smithery.ai/server/@IlyaGusev/academia_mcp)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/e818878b-c3a6-4b3d-a5b4-e54dcd1f1fed)\n\nMCP server with tools to search, fetch, analyze, and report on scientific papers and datasets.\n\n### Features\n- ArXiv search and download\n- ACL Anthology search\n- Hugging Face datasets search\n- Semantic Scholar citations and references\n- Web search via Exa, Brave, or Tavily\n- Web page crawler, LaTeX compilation, PDF reading\n- Optional LLM-powered tools for document QA and research proposal workflows\n\n### Requirements\n- Python 3.12+\n\n### Install\n- Using pip (end users):\n```bash\npip3 install academia-mcp\n```\n\n- For development (uv + Makefile):\n```bash\nuv venv .venv\nmake install\n```\n\n### Quickstart\n- Run over HTTP (default transport):\n```bash\npython -m academia_mcp --transport streamable-http\n# OR\nuv run -m academia_mcp --transport streamable-http\n```\n\n- Run over stdio (for local MCP clients like Claude Desktop):\n```bash\npython -m academia_mcp --transport stdio\n# OR\nuv run -m academia_mcp --transport stdio\n```\n\nNotes:\n- Transports: `stdio`, `sse`, `streamable-http`.\n- `host`/`port` are used for HTTP transports; ignored for `stdio`. Default port is `5056` (or `PORT`).\n\n### Authentication\n\nAcademia MCP supports optional token-based authentication for HTTP transports (`streamable-http` and `sse`). Authentication is disabled by default to maintain backward compatibility.\n\n#### Enabling Authentication\n\nSet the `ENABLE_AUTH` environment variable to `true`:\n\n```bash\nexport ENABLE_AUTH=true\nexport TOKENS_FILE=/path/to/tokens.json  # Optional, defaults to ./tokens.json\n```\n\n#### Managing Tokens\n\nIssue a new token:\n```bash\nacademia_mcp auth issue-token --client-id=my-client --description=\"Production API client\"\n\n# Issue token with 30-day expiration\nacademia_mcp auth issue-token --client-id=test-client --expires-days=30\n\n# Issue token with custom scopes\nacademia_mcp auth issue-token --client-id=admin --scopes=\"read,write,admin\"\n```\n\nList active tokens:\n```bash\nacademia_mcp auth list-tokens\n```\n\nRevoke a token:\n```bash\nacademia_mcp auth revoke-token mcp_a1b2c3d4e5f6...\n```\n\n#### Using Tokens\n\nInclude the token in the `Authorization` header with the `Bearer` scheme or as a query parameter apiKey.\n\n**Security Notes:**\n- Tokens are displayed only once during issuance. Store them securely.\n- Use HTTPS in production to protect tokens in transit.\n- The `tokens.json` file is automatically created with restrictive permissions (mode 600).\n- Tokens are stored in plaintext (standard practice for bearer tokens) - protect the tokens file.\n\n### Claude Desktop config\n```json\n{\n  \"mcpServers\": {\n    \"academia\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"-m\",\n        \"academia_mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n### Available tools (one-liners)\n- `arxiv_search`: Query arXiv with field-specific queries and filters.\n- `arxiv_download`: Fetch a paper by ID and convert to structured text (HTML/PDF modes).\n- `anthology_search`: Search ACL Anthology with fielded queries and optional date filtering.\n- `hf_datasets_search`: Find Hugging Face datasets with filters and sorting.\n- `s2_get_citations`: List papers citing a given arXiv paper (Semantic Scholar Graph).\n- `s2_get_references`: List papers referenced by a given arXiv paper.\n- `visit_webpage`: Fetch and normalize a web page.\n- `web_search`: Unified search wrapper; available when at least one of Exa/Brave/Tavily keys is set.\n- `exa_web_search`, `brave_web_search`, `tavily_web_search`: Provider-specific search.\n- `get_latex_templates_list`, `get_latex_template`: Enumerate and fetch built-in LaTeX templates.\n- `compile_latex`: Compile LaTeX to PDF in `WORKSPACE_DIR`.\n- `read_pdf`: Extract text per page from a PDF.\n- `download_pdf_paper`, `review_pdf_paper`: Download and optionally review PDFs (requires LLM + workspace).\n- `document_qa`: Answer questions over provided document chunks (requires LLM).\n- `extract_bitflip_info`, `generate_research_proposals`, `score_research_proposals`: Research proposal helpers (requires LLM).\n\nAvailability notes:\n- Set `WORKSPACE_DIR` to enable `compile_latex`, `read_pdf`, `download_pdf_paper`, and `review_pdf_paper`.\n- Set `OPENROUTER_API_KEY` to enable LLM tools (`document_qa`, `review_pdf_paper`, and bitflip tools).\n- Set one or more of `EXA_API_KEY`, `BRAVE_API_KEY`, `TAVILY_API_KEY` to enable `web_search` and provider tools.\n\n### Environment variables\nSet as needed, depending on which tools you use:\n\n- `OPENROUTER_API_KEY`: required for LLM-related tools.\n- `BASE_URL`: override OpenRouter base URL.\n- `DOCUMENT_QA_MODEL_NAME`: override default model for `document_qa`.\n- `BITFLIP_MODEL_NAME`: override default model for bitflip tools.\n- `TAVILY_API_KEY`: enables Tavily in `web_search`.\n- `EXA_API_KEY`: enables Exa in `web_search` and `visit_webpage`.\n- `BRAVE_API_KEY`: enables Brave in `web_search`.\n- `WORKSPACE_DIR`: directory for generated files (PDFs, temp artifacts).\n- `PORT`: HTTP port (default `5056`).\n\nYou can put these in a `.env` file in the project root.\n\n### Docker\nBuild the image:\n```bash\ndocker build -t academia_mcp .\n```\n\nRun the server (HTTP):\n```bash\ndocker run --rm -p 5056:5056 \\\n  -e PORT=5056 \\\n  -e OPENROUTER_API_KEY=your_key_here \\\n  -e WORKSPACE_DIR=/workspace \\\n  -v \"$PWD/workdir:/workspace\" \\\n  academia_mcp\n```\n\nOr use existing image: [`phoenix120/academia_mcp`](https://hub.docker.com/repository/docker/phoenix120/academia_mcp)\n\n### Examples\n- [Comprehensive report screencast (YouTube)](https://www.youtube.com/watch?v=4bweqQcN6w8)\n- [Single paper screencast (YouTube)](https://www.youtube.com/watch?v=IAAPMptJ5k8)\n\n### Makefile targets\n- `make install`: install the package in editable mode with uv\n- `make validate`: run black, flake8, and mypy (strict)\n- `make test`: run the test suite with pytest\n- `make publish`: build and publish using uv\n\n### LaTeX/PDF requirements\nOnly needed for LaTeX/PDF tools. Ensure a LaTeX distribution is installed and `pdflatex` is on PATH, as well as `latexmk`. On Debian/Ubuntu:\n```bash\nsudo apt install texlive-latex-base texlive-fonts-recommended texlive-latex-extra texlive-science latexmk\n```\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search and download scientific papers from ArXiv",
        "Search ACL Anthology with fielded queries and date filters",
        "Search Hugging Face datasets with filters and sorting",
        "Retrieve citations and references from Semantic Scholar",
        "Perform web searches via Exa, Brave, or Tavily providers",
        "Fetch and normalize web pages",
        "Compile LaTeX documents to PDF",
        "Extract text from PDF files per page",
        "Provide LLM-powered document question answering and research proposal workflows",
        "Manage token-based authentication for HTTP transports"
      ],
      "limitations": [
        "LLM-powered tools require setting OPENROUTER_API_KEY environment variable",
        "Web search tools require at least one of EXA_API_KEY, BRAVE_API_KEY, or TAVILY_API_KEY",
        "LaTeX and PDF tools require a LaTeX distribution installed with pdflatex and latexmk on PATH",
        "Authentication is optional and disabled by default; tokens must be securely stored",
        "Tokens are stored in plaintext and require HTTPS in production for security",
        "WORKSPACE_DIR must be set to enable LaTeX compilation and PDF-related tools"
      ],
      "requirements": [
        "Python 3.12 or higher",
        "Optional environment variables depending on features used: OPENROUTER_API_KEY, EXA_API_KEY, BRAVE_API_KEY, TAVILY_API_KEY, WORKSPACE_DIR",
        "LaTeX distribution installed with pdflatex and latexmk for LaTeX/PDF tools",
        "Optional token file (tokens.json) for enabling authentication",
        "Network access for web searches and external API calls"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear descriptions of all tools, environment variable requirements, authentication management, limitations, and Docker usage.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Academia MCP\n\n[![PyPI](https://img.shields.io/pypi/v/academia-mcp?label=PyPI%20package)](https://pypi.org/project/academia-mcp/)\n[![CI](https://github.com/IlyaGusev/academia_mcp/actions/workflows/python.yml/badge.svg)](https://github.com/IlyaGusev/academia_mcp/actions/workflows/python.yml)\n[![License](https://img.shields.io/github/license/IlyaGusev/academia_mcp)](LICENSE)\n[![smithery badge](https://smithery.ai/badge/@IlyaGusev/academia_mcp)](https://smithery.ai/server/@IlyaGusev/academia_mcp)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/e818878b-c3a6-4b3d-a5b4-e54dcd1f1fed)\n\nMCP server with tools to search, fetch, analyze, and report on scientific papers and datasets.\n\n### Features\n- ArXiv search and download\n- ACL Anthology search\n- Hugging Face datasets search\n- Semantic Scholar citations and references\n- Web search via Exa, Brave, or Tavily\n- Web page crawler, LaTeX compilation, PDF reading\n- Optional LLM-powered tools for document QA and research proposal workflows\n\n### Requirements\n- Python 3.12+\n\n### Install\n- Using pip (end users):\n```bash\npip3 install academia-mcp\n```\n\n- For development (uv + Makefile):\n```bash\nuv venv .venv\nmake install\n```\n\n### Quickstart\n- Run over HTTP (default transport):\n```bash\npython -m academia_mcp --transport streamable-http\n# OR\nuv run -m academia_mcp --transport streamable-http\n```\n\n- Run over stdio (for local MCP clients like Claude Desktop):\n```bash\npython -m academia_mcp --transport stdio\n# OR\nuv run -m academia_mcp --transport stdio\n```\n\nNotes:\n- Transports: `stdio`, `sse`, `streamable-http`.\n- `host`/`port` are used for HTTP transports; ignored for `stdio`. Default port is `5056` (or `PORT`).\n\n### Authentication\n\nAcademia MCP supports optional token-based authentication for HTTP transports (`streamable-http` and `sse`). Authentication is disabled by default to maintain backward compatibility.",
        "start_pos": 0,
        "end_pos": 1894,
        "token_count_estimate": 473,
        "source_type": "readme",
        "agent_id": "c533f01be4ff2644"
      },
      {
        "chunk_id": 1,
        "text": "# Authentication\n\nAcademia MCP supports optional token-based authentication for HTTP transports (`streamable-http` and `sse`). Authentication is disabled by default to maintain backward compatibility.\n\n#### Enabling Authentication\n\nSet the `ENABLE_AUTH` environment variable to `true`:\n\n```bash\nexport ENABLE_AUTH=true\nexport TOKENS_FILE=/path/to/tokens.json  # Optional, defaults to ./tokens.json\n```\n\n#### Managing Tokens\n\nIssue a new token:\n```bash\nacademia_mcp auth issue-token --client-id=my-client --description=\"Production API client\"\n\n# Issue token with 30-day expiration\nacademia_mcp auth issue-token --client-id=test-client --expires-days=30\n\n# Issue token with custom scopes\nacademia_mcp auth issue-token --client-id=admin --scopes=\"read,write,admin\"\n```\n\nList active tokens:\n```bash\nacademia_mcp auth list-tokens\n```\n\nRevoke a token:\n```bash\nacademia_mcp auth revoke-token mcp_a1b2c3d4e5f6...\n```\n\n#### Using Tokens\n\nInclude the token in the `Authorization` header with the `Bearer` scheme or as a query parameter apiKey.\n\n**Security Notes:**\n- Tokens are displayed only once during issuance. Store them securely.\n- Use HTTPS in production to protect tokens in transit.\n- The `tokens.json` file is automatically created with restrictive permissions (mode 600).\n- Tokens are stored in plaintext (standard practice for bearer tokens) - protect the tokens file.\n\n### Claude Desktop config\n```json\n{\n  \"mcpServers\": {\n    \"academia\": {\n      \"command\": \"python3\",\n      \"args\": [\n        \"-m\",\n        \"academia_mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n### Available tools (one-liners)\n- `arxiv_search`: Query arXiv with field-specific queries and filters.\n- `arxiv_download`: Fetch a paper by ID and convert to structured text (HTML/PDF modes).\n- `anthology_search`: Search ACL Anthology with fielded queries and optional date filtering.\n- `hf_datasets_search`: Find Hugging Face datasets with filters and sorting.\n- `s2_get_citations`: List papers citing a given arXiv paper (Semantic Scholar Graph).",
        "start_pos": 1694,
        "end_pos": 3733,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "c533f01be4ff2644"
      },
      {
        "chunk_id": 2,
        "text": "queries and optional date filtering.\n- `hf_datasets_search`: Find Hugging Face datasets with filters and sorting.\n- `s2_get_citations`: List papers citing a given arXiv paper (Semantic Scholar Graph).\n- `s2_get_references`: List papers referenced by a given arXiv paper.\n- `visit_webpage`: Fetch and normalize a web page.\n- `web_search`: Unified search wrapper; available when at least one of Exa/Brave/Tavily keys is set.\n- `exa_web_search`, `brave_web_search`, `tavily_web_search`: Provider-specific search.\n- `get_latex_templates_list`, `get_latex_template`: Enumerate and fetch built-in LaTeX templates.\n- `compile_latex`: Compile LaTeX to PDF in `WORKSPACE_DIR`.\n- `read_pdf`: Extract text per page from a PDF.\n- `download_pdf_paper`, `review_pdf_paper`: Download and optionally review PDFs (requires LLM + workspace).\n- `document_qa`: Answer questions over provided document chunks (requires LLM).\n- `extract_bitflip_info`, `generate_research_proposals`, `score_research_proposals`: Research proposal helpers (requires LLM).\n\nAvailability notes:\n- Set `WORKSPACE_DIR` to enable `compile_latex`, `read_pdf`, `download_pdf_paper`, and `review_pdf_paper`.\n- Set `OPENROUTER_API_KEY` to enable LLM tools (`document_qa`, `review_pdf_paper`, and bitflip tools).\n- Set one or more of `EXA_API_KEY`, `BRAVE_API_KEY`, `TAVILY_API_KEY` to enable `web_search` and provider tools.\n\n### Environment variables\nSet as needed, depending on which tools you use:\n\n- `OPENROUTER_API_KEY`: required for LLM-related tools.\n- `BASE_URL`: override OpenRouter base URL.\n- `DOCUMENT_QA_MODEL_NAME`: override default model for `document_qa`.\n- `BITFLIP_MODEL_NAME`: override default model for bitflip tools.\n- `TAVILY_API_KEY`: enables Tavily in `web_search`.\n- `EXA_API_KEY`: enables Exa in `web_search` and `visit_webpage`.\n- `BRAVE_API_KEY`: enables Brave in `web_search`.\n- `WORKSPACE_DIR`: directory for generated files (PDFs, temp artifacts).\n- `PORT`: HTTP port (default `5056`).\n\nYou can put these in a `.env` file in the project root.",
        "start_pos": 3533,
        "end_pos": 5556,
        "token_count_estimate": 505,
        "source_type": "readme",
        "agent_id": "c533f01be4ff2644"
      },
      {
        "chunk_id": 3,
        "text": ": enables Brave in `web_search`.\n- `WORKSPACE_DIR`: directory for generated files (PDFs, temp artifacts).\n- `PORT`: HTTP port (default `5056`).\n\nYou can put these in a `.env` file in the project root.\n\n### Docker\nBuild the image:\n```bash\ndocker build -t academia_mcp .\n```\n\nRun the server (HTTP):\n```bash\ndocker run --rm -p 5056:5056 \\\n  -e PORT=5056 \\\n  -e OPENROUTER_API_KEY=your_key_here \\\n  -e WORKSPACE_DIR=/workspace \\\n  -v \"$PWD/workdir:/workspace\" \\\n  academia_mcp\n```\n\nOr use existing image: [`phoenix120/academia_mcp`](https://hub.docker.com/repository/docker/phoenix120/academia_mcp)\n\n### Examples\n- [Comprehensive report screencast (YouTube)](https://www.youtube.com/watch?v=4bweqQcN6w8)\n- [Single paper screencast (YouTube)](https://www.youtube.com/watch?v=IAAPMptJ5k8)\n\n### Makefile targets\n- `make install`: install the package in editable mode with uv\n- `make validate`: run black, flake8, and mypy (strict)\n- `make test`: run the test suite with pytest\n- `make publish`: build and publish using uv\n\n### LaTeX/PDF requirements\nOnly needed for LaTeX/PDF tools. Ensure a LaTeX distribution is installed and `pdflatex` is on PATH, as well as `latexmk`. On Debian/Ubuntu:\n```bash\nsudo apt install texlive-latex-base texlive-fonts-recommended texlive-latex-extra texlive-science latexmk\n```",
        "start_pos": 5356,
        "end_pos": 6658,
        "token_count_estimate": 325,
        "source_type": "readme",
        "agent_id": "c533f01be4ff2644"
      }
    ]
  },
  {
    "agent_id": "a2b6ba7392fa6d9e",
    "name": "ai.smithery/ImRonAI-mcp-server-browserbase",
    "source": "mcp",
    "source_url": "https://github.com/ImRonAI/mcp-server-browserbase",
    "description": "Automate cloud browsers to navigate websites, interact with elements, and extract structured data.‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-16T06:05:33.453619Z",
    "indexed_at": "2026-02-18T04:03:43.106457",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Browserbase MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@browserbasehq/mcp-browserbase)](https://smithery.ai/server/@browserbasehq/mcp-browserbase)\n\n![cover](assets/cover-mcp.png)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\nThis server provides cloud browser automation capabilities using [Browserbase](https://www.browserbase.com/) and [Stagehand](https://github.com/browserbase/stagehand). It enables LLMs to interact with web pages, take screenshots, extract information, and perform automated actions with atomic precision.\n\n## Features\n\n| Feature            | Description                                                 |\n| ------------------ | ----------------------------------------------------------- |\n| Browser Automation | Control and orchestrate cloud browsers via Browserbase      |\n| Data Extraction    | Extract structured data from any webpage                    |\n| Web Interaction    | Navigate, click, and fill forms with ease                   |\n| Screenshots        | Capture full-page and element screenshots                   |\n| Model Flexibility  | Supports multiple models (OpenAI, Claude, Gemini, and more) |\n| Vision Support     | Use annotated screenshots for complex DOMs                  |\n| Session Management | Create, manage, and close browser sessions                  |\n| Multi-Session      | Run multiple browser sessions in parallel                   |\n\n## How to Setup\n\n### Quickstarts:\n\n#### Add to Cursor\n\nCopy and Paste this link in your Browser:\n\n```text\ncursor://anysphere.cursor-deeplink/mcp/install?name=browserbase&config=eyJjb21tYW5kIjoibnB4IEBicm93c2VyYmFzZWhxL21jcCIsImVudiI6eyJCUk9XU0VSQkFTRV9BUElfS0VZIjoiIiwiQlJPV1NFUkJBU0VfUFJPSkVDVF9JRCI6IiIsIkdFTUlOSV9BUElfS0VZIjoiIn19\n```\n\nWe currently support 2 transports for our MCP server, STDIO and SHTTP. We recommend you use SHTTP with our remote hosted url to take advantage of the server at full capacity.\n\n## SHTTP:\n\nTo use the Browserbase MCP Server through our remote hosted URL, add the following to your configuration.\n\nGo to [smithery.ai](https://smithery.ai/server/@browserbasehq/mcp-browserbase) and enter your API keys and configuration to get a remote hosted URL.\nWhen using our remote hosted server, we provide the LLM costs for Gemini, the [best performing model](https://www.stagehand.dev/evals) in [Stagehand](https://www.stagehand.dev).\n\n![Smithery Image](assets/smithery.jpg)\n\nIf your client supports SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"url\": \"your-smithery-url.com\"\n    }\n  }\n}\n```\n\nIf your client doesn't support SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"your-smithery-url.com\"]\n    }\n  }\n}\n```\n\n## STDIO:\n\nYou can either use our Server hosted on NPM or run it completely locally by cloning this repo.\n\n> **‚ùóÔ∏è Important:** If you want to use a different model you have to add --modelName to the args and provide that respective key as an arg. More info below.\n\n### To run on NPM (Recommended)\n\nGo into your MCP Config JSON and add the Browserbase Server:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThat's it! Reload your MCP client and Claude will be able to use Browserbase.\n\n### To run 100% local:\n\n```bash\n# Clone the Repo\ngit clone https://github.com/browserbase/mcp-server-browserbase.git\ncd mcp-server-browserbase\n\n# Install the dependencies and build the project\nnpm install && npm run build\n```\n\nThen in your MCP Config JSON run the server. To run locally we can use STDIO or self-host SHTTP.\n\n### STDIO:\n\nTo your MCP Config JSON file add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-server-browserbase/cli.js\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThen reload your MCP client and you should be good to go!\n\n## Configuration\n\nThe Browserbase MCP server accepts the following command-line flags:\n\n| Flag                       | Description                                                                 |\n| -------------------------- | --------------------------------------------------------------------------- |\n| `--proxies`                | Enable Browserbase proxies for the session                                  |\n| `--advancedStealth`        | Enable Browserbase Advanced Stealth (Only for Scale Plan Users)             |\n| `--keepAlive`              | Enable Browserbase Keep Alive Session                                       |\n| `--contextId <contextId>`  | Specify a Browserbase Context ID to use                                     |\n| `--persist`                | Whether to persist the Browserbase context (default: true)                  |\n| `--port <port>`            | Port to listen on for HTTP/SHTTP transport                                  |\n| `--host <host>`            | Host to bind server to (default: localhost, use 0.0.0.0 for all interfaces) |\n| `--cookies [json]`         | JSON array of cookies to inject into the browser                            |\n| `--browserWidth <width>`   | Browser viewport width (default: 1024)                                      |\n| `--browserHeight <height>` | Browser viewport height (default: 768)                                      |\n| `--modelName <model>`      | The model to use for Stagehand (default: google/gemini-2.0-flash)           |\n| `--modelApiKey <key>`      | API key for the custom model provider (required when using custom models)   |\n| `--experimental`           | Enable experimental features (default: false)                               |\n\nThese flags can be passed directly to the CLI or configured in your MCP configuration file.\n\n### NOTE:\n\nCurrently, these flags can only be used with the local server (npx @browserbasehq/mcp-server-browserbase).\n\n## Configuration Examples\n\n### Proxies\n\nHere are our docs on [Proxies](https://docs.browserbase.com/features/proxies).\n\nTo use proxies, set the --proxies flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--proxies\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Stealth\n\nHere are our docs on [Advanced Stealth](https://docs.browserbase.com/features/stealth-mode#advanced-stealth-mode).\n\nTo use advanced stealth, set the --advancedStealth flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--advancedStealth\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Contexts\n\nHere are our docs on [Contexts](https://docs.browserbase.com/features/contexts)\n\nTo use contexts, set the --contextId flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--contextId\",\n        \"<YOUR_CONTEXT_ID>\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Browser Viewport Sizing\n\nThe default viewport sizing for a browser session is 1024 x 768. You can adjust the Browser viewport sizing with browserWidth and browserHeight flags.\n\nHere's how to use it for custom browser sizing. We recommend to stick with 16:9 aspect ratios (ie: 1920 x 1080, 1280 x 720, 1024 x 768)\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--browserHeight 1080\",\n        \"--browserWidth 1920\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Model Configuration\n\nStagehand defaults to using Google's Gemini 2.0 Flash model, but you can configure it to use other models like GPT-4o, Claude, or other providers.\n\n**Important**: When using any custom model (non-default), you must provide your own API key for that model provider using the `--modelApiKey` flag.\n\nHere's how to configure different models:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--modelName\",\n        \"anthropic/claude-3-5-sonnet-latest\",\n        \"--modelApiKey\",\n        \"your-anthropic-api-key\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\"\n      }\n    }\n  }\n}\n```\n\n_Note: The model must be supported in Stagehand. Check out the docs [here](https://docs.stagehand.dev/examples/custom_llms#supported-llms). When using any custom model, you must provide your own API key for that provider._\n\n### Resources\n\nThe server provides access to screenshot resources:\n\n1. **Screenshots** (`screenshot://<screenshot-name>`)\n   - PNG images of captured screenshots\n\n## Key Features\n\n- **AI-Powered Automation**: Natural language commands for web interactions\n- **Multi-Model Support**: Works with OpenAI, Claude, Gemini, and more\n- **Advanced Session Management**: Single and multi-session support for parallel browser automation\n- **Screenshot Capture**: Full-page and element-specific screenshots\n- **Data Extraction**: Intelligent content extraction from web pages\n- **Proxy Support**: Enterprise-grade proxy capabilities\n- **Stealth Mode**: Advanced anti-detection features\n- **Context Persistence**: Maintain authentication and state across sessions\n- **Parallel Workflows**: Run multiple browser sessions simultaneously for complex automation tasks\n\nFor more information about the Model Context Protocol, visit:\n\n- [MCP Documentation](https://modelcontextprotocol.io/docs)\n- [MCP Specification](https://spec.modelcontextprotocol.io/)\n\nFor the official MCP Docs:\n\n- [Browserbase MCP](https://docs.browserbase.com/integrations/mcp/introduction)\n\n## License\n\nLicensed under the Apache 2.0 License.\n\nCopyright 2025 Browserbase, Inc.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Control and orchestrate cloud browsers via Browserbase",
        "Extract structured data from any webpage",
        "Navigate, click, and fill forms on web pages",
        "Capture full-page and element-specific screenshots",
        "Support multiple LLM models including OpenAI, Claude, Gemini, and custom models",
        "Use annotated screenshots for complex DOM interactions",
        "Create, manage, and close browser sessions with session persistence",
        "Run multiple browser sessions in parallel for complex workflows",
        "Enable proxy support and advanced stealth mode for anti-detection",
        "Maintain authentication and state across browser sessions"
      ],
      "limitations": [
        "Custom models require providing your own API key",
        "Command-line flags are only supported with the local server (npx @browserbasehq/mcp-server-browserbase)",
        "Advanced stealth mode is only available for Scale Plan users",
        "Model support is limited to those supported by Stagehand",
        "SHTTP transport requires client support; fallback to STDIO if unsupported"
      ],
      "requirements": [
        "API keys for Browserbase (BROWSERBASE_API_KEY) and Browserbase project (BROWSERBASE_PROJECT_ID)",
        "API key for Gemini model or other custom LLM providers (GEMINI_API_KEY or --modelApiKey)",
        "Node.js environment to run the server locally",
        "MCP client supporting either STDIO or SHTTP transport",
        "For custom models, valid API keys for those model providers",
        "Internet access for remote hosted server usage"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed configuration options, supported features, limitations, and requirements, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Browserbase MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@browserbasehq/mcp-browserbase)](https://smithery.ai/server/@browserbasehq/mcp-browserbase)\n\n![cover](assets/cover-mcp.png)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\nThis server provides cloud browser automation capabilities using [Browserbase](https://www.browserbase.com/) and [Stagehand](https://github.com/browserbase/stagehand). It enables LLMs to interact with web pages, take screenshots, extract information, and perform automated actions with atomic precision.",
        "start_pos": 0,
        "end_pos": 877,
        "token_count_estimate": 219,
        "source_type": "readme",
        "agent_id": "a2b6ba7392fa6d9e"
      },
      {
        "chunk_id": 1,
        "text": "cursor://anysphere.cursor-deeplink/mcp/install?name=browserbase&config=eyJjb21tYW5kIjoibnB4IEBicm93c2VyYmFzZWhxL21jcCIsImVudiI6eyJCUk9XU0VSQkFTRV9BUElfS0VZIjoiIiwiQlJPV1NFUkJBU0VfUFJPSkVDVF9JRCI6IiIsIkdFTUlOSV9BUElfS0VZIjoiIn19\n```\n\nWe currently support 2 transports for our MCP server, STDIO and SHTTP. We recommend you use SHTTP with our remote hosted url to take advantage of the server at full capacity.\n\n## SHTTP:\n\nTo use the Browserbase MCP Server through our remote hosted URL, add the following to your configuration.\n\nGo to [smithery.ai](https://smithery.ai/server/@browserbasehq/mcp-browserbase) and enter your API keys and configuration to get a remote hosted URL.\nWhen using our remote hosted server, we provide the LLM costs for Gemini, the [best performing model](https://www.stagehand.dev/evals) in [Stagehand](https://www.stagehand.dev).\n\n![Smithery Image](assets/smithery.jpg)\n\nIf your client supports SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"url\": \"your-smithery-url.com\"\n    }\n  }\n}\n```\n\nIf your client doesn't support SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"your-smithery-url.com\"]\n    }\n  }\n}\n```\n\n## STDIO:\n\nYou can either use our Server hosted on NPM or run it completely locally by cloning this repo.\n\n> **‚ùóÔ∏è Important:** If you want to use a different model you have to add --modelName to the args and provide that respective key as an arg. More info below.\n\n### To run on NPM (Recommended)\n\nGo into your MCP Config JSON and add the Browserbase Server:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThat's it! Reload your MCP client and Claude will be able to use Browserbase.",
        "start_pos": 1848,
        "end_pos": 3771,
        "token_count_estimate": 480,
        "source_type": "readme",
        "agent_id": "a2b6ba7392fa6d9e"
      },
      {
        "chunk_id": 2,
        "text": "\"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThat's it! Reload your MCP client and Claude will be able to use Browserbase.\n\n### To run 100% local:\n\n```bash\n# Clone the Repo\ngit clone https://github.com/browserbase/mcp-server-browserbase.git\ncd mcp-server-browserbase\n\n# Install the dependencies and build the project\nnpm install && npm run build\n```\n\nThen in your MCP Config JSON run the server. To run locally we can use STDIO or self-host SHTTP.\n\n### STDIO:\n\nTo your MCP Config JSON file add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-server-browserbase/cli.js\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThen reload your MCP client and you should be good to go!",
        "start_pos": 3571,
        "end_pos": 4490,
        "token_count_estimate": 228,
        "source_type": "readme",
        "agent_id": "a2b6ba7392fa6d9e"
      },
      {
        "chunk_id": 3,
        "text": "|\n| `--host <host>`            | Host to bind server to (default: localhost, use 0.0.0.0 for all interfaces) |\n| `--cookies [json]`         | JSON array of cookies to inject into the browser                            |\n| `--browserWidth <width>`   | Browser viewport width (default: 1024)                                      |\n| `--browserHeight <height>` | Browser viewport height (default: 768)                                      |\n| `--modelName <model>`      | The model to use for Stagehand (default: google/gemini-2.0-flash)           |\n| `--modelApiKey <key>`      | API key for the custom model provider (required when using custom models)   |\n| `--experimental`           | Enable experimental features (default: false)                               |\n\nThese flags can be passed directly to the CLI or configured in your MCP configuration file.\n\n### NOTE:\n\nCurrently, these flags can only be used with the local server (npx @browserbasehq/mcp-server-browserbase).\n\n## Configuration Examples\n\n### Proxies\n\nHere are our docs on [Proxies](https://docs.browserbase.com/features/proxies).\n\nTo use proxies, set the --proxies flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--proxies\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Stealth\n\nHere are our docs on [Advanced Stealth](https://docs.browserbase.com/features/stealth-mode#advanced-stealth-mode).",
        "start_pos": 5419,
        "end_pos": 7029,
        "token_count_estimate": 394,
        "source_type": "readme",
        "agent_id": "a2b6ba7392fa6d9e"
      },
      {
        "chunk_id": 4,
        "text": "nv\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Contexts\n\nHere are our docs on [Contexts](https://docs.browserbase.com/features/contexts)\n\nTo use contexts, set the --contextId flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--contextId\",\n        \"<YOUR_CONTEXT_ID>\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Browser Viewport Sizing\n\nThe default viewport sizing for a browser session is 1024 x 768. You can adjust the Browser viewport sizing with browserWidth and browserHeight flags.\n\nHere's how to use it for custom browser sizing. We recommend to stick with 16:9 aspect ratios (ie: 1920 x 1080, 1280 x 720, 1024 x 768)\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--browserHeight 1080\",\n        \"--browserWidth 1920\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Model Configuration\n\nStagehand defaults to using Google's Gemini 2.0 Flash model, but you can configure it to use other models like GPT-4o, Claude, or other providers.\n\n**Important**: When using any custom model (non-default), you must provide your own API key for that model provider using the `--modelApiKey` flag.",
        "start_pos": 7267,
        "end_pos": 8888,
        "token_count_estimate": 405,
        "source_type": "readme",
        "agent_id": "a2b6ba7392fa6d9e"
      },
      {
        "chunk_id": 5,
        "text": "aude-3-5-sonnet-latest\",\n        \"--modelApiKey\",\n        \"your-anthropic-api-key\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\"\n      }\n    }\n  }\n}\n```\n\n_Note: The model must be supported in Stagehand. Check out the docs [here](https://docs.stagehand.dev/examples/custom_llms#supported-llms). When using any custom model, you must provide your own API key for that provider._\n\n### Resources\n\nThe server provides access to screenshot resources:\n\n1. **Screenshots** (`screenshot://<screenshot-name>`)\n   - PNG images of captured screenshots\n\n## Key Features\n\n- **AI-Powered Automation**: Natural language commands for web interactions\n- **Multi-Model Support**: Works with OpenAI, Claude, Gemini, and more\n- **Advanced Session Management**: Single and multi-session support for parallel browser automation\n- **Screenshot Capture**: Full-page and element-specific screenshots\n- **Data Extraction**: Intelligent content extraction from web pages\n- **Proxy Support**: Enterprise-grade proxy capabilities\n- **Stealth Mode**: Advanced anti-detection features\n- **Context Persistence**: Maintain authentication and state across sessions\n- **Parallel Workflows**: Run multiple browser sessions simultaneously for complex automation tasks\n\nFor more information about the Model Context Protocol, visit:\n\n- [MCP Documentation](https://modelcontextprotocol.io/docs)\n- [MCP Specification](https://spec.modelcontextprotocol.io/)\n\nFor the official MCP Docs:\n\n- [Browserbase MCP](https://docs.browserbase.com/integrations/mcp/introduction)\n\n## License\n\nLicensed under the Apache 2.0 License.\n\nCopyright 2025 Browserbase, Inc.",
        "start_pos": 9115,
        "end_pos": 10775,
        "token_count_estimate": 414,
        "source_type": "readme",
        "agent_id": "a2b6ba7392fa6d9e"
      }
    ]
  },
  {
    "agent_id": "fc7cb935dbf3d6c0",
    "name": "ai.smithery/IndianAppGuy-magicslide-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@IndianAppGuy/magicslide-mcp/mcp",
    "description": "Generate professional PowerPoint presentations from text, YouTube videos, or structured JSON data.‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-08T05:29:39.856257Z",
    "indexed_at": "2026-02-18T04:03:45.553796",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Generate professional PowerPoint presentations from text",
        "Generate professional PowerPoint presentations from YouTube videos",
        "Generate professional PowerPoint presentations from structured JSON data"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of core functionality but lacks detailed examples, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "3163f51c7896961e",
    "name": "ai.smithery/IndianAppGuy-magicslide-mcp-actual-test",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@IndianAppGuy/magicslide-mcp-actual-test/mcp",
    "description": "Generate polished PowerPoint presentations from text prompts, YouTube videos, or structured outlin‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-07T12:06:34.023568Z",
    "indexed_at": "2026-02-18T04:03:47.001237",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Generate polished PowerPoint presentations from text prompts",
        "Generate polished PowerPoint presentations from YouTube videos",
        "Generate polished PowerPoint presentations from structured outlines"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of input types for generating presentations but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "d1cab4f018511278",
    "name": "ai.smithery/JMoak-chrono-mcp",
    "source": "mcp",
    "source_url": "https://github.com/JMoak/chrono-mcp",
    "description": "Convert and compare dates and times across any timezone with flexible, locale-aware formatting. Ad‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-17T02:23:35.312972Z",
    "indexed_at": "2026-02-18T04:03:48.790340",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# chrono-mcp\n\nA comprehensive Model Context Protocol (MCP) server providing advanced date, time, timezone, and calendar operations powered by Luxon. Perfect for AI agents and applications that need robust temporal data handling.\n\n[![NPM Version](https://img.shields.io/npm/v/@jmoak/chrono-mcp)](https://www.npmjs.com/package/@jmoak/chrono-mcp)\n[![Downloads](https://img.shields.io/npm/dm/@jmoak/chrono-mcp)](https://www.npmjs.com/package/@jmoak/chrono-mcp)\n[![Node Version](https://img.shields.io/node/v/@jmoak/chrono-mcp)](https://nodejs.org/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.9-blue)](https://www.typescriptlang.org/)\n[![Code Style: Biome](https://img.shields.io/badge/Code%20Style-Biome-60a5fa)](https://biomejs.dev/)\n[![Tests: Vitest](https://img.shields.io/badge/Tests-Vitest-6E9F18)](https://vitest.dev/)\n[![MCP Server](https://img.shields.io/badge/MCP-Server-0b7285)](https://modelcontextprotocol.io/)\n[![Powered by Luxon](https://img.shields.io/badge/Powered%20by-Luxon-0a3d62)](https://github.com/moment/luxon)\n[![smithery badge](https://smithery.ai/badge/@JMoak/chrono-mcp)](https://smithery.ai/server/@JMoak/chrono-mcp)\n\n## Quick Start\n\n```bash\nnpx @jmoak/chrono-mcp\n```\n\n### Run as local HTTP server\n\n```bash\nnpm install\nnpm run build\nnpm run start:http\n# Server listens on http://localhost:8000/mcp (health check at /health)\n```\n## MCP Client Configuration\n\nConfigure your MCP client to launch `chrono-mcp` via `npx`. Below are client-specific examples.\n\n### Claude Code\n\nAsk Claude! Here's the configuration:\n\n```jsonc\n{\n  \"mcpServers\": {\n    \"chrono-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@jmoak/chrono-mcp@latest\"]\n    }\n  }\n}\n```\n\n### Cursor\n\nReference: [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\n```jsonc\n{\n  \"mcpServers\": {\n    \"chrono-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@jmoak/chrono-mcp@latest\"]\n    },\n    \"chrono-mcp-http\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n\n## Features\n\n- **Global Timezone Support** - Work with all IANA timezone identifiers\n- **Time Calculations** - Add/subtract durations, calculate differences between dates\n- **Multiple Formats** - ISO, RFC2822, SQL, locale-aware, and custom formatting\n- **Type Safety** - Zod validation with comprehensive error handling\n- **Real-time** - Current time retrieval with microsecond precision\n- **Easy Integration** - Standard MCP protocol for seamless AI agent integration\n- **Token-Optimized Output** - Dynamically shaped responses that maximize information density while minimizing token usage for efficient AI interactions\n\n## Available Tools\n\n### GET TIME\n\nGet current time or convert times across timezones with flexible formatting.\n\n**Parameters:**\n- `datetime` (string, optional): ISO datetime string. Defaults to current time\n- `timezones` (array, optional): List of timezone names for conversions\n- `formats` (array, optional): Output formats (`iso`, `rfc2822`, `sql`, `local`, `localeString`, `short`, `medium`, `long`, `full`)\n- `locale` (string, optional): Locale for formatting (e.g., `en-US`, `fr-FR`, `ja-JP`)\n- `includeOffsets` (boolean, optional): Include UTC offsets in output\n\n**Example:**\n\nInput\n```json\n{\n  \"datetime\": \"2024-01-01T12:00:00Z\",\n  \"timezones\": [\"America/New_York\", \"Asia/Tokyo\"],\n  \"includeOffsets\": true\n}\n```\n\nOutput\n```json\n{\n  \"baseTime\": \"2024-01-01T12:00:00.000Z\",\n  \"America/New_York\": \"2024-01-01T07:00:00.000-05:00\",\n  \"Asia/Tokyo\": \"2024-01-01T21:00:00.000+09:00\"\n}\n```\n\n### TIME CALCULATOR\n\nPerform time arithmetic operations including duration calculations and date math.\n\n**Operations:**\n- `add` - Add duration to a datetime\n- `subtract` - Subtract duration from a datetime\n- `diff` - Calculate simple difference in various units\n- `duration_between` - Detailed duration breakdown between two times\n- `stats` - Statistical analysis of time series and durations\n- `sort` - Sort timestamps chronologically\n\n**Parameters:**\n- `operation` (required): Type of calculation\n- `interaction_mode` (optional): `auto_detect` | `single_to_many` | `many_to_single` | `pairwise` | `cross_product` | `aggregate`. Defaults to `auto_detect`.\n- `base_time` (optional): Base ISO datetime(s). String or array. Defaults to current time.\n- `compare_time` (optional): Compare ISO datetime(s) for `diff`/`duration_between`. String or array.\n- `timezone` (optional): Timezone for `base_time`\n- `compare_time_timezone` (optional): Timezone for `compare_time`\n- `years`, `months`, `days`, `hours`, `minutes`, `seconds` (optional): Duration values\n\n**Example:**\n\nInput\n```json\n{\n  \"operation\": \"add\",\n  \"base_time\": \"2024-12-25T10:00:00Z\",\n  \"days\": 5,\n  \"hours\": 3\n}\n```\n\nOutput\n```json\n{\n  \"operation\": \"add\",\n  \"interaction_mode\": \"single_to_single\",\n  \"input\": {\n    \"base_time\": \"2024-12-25T10:00:00.000Z\",\n    \"duration\": { \"days\": 5, \"hours\": 3 }\n  },\n  \"result\": \"2024-12-30T13:00:00.000Z\",\n  \"result_timezone\": \"UTC\"\n}\n```\n\n<details>\n<summary><strong>Development</strong></summary>\n\n### Prerequisites\n- Node.js >= 22.0.0\n- npm or yarn\n\n### Setup\n```bash\ngit clone https://github.com/yourusername/chrono-mcp.git\ncd chrono-mcp\nnpm install\n```\n\n### Build\n```bash\nnpm run build\n```\n\n### Testing & Inspector\n```bash\nnpm test\nnpm run test:ui\nnpm run inspector\n```\nVisit `http://localhost:6274` for the web inspector UI.\n\n### Linting\n```bash\nnpm run lint\nnpm run lint:fix\n```\n\n</details>\n\n## Supported Timezones\n\nSupports all IANA timezone identifiers including:\n\n- **Americas**: `America/New_York`, `America/Los_Angeles`, `America/Toronto`, etc.\n- **Europe**: `Europe/London`, `Europe/Paris`, `Europe/Berlin`, etc.\n- **Asia**: `Asia/Tokyo`, `Asia/Shanghai`, `Asia/Dubai`, etc.\n- **Australia**: `Australia/Sydney`, `Australia/Melbourne`, etc.\n- **And 400+ more...**\n\n## Acknowledgments\n\nThis project is powered by [Luxon](https://github.com/moment/luxon), the excellent DateTime library that provides robust timezone handling and date arithmetic. We're grateful to the Luxon team for creating such a reliable foundation for temporal operations.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Releases\n\nSee [GitHub Releases](https://github.com/JMoak/chrono-mcp/releases) for detailed changes."
    },
    "llm_extracted": {
      "capabilities": [
        "Provide current time retrieval with microsecond precision",
        "Convert times across all IANA timezones with flexible formatting",
        "Perform time arithmetic operations including add, subtract, and difference calculations",
        "Calculate detailed duration breakdowns between two times",
        "Analyze time series and durations with statistical operations",
        "Sort timestamps chronologically",
        "Support multiple output formats including ISO, RFC2822, SQL, and locale-aware formats",
        "Validate inputs with type safety using Zod and provide comprehensive error handling",
        "Integrate seamlessly with AI agents using the standard MCP protocol",
        "Optimize output to maximize information density while minimizing token usage"
      ],
      "limitations": [],
      "requirements": [
        "Node.js version 22.0.0 or higher",
        "npm or yarn package manager",
        "No API keys or authentication tokens required"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for multiple tools, clear feature descriptions, client configuration examples, and environment requirements, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# chrono-mcp\n\nA comprehensive Model Context Protocol (MCP) server providing advanced date, time, timezone, and calendar operations powered by Luxon. Perfect for AI agents and applications that need robust temporal data handling.\n\n[![NPM Version](https://img.shields.io/npm/v/@jmoak/chrono-mcp)](https://www.npmjs.com/package/@jmoak/chrono-mcp)\n[![Downloads](https://img.shields.io/npm/dm/@jmoak/chrono-mcp)](https://www.npmjs.com/package/@jmoak/chrono-mcp)\n[![Node Version](https://img.shields.io/node/v/@jmoak/chrono-mcp)](https://nodejs.org/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.9-blue)](https://www.typescriptlang.org/)\n[![Code Style: Biome](https://img.shields.io/badge/Code%20Style-Biome-60a5fa)](https://biomejs.dev/)\n[![Tests: Vitest](https://img.shields.io/badge/Tests-Vitest-6E9F18)](https://vitest.dev/)\n[![MCP Server](https://img.shields.io/badge/MCP-Server-0b7285)](https://modelcontextprotocol.io/)\n[![Powered by Luxon](https://img.shields.io/badge/Powered%20by-Luxon-0a3d62)](https://github.com/moment/luxon)\n[![smithery badge](https://smithery.ai/badge/@JMoak/chrono-mcp)](https://smithery.ai/server/@JMoak/chrono-mcp)\n\n## Quick Start\n\n```bash\nnpx @jmoak/chrono-mcp\n```\n\n### Run as local HTTP server\n\n```bash\nnpm install\nnpm run build\nnpm run start:http\n# Server listens on http://localhost:8000/mcp (health check at /health)\n```\n## MCP Client Configuration\n\nConfigure your MCP client to launch `chrono-mcp` via `npx`. Below are client-specific examples.",
        "start_pos": 0,
        "end_pos": 1598,
        "token_count_estimate": 399,
        "source_type": "readme",
        "agent_id": "d1cab4f018511278"
      },
      {
        "chunk_id": 1,
        "text": ".cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\n```jsonc\n{\n  \"mcpServers\": {\n    \"chrono-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@jmoak/chrono-mcp@latest\"]\n    },\n    \"chrono-mcp-http\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n\n## Features\n\n- **Global Timezone Support** - Work with all IANA timezone identifiers\n- **Time Calculations** - Add/subtract durations, calculate differences between dates\n- **Multiple Formats** - ISO, RFC2822, SQL, locale-aware, and custom formatting\n- **Type Safety** - Zod validation with comprehensive error handling\n- **Real-time** - Current time retrieval with microsecond precision\n- **Easy Integration** - Standard MCP protocol for seamless AI agent integration\n- **Token-Optimized Output** - Dynamically shaped responses that maximize information density while minimizing token usage for efficient AI interactions\n\n## Available Tools\n\n### GET TIME\n\nGet current time or convert times across timezones with flexible formatting.\n\n**Parameters:**\n- `datetime` (string, optional): ISO datetime string. Defaults to current time\n- `timezones` (array, optional): List of timezone names for conversions\n- `formats` (array, optional): Output formats (`iso`, `rfc2822`, `sql`, `local`, `localeString`, `short`, `medium`, `long`, `full`)\n- `locale` (string, optional): Locale for formatting (e.g., `en-US`, `fr-FR`, `ja-JP`)\n- `includeOffsets` (boolean, optional): Include UTC offsets in output\n\n**Example:**\n\nInput\n```json\n{\n  \"datetime\": \"2024-01-01T12:00:00Z\",\n  \"timezones\": [\"America/New_York\", \"Asia/Tokyo\"],\n  \"includeOffsets\": true\n}\n```\n\nOutput\n```json\n{\n  \"baseTime\": \"2024-01-01T12:00:00.000Z\",\n  \"America/New_York\": \"2024-01-01T07:00:00.000-05:00\",\n  \"Asia/Tokyo\": \"2024-01-01T21:00:00.000+09:00\"\n}\n```\n\n### TIME CALCULATOR\n\nPerform time arithmetic operations including duration calculations and date math.",
        "start_pos": 1848,
        "end_pos": 3766,
        "token_count_estimate": 479,
        "source_type": "readme",
        "agent_id": "d1cab4f018511278"
      },
      {
        "chunk_id": 2,
        "text": "ew_York\": \"2024-01-01T07:00:00.000-05:00\",\n  \"Asia/Tokyo\": \"2024-01-01T21:00:00.000+09:00\"\n}\n```\n\n### TIME CALCULATOR\n\nPerform time arithmetic operations including duration calculations and date math.\n\n**Operations:**\n- `add` - Add duration to a datetime\n- `subtract` - Subtract duration from a datetime\n- `diff` - Calculate simple difference in various units\n- `duration_between` - Detailed duration breakdown between two times\n- `stats` - Statistical analysis of time series and durations\n- `sort` - Sort timestamps chronologically\n\n**Parameters:**\n- `operation` (required): Type of calculation\n- `interaction_mode` (optional): `auto_detect` | `single_to_many` | `many_to_single` | `pairwise` | `cross_product` | `aggregate`. Defaults to `auto_detect`.\n- `base_time` (optional): Base ISO datetime(s). String or array. Defaults to current time.\n- `compare_time` (optional): Compare ISO datetime(s) for `diff`/`duration_between`. String or array.\n- `timezone` (optional): Timezone for `base_time`\n- `compare_time_timezone` (optional): Timezone for `compare_time`\n- `years`, `months`, `days`, `hours`, `minutes`, `seconds` (optional): Duration values\n\n**Example:**\n\nInput\n```json\n{\n  \"operation\": \"add\",\n  \"base_time\": \"2024-12-25T10:00:00Z\",\n  \"days\": 5,\n  \"hours\": 3\n}\n```\n\nOutput\n```json\n{\n  \"operation\": \"add\",\n  \"interaction_mode\": \"single_to_single\",\n  \"input\": {\n    \"base_time\": \"2024-12-25T10:00:00.000Z\",\n    \"duration\": { \"days\": 5, \"hours\": 3 }\n  },\n  \"result\": \"2024-12-30T13:00:00.000Z\",\n  \"result_timezone\": \"UTC\"\n}\n```\n\n<details>\n<summary><strong>Development</strong></summary>\n\n### Prerequisites\n- Node.js >= 22.0.0\n- npm or yarn\n\n### Setup\n```bash\ngit clone https://github.com/yourusername/chrono-mcp.git\ncd chrono-mcp\nnpm install\n```\n\n### Build\n```bash\nnpm run build\n```\n\n### Testing & Inspector\n```bash\nnpm test\nnpm run test:ui\nnpm run inspector\n```\nVisit `http://localhost:6274` for the web inspector UI.",
        "start_pos": 3566,
        "end_pos": 5490,
        "token_count_estimate": 481,
        "source_type": "readme",
        "agent_id": "d1cab4f018511278"
      },
      {
        "chunk_id": 3,
        "text": "chrono-mcp\nnpm install\n```\n\n### Build\n```bash\nnpm run build\n```\n\n### Testing & Inspector\n```bash\nnpm test\nnpm run test:ui\nnpm run inspector\n```\nVisit `http://localhost:6274` for the web inspector UI.\n\n### Linting\n```bash\nnpm run lint\nnpm run lint:fix\n```\n\n</details>\n\n## Supported Timezones\n\nSupports all IANA timezone identifiers including:\n\n- **Americas**: `America/New_York`, `America/Los_Angeles`, `America/Toronto`, etc.\n- **Europe**: `Europe/London`, `Europe/Paris`, `Europe/Berlin`, etc.\n- **Asia**: `Asia/Tokyo`, `Asia/Shanghai`, `Asia/Dubai`, etc.\n- **Australia**: `Australia/Sydney`, `Australia/Melbourne`, etc.\n- **And 400+ more...**\n\n## Acknowledgments\n\nThis project is powered by [Luxon](https://github.com/moment/luxon), the excellent DateTime library that provides robust timezone handling and date arithmetic. We're grateful to the Luxon team for creating such a reliable foundation for temporal operations.\n\n## License\n\nMIT License - see the [LICENSE](LICENSE) file for details.\n\n## Releases\n\nSee [GitHub Releases](https://github.com/JMoak/chrono-mcp/releases) for detailed changes.",
        "start_pos": 5290,
        "end_pos": 6390,
        "token_count_estimate": 274,
        "source_type": "readme",
        "agent_id": "d1cab4f018511278"
      }
    ]
  },
  {
    "agent_id": "a02b2a5cb947bff4",
    "name": "ai.smithery/JunoJunHyun-festival-finder-mcp",
    "source": "mcp",
    "source_url": "https://github.com/JunoJunHyun/Festival-Finder-mcp",
    "description": "Discover festivals worldwide by location, date, and genre. Compare options with key details like d‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-05T15:14:35.748005Z",
    "indexed_at": "2026-02-18T04:03:50.216824",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Festival-Finder-mcp\n\n[![smithery badge](https://smithery.ai/badge/@JunoJunHyun/festival-finder-mcp)](https://smithery.ai/server/@JunoJunHyun/festival-finder-mcp)\n\n# Festival Finder (ÌéòÏä§Ìã∞Î≤å ÌååÏù∏Îçî)\n\nÎ≤îÏö© AI ÏóêÏù¥Ï†ÑÌä∏ 'ÌéòÏä§Ìã∞Î≤å ÌååÏù∏Îçî'ÏûÖÎãàÎã§. ÌïµÏã¨ Î°úÏßÅÍ≥º ÌîåÎû´ÌèºÎ≥Ñ Ïñ¥ÎåëÌÑ∞Î•º Î∂ÑÎ¶¨ÌïòÏó¨ Ïπ¥Ïπ¥Ïò§ÌÜ°, ÏùºÎ∞ò ÏõπÏÇ¨Ïù¥Ìä∏ Îì± Îã§ÏñëÌïú ÌîåÎû´ÌèºÏóê Ïó∞Í≤∞Ìï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏóàÏäµÎãàÎã§. KOPIS(Í≥µÏó∞ÏòàÏà†ÌÜµÌï©Ï†ÑÏÇ∞Îßù) APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ï†ÑÍµ≠Ïùò Í≥µÏó∞ Î∞è Ï∂ïÏ†ú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§.\n\n---\n\n## üèóÔ∏è ÏïÑÌÇ§ÌÖçÏ≤ò\n\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî **'ÌïµÏã¨ ÏóîÏßÑ(Core Engine)'**Í≥º **'Ïó∞Í≤∞ Ïñ¥ÎåëÌÑ∞(Connection Adapters)'**ÎùºÎäî Îëê Í∞ÄÏßÄ Ï£ºÏöî Î∂ÄÎ∂ÑÏúºÎ°ú Íµ¨ÏÑ±Îêú ÌôïÏû• Í∞ÄÎä•Ìïú Íµ¨Ï°∞Î•º Îî∞Î¶ÖÎãàÎã§.\n\n### üß† ÌïµÏã¨ ÏóîÏßÑ: `core_logic.py`\n-   ÌîåÎû´ÌèºÏóê ÎèÖÎ¶ΩÏ†ÅÏù∏ ÏàúÏàò ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅÏûÖÎãàÎã§.\n-   KOPIS APIÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò§Í≥†, Í∞ÄÍ≥µÎêòÏßÄ ÏïäÏùÄ ÏàúÏàòÌïú ÌååÏù¥Ïç¨ Îç∞Ïù¥ÌÑ∞ ÌòïÌÉúÎ°ú Î∞òÌôòÌïòÎäî Ïó≠Ìï†Îßå Îã¥ÎãπÌï©ÎãàÎã§.\n-   Ïñ¥Îñ§ ÌîåÎû´ÌèºÏóê Ïó∞Í≤∞ÎêòÎäîÏßÄ Ï†ÑÌòÄ ÏïåÏßÄ Î™ªÌïòÎ©∞, Ïñ¥ÎîîÏÑúÎì† Ïû¨ÏÇ¨Ïö©Ïù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\n\n### üìû Ïó∞Í≤∞ Ïñ¥ÎåëÌÑ∞\n-   ÌïµÏã¨ ÏóîÏßÑÏùÑ Îã§ÏñëÌïú Ïô∏Î∂Ä ÌîåÎû´ÌèºÏóê Ïó∞Í≤∞ÌïòÎäî 'ÌîåÎü¨Í∑∏' Ïó≠Ìï†ÏùÑ Ìï©ÎãàÎã§.\n-   Í∞Å ÌîåÎû´ÌèºÏùò Í≥†Ïú†Ìïú ÏöîÏ≤≠/ÏùëÎãµ ÌòïÏãùÏùÑ Ï≤òÎ¶¨ÌïòÍ≥†, ÌïµÏã¨ ÏóîÏßÑÍ≥º ÌÜµÏã†Ìï©ÎãàÎã§.\n\n-   **`kakao_server.py`**: **Ïπ¥Ïπ¥Ïò§ PlayMCP**Ïö© Ïñ¥ÎåëÌÑ∞ ÏÑúÎ≤ÑÏûÖÎãàÎã§.\n-   **`web_server.py`**: **ÏùºÎ∞ò ÏõπÏÇ¨Ïù¥Ìä∏ÎÇò Ïï±**ÏùÑ ÏúÑÌïú Î≤îÏö© API Ïñ¥ÎåëÌÑ∞ ÏÑúÎ≤ÑÏûÖÎãàÎã§. \n\n---\n\n## üõ†Ô∏è ÏãúÏûëÌïòÍ∏∞\n\n1.  **ÌïÑÏöî ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n### Installing via Smithery\n\nTo install festival-finder-mcp automatically via [Smithery](https://smithery.ai/server/@JunoJunHyun/festival-finder-mcp):\n\n```bash\nnpx -y @smithery/cli install @JunoJunHyun/festival-finder-mcp\n```\n\n2.  **KOPIS API ÌÇ§ ÏÑ§Ï†ï**\n    `core_logic.py` ÌååÏùºÏùÑ Ïó¥Ïñ¥ `KOPIS_API_KEY` Î≥ÄÏàòÏóê Î∞úÍ∏âÎ∞õÏùÄ Î≥∏Ïù∏Ïùò API ÌÇ§Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.\n\n3.  **Ïñ¥ÎåëÌÑ∞ ÏÑúÎ≤Ñ Ïã§Ìñâ**\n    ÌïÑÏöîÌïú Ïñ¥ÎåëÌÑ∞ ÏÑúÎ≤ÑÎ•º ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú Ïã§ÌñâÌï©ÎãàÎã§.\n\n    -   **Ïπ¥Ïπ¥Ïò§ÌÜ° Ïó∞Îèô ÌÖåÏä§Ìä∏ Ïãú:**\n        ```bash\n        python kakao_server.py\n        ```\n        - ngrokÏùÑ `http://localhost:5000`Ïóê Ïó∞Í≤∞ÌïòÍ≥†, PlayMCP EndpointÏóê `[ngrok Ï£ºÏÜå]/kakao`Î•º Îì±Î°ùÌï©ÎãàÎã§.\n\n    -   **ÏùºÎ∞ò Ïõπ API ÌÖåÏä§Ìä∏ Ïãú:**\n        ```bash\n        python web_server.py\n        ```\n        - Ïõπ Î∏åÎùºÏö∞Ï†ÄÎÇò Îã§Î•∏ ÌîÑÎ°úÍ∑∏Îû®ÏóêÏÑú `http://localhost:5001/api/performances?stdate=...` ÌòïÏãùÏúºÎ°ú ÏöîÏ≤≠ÌïòÏó¨ ÌÖåÏä§Ìä∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide nationwide performance and festival information using KOPIS API",
        "Separate core business logic from platform-specific adapters",
        "Serve as a backend for Kakao PlayMCP platform",
        "Serve as a backend for general web or app API requests",
        "Fetch and return raw performance data in Python data structures",
        "Allow easy integration with multiple platforms via connection adapters"
      ],
      "limitations": [
        "Requires user to manually input KOPIS API key in core_logic.py",
        "No direct frontend or UI provided, only backend server components",
        "Platform adapters limited to Kakao PlayMCP and generic web API",
        "Dependent on availability and limits of KOPIS API"
      ],
      "requirements": [
        "Python environment with dependencies installed from requirements.txt",
        "Valid KOPIS API key set in core_logic.py",
        "Ngrok or similar tunneling tool for Kakao PlayMCP testing",
        "Permissions to run Python servers on localhost ports 5000 and 5001"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, architecture overview, usage examples for multiple adapters, and explicit requirements and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Festival-Finder-mcp\n\n[![smithery badge](https://smithery.ai/badge/@JunoJunHyun/festival-finder-mcp)](https://smithery.ai/server/@JunoJunHyun/festival-finder-mcp)\n\n# Festival Finder (ÌéòÏä§Ìã∞Î≤å ÌååÏù∏Îçî)\n\nÎ≤îÏö© AI ÏóêÏù¥Ï†ÑÌä∏ 'ÌéòÏä§Ìã∞Î≤å ÌååÏù∏Îçî'ÏûÖÎãàÎã§. ÌïµÏã¨ Î°úÏßÅÍ≥º ÌîåÎû´ÌèºÎ≥Ñ Ïñ¥ÎåëÌÑ∞Î•º Î∂ÑÎ¶¨ÌïòÏó¨ Ïπ¥Ïπ¥Ïò§ÌÜ°, ÏùºÎ∞ò ÏõπÏÇ¨Ïù¥Ìä∏ Îì± Îã§ÏñëÌïú ÌîåÎû´ÌèºÏóê Ïó∞Í≤∞Ìï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏóàÏäµÎãàÎã§. KOPIS(Í≥µÏó∞ÏòàÏà†ÌÜµÌï©Ï†ÑÏÇ∞Îßù) APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ï†ÑÍµ≠Ïùò Í≥µÏó∞ Î∞è Ï∂ïÏ†ú Ï†ïÎ≥¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§.\n\n---\n\n## üèóÔ∏è ÏïÑÌÇ§ÌÖçÏ≤ò\n\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî **'ÌïµÏã¨ ÏóîÏßÑ(Core Engine)'**Í≥º **'Ïó∞Í≤∞ Ïñ¥ÎåëÌÑ∞(Connection Adapters)'**ÎùºÎäî Îëê Í∞ÄÏßÄ Ï£ºÏöî Î∂ÄÎ∂ÑÏúºÎ°ú Íµ¨ÏÑ±Îêú ÌôïÏû• Í∞ÄÎä•Ìïú Íµ¨Ï°∞Î•º Îî∞Î¶ÖÎãàÎã§.\n\n### üß† ÌïµÏã¨ ÏóîÏßÑ: `core_logic.py`\n-   ÌîåÎû´ÌèºÏóê ÎèÖÎ¶ΩÏ†ÅÏù∏ ÏàúÏàò ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅÏûÖÎãàÎã§.\n-   KOPIS APIÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏Ïò§Í≥†, Í∞ÄÍ≥µÎêòÏßÄ ÏïäÏùÄ ÏàúÏàòÌïú ÌååÏù¥Ïç¨ Îç∞Ïù¥ÌÑ∞ ÌòïÌÉúÎ°ú Î∞òÌôòÌïòÎäî Ïó≠Ìï†Îßå Îã¥ÎãπÌï©ÎãàÎã§.\n-   Ïñ¥Îñ§ ÌîåÎû´ÌèºÏóê Ïó∞Í≤∞ÎêòÎäîÏßÄ Ï†ÑÌòÄ ÏïåÏßÄ Î™ªÌïòÎ©∞, Ïñ¥ÎîîÏÑúÎì† Ïû¨ÏÇ¨Ïö©Ïù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\n\n### üìû Ïó∞Í≤∞ Ïñ¥ÎåëÌÑ∞\n-   ÌïµÏã¨ ÏóîÏßÑÏùÑ Îã§ÏñëÌïú Ïô∏Î∂Ä ÌîåÎû´ÌèºÏóê Ïó∞Í≤∞ÌïòÎäî 'ÌîåÎü¨Í∑∏' Ïó≠Ìï†ÏùÑ Ìï©ÎãàÎã§.\n-   Í∞Å ÌîåÎû´ÌèºÏùò Í≥†Ïú†Ìïú ÏöîÏ≤≠/ÏùëÎãµ ÌòïÏãùÏùÑ Ï≤òÎ¶¨ÌïòÍ≥†, ÌïµÏã¨ ÏóîÏßÑÍ≥º ÌÜµÏã†Ìï©ÎãàÎã§.\n\n-   **`kakao_server.py`**: **Ïπ¥Ïπ¥Ïò§ PlayMCP**Ïö© Ïñ¥ÎåëÌÑ∞ ÏÑúÎ≤ÑÏûÖÎãàÎã§.\n-   **`web_server.py`**: **ÏùºÎ∞ò ÏõπÏÇ¨Ïù¥Ìä∏ÎÇò Ïï±**ÏùÑ ÏúÑÌïú Î≤îÏö© API Ïñ¥ÎåëÌÑ∞ ÏÑúÎ≤ÑÏûÖÎãàÎã§. \n\n---\n\n## üõ†Ô∏è ÏãúÏûëÌïòÍ∏∞\n\n1.  **ÌïÑÏöî ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n### Installing via Smithery\n\nTo install festival-finder-mcp automatically via [Smithery](https://smithery.ai/server/@JunoJunHyun/festival-finder-mcp):\n\n```bash\nnpx -y @smithery/cli install @JunoJunHyun/festival-finder-mcp\n```\n\n2.  **KOPIS API ÌÇ§ ÏÑ§Ï†ï**\n    `core_logic.py` ÌååÏùºÏùÑ Ïó¥Ïñ¥ `KOPIS_API_KEY` Î≥ÄÏàòÏóê Î∞úÍ∏âÎ∞õÏùÄ Î≥∏Ïù∏Ïùò API ÌÇ§Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.\n\n3.  **Ïñ¥ÎåëÌÑ∞ ÏÑúÎ≤Ñ Ïã§Ìñâ**\n    ÌïÑÏöîÌïú Ïñ¥ÎåëÌÑ∞ ÏÑúÎ≤ÑÎ•º ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú Ïã§ÌñâÌï©ÎãàÎã§.\n\n    -   **Ïπ¥Ïπ¥Ïò§ÌÜ° Ïó∞Îèô ÌÖåÏä§Ìä∏ Ïãú:**\n        ```bash\n        python kakao_server.py\n        ```\n        - ngrokÏùÑ `http://localhost:5000`Ïóê Ïó∞Í≤∞ÌïòÍ≥†, PlayMCP EndpointÏóê `[ngrok Ï£ºÏÜå]/kakao`Î•º Îì±Î°ùÌï©ÎãàÎã§.\n\n    -   **ÏùºÎ∞ò Ïõπ API ÌÖåÏä§Ìä∏ Ïãú:**\n        ```bash\n        python web_server.py\n        ```\n        - Ïõπ Î∏åÎùºÏö∞Ï†ÄÎÇò Îã§Î•∏ ÌîÑÎ°úÍ∑∏Îû®ÏóêÏÑú `http://localhost:5001/api/performances?stdate=...` ÌòïÏãùÏúºÎ°ú ÏöîÏ≤≠ÌïòÏó¨ ÌÖåÏä§Ìä∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.",
        "start_pos": 0,
        "end_pos": 1684,
        "token_count_estimate": 420,
        "source_type": "readme",
        "agent_id": "a02b2a5cb947bff4"
      }
    ]
  },
  {
    "agent_id": "4968f55ed6352e6d",
    "name": "ai.smithery/Kim-soung-won-mcp-smithery-exam",
    "source": "mcp",
    "source_url": "https://github.com/Kim-soung-won/mcp-smithery-exam",
    "description": "Craft quick, personalized greetings by name. Generate ready-to-use greeting prompts for a consiste‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-16T06:32:38.498462Z",
    "indexed_at": "2026-02-18T04:03:51.798417",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# smithery-mcp-exam\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server built with Smithery",
        "Test server interactions interactively via playground",
        "Add or update server capabilities through server code",
        "Deploy the MCP server to Smithery platform"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai/account/api-keys",
        "GitHub account for repository creation and code push",
        "uv tool for running and testing the server"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation steps, usage examples, development guidance, and deployment instructions but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# smithery-mcp-exam\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 924,
        "token_count_estimate": 230,
        "source_type": "readme",
        "agent_id": "4968f55ed6352e6d"
      }
    ]
  },
  {
    "agent_id": "3ae0b27ea8a6ede1",
    "name": "ai.smithery/Kryptoskatt-mcp-server",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@Kryptoskatt/mcp-server/mcp",
    "description": "Enable AI assistants to interact seamlessly with the DefiLlama API by translating MCP tool calls i‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-17T10:41:17.536569Z",
    "indexed_at": "2026-02-18T04:03:53.676321",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Enable AI assistants to interact with the DefiLlama API",
        "Translate MCP tool calls for seamless integration with DefiLlama"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's purpose but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "7956c2ad89f90edb",
    "name": "ai.smithery/Leghis-smart-thinking",
    "source": "mcp",
    "source_url": "https://github.com/Leghis/Smart-Thinking",
    "description": "Find relevant Smart‚ÄëThinking memories fast. Fetch full entries by ID to get complete context. Spee‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T14:04:13.933299Z",
    "indexed_at": "2026-02-18T04:03:54.956283",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/leghis-smart-thinking-badge.png)](https://mseep.ai/app/leghis-smart-thinking)\n\n# Smart-Thinking\n\n[![npm version](https://img.shields.io/npm/v/smart-thinking-mcp.svg)](https://www.npmjs.com/package/smart-thinking-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.1.6-blue)](https://www.typescriptlang.org/)\n[![Platform: Windows](https://img.shields.io/badge/Platform-Windows-blue)](https://github.com/Leghis/smart-thinking-mcp)\n[![Platform: macOS](https://img.shields.io/badge/Platform-macOS-blue)](https://github.com/Leghis/smart-thinking-mcp)\n[![Platform: Linux](https://img.shields.io/badge/Platform-Linux-blue)](https://github.com/Leghis/smart-thinking-mcp)\n\nSmart-Thinking is a Model Context Protocol (MCP) server that delivers graph-based, multi-step reasoning without relying on external AI APIs. Everything happens locally: similarity search, heuristic-based scoring, verification tracking, memory, and visualization all run in a deterministic pipeline designed for transparency and reproducibility.\n\n## Core Capabilities\n- Graph-first reasoning that connects thoughts with rich relationships (supports, contradicts, refines, contextual links, and more).\n- Local TF-IDF + cosine similarity engine powering memory lookups and graph expansion without third-party embedding services.\n- Heuristic quality evaluation that scores confidence, relevance, and quality using transparent rules instead of LLM calls.\n- Verification workflow with detailed statuses and calculation tracing to surface facts, guardrails, and uncertainties.\n- Persistent sessions that can be resumed across runs, keeping both the reasoning graph and verification ledger in sync.\n\n## Reasoning Flow\n1. **Session bootstrap** ‚Äì `ReasoningOrchestrator` initializes a session, restores any saved graph state, and prepares feature flags.\n2. **Pre-verification** ‚Äì deterministic guards inspect the incoming thought, perform light-weight calculation checks, and annotate the payload.\n3. **Graph integration** ‚Äì the thought is inserted into `ThoughtGraph`, linking to context, prior thoughts, and relevant memories.\n4. **Heuristic evaluation** ‚Äì `QualityEvaluator` and `MetricsCalculator` compute weighted scores and traces that explain the decision path.\n5. **Verification feedback** ‚Äì statuses from `VerificationService` and heuristic traces are attached to the node and propagated across connections.\n6. **Persistence & response** ‚Äì updates are written to `MemoryManager`/`VerificationMemory`, and a structured MCP response is returned with a timeline of reasoning steps.\n\nEach step is logged with structured metadata so you can visualize the reasoning fabric, audit decisions, and replay sessions deterministically.\n\n## Installation\nSmart-Thinking ships as an npm package compatible with Windows, macOS, and Linux.\n\n### Global install (recommended)\n```bash\nnpm install -g smart-thinking-mcp\n```\n\n### Run with npx\n```bash\nnpx -y smart-thinking-mcp\n```\n\n### From source\n```bash\ngit clone https://github.com/Leghis/Smart-Thinking.git\ncd Smart-Thinking\nnpm install\nnpm run build\nnpm link\n```\n\n> Need platform-specific configuration details? See `GUIDE_INSTALLATION.md` for step-by-step instructions covering Windows, macOS, Linux, and Claude Desktop integration.\n\n## Quick Tour\n- `smart-thinking-mcp` ‚Äî start the MCP server (globally installed package).\n- `npx -y smart-thinking-mcp` ‚Äî launch without a global install.\n- `npm run start` ‚Äî execute the built server from source.\n- `npm run demo:session` ‚Äî run the built-in CLI walkthrough that feeds sample thoughts through the reasoning pipeline and prints the resulting timeline.\n\nThe demo script showcases how the orchestrator adds nodes, evaluates heuristics, and records verification feedback step by step.\n\n## MCP Client Compatibility\nSmart-Thinking is validated across the most popular MCP clients and operating systems. Use the new connector mode (`--mode=connector` or `SMART_THINKING_MODE=connector`) when a client only accepts the `search` and `fetch` tools required by ChatGPT connectors.[^openai-mcp]\n\n| Client | Transport | Notes |\n| --- | --- | --- |\n| **ChatGPT Connectors & Deep Research** | HTTP + SSE | Deploy with `SMART_THINKING_MODE=connector node build/index.js --transport=http --host 0.0.0.0 --port 8000`. Point ChatGPT to `https://<host>/sse` and keep only `search`/`fetch` enabled, aligning with OpenAI‚Äôs remote MCP guidance.[^openai-mcp] |\n| **OpenAI Codex CLI & Agents SDK** | Streamable HTTP / SSE | Configure the Codex agent with `http://localhost:3000/mcp` or `http://localhost:3000/sse` and set `SMART_THINKING_MODE=connector` when only knowledge retrieval is needed.[^openai-agents] |\n| **Claude Desktop / Claude Code** | stdio | Add `\"command\": \"smart-thinking-mcp\"` (or an `npx` command) to `claude_desktop_config.json`. Full toolset is available.[^mcp-clients] |\n| **Cursor IDE** | stdio / SSE / Streamable HTTP | Add the server to `~/.cursor/mcp.json` or the project `.cursor/mcp.json`. Cursor supports prompts, roots, elicitation, and streaming.[^cursor-mcp] |\n| **Cline (VS Code)** | stdio | Place the command in `~/Documents/Cline/MCP/smart-thinking.json` or use the in-app marketplace to register the toolset.[^mcp-clients] |\n| **Kilo Code** | stdio | Register via the MCP marketplace and run the server locally; Smart-Thinking exposes deterministic tooling for autonomous edits.[^mcp-clients] |\n\n> Need a minimal deployment footprint? Combine `--transport=http --mode=connector` with a reverse proxy (ngrok, fly.io, render, etc.) so remote clients can consume the server without exposing the full toolset.\n\nFor registry scanners and fallback metadata extraction, Smart-Thinking also exposes:\n\n- `GET /.well-known/mcp/server-card.json`\n\n## Configuration & Feature Flags\n- `feature-flags.ts` toggles advanced behaviours such as external integrations (disabled by default) and verbose tracing.\n- `config.ts` aligns platform-specific paths and verification thresholds.\n- `memory-manager.ts` and `verification-memory.ts` store session graphs, metrics, and calculation results using deterministic JSON snapshots.\n\n## Zero-API-Key Mode (Default)\n- Smart-Thinking runs fully in local deterministic mode without any API key.\n- External verification/search connectors are disabled by default in `ToolIntegrator`.\n- To explicitly enable external connectors, set:\n\n```bash\nexport SMART_THINKING_ENABLE_EXTERNAL_TOOLS=true\n```\n\n- If external connectors are disabled (default), verification suggestions stay local (`executePython`, `executeJavaScript`) and external tool calls return a local fallback result.\n- `FeatureFlags.externalLlmEnabled` and `FeatureFlags.externalEmbeddingEnabled` remain disabled by default, so no remote LLM/embedding provider is required.\n\n## Development Workflow\n```bash\nnpm run build           # Compile TypeScript sources\nnpm run lint            # ESLint across src/\nnpm run test            # Jest test suite\nnpm run test:coverage   # Jest coverage report\nnpm run watch           # Incremental TypeScript compilation\n```\n\nSee `docs/modernisation-smart-thinking-v12-plan.md` for the modernization checklist and rollout tracking.\n\n## Quality & Support\n- Deterministic heuristics and verification eliminate dependency on remote LLMs.\n- Latest validation (February 6, 2026): `80.47%` statements, `81.59%` lines, `84.34%` functions, `63.48%` branches.\n- CI recommendations: run `npm run lint` and `npm run test:coverage` before each release candidate.\n\n## Contributing\nContributions are welcome. Please open an issue or pull request describing the change, and run the quality checks above before submitting.\n\n## License\n[MIT](./LICENSE)\n\n[^openai-mcp]: OpenAI, ‚ÄúBuilding MCP servers for ChatGPT and API integrations,‚Äù highlights that connectors require `search` and `fetch` tools for remote use. (https://platform.openai.com/docs/mcp)\n[^openai-agents]: OpenAI Agents SDK documentation on MCP transports (stdio, SSE, streamable HTTP). (https://openai.github.io/openai-agents-python/mcp/)\n[^mcp-clients]: Model Context Protocol client catalogue listing Claude, Cline, Kilo Code, and other MCP-compatible applications. (https://modelcontextprotocol.io/clients)\n[^cursor-mcp]: Cursor documentation for configuring MCP servers via stdio/SSE/HTTP transports. (https://cursor.com/docs/context/mcp)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Perform graph-first reasoning connecting thoughts with rich relationships",
        "Execute local TF-IDF and cosine similarity searches for memory lookups and graph expansion",
        "Evaluate heuristic quality scores for confidence, relevance, and quality using transparent rules",
        "Track verification statuses with detailed calculation tracing and verification workflow",
        "Persist and resume sessions maintaining reasoning graph and verification ledger state",
        "Log structured metadata for visualization, auditing, and deterministic replay of reasoning steps",
        "Support multiple MCP clients and transports including HTTP, SSE, and stdio",
        "Operate fully locally without external AI API dependencies by default"
      ],
      "limitations": [
        "Does not use external LLM or embedding providers by default",
        "External verification and search connectors are disabled by default and require explicit enabling",
        "Limited to heuristic and deterministic evaluation without probabilistic LLM inference",
        "Connector mode restricts tools to only 'search' and 'fetch' for compatibility with some clients",
        "Verification suggestions fallback to local execution of Python/JavaScript when external tools are disabled"
      ],
      "requirements": [
        "Node.js environment with npm for installation and running",
        "No API keys or external authentication required by default",
        "Optional environment variable SMART_THINKING_ENABLE_EXTERNAL_TOOLS=true to enable external connectors",
        "Compatible with Windows, macOS, and Linux platforms",
        "Recommended to run lint and test coverage scripts for quality assurance during development"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed capability descriptions, usage examples including demo scripts, client compatibility notes, configuration options, limitations, and development workflow, making it excellent overall.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/leghis-smart-thinking-badge.png)](https://mseep.ai/app/leghis-smart-thinking)\n\n# Smart-Thinking\n\n[![npm version](https://img.shields.io/npm/v/smart-thinking-mcp.svg)](https://www.npmjs.com/package/smart-thinking-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.1.6-blue)](https://www.typescriptlang.org/)\n[![Platform: Windows](https://img.shields.io/badge/Platform-Windows-blue)](https://github.com/Leghis/smart-thinking-mcp)\n[![Platform: macOS](https://img.shields.io/badge/Platform-macOS-blue)](https://github.com/Leghis/smart-thinking-mcp)\n[![Platform: Linux](https://img.shields.io/badge/Platform-Linux-blue)](https://github.com/Leghis/smart-thinking-mcp)\n\nSmart-Thinking is a Model Context Protocol (MCP) server that delivers graph-based, multi-step reasoning without relying on external AI APIs. Everything happens locally: similarity search, heuristic-based scoring, verification tracking, memory, and visualization all run in a deterministic pipeline designed for transparency and reproducibility.\n\n## Core Capabilities\n- Graph-first reasoning that connects thoughts with rich relationships (supports, contradicts, refines, contextual links, and more).\n- Local TF-IDF + cosine similarity engine powering memory lookups and graph expansion without third-party embedding services.\n- Heuristic quality evaluation that scores confidence, relevance, and quality using transparent rules instead of LLM calls.\n- Verification workflow with detailed statuses and calculation tracing to surface facts, guardrails, and uncertainties.\n- Persistent sessions that can be resumed across runs, keeping both the reasoning graph and verification ledger in sync.\n\n## Reasoning Flow\n1. **Session bootstrap** ‚Äì `ReasoningOrchestrator` initializes a session, restores any saved graph state, and prepares feature flags.\n2.",
        "start_pos": 0,
        "end_pos": 1986,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "7956c2ad89f90edb"
      },
      {
        "chunk_id": 1,
        "text": "oning graph and verification ledger in sync.\n\n## Reasoning Flow\n1. **Session bootstrap** ‚Äì `ReasoningOrchestrator` initializes a session, restores any saved graph state, and prepares feature flags.\n2. **Pre-verification** ‚Äì deterministic guards inspect the incoming thought, perform light-weight calculation checks, and annotate the payload.\n3. **Graph integration** ‚Äì the thought is inserted into `ThoughtGraph`, linking to context, prior thoughts, and relevant memories.\n4. **Heuristic evaluation** ‚Äì `QualityEvaluator` and `MetricsCalculator` compute weighted scores and traces that explain the decision path.\n5. **Verification feedback** ‚Äì statuses from `VerificationService` and heuristic traces are attached to the node and propagated across connections.\n6. **Persistence & response** ‚Äì updates are written to `MemoryManager`/`VerificationMemory`, and a structured MCP response is returned with a timeline of reasoning steps.\n\nEach step is logged with structured metadata so you can visualize the reasoning fabric, audit decisions, and replay sessions deterministically.\n\n## Installation\nSmart-Thinking ships as an npm package compatible with Windows, macOS, and Linux.\n\n### Global install (recommended)\n```bash\nnpm install -g smart-thinking-mcp\n```\n\n### Run with npx\n```bash\nnpx -y smart-thinking-mcp\n```\n\n### From source\n```bash\ngit clone https://github.com/Leghis/Smart-Thinking.git\ncd Smart-Thinking\nnpm install\nnpm run build\nnpm link\n```\n\n> Need platform-specific configuration details? See `GUIDE_INSTALLATION.md` for step-by-step instructions covering Windows, macOS, Linux, and Claude Desktop integration.\n\n## Quick Tour\n- `smart-thinking-mcp` ‚Äî start the MCP server (globally installed package).\n- `npx -y smart-thinking-mcp` ‚Äî launch without a global install.\n- `npm run start` ‚Äî execute the built server from source.\n- `npm run demo:session` ‚Äî run the built-in CLI walkthrough that feeds sample thoughts through the reasoning pipeline and prints the resulting timeline.",
        "start_pos": 1786,
        "end_pos": 3772,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "7956c2ad89f90edb"
      },
      {
        "chunk_id": 2,
        "text": "start` ‚Äî execute the built server from source.\n- `npm run demo:session` ‚Äî run the built-in CLI walkthrough that feeds sample thoughts through the reasoning pipeline and prints the resulting timeline.\n\nThe demo script showcases how the orchestrator adds nodes, evaluates heuristics, and records verification feedback step by step.\n\n## MCP Client Compatibility\nSmart-Thinking is validated across the most popular MCP clients and operating systems. Use the new connector mode (`--mode=connector` or `SMART_THINKING_MODE=connector`) when a client only accepts the `search` and `fetch` tools required by ChatGPT connectors.[^openai-mcp]\n\n| Client | Transport | Notes |\n| --- | --- | --- |\n| **ChatGPT Connectors & Deep Research** | HTTP + SSE | Deploy with `SMART_THINKING_MODE=connector node build/index.js --transport=http --host 0.0.0.0 --port 8000`. Point ChatGPT to `https://<host>/sse` and keep only `search`/`fetch` enabled, aligning with OpenAI‚Äôs remote MCP guidance.[^openai-mcp] |\n| **OpenAI Codex CLI & Agents SDK** | Streamable HTTP / SSE | Configure the Codex agent with `http://localhost:3000/mcp` or `http://localhost:3000/sse` and set `SMART_THINKING_MODE=connector` when only knowledge retrieval is needed.[^openai-agents] |\n| **Claude Desktop / Claude Code** | stdio | Add `\"command\": \"smart-thinking-mcp\"` (or an `npx` command) to `claude_desktop_config.json`. Full toolset is available.[^mcp-clients] |\n| **Cursor IDE** | stdio / SSE / Streamable HTTP | Add the server to `~/.cursor/mcp.json` or the project `.cursor/mcp.json`.",
        "start_pos": 3572,
        "end_pos": 5115,
        "token_count_estimate": 385,
        "source_type": "readme",
        "agent_id": "7956c2ad89f90edb"
      },
      {
        "chunk_id": 3,
        "text": "d run the server locally; Smart-Thinking exposes deterministic tooling for autonomous edits.[^mcp-clients] |\n\n> Need a minimal deployment footprint? Combine `--transport=http --mode=connector` with a reverse proxy (ngrok, fly.io, render, etc.) so remote clients can consume the server without exposing the full toolset.\n\nFor registry scanners and fallback metadata extraction, Smart-Thinking also exposes:\n\n- `GET /.well-known/mcp/server-card.json`\n\n## Configuration & Feature Flags\n- `feature-flags.ts` toggles advanced behaviours such as external integrations (disabled by default) and verbose tracing.\n- `config.ts` aligns platform-specific paths and verification thresholds.\n- `memory-manager.ts` and `verification-memory.ts` store session graphs, metrics, and calculation results using deterministic JSON snapshots.\n\n## Zero-API-Key Mode (Default)\n- Smart-Thinking runs fully in local deterministic mode without any API key.\n- External verification/search connectors are disabled by default in `ToolIntegrator`.\n- To explicitly enable external connectors, set:\n\n```bash\nexport SMART_THINKING_ENABLE_EXTERNAL_TOOLS=true\n```\n\n- If external connectors are disabled (default), verification suggestions stay local (`executePython`, `executeJavaScript`) and external tool calls return a local fallback result.\n- `FeatureFlags.externalLlmEnabled` and `FeatureFlags.externalEmbeddingEnabled` remain disabled by default, so no remote LLM/embedding provider is required.\n\n## Development Workflow\n```bash\nnpm run build           # Compile TypeScript sources\nnpm run lint            # ESLint across src/\nnpm run test            # Jest test suite\nnpm run test:coverage   # Jest coverage report\nnpm run watch           # Incremental TypeScript compilation\n```\n\nSee `docs/modernisation-smart-thinking-v12-plan.md` for the modernization checklist and rollout tracking.\n\n## Quality & Support\n- Deterministic heuristics and verification eliminate dependency on remote LLMs.",
        "start_pos": 5420,
        "end_pos": 7380,
        "token_count_estimate": 490,
        "source_type": "readme",
        "agent_id": "7956c2ad89f90edb"
      },
      {
        "chunk_id": 4,
        "text": "s/modernisation-smart-thinking-v12-plan.md` for the modernization checklist and rollout tracking.\n\n## Quality & Support\n- Deterministic heuristics and verification eliminate dependency on remote LLMs.\n- Latest validation (February 6, 2026): `80.47%` statements, `81.59%` lines, `84.34%` functions, `63.48%` branches.\n- CI recommendations: run `npm run lint` and `npm run test:coverage` before each release candidate.\n\n## Contributing\nContributions are welcome. Please open an issue or pull request describing the change, and run the quality checks above before submitting.\n\n## License\n[MIT](./LICENSE)\n\n[^openai-mcp]: OpenAI, ‚ÄúBuilding MCP servers for ChatGPT and API integrations,‚Äù highlights that connectors require `search` and `fetch` tools for remote use. (https://platform.openai.com/docs/mcp)\n[^openai-agents]: OpenAI Agents SDK documentation on MCP transports (stdio, SSE, streamable HTTP). (https://openai.github.io/openai-agents-python/mcp/)\n[^mcp-clients]: Model Context Protocol client catalogue listing Claude, Cline, Kilo Code, and other MCP-compatible applications. (https://modelcontextprotocol.io/clients)\n[^cursor-mcp]: Cursor documentation for configuring MCP servers via stdio/SSE/HTTP transports. (https://cursor.com/docs/context/mcp)",
        "start_pos": 7180,
        "end_pos": 8436,
        "token_count_estimate": 313,
        "source_type": "readme",
        "agent_id": "7956c2ad89f90edb"
      }
    ]
  },
  {
    "agent_id": "5195375ae53ea296",
    "name": "ai.smithery/LinkupPlatform-linkup-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/LinkupPlatform/linkup-mcp-server",
    "description": "Search the web in real time to get trustworthy, source-backed answers. Find the latest news and co‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-27T11:25:12.402429Z",
    "indexed_at": "2026-02-18T04:03:57.498058",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Linkup MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@LinkupPlatform/linkup-mcp-server)](https://smithery.ai/server/@LinkupPlatform/linkup-mcp-server)\n\nA Model Context Protocol (MCP) server that provides web search and page fetching capabilities through [Linkup's](https://www.linkup.so/) advanced API. This server enables AI assistants like Claude to perform intelligent web searches with natural language queries and fetch content from any webpage, accessing real-time information from trusted sources across the web.\n\n## Features\n\n- üîç **Real-time Web Search**: Search the web for current information, news, and data\n- üåê **Page Fetching**: Fetch and extract content from any webpage\n- üéØ **Natural Language Queries**: Use full questions for best results\n- üìä **Flexible Search Depth**:\n  - `standard` - For queries with direct answers\n  - `deep` - For complex research requiring analysis across multiple sources\n- üñ•Ô∏è **JavaScript Rendering**: Optional JS rendering for dynamic content\n- ‚ö° **Fast**: Powered by Linkup's optimized infrastructure\n\n## Installation\n\n- Cursor, VSCode, Claude Code, or another MCP compatible client\n- Linkup API key\n\n### Getting Your API Key\n\n1. Create a Linkup account for free at [app.linkup.so](https://app.linkup.so/)\n2. Copy the API key from your dashboard\n\n### Remote MCP Server (recommended)\n\nYou can access the MCP server directly through [Smithery](https://smithery.ai/server/@LinkupPlatform/linkup-mcp-server). From there, you'll be able to install the server into your favorite MCP compatible client. The remote MCP server is using the Streamable HTTP transport.\n\nYou can also use the Smithery CLI to install the server into your favorite MCP compatible client.\n```bash\nnpx @smithery/cli login # If you haven't already\nnpx -y @smithery/cli@latest install linkup-mcp-server --client <CLIENT_NAME> --config '{\"apiKey\":<LINKUP_API_KEY>}'\n```\n\n**Finally, if your client supports OAuth protocol, you can reference directly the remote MCP server URL. See examples below:**\n\n#### Cursor\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en-US/install-mcp?name=linkup&config=eyJ0eXBlIjoiaHR0cCIsInVybCI6Imh0dHBzOi8vbWNwLmxpbmt1cC5zby9tY3A%2FYXBpS2V5PVlPVVJfQVBJX0tFWSJ9)\n\n\nIn your `~/.cursor/mcp.json` file, add the following:\n```json\n{\n  \"mcpServers\": {\n    // ... other MCP servers\n    \"linkup\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.linkup.so/mcp?apiKey=LINKUP_API_KEY\"\n    }\n  }\n}\n```\n\n#### VSCode\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n```json\n{\n  \"servers\": {\n    // ... other MCP servers\n    \"linkup\": {\n      \"url\": \"https://mcp.linkup.so/mcp?apiKey=LINKUP_API_KEY\",\n      \"type\": \"http\"\n    }\n  }\n}\n```\n\n### MCP Bundle (recommanded for Claude Desktop)\n\nDownload the pre-built MCP bundle, a self-contained package that works across compatible MCP clients (like Claude Desktop for example). MCP Bundles are developed by Anthropics see [here](https://github.com/anthropics/mcpb?tab=readme-ov-file#mcp-bundles-mcpb) for more info.\n\n**Quick Download:**\n```bash\ncurl -L -o linkup-mcp-server.mcpb https://github.com/LinkupPlatform/linkup-mcp-server/releases/latest/download/linkup-mcp-server.mcpb\n```\n\n**Installation:**\n1. Download `linkup-mcp-server.mcpb` from releases (or use the curl command above)\n2. Click on the file to install\n3. Configure your API key when prompted\n\n### Local MCP Server\n\nYou can also run the MCP server locally through the stdio transport.\n\n#### Cursor\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en-US/install-mcp?name=linkup&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsImxpbmt1cC1tY3Atc2VydmVyIiwiYXBpS2V5PVlPVVJfTElOS1VQX0FQSV9LRVkiXX0%3D)\n\n```json\n{\n  \"mcpServers\": {\n    // ... other MCP servers\n    \"linkup\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"linkup-mcp-server\",\n        \"apiKey=LINKUP_API_KEY\"\n      ]\n    }\n  }\n}\n```\n\n#### VSCode\n\n```json\n{\n  \"servers\": {\n    // ... other MCP servers\n    \"linkup\": {\n      \"command\": \"npx\",\n      \"type\": \"stdio\",\n      \"args\": [\n        \"-y\",\n        \"linkup-mcp-server\",\n        \"apiKey=LINKUP_API_KEY\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\nOnce configured, you can ask your AI agent to search the web or fetch webpage content:\n\n**Search Examples:**\n- \"Search the web for the latest news about AI developments\"\n- \"What's the current weather in Tokyo?\"\n- \"Find information about the new EU AI Act and how it affects startups\"\n- \"Search for the latest stock price of NVIDIA\"\n\n**Fetch Examples:**\n- \"Fetch the content from https://example.com/article\"\n- \"Get the content of this blog post: https://blog.example.com/post and make a summary of it\"\n- \"Fetch https://example.com with JavaScript rendering enabled\"\n\n### Search Depths\n\n- **Standard Search**: Best for queries with direct answers (weather, stock prices, simple facts)\n- **Deep Search**: Best for complex research requiring analysis across multiple sources (comprehensive guides, comparative analysis, in-depth research)\n\n## Tools Available\n\n### `linkup-search`\n\nSearch the web in real time using Linkup to retrieve current information, facts, and news from trusted sources.\n\n**Parameters:**\n- `query` (required): Natural language search query. Full questions work best.\n- `depth` (optional): Search depth - \"standard\" (default) or \"deep\"\n\n**Use cases:**\n- Real-time data (weather, stocks, sports scores, events)\n- Breaking news and current events\n- Recent research and publications\n- Product information and up-to-date prices\n- Schedules and availability\n- Any information not available in the AI's knowledge base\n\n### `linkup-fetch`\n\nFetch and extract content from any webpage URL.\n\n**Parameters:**\n- `url` (required): The URL to fetch content from.\n- `renderJs` (optional): Whether to render JavaScript content (default: false). Enable this for dynamic pages that load content via JavaScript. Note: This makes the request slower.\n\n**Use cases:**\n- Retrieve page content for analysis or summarization\n- Extract article content from news sites\n- Get documentation from technical websites\n- Fetch blog posts and written content\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18.0.0\n- npm\n\n### Setup\n\n```bash\n# Install dependencies\nnpm install\n```\n\n### Running with Smithery\n\n```bash\nnpm run dev\n```\n\n### Running with stdio transport\n```bash\nnpm run build:stdio\nnpm run start:stdio apiKey=YOUR_API_KEY\n```\n\n### Testing with MCP Inspector\n\n```bash\nnpm run build:stdio\nnpm run inspector apiKey=YOUR_API_KEY\n```\n\nThis will open the MCP Inspector in your browser where you can test the search tool interactively.\n\n## License\n\nMIT\n\n## Links\n\n- [Linkup Website](https://www.linkup.so/)\n- [Linkup Documentation](https://docs.linkup.so/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n\n## Support\n\nIf you have issues, contact us via email at [support@linkup.so](mailto:support@linkup.so) or join our [Discord server](https://discord.com/invite/9q9mCYJa86).\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Perform real-time web searches using natural language queries",
        "Fetch and extract content from any webpage URL",
        "Render JavaScript content on webpages for dynamic content fetching",
        "Support flexible search depths including standard and deep searches",
        "Integrate with MCP compatible clients via HTTP or stdio transport",
        "Provide up-to-date information from trusted sources across the web",
        "Enable AI assistants to access current news, weather, stock prices, and research",
        "Offer pre-built MCP bundles for easy installation in compatible clients"
      ],
      "limitations": [
        "JavaScript rendering makes requests slower",
        "Requires an API key from Linkup to function",
        "No explicit mention of rate limits or usage quotas",
        "No offline or cached data capabilities mentioned"
      ],
      "requirements": [
        "Linkup API key for authentication",
        "Node.js version 18.0.0 or higher for local server setup",
        "npm for dependency management and running scripts",
        "MCP compatible client (e.g., Cursor, VSCode, Claude Desktop)",
        "Optional Smithery CLI for installation and management"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, configuration options, development setup, and contact/support information.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Linkup MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@LinkupPlatform/linkup-mcp-server)](https://smithery.ai/server/@LinkupPlatform/linkup-mcp-server)\n\nA Model Context Protocol (MCP) server that provides web search and page fetching capabilities through [Linkup's](https://www.linkup.so/) advanced API. This server enables AI assistants like Claude to perform intelligent web searches with natural language queries and fetch content from any webpage, accessing real-time information from trusted sources across the web.\n\n## Features\n\n- üîç **Real-time Web Search**: Search the web for current information, news, and data\n- üåê **Page Fetching**: Fetch and extract content from any webpage\n- üéØ **Natural Language Queries**: Use full questions for best results\n- üìä **Flexible Search Depth**:\n  - `standard` - For queries with direct answers\n  - `deep` - For complex research requiring analysis across multiple sources\n- üñ•Ô∏è **JavaScript Rendering**: Optional JS rendering for dynamic content\n- ‚ö° **Fast**: Powered by Linkup's optimized infrastructure\n\n## Installation\n\n- Cursor, VSCode, Claude Code, or another MCP compatible client\n- Linkup API key\n\n### Getting Your API Key\n\n1. Create a Linkup account for free at [app.linkup.so](https://app.linkup.so/)\n2. Copy the API key from your dashboard\n\n### Remote MCP Server (recommended)\n\nYou can access the MCP server directly through [Smithery](https://smithery.ai/server/@LinkupPlatform/linkup-mcp-server). From there, you'll be able to install the server into your favorite MCP compatible client. The remote MCP server is using the Streamable HTTP transport.\n\nYou can also use the Smithery CLI to install the server into your favorite MCP compatible client.\n```bash\nnpx @smithery/cli login # If you haven't already\nnpx -y @smithery/cli@latest install linkup-mcp-server --client <CLIENT_NAME> --config '{\"apiKey\":<LINKUP_API_KEY>}'\n```\n\n**Finally, if your client supports OAuth protocol, you can reference directly the remote MCP server URL.",
        "start_pos": 0,
        "end_pos": 1995,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "5195375ae53ea296"
      },
      {
        "chunk_id": 1,
        "text": "st install linkup-mcp-server --client <CLIENT_NAME> --config '{\"apiKey\":<LINKUP_API_KEY>}'\n```\n\n**Finally, if your client supports OAuth protocol, you can reference directly the remote MCP server URL. See examples below:**\n\n#### Cursor\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en-US/install-mcp?name=linkup&config=eyJ0eXBlIjoiaHR0cCIsInVybCI6Imh0dHBzOi8vbWNwLmxpbmt1cC5zby9tY3A%2FYXBpS2V5PVlPVVJfQVBJX0tFWSJ9)\n\n\nIn your `~/.cursor/mcp.json` file, add the following:\n```json\n{\n  \"mcpServers\": {\n    // ... other MCP servers\n    \"linkup\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.linkup.so/mcp?apiKey=LINKUP_API_KEY\"\n    }\n  }\n}\n```\n\n#### VSCode\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n```json\n{\n  \"servers\": {\n    // ... other MCP servers\n    \"linkup\": {\n      \"url\": \"https://mcp.linkup.so/mcp?apiKey=LINKUP_API_KEY\",\n      \"type\": \"http\"\n    }\n  }\n}\n```\n\n### MCP Bundle (recommanded for Claude Desktop)\n\nDownload the pre-built MCP bundle, a self-contained package that works across compatible MCP clients (like Claude Desktop for example). MCP Bundles are developed by Anthropics see [here](https://github.com/anthropics/mcpb?tab=readme-ov-file#mcp-bundles-mcpb) for more info.\n\n**Quick Download:**\n```bash\ncurl -L -o linkup-mcp-server.mcpb https://github.com/LinkupPlatform/linkup-mcp-server/releases/latest/download/linkup-mcp-server.mcpb\n```\n\n**Installation:**\n1. Download `linkup-mcp-server.mcpb` from releases (or use the curl command above)\n2. Click on the file to install\n3. Configure your API key when prompted\n\n### Local MCP Server\n\nYou can also run the MCP server locally through the stdio transport.",
        "start_pos": 1795,
        "end_pos": 3569,
        "token_count_estimate": 443,
        "source_type": "readme",
        "agent_id": "5195375ae53ea296"
      },
      {
        "chunk_id": 2,
        "text": "all-dark.svg)](https://cursor.com/en-US/install-mcp?name=linkup&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsImxpbmt1cC1tY3Atc2VydmVyIiwiYXBpS2V5PVlPVVJfTElOS1VQX0FQSV9LRVkiXX0%3D)\n\n```json\n{\n  \"mcpServers\": {\n    // ... other MCP servers\n    \"linkup\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"linkup-mcp-server\",\n        \"apiKey=LINKUP_API_KEY\"\n      ]\n    }\n  }\n}\n```\n\n#### VSCode\n\n```json\n{\n  \"servers\": {\n    // ... other MCP servers\n    \"linkup\": {\n      \"command\": \"npx\",\n      \"type\": \"stdio\",\n      \"args\": [\n        \"-y\",\n        \"linkup-mcp-server\",\n        \"apiKey=LINKUP_API_KEY\"\n      ]\n    }\n  }\n}\n```\n\n## Usage\n\nOnce configured, you can ask your AI agent to search the web or fetch webpage content:\n\n**Search Examples:**\n- \"Search the web for the latest news about AI developments\"\n- \"What's the current weather in Tokyo?\"\n- \"Find information about the new EU AI Act and how it affects startups\"\n- \"Search for the latest stock price of NVIDIA\"\n\n**Fetch Examples:**\n- \"Fetch the content from https://example.com/article\"\n- \"Get the content of this blog post: https://blog.example.com/post and make a summary of it\"\n- \"Fetch https://example.com with JavaScript rendering enabled\"\n\n### Search Depths\n\n- **Standard Search**: Best for queries with direct answers (weather, stock prices, simple facts)\n- **Deep Search**: Best for complex research requiring analysis across multiple sources (comprehensive guides, comparative analysis, in-depth research)\n\n## Tools Available\n\n### `linkup-search`\n\nSearch the web in real time using Linkup to retrieve current information, facts, and news from trusted sources.\n\n**Parameters:**\n- `query` (required): Natural language search query. Full questions work best.",
        "start_pos": 3643,
        "end_pos": 5381,
        "token_count_estimate": 434,
        "source_type": "readme",
        "agent_id": "5195375ae53ea296"
      },
      {
        "chunk_id": 3,
        "text": ", stocks, sports scores, events)\n- Breaking news and current events\n- Recent research and publications\n- Product information and up-to-date prices\n- Schedules and availability\n- Any information not available in the AI's knowledge base\n\n### `linkup-fetch`\n\nFetch and extract content from any webpage URL.\n\n**Parameters:**\n- `url` (required): The URL to fetch content from.\n- `renderJs` (optional): Whether to render JavaScript content (default: false). Enable this for dynamic pages that load content via JavaScript. Note: This makes the request slower.\n\n**Use cases:**\n- Retrieve page content for analysis or summarization\n- Extract article content from news sites\n- Get documentation from technical websites\n- Fetch blog posts and written content\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18.0.0\n- npm\n\n### Setup\n\n```bash\n# Install dependencies\nnpm install\n```\n\n### Running with Smithery\n\n```bash\nnpm run dev\n```\n\n### Running with stdio transport\n```bash\nnpm run build:stdio\nnpm run start:stdio apiKey=YOUR_API_KEY\n```\n\n### Testing with MCP Inspector\n\n```bash\nnpm run build:stdio\nnpm run inspector apiKey=YOUR_API_KEY\n```\n\nThis will open the MCP Inspector in your browser where you can test the search tool interactively.\n\n## License\n\nMIT\n\n## Links\n\n- [Linkup Website](https://www.linkup.so/)\n- [Linkup Documentation](https://docs.linkup.so/)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n\n## Support\n\nIf you have issues, contact us via email at [support@linkup.so](mailto:support@linkup.so) or join our [Discord server](https://discord.com/invite/9q9mCYJa86).",
        "start_pos": 5491,
        "end_pos": 7075,
        "token_count_estimate": 395,
        "source_type": "readme",
        "agent_id": "5195375ae53ea296"
      }
    ]
  },
  {
    "agent_id": "1e3a50fe4bf41765",
    "name": "ai.smithery/MetehanGZL-pokemcp",
    "source": "mcp",
    "source_url": "https://github.com/MetehanGZL/PokeMCP",
    "description": "Provide detailed Pok√©mon data and information through a standardized MCP interface. Enable LLMs an‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-15T17:56:16.25232Z",
    "indexed_at": "2026-02-18T04:03:59.574745",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# PokeMCP\r\n# PokeMCP\r\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide detailed Pok√©mon data",
        "Provide Pok√©mon information",
        "Serve data through a standardized MCP interface",
        "Enable large language models (LLMs) to access Pok√©mon data"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence providing a basic overview without examples, structure, or detailed capabilities.",
    "llm_text_source": "description_only",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# PokeMCP\r\n# PokeMCP",
        "start_pos": 0,
        "end_pos": 22,
        "token_count_estimate": 5,
        "source_type": "readme",
        "agent_id": "1e3a50fe4bf41765"
      }
    ]
  },
  {
    "agent_id": "b059b97de3e5c155",
    "name": "ai.smithery/MisterSandFR-supabase-mcp-selfhosted",
    "source": "mcp",
    "source_url": "https://github.com/MisterSandFR/Supabase-MCP-SelfHosted",
    "description": "Query and manage your Supabase database directly from your workspace. Execute SQL statements, brow‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T14:42:06.205285Z",
    "indexed_at": "2026-02-18T04:04:03.262531",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Supabase MCP Server - Self-Hosted Edition\r\n\r\nüóÑÔ∏è **Serveur MCP Supabase Self-Hosted** - Gestion compl√®te de votre instance Supabase priv√©e\r\n\r\n## üåü Fonctionnalit√©s\r\n\r\n- üîê **Gestion compl√®te** de votre instance Supabase priv√©e\r\n- üõ†Ô∏è **54+ outils MCP** pour l'administration Supabase\r\n- üìä **Monitoring** et m√©triques en temps r√©el\r\n- üöÄ **D√©ploiement automatique** sur Railway\r\n- üîí **S√©curit√© renforc√©e** avec pr√©vention SQL injection\r\n- ‚ö° **Performance optimis√©e** pour la production\r\n\r\n## üèóÔ∏è Architecture\r\n\r\nCe repository contient **uniquement** le serveur MCP Supabase pur, sans interface web ni hub central.\r\n\r\n```\r\nSupabase MCP Server (Port 8000)\r\n‚îú‚îÄ‚îÄ üóÑÔ∏è Gestion de base de donn√©es\r\n‚îú‚îÄ‚îÄ üîê Authentification et autorisation\r\n‚îú‚îÄ‚îÄ üìÅ Stockage et fichiers\r\n‚îú‚îÄ‚îÄ üîÑ Temps r√©el et subscriptions\r\n‚îú‚îÄ‚îÄ üõ†Ô∏è Migrations et sch√©mas\r\n‚îú‚îÄ‚îÄ üìä Monitoring et logs\r\n‚îî‚îÄ‚îÄ üöÄ D√©ploiement automatique\r\n```\r\n\r\n## üöÄ D√©marrage Rapide\r\n\r\n### Pr√©requis\r\n- Python 3.11+\r\n- Instance Supabase (self-hosted ou cloud)\r\n- Variables d'environnement Supabase\r\n\r\n### Option A ‚Äî SDK Smithery (recommand√©)\r\n\r\n```bash\r\n# Cloner le repository\r\ngit clone https://github.com/MisterSandFR/Supabase-MCP-SelfHosted.git\r\ncd Supabase-MCP-SelfHosted\r\n\r\n# Installer les d√©pendances\r\npip install -r requirements.txt\r\n\r\n# (Facultatif) Installer la CLI Smithery si besoin\r\nnpm i -g @smithery/cli\r\n\r\n# Lancer le dev SDK (selon votre environnement)\r\nsmithery dev   # ou: smithery playground\r\n```\r\n\r\n- Le serveur SDK est d√©fini dans `pyproject.toml` via:\r\n  - `[tool.smithery] server = \"supabase_mcp_server.server:create_server\"`\r\n- Lors du d√©ploiement dans l‚Äôinterface Smithery, configurez le Test Profile puis lancez le Scan.\r\n\r\n### Option B ‚Äî HTTP self-hosted (compat)\r\n\r\n```bash\r\n# Cloner le repository\r\ngit clone https://github.com/MisterSandFR/Supabase-MCP-SelfHosted.git\r\ncd Supabase-MCP-SelfHosted\r\n\r\n# Installer les d√©pendances Python\r\npip install -r requirements.txt\r\n\r\n# Configurer les variables d'environnement\r\nexport SUPABASE_URL=\"https://your-project.supabase.co\"\r\nexport SUPABASE_ANON_KEY=\"your-anon-key\"\r\nexport SUPABASE_SERVICE_KEY=\"your-service-key\"  # Optionnel\r\n\r\n# D√©marrer le serveur HTTP externe\r\npython src/supabase_server.py\r\n```\r\n\r\n### Avec Docker (Railway / self-hosted)\r\n\r\n```bash\r\n# Build et d√©marrage (utilisez Dockerfile.railway si besoin)\r\ndocker build -f Dockerfile.railway -t supabase-mcp-server .\r\ndocker run -p 8000:8000 \\\r\n  -e SUPABASE_URL=\"https://your-project.supabase.co\" \\\r\n  -e SUPABASE_ANON_KEY=\"your-anon-key\" \\\r\n  supabase-mcp-server\r\n```\r\n\r\n## ‚öôÔ∏è Configuration\r\n\r\n### Variables d'Environnement\r\n\r\n```bash\r\n# Supabase Configuration\r\nSUPABASE_URL=https://your-project.supabase.co\r\nSUPABASE_ANON_KEY=eyJ... (votre cl√© anonyme)\r\nSUPABASE_SERVICE_KEY=eyJ... (optionnel, pour op√©rations privil√©gi√©es)\r\n\r\n# Server Configuration\r\nPORT=8000\r\nPYTHONUNBUFFERED=1\r\n```\r\n\r\n## üõ†Ô∏è Outils MCP Disponibles\r\n\r\n### Base de Donn√©es (15 outils)\r\n- `execute_sql` - Ex√©cution de requ√™tes SQL\r\n- `list_tables` - Liste des tables\r\n- `inspect_schema` - Inspection du sch√©ma\r\n- `apply_migration` - Application de migrations\r\n- `backup_database` - Sauvegarde de base\r\n- `restore_database` - Restauration de base\r\n- `vacuum_analyze` - Optimisation de base\r\n- `get_database_stats` - Statistiques de base\r\n- `create_index` - Cr√©ation d'index\r\n- `drop_index` - Suppression d'index\r\n- `list_extensions` - Liste des extensions\r\n- `manage_extensions` - Gestion des extensions\r\n- `execute_psql` - Commandes psql\r\n- `check_health` - V√©rification de sant√©\r\n- `get_database_connections` - Connexions de base\r\n\r\n### Authentification (8 outils)\r\n- `list_auth_users` - Liste des utilisateurs\r\n- `create_auth_user` - Cr√©ation d'utilisateur\r\n- `update_auth_user` - Mise √† jour d'utilisateur\r\n- `delete_auth_user` - Suppression d'utilisateur\r\n- `get_auth_user` - R√©cup√©ration d'utilisateur\r\n- `verify_jwt_secret` - V√©rification JWT\r\n- `manage_roles` - Gestion des r√¥les\r\n- `manage_rls_policies` - Gestion des politiques RLS\r\n\r\n### Stockage (6 outils)\r\n- `list_storage_buckets` - Liste des buckets\r\n- `list_storage_objects` - Liste des objets\r\n- `manage_storage_policies` - Gestion des politiques\r\n- `upload_file` - Upload de fichier\r\n- `download_file` - T√©l√©chargement de fichier\r\n- `delete_file` - Suppression de fichier\r\n\r\n### Temps R√©el (4 outils)\r\n- `list_realtime_publications` - Liste des publications\r\n- `manage_realtime` - Gestion du temps r√©el\r\n- `create_subscription` - Cr√©ation de subscription\r\n- `delete_subscription` - Suppression de subscription\r\n\r\n### Migrations (8 outils)\r\n- `create_migration` - Cr√©ation de migration\r\n- `list_migrations` - Liste des migrations\r\n- `push_migrations` - Push des migrations\r\n- `validate_migration` - Validation de migration\r\n- `smart_migration` - Migration intelligente\r\n- `auto_migrate` - Migration automatique\r\n- `sync_schema` - Synchronisation de sch√©ma\r\n- `import_schema` - Import de sch√©ma\r\n\r\n### Monitoring (5 outils)\r\n- `get_logs` - R√©cup√©ration des logs\r\n- `metrics_dashboard` - Tableau de bord m√©triques\r\n- `analyze_performance` - Analyse de performance\r\n- `analyze_rls_coverage` - Analyse couverture RLS\r\n- `audit_security` - Audit de s√©curit√©\r\n\r\n### Utilitaires (8 outils)\r\n- `generate_typescript_types` - G√©n√©ration de types TS\r\n- `generate_crud_api` - G√©n√©ration d'API CRUD\r\n- `cache_management` - Gestion du cache\r\n- `environment_management` - Gestion d'environnement\r\n- `manage_secrets` - Gestion des secrets\r\n- `manage_functions` - Gestion des fonctions\r\n- `manage_triggers` - Gestion des triggers\r\n- `manage_webhooks` - Gestion des webhooks\r\n\r\n## üîß API Endpoints\r\n\r\n### Serveur MCP (HTTP self-hosted)\r\n- `GET /health` - Health check\r\n- `POST /mcp` - Endpoint JSON-RPC principal\r\n- `GET /.well-known/mcp-config` - Configuration MCP\r\n- `GET /mcp/tools.json` - D√©couverte des outils (JSON)\r\n\r\n### Outils Sp√©cialis√©s\r\n- `GET /api/tools` - Liste des outils disponibles\r\n- `POST /api/execute` - Ex√©cution d'outils\r\n\r\n## üöÄ D√©ploiement\r\n\r\n### Smithery (SDK Python) ‚Äî Recommand√©\r\n1. V√©rifiez que `pyproject.toml` contient:\r\n   - `[tool.smithery] server = \"supabase_mcp_server.server:create_server\"`\r\n2. Dans Smithery ‚Üí Deploy, s√©lectionnez SDK Python\r\n3. Configurez le Test Profile puis lancez le Scan\r\n   - Requis: `SUPABASE_URL`, `SUPABASE_ANON_KEY`\r\n\r\n### Railway (Self-hosted)\r\n```bash\r\n# D√©ployer sur Railway\r\nrailway login\r\nrailway init\r\nrailway up\r\n```\r\n\r\n### Docker\r\n```bash\r\n# Build et d√©ploiement\r\ndocker build -t supabase-mcp-server .\r\ndocker run -p 8000:8000 supabase-mcp-server\r\n```\r\n\r\n### Int√©gration avec Hub Central\r\n\r\nCe serveur est con√ßu pour √™tre int√©gr√© avec le [MCP Hub Central](https://github.com/coupaul/mcp-hub-central) :\r\n\r\n```json\r\n{\r\n  \"servers\": {\r\n    \"supabase\": {\r\n      \"name\": \"Supabase MCP Server\",\r\n      \"host\": \"supabase.mcp.coupaul.fr\",\r\n      \"port\": 8000,\r\n      \"path\": \"/\",\r\n      \"categories\": [\"database\", \"auth\", \"storage\", \"realtime\", \"security\", \"migration\", \"monitoring\", \"performance\"]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## üß™ Test Profile (Smithery)\r\n\r\nLors de la connexion c√¥t√© Smithery, fournissez les cl√©s suivantes dans le Test Profile:\r\n\r\n```json\r\n{\r\n  \"SUPABASE_URL\": \"https://your-project.supabase.co\",\r\n  \"SUPABASE_ANON_KEY\": \"eyJ...\"\r\n}\r\n```\r\n\r\nEn cas de cache, re-sauvegardez le profil et relancez le Scan.\r\n\r\n## üîí S√©curit√©\r\n\r\n- **Validation des entr√©es** pour pr√©venir les injections SQL\r\n- **Rate limiting** par IP et utilisateur\r\n- **Audit logs** de toutes les op√©rations\r\n- **Chiffrement HTTPS** obligatoire en production\r\n- **Gestion des secrets** s√©curis√©e\r\n- **Politiques RLS** pour la s√©curit√© des donn√©es\r\n\r\n## üìä Monitoring\r\n\r\nLe serveur fournit un monitoring complet :\r\n\r\n- **M√©triques de performance** en temps r√©el\r\n- **Logs structur√©s** avec niveaux configurables\r\n- **Health checks** automatiques\r\n- **Alertes** en cas de probl√®me\r\n- **Tableau de bord** m√©triques\r\n- **Analyse de performance** d√©taill√©e\r\n\r\n## ü§ù Contribution\r\n\r\n1. Fork le repository\r\n2. Cr√©er une branche feature (`git checkout -b feature/amazing-feature`)\r\n3. Commit vos changements (`git commit -m 'Add amazing feature'`)\r\n4. Push vers la branche (`git push origin feature/amazing-feature`)\r\n5. Ouvrir une Pull Request\r\n\r\n## üìÑ Licence\r\n\r\nCe projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.\r\n\r\n## üôè Remerciements\r\n\r\n- [Supabase](https://supabase.com/) pour la plateforme\r\n- [Smithery](https://smithery.ai) pour l'√©cosyst√®me MCP\r\n- La communaut√© Supabase pour les contributions\r\n\r\n## üìû Support\r\n\r\n- üìß Email : contact@coupaul.fr\r\n- üí¨ Discord : [Serveur MCP Community](https://discord.gg/mcp)\r\n- üêõ Issues : [GitHub Issues](https://github.com/MisterSandFR/Supabase-MCP-SelfHosted/issues)\r\n\r\n---\r\n\r\n**Fait avec ‚ù§Ô∏è par [coupaul](https://github.com/coupaul)**"
    },
    "llm_extracted": {
      "capabilities": [
        "Manage a private Supabase instance comprehensively",
        "Execute over 54 MCP tools for Supabase administration",
        "Perform real-time monitoring and metrics collection",
        "Deploy automatically on Railway platform",
        "Prevent SQL injection through input validation",
        "Optimize performance for production environments",
        "Execute SQL queries and manage database schema and migrations",
        "Manage authentication users, roles, and RLS policies",
        "Handle storage buckets and files including upload and download",
        "Manage real-time subscriptions and publications",
        "Generate TypeScript types and CRUD APIs",
        "Provide detailed audit logs and security audits"
      ],
      "limitations": [
        "No web interface or central hub included in this repository",
        "Requires external MCP Hub Central for integration",
        "Rate limiting applies per IP and user",
        "HTTPS encryption mandatory in production",
        "Service key usage is optional but recommended for privileged operations"
      ],
      "requirements": [
        "Python 3.11 or higher",
        "Supabase instance (self-hosted or cloud)",
        "Supabase environment variables: SUPABASE_URL, SUPABASE_ANON_KEY, optionally SUPABASE_SERVICE_KEY",
        "Smithery CLI for SDK usage (optional)",
        "Docker for containerized deployment (optional)",
        "Railway account for deployment (optional)"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, extensive tool descriptions, explicit limitations, and clear environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Supabase MCP Server - Self-Hosted Edition\r\n\r\nüóÑÔ∏è **Serveur MCP Supabase Self-Hosted** - Gestion compl√®te de votre instance Supabase priv√©e\r\n\r\n## üåü Fonctionnalit√©s\r\n\r\n- üîê **Gestion compl√®te** de votre instance Supabase priv√©e\r\n- üõ†Ô∏è **54+ outils MCP** pour l'administration Supabase\r\n- üìä **Monitoring** et m√©triques en temps r√©el\r\n- üöÄ **D√©ploiement automatique** sur Railway\r\n- üîí **S√©curit√© renforc√©e** avec pr√©vention SQL injection\r\n- ‚ö° **Performance optimis√©e** pour la production\r\n\r\n## üèóÔ∏è Architecture\r\n\r\nCe repository contient **uniquement** le serveur MCP Supabase pur, sans interface web ni hub central.\r\n\r\n```\r\nSupabase MCP Server (Port 8000)\r\n‚îú‚îÄ‚îÄ üóÑÔ∏è Gestion de base de donn√©es\r\n‚îú‚îÄ‚îÄ üîê Authentification et autorisation\r\n‚îú‚îÄ‚îÄ üìÅ Stockage et fichiers\r\n‚îú‚îÄ‚îÄ üîÑ Temps r√©el et subscriptions\r\n‚îú‚îÄ‚îÄ üõ†Ô∏è Migrations et sch√©mas\r\n‚îú‚îÄ‚îÄ üìä Monitoring et logs\r\n‚îî‚îÄ‚îÄ üöÄ D√©ploiement automatique\r\n```\r\n\r\n## üöÄ D√©marrage Rapide\r\n\r\n### Pr√©requis\r\n- Python 3.11+\r\n- Instance Supabase (self-hosted ou cloud)\r\n- Variables d'environnement Supabase\r\n\r\n### Option A ‚Äî SDK Smithery (recommand√©)\r\n\r\n```bash\r\n# Cloner le repository\r\ngit clone https://github.com/MisterSandFR/Supabase-MCP-SelfHosted.git\r\ncd Supabase-MCP-SelfHosted\r\n\r\n# Installer les d√©pendances\r\npip install -r requirements.txt\r\n\r\n# (Facultatif) Installer la CLI Smithery si besoin\r\nnpm i -g @smithery/cli\r\n\r\n# Lancer le dev SDK (selon votre environnement)\r\nsmithery dev   # ou: smithery playground\r\n```\r\n\r\n- Le serveur SDK est d√©fini dans `pyproject.toml` via:\r\n  - `[tool.smithery] server = \"supabase_mcp_server.server:create_server\"`\r\n- Lors du d√©ploiement dans l‚Äôinterface Smithery, configurez le Test Profile puis lancez le Scan.\r\n\r\n### Option B ‚Äî HTTP self-hosted (compat)\r\n\r\n```bash\r\n# Cloner le repository\r\ngit clone https://github.com/MisterSandFR/Supabase-MCP-SelfHosted.git\r\ncd Supabase-MCP-SelfHosted\r\n\r\n# Installer les d√©pendances Python\r\npip install -r requirements.txt\r\n\r\n# Configurer les variables d'environnement\r\nexport SUPABASE_URL=\"https://your-project.supabase.co\"\r\nexport SUPABASE_ANON_KEY=\"yo",
        "start_pos": 0,
        "end_pos": 2048,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "b059b97de3e5c155"
      },
      {
        "chunk_id": 1,
        "text": "# Installer les d√©pendances Python\r\npip install -r requirements.txt\r\n\r\n# Configurer les variables d'environnement\r\nexport SUPABASE_URL=\"https://your-project.supabase.co\"\r\nexport SUPABASE_ANON_KEY=\"your-anon-key\"\r\nexport SUPABASE_SERVICE_KEY=\"your-service-key\"  # Optionnel\r\n\r\n# D√©marrer le serveur HTTP externe\r\npython src/supabase_server.py\r\n```\r\n\r\n### Avec Docker (Railway / self-hosted)\r\n\r\n```bash\r\n# Build et d√©marrage (utilisez Dockerfile.railway si besoin)\r\ndocker build -f Dockerfile.railway -t supabase-mcp-server .\r\ndocker run -p 8000:8000 \\\r\n  -e SUPABASE_URL=\"https://your-project.supabase.co\" \\\r\n  -e SUPABASE_ANON_KEY=\"your-anon-key\" \\\r\n  supabase-mcp-server\r\n```\r\n\r\n## ‚öôÔ∏è Configuration\r\n\r\n### Variables d'Environnement\r\n\r\n```bash\r\n# Supabase Configuration\r\nSUPABASE_URL=https://your-project.supabase.co\r\nSUPABASE_ANON_KEY=eyJ... (votre cl√© anonyme)\r\nSUPABASE_SERVICE_KEY=eyJ...",
        "start_pos": 1848,
        "end_pos": 2740,
        "token_count_estimate": 222,
        "source_type": "readme",
        "agent_id": "b059b97de3e5c155"
      },
      {
        "chunk_id": 2,
        "text": "auth_user` - Mise √† jour d'utilisateur\r\n- `delete_auth_user` - Suppression d'utilisateur\r\n- `get_auth_user` - R√©cup√©ration d'utilisateur\r\n- `verify_jwt_secret` - V√©rification JWT\r\n- `manage_roles` - Gestion des r√¥les\r\n- `manage_rls_policies` - Gestion des politiques RLS\r\n\r\n### Stockage (6 outils)\r\n- `list_storage_buckets` - Liste des buckets\r\n- `list_storage_objects` - Liste des objets\r\n- `manage_storage_policies` - Gestion des politiques\r\n- `upload_file` - Upload de fichier\r\n- `download_file` - T√©l√©chargement de fichier\r\n- `delete_file` - Suppression de fichier\r\n\r\n### Temps R√©el (4 outils)\r\n- `list_realtime_publications` - Liste des publications\r\n- `manage_realtime` - Gestion du temps r√©el\r\n- `create_subscription` - Cr√©ation de subscription\r\n- `delete_subscription` - Suppression de subscription\r\n\r\n### Migrations (8 outils)\r\n- `create_migration` - Cr√©ation de migration\r\n- `list_migrations` - Liste des migrations\r\n- `push_migrations` - Push des migrations\r\n- `validate_migration` - Validation de migration\r\n- `smart_migration` - Migration intelligente\r\n- `auto_migrate` - Migration automatique\r\n- `sync_schema` - Synchronisation de sch√©ma\r\n- `import_schema` - Import de sch√©ma\r\n\r\n### Monitoring (5 outils)\r\n- `get_logs` - R√©cup√©ration des logs\r\n- `metrics_dashboard` - Tableau de bord m√©triques\r\n- `analyze_performance` - Analyse de performance\r\n- `analyze_rls_coverage` - Analyse couverture RLS\r\n- `audit_security` - Audit de s√©curit√©\r\n\r\n### Utilitaires (8 outils)\r\n- `generate_typescript_types` - G√©n√©ration de types TS\r\n- `generate_crud_api` - G√©n√©ration d'API CRUD\r\n- `cache_management` - Gestion du cache\r\n- `environment_management` - Gestion d'environnement\r\n- `manage_secrets` - Gestion des secrets\r\n- `manage_functions` - Gestion des fonctions\r\n- `manage_triggers` - Gestion des triggers\r\n- `manage_webhooks` - Gestion des webhooks\r\n\r\n## üîß API Endpoints\r\n\r\n### Serveur MCP (HTTP self-hosted)\r\n- `GET /health` - Health check\r\n- `POST /mcp` - Endpoint JSON-RPC principal\r\n- `GET /.well-known/mcp-config` - Configuration MCP\r\n- `G",
        "start_pos": 3696,
        "end_pos": 5744,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "b059b97de3e5c155"
      },
      {
        "chunk_id": 3,
        "text": "ooks\r\n\r\n## üîß API Endpoints\r\n\r\n### Serveur MCP (HTTP self-hosted)\r\n- `GET /health` - Health check\r\n- `POST /mcp` - Endpoint JSON-RPC principal\r\n- `GET /.well-known/mcp-config` - Configuration MCP\r\n- `GET /mcp/tools.json` - D√©couverte des outils (JSON)\r\n\r\n### Outils Sp√©cialis√©s\r\n- `GET /api/tools` - Liste des outils disponibles\r\n- `POST /api/execute` - Ex√©cution d'outils\r\n\r\n## üöÄ D√©ploiement\r\n\r\n### Smithery (SDK Python) ‚Äî Recommand√©\r\n1. V√©rifiez que `pyproject.toml` contient:\r\n   - `[tool.smithery] server = \"supabase_mcp_server.server:create_server\"`\r\n2. Dans Smithery ‚Üí Deploy, s√©lectionnez SDK Python\r\n3.",
        "start_pos": 5544,
        "end_pos": 6153,
        "token_count_estimate": 152,
        "source_type": "readme",
        "agent_id": "b059b97de3e5c155"
      },
      {
        "chunk_id": 4,
        "text": "s** de toutes les op√©rations\r\n- **Chiffrement HTTPS** obligatoire en production\r\n- **Gestion des secrets** s√©curis√©e\r\n- **Politiques RLS** pour la s√©curit√© des donn√©es\r\n\r\n## üìä Monitoring\r\n\r\nLe serveur fournit un monitoring complet :\r\n\r\n- **M√©triques de performance** en temps r√©el\r\n- **Logs structur√©s** avec niveaux configurables\r\n- **Health checks** automatiques\r\n- **Alertes** en cas de probl√®me\r\n- **Tableau de bord** m√©triques\r\n- **Analyse de performance** d√©taill√©e\r\n\r\n## ü§ù Contribution\r\n\r\n1. Fork le repository\r\n2. Cr√©er une branche feature (`git checkout -b feature/amazing-feature`)\r\n3. Commit vos changements (`git commit -m 'Add amazing feature'`)\r\n4. Push vers la branche (`git push origin feature/amazing-feature`)\r\n5. Ouvrir une Pull Request\r\n\r\n## üìÑ Licence\r\n\r\nCe projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.\r\n\r\n## üôè Remerciements\r\n\r\n- [Supabase](https://supabase.com/) pour la plateforme\r\n- [Smithery](https://smithery.ai) pour l'√©cosyst√®me MCP\r\n- La communaut√© Supabase pour les contributions\r\n\r\n## üìû Support\r\n\r\n- üìß Email : contact@coupaul.fr\r\n- üí¨ Discord : [Serveur MCP Community](https://discord.gg/mcp)\r\n- üêõ Issues : [GitHub Issues](https://github.com/MisterSandFR/Supabase-MCP-SelfHosted/issues)\r\n\r\n---\r\n\r\n**Fait avec ‚ù§Ô∏è par [coupaul](https://github.com/coupaul)**",
        "start_pos": 7392,
        "end_pos": 8717,
        "token_count_estimate": 331,
        "source_type": "readme",
        "agent_id": "b059b97de3e5c155"
      }
    ]
  },
  {
    "agent_id": "1ee7750859038dd6",
    "name": "ai.smithery/Nekzus-npm-sentinel-mcp",
    "source": "mcp",
    "source_url": "https://github.com/Nekzus/npm-sentinel-mcp",
    "description": "Provide AI-powered real-time analysis and intelligence on NPM packages, including security, depend‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T21:32:48.552Z",
    "indexed_at": "2026-02-18T04:04:04.544563",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# NPM Sentinel MCP\n\n<div align=\"center\">\n\n[![smithery badge](https://smithery.ai/badge/@Nekzus/npm-sentinel-mcp)](https://smithery.ai/server/@Nekzus/npm-sentinel-mcp)\n[![Github Workflow](https://github.com/nekzus/NPM-Sentinel-MCP/actions/workflows/publish.yml/badge.svg?event=push)](https://github.com/Nekzus/npm-sentinel-mcp/actions/workflows/publish.yml)\n[![npm version](https://img.shields.io/npm/v/@nekzus/mcp-server.svg)](https://www.npmjs.com/package/@nekzus/mcp-server)\n[![npm-month](https://img.shields.io/npm/dm/@nekzus/mcp-server.svg)](https://www.npmjs.com/package/@nekzus/mcp-server)\n[![npm-total](https://img.shields.io/npm/dt/@nekzus/mcp-server.svg?style=flat)](https://www.npmjs.com/package/@nekzus/mcp-server)\n[![Docker Hub](https://img.shields.io/docker/pulls/mcp/npm-sentinel.svg?label=Docker%20Hub)](https://hub.docker.com/r/mcp/npm-sentinel)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Nekzus/npm-sentinel-mcp)\n[![Donate](https://img.shields.io/badge/donate-paypal-blue.svg?style=flat-square)](https://paypal.me/maseortega)\n\n</div>\n\nA powerful Model Context Protocol (MCP) server that revolutionizes NPM package analysis through AI. Built to integrate with Claude and Anthropic AI, it provides real-time intelligence on package security, dependencies, and performance. This MCP server delivers instant insights and smart analysis to safeguard and optimize your npm ecosystem, making package management decisions faster and safer for modern development workflows.\n\n## Features\n\n- **Version analysis and tracking**\n- **Dependency analysis and mapping**\n- **Advanced Security Scanning**: Recursive dependency checks, ecosystem awareness (e.g., React), and accurate version resolution.\n- **Strict Input Validation**: Protection against Path Traversal, SSRF, and Command Injection via rigorous input sanitization.\n- **Package quality metrics**\n- **Download trends and statistics**\n- **TypeScript support verification**\n- **Package size analysis**\n- **Maintenance metrics**\n- **Real-time package comparisons**\n- **Standardized error handling and MCP response formats**\n- **Efficient caching for improved performance and API rate limit management**\n- **Rigorous schema validation and type safety using Zod**\n\nNote: The server provides AI-assisted analysis through MCP integration.\n\n## Caching and Invalidation\n\nTo ensure data accuracy while maintaining performance, the server implements robust caching strategies:\n- **Automatic Invalidation**: The cache is automatically invalidated whenever `pnpm-lock.yaml`, `package-lock.json`, or `yarn.lock` changes in your workspace. This ensures you always get fresh data after installing or updating dependencies.\n- **Force Refresh**: All tools accept an optional `ignoreCache: true` parameter to bypass the cache and force a fresh lookup from the registry.\n\n### Example Usage (JSON-RPC)\n\nWhen calling a tool, simply include `ignoreCache: true` in the arguments:\n\n```json\n{\n  \"name\": \"npmVersions\",\n  \"arguments\": {\n    \"packages\": [\"react\"],\n    \"ignoreCache\": true\n  }\n}\n```\n\n## Installation\n\n### Migration to HTTP Streamable\n\nThis MCP server now supports both STDIO and HTTP streamable transport. Your existing STDIO configuration will continue to work without changes.\n\n**New capabilities:**\n- HTTP streamable transport via Smithery.ai\n- Enhanced scalability and performance\n- Interactive testing playground\n\n**Development commands:**\n```bash\n# Development server with playground\nnpm run dev\n\n# Build for HTTP\nnpm run build:http\n\n# Start HTTP server\nnpm run start:http\n```\n\n### Install in VS Code\n\n[<img alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20NPM%20Sentinel%20MCP&color=0098FF\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522npm-sentinel%2522%252C%2522config%2522%253A%257B%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522%2540nekzus%252Fmcp-server%2540latest%2522%255D%257D%257D)\n[<img alt=\"Install in VS Code Insiders (npx)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20NPM%20Sentinel%20MCP&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522npm-sentinel%2522%252C%2522config%2522%253A%257B%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522%2540nekzus%252Fmcp-server%2540latest%2522%255D%257D%257D)\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n```json\n{\n  \"servers\": {\n    \"npm-sentinel\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@nekzus/mcp-server@latest\"]\n    }\n  }\n}\n```\n\n### Smithery.ai Deployment (HTTP Streamable)\n\nThis MCP server now supports HTTP streamable transport through Smithery.ai for enhanced scalability and performance. You can deploy it directly on Smithery.ai:\n**Benefits of HTTP deployment:**\n- **Scalable**: Handles multiple concurrent connections\n- **Streamable**: Real-time streaming responses\n- **Managed**: Automatic deployment and monitoring\n- **Backward Compatible**: Still supports STDIO for local development\n- **Interactive Testing**: Built-in playground for testing tools\n\n**Configuration for Smithery.ai:**\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel\": {\n      \"type\": \"http\",\n      \"url\": \"https://smithery.ai/server/@Nekzus/npm-sentinel-mcp\"\n    }\n  }\n}\n```\n\n### Configuration\n\nThe server supports the following configuration options:\n\n| Environment Variable | CLI Argument | Default | Description |\n| -------------------- | ------------ | ------- | ----------- |\n| `NPM_REGISTRY_URL` | `config.NPM_REGISTRY_URL` | `https://registry.npmjs.org` | URL of the NPM registry to use for all requests |\n\n#### HTTP Deployment (Smithery/Docker)\n\nWhen deploying via Smithery or Docker, you can configure these options in your configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel\": {\n      \"type\": \"http\",\n      \"url\": \"https://smithery.ai/server/@Nekzus/npm-sentinel-mcp\",\n      \"config\": {\n        \"NPM_REGISTRY_URL\": \"https://registry.npmjs.org\"\n      }\n    }\n  }\n}\n```\n### Docker\n\n#### Build\n```bash\n# Build the Docker image\ndocker build -t nekzus/npm-sentinel-mcp .\n```\n\n#### Usage\n\nYou can run the MCP server using Docker with directory mounting to `/projects`:\n\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-w\", \"/projects\",\n        \"--mount\", \"type=bind,src=${PWD},dst=/projects\",\n        \"nekzus/npm-sentinel-mcp\",\n        \"node\",\n        \"dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nFor multiple directories:\n\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-w\", \"/projects\",\n        \"--mount\", \"type=bind,src=/path/to/workspace,dst=/projects/workspace\",\n        \"--mount\", \"type=bind,src=/path/to/other/dir,dst=/projects/other/dir,ro\",\n        \"nekzus/npm-sentinel-mcp\",\n        \"node\",\n        \"dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nNote: All mounted directories must be under `/projects` for proper access.\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"npmsentinel\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@nekzus/mcp-server@latest\"]\n    }\n  }\n}\n```\n\nConfiguration file locations:\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Linux: (Claude for Desktop does not officially support Linux at this time)\n\n### NPX\n\n<!-- [![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.png)](cursor://anysphere.cursor-deeplink/mcp/install?name=npm-sentinel-mcp&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBuZWt6dXMvbWNwLXNlcnZlckBsYXRlc3QiXX0=) -->\n\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@nekzus/mcp-server@latest\"\n      ]\n    }\n  }\n}\n```\n\n## API\n\nThe server exposes its tools via the Model Context Protocol. All tools adhere to a standardized response format:\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"string\",\n      \"isError\": boolean // Optional\n    }\n    // ... more content items if necessary\n  ]\n}\n```\n\n### Resources\n\n- `npm://registry`: NPM Registry interface\n- `npm://security`: Security analysis interface\n- `npm://metrics`: Package metrics interface\n\n### Server Resources\n\nThe server also provides the following informational resources accessible via MCP `GetResource` requests:\n\n- `doc://server/readme`:\n  - **Description**: Retrieves the main `README.md` file content for this NPM Sentinel MCP server.\n  - **MIME Type**: `text/markdown`\n- `doc://mcp/specification`:\n  - **Description**: Retrieves the `llms-full.txt` content, providing the comprehensive Model Context Protocol specification.\n  - **MIME Type**: `text/plain`\n\n### Tools\n\n#### npmVersions\n- Get all versions of a package\n- Input: `packages` (string[])\n- Returns: Version history with release dates\n\n#### npmLatest\n- Get latest version information\n- Input: `packages` (string[])\n- Returns: Latest version details and changelog\n\n#### npmDeps\n- Analyze package dependencies\n- Input: `packages` (string[])\n- Returns: Complete dependency tree analysis\n\n#### npmTypes\n- Check TypeScript support\n- Input: `packages` (string[])\n- Returns: TypeScript compatibility status\n\n#### npmSize\n- Analyze package size\n- Input: `packages` (string[])\n- Returns: Bundle size and import cost analysis\n\n#### npmVulnerabilities\n- Scan for security vulnerabilities\n- Features: \n  - **Transitive Scanning**: Checks dependencies up to depth 2.\n  - **Ecosystem Awareness**: Automatically scans related packages (e.g., React Server Components).\n  - **Rich Reports**: Includes CVE IDs and full summaries.\n- Input: `packages` (string[])\n- Returns: Detailed security advisories, CVEs, and severity ratings\n\n#### npmTrends\n- Get download trends\n- Input:\n  - `packages` (string[])\n  - `period` (\"last-week\" | \"last-month\" | \"last-year\")\n- Returns: Download statistics over time\n\n#### npmCompare\n- Compare multiple packages\n- Input: `packages` (string[])\n- Returns: Detailed comparison metrics\n\n#### npmMaintainers\n- Get package maintainers\n- Input: `packages` (string[])\n- Returns: Maintainer information and activity\n\n#### npmScore\n- Get package quality score\n- Input: `packages` (string[])\n- Returns: Comprehensive quality metrics\n\n#### npmPackageReadme\n- Get package README\n- Input: `packages` (string[])\n- Returns: Formatted README content\n\n#### npmSearch\n- Search for packages\n- Input:\n  - `query` (string)\n  - `limit` (number, optional)\n- Returns: Matching packages with metadata\n\n#### npmLicenseCompatibility\n- Check license compatibility\n- Input: `packages` (string[])\n- Returns: License analysis and compatibility info\n\n#### npmRepoStats\n- Get repository statistics\n- Input: `packages` (string[])\n- Returns: GitHub/repository metrics\n\n#### npmDeprecated\n- Check for deprecation\n- Input: `packages` (string[])\n- Returns: Deprecation status and alternatives\n\n#### npmChangelogAnalysis\n- Analyze package changelogs\n- Input: `packages` (string[])\n- Returns: Changelog summaries and impact analysis\n\n#### npmAlternatives\n- Find package alternatives\n- Input: `packages` (string[])\n- Returns: Similar packages with comparisons\n\n#### npmQuality\n- Assess package quality\n- Input: `packages` (string[])\n- Returns: Quality metrics and scores\n\n#### npmMaintenance\n- Check maintenance status\n- Input: `packages` (string[])\n- Returns: Maintenance activity metrics\n\n## Build\n\n```bash\n# Install dependencies\nnpm install\n\n# Build for STDIO (traditional)\nnpm run build:stdio\n\n# Build for HTTP (Smithery)\nnpm run build:http\n\n# Development server\nnpm run dev\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n---\n\nMIT ¬© [nekzus](https://github.com/nekzus)"
    },
    "llm_extracted": {
      "capabilities": [
        "Analyze NPM package versions and track version history",
        "Map and analyze package dependencies including recursive checks",
        "Perform advanced security scanning with transitive vulnerability detection and ecosystem awareness",
        "Validate inputs strictly to prevent security issues like Path Traversal, SSRF, and Command Injection",
        "Provide package quality metrics and scores",
        "Retrieve download trends and statistics over configurable periods",
        "Verify TypeScript support for packages",
        "Analyze package size including bundle size and import cost",
        "Compare multiple packages with detailed metrics",
        "Expose standardized MCP tools and resources with efficient caching and schema validation"
      ],
      "limitations": [
        "Security scanning depth limited to dependency depth 2",
        "Linux is not officially supported for Claude Desktop integration",
        "All mounted directories in Docker must be under /projects for access"
      ],
      "requirements": [
        "Node.js environment to run via npx or locally",
        "Optional Docker for containerized deployment",
        "Access to NPM registry (default https://registry.npmjs.org) or configurable alternative",
        "Claude or Anthropic AI integration for AI-assisted analysis",
        "VS Code MCP configuration or Smithery.ai deployment for HTTP streamable transport"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with inputs and outputs, usage examples, configuration options, deployment methods, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# NPM Sentinel MCP\n\n<div align=\"center\">\n\n[![smithery badge](https://smithery.ai/badge/@Nekzus/npm-sentinel-mcp)](https://smithery.ai/server/@Nekzus/npm-sentinel-mcp)\n[![Github Workflow](https://github.com/nekzus/NPM-Sentinel-MCP/actions/workflows/publish.yml/badge.svg?event=push)](https://github.com/Nekzus/npm-sentinel-mcp/actions/workflows/publish.yml)\n[![npm version](https://img.shields.io/npm/v/@nekzus/mcp-server.svg)](https://www.npmjs.com/package/@nekzus/mcp-server)\n[![npm-month](https://img.shields.io/npm/dm/@nekzus/mcp-server.svg)](https://www.npmjs.com/package/@nekzus/mcp-server)\n[![npm-total](https://img.shields.io/npm/dt/@nekzus/mcp-server.svg?style=flat)](https://www.npmjs.com/package/@nekzus/mcp-server)\n[![Docker Hub](https://img.shields.io/docker/pulls/mcp/npm-sentinel.svg?label=Docker%20Hub)](https://hub.docker.com/r/mcp/npm-sentinel)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Nekzus/npm-sentinel-mcp)\n[![Donate](https://img.shields.io/badge/donate-paypal-blue.svg?style=flat-square)](https://paypal.me/maseortega)\n\n</div>\n\nA powerful Model Context Protocol (MCP) server that revolutionizes NPM package analysis through AI. Built to integrate with Claude and Anthropic AI, it provides real-time intelligence on package security, dependencies, and performance. This MCP server delivers instant insights and smart analysis to safeguard and optimize your npm ecosystem, making package management decisions faster and safer for modern development workflows.\n\n## Features\n\n- **Version analysis and tracking**\n- **Dependency analysis and mapping**\n- **Advanced Security Scanning**: Recursive dependency checks, ecosystem awareness (e.g., React), and accurate version resolution.\n- **Strict Input Validation**: Protection against Path Traversal, SSRF, and Command Injection via rigorous input sanitization.",
        "start_pos": 0,
        "end_pos": 1855,
        "token_count_estimate": 463,
        "source_type": "readme",
        "agent_id": "1ee7750859038dd6"
      },
      {
        "chunk_id": 1,
        "text": "ecks, ecosystem awareness (e.g., React), and accurate version resolution.\n- **Strict Input Validation**: Protection against Path Traversal, SSRF, and Command Injection via rigorous input sanitization.\n- **Package quality metrics**\n- **Download trends and statistics**\n- **TypeScript support verification**\n- **Package size analysis**\n- **Maintenance metrics**\n- **Real-time package comparisons**\n- **Standardized error handling and MCP response formats**\n- **Efficient caching for improved performance and API rate limit management**\n- **Rigorous schema validation and type safety using Zod**\n\nNote: The server provides AI-assisted analysis through MCP integration.\n\n## Caching and Invalidation\n\nTo ensure data accuracy while maintaining performance, the server implements robust caching strategies:\n- **Automatic Invalidation**: The cache is automatically invalidated whenever `pnpm-lock.yaml`, `package-lock.json`, or `yarn.lock` changes in your workspace. This ensures you always get fresh data after installing or updating dependencies.\n- **Force Refresh**: All tools accept an optional `ignoreCache: true` parameter to bypass the cache and force a fresh lookup from the registry.\n\n### Example Usage (JSON-RPC)\n\nWhen calling a tool, simply include `ignoreCache: true` in the arguments:\n\n```json\n{\n  \"name\": \"npmVersions\",\n  \"arguments\": {\n    \"packages\": [\"react\"],\n    \"ignoreCache\": true\n  }\n}\n```\n\n## Installation\n\n### Migration to HTTP Streamable\n\nThis MCP server now supports both STDIO and HTTP streamable transport. Your existing STDIO configuration will continue to work without changes.",
        "start_pos": 1655,
        "end_pos": 3254,
        "token_count_estimate": 399,
        "source_type": "readme",
        "agent_id": "1ee7750859038dd6"
      },
      {
        "chunk_id": 2,
        "text": "build:http\n\n# Start HTTP server\nnpm run start:http\n```\n\n### Install in VS Code\n\n[<img alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20NPM%20Sentinel%20MCP&color=0098FF\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522npm-sentinel%2522%252C%2522config%2522%253A%257B%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522%2540nekzus%252Fmcp-server%2540latest%2522%255D%257D%257D)\n[<img alt=\"Install in VS Code Insiders (npx)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20NPM%20Sentinel%20MCP&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522npm-sentinel%2522%252C%2522config%2522%253A%257B%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522-y%2522%252C%2522%2540nekzus%252Fmcp-server%2540latest%2522%255D%257D%257D)\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n```json\n{\n  \"servers\": {\n    \"npm-sentinel\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@nekzus/mcp-server@latest\"]\n    }\n  }\n}\n```\n\n### Smithery.ai Deployment (HTTP Streamable)\n\nThis MCP server now supports HTTP streamable transport through Smithery.ai for enhanced scalability and performance.",
        "start_pos": 3503,
        "end_pos": 4978,
        "token_count_estimate": 368,
        "source_type": "readme",
        "agent_id": "1ee7750859038dd6"
      },
      {
        "chunk_id": 3,
        "text": "figuration for Smithery.ai:**\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel\": {\n      \"type\": \"http\",\n      \"url\": \"https://smithery.ai/server/@Nekzus/npm-sentinel-mcp\"\n    }\n  }\n}\n```\n\n### Configuration\n\nThe server supports the following configuration options:\n\n| Environment Variable | CLI Argument | Default | Description |\n| -------------------- | ------------ | ------- | ----------- |\n| `NPM_REGISTRY_URL` | `config.NPM_REGISTRY_URL` | `https://registry.npmjs.org` | URL of the NPM registry to use for all requests |\n\n#### HTTP Deployment (Smithery/Docker)\n\nWhen deploying via Smithery or Docker, you can configure these options in your configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel\": {\n      \"type\": \"http\",\n      \"url\": \"https://smithery.ai/server/@Nekzus/npm-sentinel-mcp\",\n      \"config\": {\n        \"NPM_REGISTRY_URL\": \"https://registry.npmjs.org\"\n      }\n    }\n  }\n}\n```\n### Docker\n\n#### Build\n```bash\n# Build the Docker image\ndocker build -t nekzus/npm-sentinel-mcp .\n```\n\n#### Usage\n\nYou can run the MCP server using Docker with directory mounting to `/projects`:\n\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-w\", \"/projects\",\n        \"--mount\", \"type=bind,src=${PWD},dst=/projects\",\n        \"nekzus/npm-sentinel-mcp\",\n        \"node\",\n        \"dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nFor multiple directories:\n\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-w\", \"/projects\",\n        \"--mount\", \"type=bind,src=/path/to/workspace,dst=/projects/workspace\",\n        \"--mount\", \"type=bind,src=/path/to/other/dir,dst=/projects/other/dir,ro\",\n        \"nekzus/npm-sentinel-mcp\",\n        \"node\",\n        \"dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nNote: All mounted directories must be under `/projects` for proper access.",
        "start_pos": 5351,
        "end_pos": 7307,
        "token_count_estimate": 489,
        "source_type": "readme",
        "agent_id": "1ee7750859038dd6"
      },
      {
        "chunk_id": 4,
        "text": "=/projects/other/dir,ro\",\n        \"nekzus/npm-sentinel-mcp\",\n        \"node\",\n        \"dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\nNote: All mounted directories must be under `/projects` for proper access.\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"npmsentinel\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@nekzus/mcp-server@latest\"]\n    }\n  }\n}\n```\n\nConfiguration file locations:\n- Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`\n- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- Linux: (Claude for Desktop does not officially support Linux at this time)\n\n### NPX\n\n<!-- [![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.png)](cursor://anysphere.cursor-deeplink/mcp/install?name=npm-sentinel-mcp&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBuZWt6dXMvbWNwLXNlcnZlckBsYXRlc3QiXX0=) -->\n\n```json\n{\n  \"mcpServers\": {\n    \"npm-sentinel-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@nekzus/mcp-server@latest\"\n      ]\n    }\n  }\n}\n```\n\n## API\n\nThe server exposes its tools via the Model Context Protocol. All tools adhere to a standardized response format:\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"string\",\n      \"isError\": boolean // Optional\n    }\n    // ... more content items if necessary\n  ]\n}\n```\n\n### Resources\n\n- `npm://registry`: NPM Registry interface\n- `npm://security`: Security analysis interface\n- `npm://metrics`: Package metrics interface\n\n### Server Resources\n\nThe server also provides the following informational resources accessible via MCP `GetResource` requests:\n\n- `doc://server/readme`:\n  - **Description**: Retrieves the main `README.md` file content for this NPM Sentinel MCP server.\n  - **MIME Type**: `text/markdown`\n- `doc://mcp/specification`:\n  - **Description**: Retrieves the `llms-full.txt` content, providing the comprehensive Model Context Protocol specification.",
        "start_pos": 7107,
        "end_pos": 9077,
        "token_count_estimate": 492,
        "source_type": "readme",
        "agent_id": "1ee7750859038dd6"
      },
      {
        "chunk_id": 5,
        "text": "CP server.\n  - **MIME Type**: `text/markdown`\n- `doc://mcp/specification`:\n  - **Description**: Retrieves the `llms-full.txt` content, providing the comprehensive Model Context Protocol specification.\n  - **MIME Type**: `text/plain`\n\n### Tools\n\n#### npmVersions\n- Get all versions of a package\n- Input: `packages` (string[])\n- Returns: Version history with release dates\n\n#### npmLatest\n- Get latest version information\n- Input: `packages` (string[])\n- Returns: Latest version details and changelog\n\n#### npmDeps\n- Analyze package dependencies\n- Input: `packages` (string[])\n- Returns: Complete dependency tree analysis\n\n#### npmTypes\n- Check TypeScript support\n- Input: `packages` (string[])\n- Returns: TypeScript compatibility status\n\n#### npmSize\n- Analyze package size\n- Input: `packages` (string[])\n- Returns: Bundle size and import cost analysis\n\n#### npmVulnerabilities\n- Scan for security vulnerabilities\n- Features: \n  - **Transitive Scanning**: Checks dependencies up to depth 2.\n  - **Ecosystem Awareness**: Automatically scans related packages (e.g., React Server Components).\n  - **Rich Reports**: Includes CVE IDs and full summaries.",
        "start_pos": 8877,
        "end_pos": 10024,
        "token_count_estimate": 286,
        "source_type": "readme",
        "agent_id": "1ee7750859038dd6"
      },
      {
        "chunk_id": 6,
        "text": "Returns: Formatted README content\n\n#### npmSearch\n- Search for packages\n- Input:\n  - `query` (string)\n  - `limit` (number, optional)\n- Returns: Matching packages with metadata\n\n#### npmLicenseCompatibility\n- Check license compatibility\n- Input: `packages` (string[])\n- Returns: License analysis and compatibility info\n\n#### npmRepoStats\n- Get repository statistics\n- Input: `packages` (string[])\n- Returns: GitHub/repository metrics\n\n#### npmDeprecated\n- Check for deprecation\n- Input: `packages` (string[])\n- Returns: Deprecation status and alternatives\n\n#### npmChangelogAnalysis\n- Analyze package changelogs\n- Input: `packages` (string[])\n- Returns: Changelog summaries and impact analysis\n\n#### npmAlternatives\n- Find package alternatives\n- Input: `packages` (string[])\n- Returns: Similar packages with comparisons\n\n#### npmQuality\n- Assess package quality\n- Input: `packages` (string[])\n- Returns: Quality metrics and scores\n\n#### npmMaintenance\n- Check maintenance status\n- Input: `packages` (string[])\n- Returns: Maintenance activity metrics\n\n## Build\n\n```bash\n# Install dependencies\nnpm install\n\n# Build for STDIO (traditional)\nnpm run build:stdio\n\n# Build for HTTP (Smithery)\nnpm run build:http\n\n# Development server\nnpm run dev\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n\n---\n\nMIT ¬© [nekzus](https://github.com/nekzus)",
        "start_pos": 10725,
        "end_pos": 12277,
        "token_count_estimate": 387,
        "source_type": "readme",
        "agent_id": "1ee7750859038dd6"
      }
    ]
  },
  {
    "agent_id": "9b02d4af84170915",
    "name": "ai.smithery/Open-Scout-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@Open-Scout/mcp/mcp",
    "description": "Create and publish one-pagers and boards for your organization. Upload images from the web, update‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-30T15:08:39.247411Z",
    "indexed_at": "2026-02-18T04:04:06.642420",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create one-pagers for your organization",
        "Publish one-pagers for your organization",
        "Create boards for your organization",
        "Upload images from the web",
        "Update content on one-pagers and boards"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of core functionalities but lacks detail, examples, or explicit limitations and requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "c7f9561c595c0e91",
    "name": "ai.smithery/PabloLec-keyprobe-mcp",
    "source": "mcp",
    "source_url": "https://github.com/PabloLec/KeyProbe-MCP",
    "description": "Audit certificates and keystores to surface expiry risks, weak algorithms, and misconfigurations.‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T18:08:11.479456Z",
    "indexed_at": "2026-02-18T04:04:08.287132",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "Local testing : \n```commandline\nnpx @modelcontextprotocol/inspector uv run python -m keyprobe.server\n```"
    },
    "llm_extracted": {
      "capabilities": [],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.1,
    "quality_rationale": "The documentation provides only a single command for local testing without any descriptions, capabilities, limitations, or requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "Local testing : \n```commandline\nnpx @modelcontextprotocol/inspector uv run python -m keyprobe.server\n```",
        "start_pos": 0,
        "end_pos": 104,
        "token_count_estimate": 26,
        "source_type": "readme",
        "agent_id": "c7f9561c595c0e91"
      }
    ]
  },
  {
    "agent_id": "f4e914d4d009d640",
    "name": "ai.smithery/Parc-Dev-task-breakdown-server",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@Parc-Dev/task-breakdown-server/mcp",
    "description": "Break down complex problems into clear, actionable steps. Adapt on the fly by iterating, revising,‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T18:37:20.988548Z",
    "indexed_at": "2026-02-18T04:04:10.826545",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Break down complex problems into clear, actionable steps",
        "Adapt on the fly by iterating and revising"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's function but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "a77d06324e198e79",
    "name": "ai.smithery/Phionx-mcp-hello-server",
    "source": "mcp",
    "source_url": "https://github.com/Phionx/mcp-hello-server",
    "description": "Send personalized greetings to anyone. Enable Pirate Mode for swashbuckling salutations. Explore t‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T06:11:06.81562Z",
    "indexed_at": "2026-02-18T04:04:12.363700",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# mcp\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server using Smithery CLI",
        "Respond to interactive test commands such as 'Say hello to John'",
        "Allow development and customization of server capabilities via Python code"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai/account/api-keys",
        "Python environment capable of running uvicorn commands",
        "GitHub account for deployment"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "The documentation provides installation steps, usage examples, development guidance, and deployment instructions but lacks explicit limitations or detailed tool descriptions.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# mcp\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 981,
        "token_count_estimate": 245,
        "source_type": "readme",
        "agent_id": "a77d06324e198e79"
      }
    ]
  },
  {
    "agent_id": "e784c5ee6afb0d2a",
    "name": "ai.smithery/PixdataOrg-coderide",
    "source": "mcp",
    "source_url": "https://github.com/PixdataOrg/coderide-mcp",
    "description": "CodeRide eliminates the context reset cycle once and for all. Through MCP integration, it seamless‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-19T14:23:37.184036Z",
    "indexed_at": "2026-02-18T04:04:14.393622",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Supercharge Your AI Assistant or IDE with CodeRide Task Management\n\n[![npm version](https://img.shields.io/npm/v/@coderide/mcp.svg)](https://www.npmjs.com/package/@coderide/mcp)\n[![smithery badge](https://smithery.ai/badge/@PixdataOrg/coderide)](https://smithery.ai/server/@PixdataOrg/coderide)\n\n<a href=\"https://glama.ai/mcp/servers/@PixdataOrg/coderide-mcp\">\n  <img width=\"300\" height=\"157\" src=\"https://glama.ai/mcp/servers/@PixdataOrg/coderide-mcp/badge\" alt=\"coderide MCP server\" />\n</a>\n\n<p align=\"center\">\n  <a href=\"https://coderide.ai\" target=\"_blank\">\n    <img src=\"https://ideybnueizkxwqmjowpy.supabase.co/storage/v1/object/sign/coderide-website/Coderide-og-Facebook.jpg?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InN0b3JhZ2UtdXJsLXNpZ25pbmcta2V5X2M5OWNmMjY4LTg5MTMtNGFiOS1iYjhhLTIxMTUyNDZjNGM2NCJ9.eyJ1cmwiOiJjb2RlcmlkZS13ZWJzaXRlL0NvZGVyaWRlLW9nLUZhY2Vib29rLmpwZyIsImlhdCI6MTc0ODM3ODg1MiwiZXhwIjoxNzc5OTE0ODUyfQ.jBb-x5f2MACBNBsls0u_9seYIiynektHqef2Y_vSMHQ\" alt=\"CodeRide\" width=\"100%\"/>\n  </a>\n</p>\n\n<!-- Suggestion: Add badges here: npm version, license, build status, etc. -->\n\n**Give your AI coding sidekick the power of CodeRide!** CodeRide MCP connects your favorite AI development tools (like Cursor, Cline, Windsurf, and other MCP clients) directly to CodeRide, the AI-native task management system.\n\nImagine your AI not just writing code, but truly understanding project context, managing its own tasks, and collaborating seamlessly with you. No more endless copy-pasting or manual updates. With CodeRide MCP, your AI becomes a first-class citizen in your CodeRide workflow.\n\n## üöÄ Why CodeRide MCP is a Game-Changer\n\n*   **Deep Project Understanding for Your AI:** Equip your AI agents with rich, structured context from your CodeRide projects and tasks. Let them see the bigger picture.\n*   **Seamless AI-Powered Task Automation:** Empower AIs to fetch, interpret, and update tasks directly in CodeRide, automating routine project management.\n*   **Bridge the Gap Between Human & AI Developers:** Foster true collaboration with smoother handoffs, consistent task understanding, and aligned efforts.\n*   **Optimized for LLM Efficiency:** Compact JSON responses minimize token usage, ensuring faster, more cost-effective AI interactions.\n*   **Secure by Design:** Workspace-scoped API key authentication ensures your data's integrity and that AI operations are confined to the correct project context.\n*   **Plug & Play Integration:** Effortlessly set up with `npx` in any MCP-compatible environment. Get your AI connected in minutes!\n*   **Future-Proof Your Workflow:** Embrace an AI-native approach to development, built on the open Model Context Protocol standard.\n\n## ‚ú® Core Capabilities\n\nThe CodeRide MCP server provides your AI with the following capabilities:\n\n*   **Task Management:** Fetch specific tasks, list all tasks in a project, and get the next task in sequence.\n*   **Task Updates:** Modify task descriptions and statuses.\n*   **Prompt Access:** Get tailored prompts and instructions for specific tasks.\n*   **Project Management:** List all projects, retrieve project details, and manage project knowledge.\n*   **Project Knowledge Management:** Update a project's knowledge graph and architecture diagrams.\n*   **Project Initiation:** Get the first task of a project to kickstart work.\n*   **Workflow Automation:** Navigate through task sequences with smart next-task suggestions.\n\n## ‚öôÔ∏è Getting Started\n\n### Installing via Smithery (Recommended)\n\nTo install Coderide MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@PixdataOrg/coderide):\n\n```bash\nnpx -y @smithery/cli install @PixdataOrg/coderide --client claude\n```\n\n### Smithery Deployment Modes\n\nCodeRide MCP supports **dual-mode operation** for both development and production use:\n\n#### üîß **Development/Testing Mode (Mock)**\nPerfect for exploring features, testing integrations, or contributing to the project without needing a real CodeRide account.\n\n**How to activate:** In the Smithery playground or configuration, either:\n- Leave the `CODERIDE_API_KEY` field empty\n- Provide any placeholder value (e.g., `mock-key`, `test`, etc.)\n\n**What you get:**\n- All 9 tools available with realistic mock data\n- Sample projects (ABC, XYZ) and tasks (ABC-1, ABC-2, etc.)\n- Full MCP functionality for testing and development\n- No real API calls - completely safe for experimentation\n\n#### üöÄ **Production Mode (Real API)**\nFor actual CodeRide users who want to integrate with their real projects and tasks.\n\n**How to activate:** Provide a valid CodeRide API key that starts with `CR_API_KEY_`\n\n**What you get:**\n- Full integration with your CodeRide workspace\n- Real project and task data\n- Ability to update tasks and projects\n- Complete workflow automation\n\n### Traditional MCP Configuration\n\nFor non-Smithery deployments, add this configuration to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"CodeRide\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@coderide/mcp\"],\n      \"env\": {\n        \"CODERIDE_API_KEY\": \"YOUR_CODERIDE_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n**Prerequisites:**\n\n1.  **Node.js and npm:** Ensure you have Node.js (which includes npm) installed.\n2.  **CodeRide Account & API Key (Production only):** For production use, you'll need an active CodeRide account and API key from [app.coderide.ai](https://app.coderide.ai).\n\nOnce configured, your MCP client will automatically connect to the CodeRide MCP server with the appropriate mode based on your configuration.\n\n## ü§ñ Who is this for?\n\nCodeRide MCP is for:\n\n*   **Developers using AI coding assistants:** Integrate your AI tools (Cursor, Cline, Windsurf, etc.) deeply with your CodeRide task management.\n*   **Teams adopting AI-driven development:** Standardize how AI agents access project information and contribute to tasks.\n*   **Anyone building with MCP:** Leverage a powerful example of an MCP server that connects to a real-world SaaS platform.\n\nIf you're looking to make your AI assistant a more productive and integrated member of your development team, CodeRide MCP is for you.\n\n## üî® Available Tools\n\nHere's a breakdown of the tools provided by CodeRide MCP and how they can be used:\n\n### `get_task`\n\nRetrieves detailed information about a specific task by its number (e.g., \"TCA-3\").\n\n**Input Schema:**\n```json\n{\n  \"number\": \"task-number (e.g., 'TCA-3')\",\n  \"status\": \"to-do|in-progress|done\", // Optional: filter by status\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"Hey AI, what are the details for task APP-101?\"\n*   **AI Action:** Calls `get_task` with `arguments: { \"number\": \"APP-101\" }`.\n*   **Outcome:** AI receives the title, description, status, priority, and other context for task APP-101.\n\n### `update_task`\n\nUpdates an existing task's description, status, or other mutable fields.\n\n**Input Schema:**\n```json\n{\n  \"number\": \"task-number-identifier\",\n  \"description\": \"updated-task-description\", // Optional\n  \"status\": \"to-do|in-progress|done\"   // Optional\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, please mark task BUG-42 as 'done' and add a note: 'Fixed the off-by-one error.'\"\n*   **AI Action:** Calls `update_task` with `arguments: { \"number\": \"BUG-42\", \"status\": \"done\", \"description\": \"Fixed the off-by-one error.\" }`.\n*   **Outcome:** Task BUG-42 is updated in CodeRide.\n\n### `get_prompt`\n\nRetrieves the specific prompt or instructions tailored for an AI agent to work on a given task.\n\n**Input Schema:**\n```json\n{\n  \"number\": \"task-number (e.g., 'TCA-3')\"\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, I'm ready to start on task ETF-7. What's the main objective?\"\n*   **AI Action:** Calls `get_prompt` with `arguments: { \"slug\": \"ETF\", \"number\": \"ETF-7\" }`.\n*   **Outcome:** AI receives the specific, actionable prompt for FEAT-7, enabling it to begin work with clear direction.\n\n### `get_project`\n\nRetrieves details about a specific project using its slug.\n\n**Input Schema:**\n```json\n{\n  \"slug\": \"project-slug (e.g., 'TCA')\",\n  \"name\": \"optional-project-name\" // Can also retrieve by name\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, can you give me an overview of the 'Omega Initiative' project?\"\n*   **AI Action:** Calls `get_project` with `arguments: { \"slug\": \"omega-initiative\" }`.\n*   **Outcome:** AI receives the project's name, description, and potentially links to its knowledge base or diagrams.\n\n### `update_project`\n\nUpdates a project's high-level information, such as its knowledge graph or system architecture diagram.\n\n**Input Schema:**\n```json\n{\n  \"slug\": \"project-slug-identifier\",\n  \"project_knowledge\": { /* JSON object representing the knowledge graph */ }, // Optional\n  \"project_diagram\": \"/* Mermaid diagram string or similar */\"             // Optional\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, I've updated the user authentication flow. Please update the project diagram for project 'APB'.\"\n*   **AI Action:** (After generating/receiving the new diagram) Calls `update_project` with `arguments: { \"slug\": \"APB\", \"project_diagram\": \"/* new mermaid diagram */\" }`.\n*   **Outcome:** The 'AlphaProject' in CodeRide now has the updated architecture diagram.\n\n### `start_project`\n\nRetrieves the first or next recommended task for a given project, allowing an AI to begin work.\n\n**Input Schema:**\n```json\n{\n  \"slug\": \"project-slug (e.g., 'TCA')\"\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, let's get started on the 'MobileAppV2' project. What's the first task?\"\n*   **AI Action:** Calls `start_project` with `arguments: { \"slug\": \"MBC\" }`.\n*   **Outcome:** AI receives details for the initial task in the 'MBC' project, ready to begin.\n\n### `list_projects` ‚ú® NEW\n\nLists all projects in the user's workspace, providing an overview of available projects with intelligent workflow guidance.\n\n**Input Schema:**\n```json\n{\n  // No input required - automatically uses workspace from API key\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, show me all my projects.\"\n*   **AI Action:** Calls `list_projects` with no arguments.\n*   **Outcome:** AI receives a list of all projects in the workspace with their slugs, names, and basic details, plus intelligent guidance for project selection and workflow initiation.\n\n### `list_tasks` ‚ú® NEW\n\nShows all tasks within a specific project, organized by status with smart numerical sorting and workflow optimization.\n\n**Input Schema:**\n```json\n{\n  \"slug\": \"project-slug (e.g., 'CRD')\"\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, what tasks are available in the CRD project?\"\n*   **AI Action:** Calls `list_tasks` with `arguments: { \"slug\": \"CRD\" }`.\n*   **Outcome:** AI receives all tasks in the CRD project, sorted numerically (CRD-1, CRD-2, CRD-3...) and organized by status columns, with intelligent task selection guidance.\n\n### `next_task` ‚ú® NEW\n\nRetrieves the next task in sequence for workflow automation, perfect for continuous development flows with intelligent sequencing.\n\n**Input Schema:**\n```json\n{\n  \"number\": \"current-task-number (e.g., 'CRD-1')\"\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, I just finished CRD-1. What's next?\"\n*   **AI Action:** Calls `next_task` with `arguments: { \"number\": \"CRD-1\" }`.\n*   **Outcome:** AI receives details for CRD-2, enabling seamless workflow continuation with prerequisite validation and contextual guidance.\n\n## üë©‚Äçüíª For Contributors & Developers\n\nWant to add new tools or modify existing ones? Check out our comprehensive [Tool Development Guide](docs/tools.md) which covers:\n\n- **Naming conventions** for consistent tool design\n- **Description guidelines** (what + when pattern)\n- **Input schema best practices** with detailed property descriptions\n- **Metadata structure** for enhanced tool discoverability\n- **Complete examples** to get you started quickly\n\nThe guide ensures all tools follow Anthropic's tool search best practices while maintaining backwards compatibility.\n\n## üí° Technical Highlights\n\n*   **üß† AI-Native Workflow Optimization:** Advanced agent instruction system with MCP 2025 structured tool output and intelligent workflow orchestration for enhanced AI productivity.\n*   **üîÑ Intelligent Task Sequencing:** Prerequisite validation, status-aware guidance, and automated workflow phases (Discovery ‚Üí Context ‚Üí Analysis ‚Üí Implementation ‚Üí Completion).\n*   **Security-First Design:** Comprehensive input validation, secure API client with retry logic, and workspace-scoped authentication ensure data integrity and safe AI operations.\n*   **Production-Ready Reliability:** Exponential backoff retry mechanisms, 90-second timeouts, and stable MCP SDK version (^1.0.3) provide enterprise-grade stability.\n*   **Workspace-Centered Authentication:** API keys are tied to specific workspaces. All operations are automatically scoped, simplifying requests and enhancing security. No need to pass `workspaceId`!\n*   **User-Friendly Identifiers:** Interact with tasks and projects using human-readable numbers (e.g., \"TCA-3\") and slugs (e.g., \"TCA\") instead of internal UUIDs.\n*   **Optimized for AI Efficiency:** Compact JSON responses minimize token usage, while smart error handling prevents unnecessary retries on validation errors.\n*   **Robust API Interaction:** Uses the official CodeRide API (`https://api.coderide.ai` by default) with comprehensive logging and error handling for all operations.\n\n## üõ°Ô∏è Security\n\nCodeRide MCP takes security seriously. We welcome responsible security research and have established a comprehensive bug bounty program with rewards ranging from ‚Ç¨9 to ‚Ç¨119.\n\nFor security vulnerabilities, please see our [Security Policy](SECURITY.md) for reporting guidelines and reward information.\n\n## üîß Recent Improvements\n\n### v0.9.3 - Interactive Installation Wizard üßô\n- **üéØ CLI Installation Wizard**: Interactive setup with `npx @coderide/mcp add` for easy configuration\n- **üîç Auto-Detection**: Automatically detects installed MCP clients (Cursor, Claude Desktop, Claude Code, VS Code, Codex CLI)\n- **üé® Enhanced UX**: Clean wizard output with status indicators and brand colors\n- **‚öôÔ∏è Smart Config Management**: Detects unchanged configs to avoid unnecessary writes, creates backups automatically\n- **üöÄ CLI Flags**: `--no-open` to skip browser opening, `--force` to show all clients\n- **üìù Multi-Format Support**: Handles both JSON (most clients) and TOML (Codex CLI) configuration formats\n- **üîí Secure by Default**: Config files written with restrictive permissions (0o600), API key validation\n\n### v0.9.0 - Comprehensive Agent Workflow Enhancement üöÄ\n- **üß† NLP Recognition System**: Smart detection of project references and user intent in natural language for improved agent understanding\n- **üß™ Mandatory Testing Verification**: Comprehensive testing pipeline with unit + integration test support, coverage validation (80% unit, 90% integration)\n- **üìù Automatic Git Commits**: Full Conventional Commits 1.0.0 compliance with semantic versioning and automated commit generation\n- **üéØ Project Knowledge Optimization**: Intelligent project knowledge management with consistency validation and standardized schemas\n- **‚ö° Enhanced Agent Instructions**: MCP 2025 structured tool output with workflow intelligence and phase-based guidance\n- **üîí Security & Performance**: Input validation, command injection prevention, singleton patterns, and comprehensive error handling\n- **üîÑ Complete Workflow Automation**: Discovery ‚Üí Context ‚Üí Analysis ‚Üí Implementation ‚Üí Completion phases with intelligent sequencing\n\n### v0.8.0 - Agent Workflow Optimization\n- **üß† Intelligent Agent Instructions**: Implemented comprehensive agent instruction system with MCP 2025 structured tool output\n- **üîÑ Workflow Orchestration**: Added prerequisite validation, status-aware guidance, and automated workflow phases\n- **üìÅ Enhanced Tool Organization**: Renamed tools for consistency (`project_list` ‚Üí `list_projects`, `task_list` ‚Üí `list_tasks`)\n- **‚ö° AI Productivity Boost**: All 9 tools now include contextual guidance, automation hints, and workflow intelligence\n- **üîí Maintained Compatibility**: 100% backward compatibility with existing integrations and security systems\n\n### Previous Improvements\n- **Enhanced Reliability**: Implemented exponential backoff retry logic for improved connection stability\n- **Optimized Timeouts**: Increased API timeouts to 90 seconds for better handling of complex operations\n- **Stable Dependencies**: Pinned MCP SDK to stable version (^1.0.3) for consistent behavior\n- **MCP Protocol Optimizations**: Reduced timeout errors (-32001) and SSE stream disconnections\n\n## üî• About CodeRide\n\n**CodeRide is where AI and human developers unite to build better software, faster.**\n\nIt's more than just task management; it's an AI-native platform built from the ground up to support the unique workflows of AI-assisted software development. CodeRide provides the essential structured context, project knowledge, and external memory that AI agents require to understand complex projects, contribute meaningfully, and collaborate effectively with their human counterparts.\n\nTransform your development process with a tool that truly understands the synergy between human ingenuity and artificial intelligence.\n\nDiscover the future of software development at [coderide.ai](https://coderide.ai).\n\n## üîß Troubleshooting\n\n### Common Issues\n\n**Connection Timeouts or MCP Error -32001:**\n- The recent improvements have significantly reduced these issues\n- Ensure you're using the latest version: `npx -y @coderide/mcp@latest`\n- Check your API key is correctly set in the environment configuration\n\n**Authentication Errors:**\n- Verify your CodeRide API key is valid and has the correct permissions\n- Ensure the API key is properly set in your MCP client configuration\n- API keys are workspace-scoped - make sure you're using the right workspace\n\n**Tool Not Found Errors:**\n- Restart your MCP client after configuration changes\n- Verify the server is properly configured in your MCP client settings\n\nFor additional support, please [open an issue](https://github.com/PixdataOrg/coderide-mcp/issues) with detailed error information.\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit issues, fork the repository, and create pull requests.\n\nFor security-related contributions, please see our [Security Policy](SECURITY.md).\n\n## üîñ License\n\nAll rights reserved. See the [LICENSE](LICENSE) file for details.\n\n## ü§ó Support & Community\n\n*   Have questions or need help with `@coderide/mcp`? [Open an issue](https://github.com/PixdataOrg/coderide-mcp/issues) on our GitHub repository.\n*   Want to learn more about CodeRide? Visit [coderide.ai](https://coderide.ai) or join our community (Link to community forum/Discord if available).\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Fetch specific tasks by task number with detailed information",
        "Update task descriptions and statuses",
        "Retrieve tailored prompts and instructions for specific tasks",
        "List all projects in the workspace with overview details",
        "Retrieve detailed information about specific projects",
        "Update project knowledge graphs and architecture diagrams",
        "Get the first or next recommended task to start a project",
        "List all tasks within a project organized by status and numerical order",
        "Retrieve the next task in sequence for workflow automation"
      ],
      "limitations": [
        "Requires a valid CodeRide API key for production mode; mock mode does not connect to real data",
        "API key must be workspace-scoped, limiting operations to the correct project context",
        "No explicit mention of rate limits or concurrency constraints",
        "Functionality limited to CodeRide task and project management; does not handle code generation or execution"
      ],
      "requirements": [
        "Node.js and npm installed",
        "For production use, an active CodeRide account and a valid API key starting with 'CR_API_KEY_'",
        "MCP-compatible environment or client to run the server",
        "Optional: Smithery CLI for easy installation and deployment"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for all tools, clear descriptions of capabilities, explicit requirements, and notes on operational modes and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Supercharge Your AI Assistant or IDE with CodeRide Task Management\n\n[![npm version](https://img.shields.io/npm/v/@coderide/mcp.svg)](https://www.npmjs.com/package/@coderide/mcp)\n[![smithery badge](https://smithery.ai/badge/@PixdataOrg/coderide)](https://smithery.ai/server/@PixdataOrg/coderide)\n\n<a href=\"https://glama.ai/mcp/servers/@PixdataOrg/coderide-mcp\">\n  <img width=\"300\" height=\"157\" src=\"https://glama.ai/mcp/servers/@PixdataOrg/coderide-mcp/badge\" alt=\"coderide MCP server\" />\n</a>\n\n<p align=\"center\">\n  <a href=\"https://coderide.ai\" target=\"_blank\">\n    <img src=\"https://ideybnueizkxwqmjowpy.supabase.co/storage/v1/object/sign/coderide-website/Coderide-og-Facebook.jpg?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InN0b3JhZ2UtdXJsLXNpZ25pbmcta2V5X2M5OWNmMjY4LTg5MTMtNGFiOS1iYjhhLTIxMTUyNDZjNGM2NCJ9.eyJ1cmwiOiJjb2RlcmlkZS13ZWJzaXRlL0NvZGVyaWRlLW9nLUZhY2Vib29rLmpwZyIsImlhdCI6MTc0ODM3ODg1MiwiZXhwIjoxNzc5OTE0ODUyfQ.jBb-x5f2MACBNBsls0u_9seYIiynektHqef2Y_vSMHQ\" alt=\"CodeRide\" width=\"100%\"/>\n  </a>\n</p>\n\n<!-- Suggestion: Add badges here: npm version, license, build status, etc. -->\n\n**Give your AI coding sidekick the power of CodeRide!** CodeRide MCP connects your favorite AI development tools (like Cursor, Cline, Windsurf, and other MCP clients) directly to CodeRide, the AI-native task management system.\n\nImagine your AI not just writing code, but truly understanding project context, managing its own tasks, and collaborating seamlessly with you. No more endless copy-pasting or manual updates. With CodeRide MCP, your AI becomes a first-class citizen in your CodeRide workflow.\n\n## üöÄ Why CodeRide MCP is a Game-Changer\n\n*   **Deep Project Understanding for Your AI:** Equip your AI agents with rich, structured context from your CodeRide projects and tasks. Let them see the bigger picture.\n*   **Seamless AI-Powered Task Automation:** Empower AIs to fetch, interpret, and update tasks directly in CodeRide, automating routine project management.",
        "start_pos": 0,
        "end_pos": 1973,
        "token_count_estimate": 493,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 1,
        "text": "and tasks. Let them see the bigger picture.\n*   **Seamless AI-Powered Task Automation:** Empower AIs to fetch, interpret, and update tasks directly in CodeRide, automating routine project management.\n*   **Bridge the Gap Between Human & AI Developers:** Foster true collaboration with smoother handoffs, consistent task understanding, and aligned efforts.\n*   **Optimized for LLM Efficiency:** Compact JSON responses minimize token usage, ensuring faster, more cost-effective AI interactions.\n*   **Secure by Design:** Workspace-scoped API key authentication ensures your data's integrity and that AI operations are confined to the correct project context.\n*   **Plug & Play Integration:** Effortlessly set up with `npx` in any MCP-compatible environment. Get your AI connected in minutes!\n*   **Future-Proof Your Workflow:** Embrace an AI-native approach to development, built on the open Model Context Protocol standard.\n\n## ‚ú® Core Capabilities\n\nThe CodeRide MCP server provides your AI with the following capabilities:\n\n*   **Task Management:** Fetch specific tasks, list all tasks in a project, and get the next task in sequence.\n*   **Task Updates:** Modify task descriptions and statuses.\n*   **Prompt Access:** Get tailored prompts and instructions for specific tasks.\n*   **Project Management:** List all projects, retrieve project details, and manage project knowledge.\n*   **Project Knowledge Management:** Update a project's knowledge graph and architecture diagrams.\n*   **Project Initiation:** Get the first task of a project to kickstart work.\n*   **Workflow Automation:** Navigate through task sequences with smart next-task suggestions.",
        "start_pos": 1773,
        "end_pos": 3426,
        "token_count_estimate": 413,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 2,
        "text": ":\n\n```bash\nnpx -y @smithery/cli install @PixdataOrg/coderide --client claude\n```\n\n### Smithery Deployment Modes\n\nCodeRide MCP supports **dual-mode operation** for both development and production use:\n\n#### üîß **Development/Testing Mode (Mock)**\nPerfect for exploring features, testing integrations, or contributing to the project without needing a real CodeRide account.\n\n**How to activate:** In the Smithery playground or configuration, either:\n- Leave the `CODERIDE_API_KEY` field empty\n- Provide any placeholder value (e.g., `mock-key`, `test`, etc.)\n\n**What you get:**\n- All 9 tools available with realistic mock data\n- Sample projects (ABC, XYZ) and tasks (ABC-1, ABC-2, etc.)\n- Full MCP functionality for testing and development\n- No real API calls - completely safe for experimentation\n\n#### üöÄ **Production Mode (Real API)**\nFor actual CodeRide users who want to integrate with their real projects and tasks.\n\n**How to activate:** Provide a valid CodeRide API key that starts with `CR_API_KEY_`\n\n**What you get:**\n- Full integration with your CodeRide workspace\n- Real project and task data\n- Ability to update tasks and projects\n- Complete workflow automation\n\n### Traditional MCP Configuration\n\nFor non-Smithery deployments, add this configuration to your MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"CodeRide\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@coderide/mcp\"],\n      \"env\": {\n        \"CODERIDE_API_KEY\": \"YOUR_CODERIDE_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n**Prerequisites:**\n\n1.  **Node.js and npm:** Ensure you have Node.js (which includes npm) installed.\n2.  **CodeRide Account & API Key (Production only):** For production use, you'll need an active CodeRide account and API key from [app.coderide.ai](https://app.coderide.ai).\n\nOnce configured, your MCP client will automatically connect to the CodeRide MCP server with the appropriate mode based on your configuration.\n\n## ü§ñ Who is this for?",
        "start_pos": 3621,
        "end_pos": 5544,
        "token_count_estimate": 480,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 3,
        "text": "ride.ai](https://app.coderide.ai).\n\nOnce configured, your MCP client will automatically connect to the CodeRide MCP server with the appropriate mode based on your configuration.\n\n## ü§ñ Who is this for?\n\nCodeRide MCP is for:\n\n*   **Developers using AI coding assistants:** Integrate your AI tools (Cursor, Cline, Windsurf, etc.) deeply with your CodeRide task management.\n*   **Teams adopting AI-driven development:** Standardize how AI agents access project information and contribute to tasks.\n*   **Anyone building with MCP:** Leverage a powerful example of an MCP server that connects to a real-world SaaS platform.\n\nIf you're looking to make your AI assistant a more productive and integrated member of your development team, CodeRide MCP is for you.\n\n## üî® Available Tools\n\nHere's a breakdown of the tools provided by CodeRide MCP and how they can be used:\n\n### `get_task`\n\nRetrieves detailed information about a specific task by its number (e.g., \"TCA-3\").\n\n**Input Schema:**\n```json\n{\n  \"number\": \"task-number (e.g., 'TCA-3')\",\n  \"status\": \"to-do|in-progress|done\", // Optional: filter by status\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"Hey AI, what are the details for task APP-101?\"\n*   **AI Action:** Calls `get_task` with `arguments: { \"number\": \"APP-101\" }`.\n*   **Outcome:** AI receives the title, description, status, priority, and other context for task APP-101.\n\n### `update_task`\n\nUpdates an existing task's description, status, or other mutable fields.\n\n**Input Schema:**\n```json\n{\n  \"number\": \"task-number-identifier\",\n  \"description\": \"updated-task-description\", // Optional\n  \"status\": \"to-do|in-progress|done\"   // Optional\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, please mark task BUG-42 as 'done' and add a note: 'Fixed the off-by-one error.'\"\n*   **AI Action:** Calls `update_task` with `arguments: { \"number\": \"BUG-42\", \"status\": \"done\", \"description\": \"Fixed the off-by-one error.\" }`.\n*   **Outcome:** Task BUG-42 is updated in CodeRide.",
        "start_pos": 5344,
        "end_pos": 7331,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 4,
        "text": "'\"\n*   **AI Action:** Calls `update_task` with `arguments: { \"number\": \"BUG-42\", \"status\": \"done\", \"description\": \"Fixed the off-by-one error.\" }`.\n*   **Outcome:** Task BUG-42 is updated in CodeRide.\n\n### `get_prompt`\n\nRetrieves the specific prompt or instructions tailored for an AI agent to work on a given task.\n\n**Input Schema:**\n```json\n{\n  \"number\": \"task-number (e.g., 'TCA-3')\"\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, I'm ready to start on task ETF-7. What's the main objective?\"\n*   **AI Action:** Calls `get_prompt` with `arguments: { \"slug\": \"ETF\", \"number\": \"ETF-7\" }`.\n*   **Outcome:** AI receives the specific, actionable prompt for FEAT-7, enabling it to begin work with clear direction.\n\n### `get_project`\n\nRetrieves details about a specific project using its slug.\n\n**Input Schema:**\n```json\n{\n  \"slug\": \"project-slug (e.g., 'TCA')\",\n  \"name\": \"optional-project-name\" // Can also retrieve by name\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, can you give me an overview of the 'Omega Initiative' project?\"\n*   **AI Action:** Calls `get_project` with `arguments: { \"slug\": \"omega-initiative\" }`.\n*   **Outcome:** AI receives the project's name, description, and potentially links to its knowledge base or diagrams.\n\n### `update_project`\n\nUpdates a project's high-level information, such as its knowledge graph or system architecture diagram.\n\n**Input Schema:**\n```json\n{\n  \"slug\": \"project-slug-identifier\",\n  \"project_knowledge\": { /* JSON object representing the knowledge graph */ }, // Optional\n  \"project_diagram\": \"/* Mermaid diagram string or similar */\"             // Optional\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, I've updated the user authentication flow. Please update the project diagram for project 'APB'.\"\n*   **AI Action:** (After generating/receiving the new diagram) Calls `update_project` with `arguments: { \"slug\": \"APB\", \"project_diagram\": \"/* new mermaid diagram */\" }`.\n*   **Outcome:** The 'AlphaProject' in CodeRide now has the updated architecture diagram.",
        "start_pos": 7131,
        "end_pos": 9172,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 5,
        "text": "m) Calls `update_project` with `arguments: { \"slug\": \"APB\", \"project_diagram\": \"/* new mermaid diagram */\" }`.\n*   **Outcome:** The 'AlphaProject' in CodeRide now has the updated architecture diagram.\n\n### `start_project`\n\nRetrieves the first or next recommended task for a given project, allowing an AI to begin work.\n\n**Input Schema:**\n```json\n{\n  \"slug\": \"project-slug (e.g., 'TCA')\"\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, let's get started on the 'MobileAppV2' project. What's the first task?\"\n*   **AI Action:** Calls `start_project` with `arguments: { \"slug\": \"MBC\" }`.\n*   **Outcome:** AI receives details for the initial task in the 'MBC' project, ready to begin.\n\n### `list_projects` ‚ú® NEW\n\nLists all projects in the user's workspace, providing an overview of available projects with intelligent workflow guidance.\n\n**Input Schema:**\n```json\n{\n  // No input required - automatically uses workspace from API key\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, show me all my projects.\"\n*   **AI Action:** Calls `list_projects` with no arguments.\n*   **Outcome:** AI receives a list of all projects in the workspace with their slugs, names, and basic details, plus intelligent guidance for project selection and workflow initiation.\n\n### `list_tasks` ‚ú® NEW\n\nShows all tasks within a specific project, organized by status with smart numerical sorting and workflow optimization.\n\n**Input Schema:**\n```json\n{\n  \"slug\": \"project-slug (e.g., 'CRD')\"\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, what tasks are available in the CRD project?\"\n*   **AI Action:** Calls `list_tasks` with `arguments: { \"slug\": \"CRD\" }`.\n*   **Outcome:** AI receives all tasks in the CRD project, sorted numerically (CRD-1, CRD-2, CRD-3...) and organized by status columns, with intelligent task selection guidance.\n\n### `next_task` ‚ú® NEW\n\nRetrieves the next task in sequence for workflow automation, perfect for continuous development flows with intelligent sequencing.",
        "start_pos": 8972,
        "end_pos": 10955,
        "token_count_estimate": 495,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 6,
        "text": "ns, with intelligent task selection guidance.\n\n### `next_task` ‚ú® NEW\n\nRetrieves the next task in sequence for workflow automation, perfect for continuous development flows with intelligent sequencing.\n\n**Input Schema:**\n```json\n{\n  \"number\": \"current-task-number (e.g., 'CRD-1')\"\n}\n```\n\n**Example Use Case:**\n*   **User Prompt:** \"AI, I just finished CRD-1. What's next?\"\n*   **AI Action:** Calls `next_task` with `arguments: { \"number\": \"CRD-1\" }`.\n*   **Outcome:** AI receives details for CRD-2, enabling seamless workflow continuation with prerequisite validation and contextual guidance.\n\n## üë©‚Äçüíª For Contributors & Developers\n\nWant to add new tools or modify existing ones? Check out our comprehensive [Tool Development Guide](docs/tools.md) which covers:\n\n- **Naming conventions** for consistent tool design\n- **Description guidelines** (what + when pattern)\n- **Input schema best practices** with detailed property descriptions\n- **Metadata structure** for enhanced tool discoverability\n- **Complete examples** to get you started quickly\n\nThe guide ensures all tools follow Anthropic's tool search best practices while maintaining backwards compatibility.\n\n## üí° Technical Highlights\n\n*   **üß† AI-Native Workflow Optimization:** Advanced agent instruction system with MCP 2025 structured tool output and intelligent workflow orchestration for enhanced AI productivity.\n*   **üîÑ Intelligent Task Sequencing:** Prerequisite validation, status-aware guidance, and automated workflow phases (Discovery ‚Üí Context ‚Üí Analysis ‚Üí Implementation ‚Üí Completion).\n*   **Security-First Design:** Comprehensive input validation, secure API client with retry logic, and workspace-scoped authentication ensure data integrity and safe AI operations.\n*   **Production-Ready Reliability:** Exponential backoff retry mechanisms, 90-second timeouts, and stable MCP SDK version (^1.0.3) provide enterprise-grade stability.\n*   **Workspace-Centered Authentication:** API keys are tied to specific workspaces.",
        "start_pos": 10755,
        "end_pos": 12742,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 7,
        "text": "ckoff retry mechanisms, 90-second timeouts, and stable MCP SDK version (^1.0.3) provide enterprise-grade stability.\n*   **Workspace-Centered Authentication:** API keys are tied to specific workspaces. All operations are automatically scoped, simplifying requests and enhancing security. No need to pass `workspaceId`!\n*   **User-Friendly Identifiers:** Interact with tasks and projects using human-readable numbers (e.g., \"TCA-3\") and slugs (e.g., \"TCA\") instead of internal UUIDs.\n*   **Optimized for AI Efficiency:** Compact JSON responses minimize token usage, while smart error handling prevents unnecessary retries on validation errors.\n*   **Robust API Interaction:** Uses the official CodeRide API (`https://api.coderide.ai` by default) with comprehensive logging and error handling for all operations.\n\n## üõ°Ô∏è Security\n\nCodeRide MCP takes security seriously. We welcome responsible security research and have established a comprehensive bug bounty program with rewards ranging from ‚Ç¨9 to ‚Ç¨119.\n\nFor security vulnerabilities, please see our [Security Policy](SECURITY.md) for reporting guidelines and reward information.",
        "start_pos": 12542,
        "end_pos": 13668,
        "token_count_estimate": 281,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 8,
        "text": "Config files written with restrictive permissions (0o600), API key validation\n\n### v0.9.0 - Comprehensive Agent Workflow Enhancement üöÄ\n- **üß† NLP Recognition System**: Smart detection of project references and user intent in natural language for improved agent understanding\n- **üß™ Mandatory Testing Verification**: Comprehensive testing pipeline with unit + integration test support, coverage validation (80% unit, 90% integration)\n- **üìù Automatic Git Commits**: Full Conventional Commits 1.0.0 compliance with semantic versioning and automated commit generation\n- **üéØ Project Knowledge Optimization**: Intelligent project knowledge management with consistency validation and standardized schemas\n- **‚ö° Enhanced Agent Instructions**: MCP 2025 structured tool output with workflow intelligence and phase-based guidance\n- **üîí Security & Performance**: Input validation, command injection prevention, singleton patterns, and comprehensive error handling\n- **üîÑ Complete Workflow Automation**: Discovery ‚Üí Context ‚Üí Analysis ‚Üí Implementation ‚Üí Completion phases with intelligent sequencing\n\n### v0.8.0 - Agent Workflow Optimization\n- **üß† Intelligent Agent Instructions**: Implemented comprehensive agent instruction system with MCP 2025 structured tool output\n- **üîÑ Workflow Orchestration**: Added prerequisite validation, status-aware guidance, and automated workflow phases\n- **üìÅ Enhanced Tool Organization**: Renamed tools for consistency (`project_list` ‚Üí `list_projects`, `task_list` ‚Üí `list_tasks`)\n- **‚ö° AI Productivity Boost**: All 9 tools now include contextual guidance, automation hints, and workflow intelligence\n- **üîí Maintained Compatibility**: 100% backward compatibility with existing integrations and security systems\n\n### Previous Improvements\n- **Enhanced Reliability**: Implemented exponential backoff retry logic for improved connection stability\n- **Optimized Timeouts**: Increased API timeouts to 90 seconds for better handling of complex operations\n- **Stable Dependencies**: Pinned MCP SDK to stable version (^1.0.3) for consist",
        "start_pos": 14390,
        "end_pos": 16438,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 9,
        "text": "tion stability\n- **Optimized Timeouts**: Increased API timeouts to 90 seconds for better handling of complex operations\n- **Stable Dependencies**: Pinned MCP SDK to stable version (^1.0.3) for consistent behavior\n- **MCP Protocol Optimizations**: Reduced timeout errors (-32001) and SSE stream disconnections\n\n## üî• About CodeRide\n\n**CodeRide is where AI and human developers unite to build better software, faster.**\n\nIt's more than just task management; it's an AI-native platform built from the ground up to support the unique workflows of AI-assisted software development. CodeRide provides the essential structured context, project knowledge, and external memory that AI agents require to understand complex projects, contribute meaningfully, and collaborate effectively with their human counterparts.\n\nTransform your development process with a tool that truly understands the synergy between human ingenuity and artificial intelligence.\n\nDiscover the future of software development at [coderide.ai](https://coderide.ai).\n\n## üîß Troubleshooting\n\n### Common Issues\n\n**Connection Timeouts or MCP Error -32001:**\n- The recent improvements have significantly reduced these issues\n- Ensure you're using the latest version: `npx -y @coderide/mcp@latest`\n- Check your API key is correctly set in the environment configuration\n\n**Authentication Errors:**\n- Verify your CodeRide API key is valid and has the correct permissions\n- Ensure the API key is properly set in your MCP client configuration\n- API keys are workspace-scoped - make sure you're using the right workspace\n\n**Tool Not Found Errors:**\n- Restart your MCP client after configuration changes\n- Verify the server is properly configured in your MCP client settings\n\nFor additional support, please [open an issue](https://github.com/PixdataOrg/coderide-mcp/issues) with detailed error information.\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit issues, fork the repository, and create pull requests.",
        "start_pos": 16238,
        "end_pos": 18220,
        "token_count_estimate": 495,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      },
      {
        "chunk_id": 10,
        "text": "ub.com/PixdataOrg/coderide-mcp/issues) with detailed error information.\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit issues, fork the repository, and create pull requests.\n\nFor security-related contributions, please see our [Security Policy](SECURITY.md).\n\n## üîñ License\n\nAll rights reserved. See the [LICENSE](LICENSE) file for details.\n\n## ü§ó Support & Community\n\n*   Have questions or need help with `@coderide/mcp`? [Open an issue](https://github.com/PixdataOrg/coderide-mcp/issues) on our GitHub repository.\n*   Want to learn more about CodeRide? Visit [coderide.ai](https://coderide.ai) or join our community (Link to community forum/Discord if available).",
        "start_pos": 18020,
        "end_pos": 18710,
        "token_count_estimate": 172,
        "source_type": "readme",
        "agent_id": "e784c5ee6afb0d2a"
      }
    ]
  },
  {
    "agent_id": "1987028afff03494",
    "name": "ai.smithery/Pratiksha-Kanoja-magicslide-mcp-test",
    "source": "mcp",
    "source_url": "https://github.com/Pratiksha-Kanoja/magicslide-mcp-test",
    "description": "Create polished slide decks from text or YouTube links in seconds. Fetch video transcripts to tran‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-07T10:10:48.100762Z",
    "indexed_at": "2026-02-18T04:04:16.017627",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create polished slide decks from text",
        "Create polished slide decks from YouTube links",
        "Fetch video transcripts"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's main functions but lacks detailed examples, structure, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "ce9457378300aded",
    "name": "ai.smithery/ProfessionalWiki-mediawiki-mcp-server",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@ProfessionalWiki/mediawiki-mcp-server/mcp",
    "description": "Enable Large Language Model clients to interact seamlessly with any MediaWiki wiki. Perform action‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-18T12:49:06.317403Z",
    "indexed_at": "2026-02-18T04:04:20.662015",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Enable Large Language Model clients to interact with any MediaWiki wiki",
        "Perform actions on MediaWiki wikis"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence providing a basic overview without details, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "3f4a1d8abad2d945",
    "name": "ai.smithery/RectiFlex-centerassist-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@RectiFlex/centerassist-mcp/mcp",
    "description": "Streamline field service and construction operations with CenterPoint Connect. Manage companies, o‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-03T15:38:17.960766Z",
    "indexed_at": "2026-02-18T04:04:22.451686",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Streamline field service operations",
        "Streamline construction operations",
        "Manage companies"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence fragment providing a very basic overview without detailed capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "44f56a0f55b3ff3a",
    "name": "ai.smithery/RectiFlex-centerassist-mcp-cp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@RectiFlex/centerassist-mcp-cp/mcp",
    "description": "Streamline property management, construction, and service workflows with CenterPoint Connect. Sear‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-07T03:31:56.911791Z",
    "indexed_at": "2026-02-18T04:04:23.963191",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Streamline property management workflows",
        "Streamline construction workflows",
        "Streamline service workflows"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence fragment providing only a very basic overview without details, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "ee672bfa068c5ce3",
    "name": "ai.smithery/RectiFlex-centerassist-mcp-cp1",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@RectiFlex/centerassist-mcp-cp1/mcp",
    "description": "Access and manage CenterPoint Connect data for property management, construction, and service oper‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-07T03:54:36.038569Z",
    "indexed_at": "2026-02-18T04:04:26.056188",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Access CenterPoint Connect data",
        "Manage CenterPoint Connect data",
        "Support property management workflows",
        "Support construction workflows",
        "Support service operations workflows"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of data access and management for specific domains but lacks detail, examples, or explicit limitations and requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "6867ad565e9f20fc",
    "name": "ai.smithery/RectiFlex-centerassist-mcp1",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@RectiFlex/centerassist-mcp1/mcp",
    "description": "Manage CenterPoint Connect data across properties, companies, employees, invoices, materials, and‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-08T05:13:44.681143Z",
    "indexed_at": "2026-02-18T04:04:27.643519",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage CenterPoint Connect data across properties",
        "Manage CenterPoint Connect data across companies",
        "Manage CenterPoint Connect data across employees",
        "Manage CenterPoint Connect data across invoices",
        "Manage CenterPoint Connect data across materials"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of data management capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "43bf2819dd7b70db",
    "name": "ai.smithery/STUzhy-py_execute_mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@STUzhy/py_execute_mcp/mcp",
    "description": "Run Python code in a secure sandbox without local setup. Declare inline dependencies and execute s‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-17T04:55:34.956438Z",
    "indexed_at": "2026-02-18T04:04:28.942940",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Run Python code in a secure sandbox",
        "Execute Python code without local setup",
        "Declare inline dependencies for Python execution"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functionality but lacks detailed information, examples, or explicit limitations and requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "af98ba11fe5f2ea6",
    "name": "ai.smithery/ScrapeGraphAI-scrapegraph-mcp",
    "source": "mcp",
    "source_url": "https://github.com/ScrapeGraphAI/scrapegraph-mcp",
    "description": "Enable language models to perform advanced AI-powered web scraping with enterprise-grade reliabili‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-25T21:42:44.755249Z",
    "indexed_at": "2026-02-18T04:04:30.089375",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# ScrapeGraph MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)\n[![smithery badge](https://smithery.ai/badge/@ScrapeGraphAI/scrapegraph-mcp)](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp)\n\n\nA production-ready [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server that provides seamless integration with the [ScrapeGraph AI](https://scrapegraphai.com) API. This server enables language models to leverage advanced AI-powered web scraping capabilities with enterprise-grade reliability.\n\n[![API Banner](https://raw.githubusercontent.com/ScrapeGraphAI/Scrapegraph-ai/main/docs/assets/api_banner.png)](https://scrapegraphai.com/?utm_source=github&utm_medium=readme&utm_campaign=api_banner&utm_content=api_banner_image)\n## Table of Contents\n\n- [Key Features](#key-features)\n- [Quick Start](#quick-start)\n- [Available Tools](#available-tools)\n- [Setup Instructions](#setup-instructions)\n- [Remote Server Usage](#remote-server-usage)\n- [Local Usage](#local-usage)\n- [Google ADK Integration](#google-adk-integration)\n- [Example Use Cases](#example-use-cases)\n- [Error Handling](#error-handling)\n- [Common Issues](#common-issues)\n- [Development](#development)\n- [Contributing](#contributing)\n- [Documentation](#documentation)\n- [Technology Stack](#technology-stack)\n- [License](#license)\n\n## Key Features\n\n- **8 Powerful Tools**: From simple markdown conversion to complex multi-page crawling and agentic workflows\n- **AI-Powered Extraction**: Intelligently extract structured data using natural language prompts\n- **Multi-Page Crawling**: SmartCrawler supports asynchronous crawling with configurable depth and page limits\n- **Infinite Scroll Support**: Handle dynamic content loading with configurable scroll counts\n- **JavaScript Rendering**: Full support for JavaScript-heavy websites\n- **Flexible Output Formats**: Get results as markdown, structured JSON, or custom schemas\n- **Easy Integration**: Works seamlessly with Claude Desktop, Cursor, and any MCP-compatible client\n- **Enterprise-Ready**: Robust error handling, timeout management, and production-tested reliability\n- **Simple Deployment**: One-command installation via Smithery or manual setup\n- **Comprehensive Documentation**: Detailed developer docs in `.agent/` folder\n\n## Quick Start\n\n### 1. Get Your API Key\n\nSign up and get your API key from the [ScrapeGraph Dashboard](https://dashboard.scrapegraphai.com)\n\n### 2. Install with Smithery (Recommended)\n\n```bash\nnpx -y @smithery/cli install @ScrapeGraphAI/scrapegraph-mcp --client claude\n```\n\n### 3. Start Using\n\nAsk Claude or Cursor:\n- \"Convert https://scrapegraphai.com to markdown\"\n- \"Extract all product prices from this e-commerce page\"\n- \"Research the latest AI developments and summarize findings\"\n\nThat's it! The server is now available to your AI assistant.\n\n## Available Tools\n\nThe server provides **8 enterprise-ready tools** for AI-powered web scraping:\n\n### Core Scraping Tools\n\n#### 1. `markdownify`\nTransform any webpage into clean, structured markdown format.\n\n```python\nmarkdownify(website_url: str)\n```\n- **Credits**: 2 per request\n- **Use case**: Quick webpage content extraction in markdown\n\n#### 2. `smartscraper`\nLeverage AI to extract structured data from any webpage with support for infinite scrolling.\n\n```python\nsmartscraper(\n    user_prompt: str,\n    website_url: str,\n    number_of_scrolls: int = None,\n    markdown_only: bool = None\n)\n```\n- **Credits**: 10+ (base) + variable based on scrolling\n- **Use case**: AI-powered data extraction with custom prompts\n\n#### 3. `searchscraper`\nExecute AI-powered web searches with structured, actionable results.\n\n```python\nsearchscraper(\n    user_prompt: str,\n    num_results: int = None,\n    number_of_scrolls: int = None,\n    time_range: str = None  # Filter by: past_hour, past_24_hours, past_week, past_month, past_year\n)\n```\n- **Credits**: Variable (3-20 websites √ó 10 credits)\n- **Use case**: Multi-source research and data aggregation\n- **Time filtering**: Use `time_range` to filter results by recency (e.g., `\"past_week\"` for recent results)\n\n### Advanced Scraping Tools\n\n#### 4. `scrape`\nBasic scraping endpoint to fetch page content with optional heavy JavaScript rendering.\n\n```python\nscrape(website_url: str, render_heavy_js: bool = None)\n```\n- **Use case**: Simple page content fetching with JS rendering support\n\n#### 5. `sitemap`\nExtract sitemap URLs and structure for any website.\n\n```python\nsitemap(website_url: str)\n```\n- **Use case**: Website structure analysis and URL discovery\n\n### Multi-Page Crawling\n\n#### 6. `smartcrawler_initiate`\nInitiate intelligent multi-page web crawling (asynchronous operation).\n\n```python\nsmartcrawler_initiate(\n    url: str,\n    prompt: str = None,\n    extraction_mode: str = \"ai\",\n    depth: int = None,\n    max_pages: int = None,\n    same_domain_only: bool = None\n)\n```\n- **AI Extraction Mode**: 10 credits per page - extracts structured data\n- **Markdown Mode**: 2 credits per page - converts to markdown\n- **Returns**: `request_id` for polling\n- **Use case**: Large-scale website crawling and data extraction\n\n#### 7. `smartcrawler_fetch_results`\nRetrieve results from asynchronous crawling operations.\n\n```python\nsmartcrawler_fetch_results(request_id: str)\n```\n- **Returns**: Status and results when crawling is complete\n- **Use case**: Poll for crawl completion and retrieve results\n\n### Intelligent Agent-Based Scraping\n\n#### 8. `agentic_scrapper`\nRun advanced agentic scraping workflows with customizable steps and structured output schemas.\n\n```python\nagentic_scrapper(\n    url: str,\n    user_prompt: str = None,\n    output_schema: dict = None,\n    steps: list = None,\n    ai_extraction: bool = None,\n    persistent_session: bool = None,\n    timeout_seconds: float = None\n)\n```\n- **Use case**: Complex multi-step workflows with custom schemas and persistent sessions\n\n## Setup Instructions\n\nTo utilize this server, you'll need a ScrapeGraph API key. Follow these steps to obtain one:\n\n1. Navigate to the [ScrapeGraph Dashboard](https://dashboard.scrapegraphai.com)\n2. Create an account and generate your API key\n\n### Automated Installation via Smithery\n\nFor automated installation of the ScrapeGraph API Integration Server using [Smithery](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp):\n\n```bash\nnpx -y @smithery/cli install @ScrapeGraphAI/scrapegraph-mcp --client claude\n```\n\n### Claude Desktop Configuration\n\nUpdate your Claude Desktop configuration file with the following settings (located on the top rigth of the Cursor page):\n\n(remember to add your API key inside the config)\n\n```json\n{\n    \"mcpServers\": {\n        \"@ScrapeGraphAI-scrapegraph-mcp\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"@smithery/cli@latest\",\n                \"run\",\n                \"@ScrapeGraphAI/scrapegraph-mcp\",\n                \"--config\",\n                \"\\\"{\\\\\\\"scrapegraphApiKey\\\\\\\":\\\\\\\"YOUR-SGAI-API-KEY\\\\\\\"}\\\"\"\n            ]\n        }\n    }\n}\n```\n\nThe configuration file is located at:\n- Windows: `%APPDATA%/Claude/claude_desktop_config.json`\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\n### Cursor Integration\n\nAdd the ScrapeGraphAI MCP server on the settings:\n\n![Cursor MCP Integration](assets/cursor_mcp.png)\n\n## Remote Server Usage\n\nConnect to our hosted MCP server - no local installation required!\n\n### Claude Desktop Configuration (Remote)\n\nAdd this to your Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS):\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapegraph-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote@0.1.25\",\n        \"https://scrapegraph-mcp.onrender.com/mcp\",\n        \"--header\",\n        \"X-API-Key:YOUR_API_KEY\"\n      ]\n    }\n  }\n}\n```\n\n### Cursor Configuration (Remote)\n\nCursor supports native HTTP MCP connections. Add to your Cursor MCP settings (`~/.cursor/mcp.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapegraph-mcp\": {\n      \"url\": \"https://scrapegraph-mcp.onrender.com/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### Benefits of Remote Server\n\n- **No local setup** - Just configure and start using\n- **Always up-to-date** - Automatically receives latest updates\n- **Cross-platform** - Works on any OS with Node.js\n\n## Local Usage\n\nTo run the MCP server locally for development or testing, follow these steps:\n\n### Prerequisites\n\n- Python 3.13 or higher\n- pip or uv package manager\n- ScrapeGraph API key\n\n### Installation\n\n1. **Clone the repository** (if you haven't already):\n\n```bash\ngit clone https://github.com/ScrapeGraphAI/scrapegraph-mcp\ncd scrapegraph-mcp\n```\n\n2. **Install the package**:\n\n```bash\n# Using pip\npip install -e .\n\n# Or using uv (faster)\nuv pip install -e .\n```\n\n3. **Set your API key**:\n\n```bash\n# macOS/Linux\nexport SGAI_API_KEY=your-api-key-here\n\n# Windows (PowerShell)\n$env:SGAI_API_KEY=\"your-api-key-here\"\n\n# Windows (CMD)\nset SGAI_API_KEY=your-api-key-here\n```\n\n### Running the Server Locally\n\nYou can run the server directly:\n\n```bash\n# Using the installed command\nscrapegraph-mcp\n\n# Or using Python module\npython -m scrapegraph_mcp.server\n```\n\nThe server will start and communicate via stdio (standard input/output), which is the standard MCP transport method.\n\n### Testing with MCP Inspector\n\nTest your local server using the MCP Inspector tool:\n\n```bash\nnpx @modelcontextprotocol/inspector python -m scrapegraph_mcp.server\n```\n\nThis provides a web interface to test all available tools interactively.\n\n### Configuring Claude Desktop for Local Server\n\nTo use your locally running server with Claude Desktop, update your configuration file:\n\n**macOS/Linux** (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n    \"mcpServers\": {\n        \"scrapegraph-mcp-local\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"-m\",\n                \"scrapegraph_mcp.server\"\n            ],\n            \"env\": {\n                \"SGAI_API_KEY\": \"your-api-key-here\"\n            }\n        }\n    }\n}\n```\n\n**Windows** (`%APPDATA%\\Claude\\claude_desktop_config.json`):\n\n```json\n{\n    \"mcpServers\": {\n        \"scrapegraph-mcp-local\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"-m\",\n                \"scrapegraph_mcp.server\"\n            ],\n            \"env\": {\n                \"SGAI_API_KEY\": \"your-api-key-here\"\n            }\n        }\n    }\n}\n```\n\n**Note**: Make sure Python is in your PATH. You can verify by running `python --version` in your terminal.\n\n### Configuring Cursor for Local Server\n\nIn Cursor's MCP settings, add a new server with:\n\n- **Command**: `python`\n- **Args**: `[\"-m\", \"scrapegraph_mcp.server\"]`\n- **Environment Variables**: `{\"SGAI_API_KEY\": \"your-api-key-here\"}`\n\n### Troubleshooting Local Setup\n\n**Server not starting:**\n- Verify Python is installed: `python --version`\n- Check that the package is installed: `pip list | grep scrapegraph-mcp`\n- Ensure API key is set: `echo $SGAI_API_KEY` (macOS/Linux) or `echo %SGAI_API_KEY%` (Windows)\n\n**Tools not appearing:**\n- Check Claude Desktop logs:\n  - macOS: `~/Library/Logs/Claude/`\n  - Windows: `%APPDATA%\\Claude\\Logs\\`\n- Verify the server starts without errors when run directly\n- Check that the configuration JSON is valid\n\n**Import errors:**\n- Reinstall the package: `pip install -e . --force-reinstall`\n- Verify dependencies: `pip install -r requirements.txt` (if available)\n\n## Google ADK Integration\n\nThe ScrapeGraph MCP server can be integrated with [Google ADK (Agent Development Kit)](https://github.com/google/adk) to create AI agents with web scraping capabilities.\n\n### Prerequisites\n\n- Python 3.13 or higher\n- Google ADK installed\n- ScrapeGraph API key\n\n### Installation\n\n1. **Install Google ADK** (if not already installed):\n\n```bash\npip install google-adk\n```\n\n2. **Set your API key**:\n\n```bash\nexport SGAI_API_KEY=your-api-key-here\n```\n\n### Basic Integration Example\n\nCreate an agent file (e.g., `agent.py`) with the following configuration:\n\n```python\nimport os\nfrom google.adk.agents import LlmAgent\nfrom google.adk.tools.mcp_tool.mcp_toolset import MCPToolset\nfrom google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams\nfrom mcp import StdioServerParameters\n\n# Path to the scrapegraph-mcp server directory\nSCRAPEGRAPH_MCP_PATH = \"/path/to/scrapegraph-mcp\"\n\n# Path to the server.py file\nSERVER_SCRIPT_PATH = os.path.join(\n    SCRAPEGRAPH_MCP_PATH, \n    \"src\", \n    \"scrapegraph_mcp\", \n    \"server.py\"\n)\n\nroot_agent = LlmAgent(\n    model='gemini-2.0-flash',\n    name='scrapegraph_assistant_agent',\n    instruction='Help the user with web scraping and data extraction using ScrapeGraph AI. '\n                'You can convert webpages to markdown, extract structured data using AI, '\n                'perform web searches, crawl multiple pages, and automate complex scraping workflows.',\n    tools=[\n        MCPToolset(\n            connection_params=StdioConnectionParams(\n                server_params=StdioServerParameters(\n                    command='python3',\n                    args=[\n                        SERVER_SCRIPT_PATH,\n                    ],\n                    env={\n                        'SGAI_API_KEY': os.getenv('SGAI_API_KEY'),\n                    },\n                ),\n                timeout=300.0,)\n            ),\n            # Optional: Filter which tools from the MCP server are exposed\n            # tool_filter=['markdownify', 'smartscraper', 'searchscraper']\n        )\n    ],\n)\n```\n\n### Configuration Options\n\n**Timeout Settings:**\n- Default timeout is 5 seconds, which may be too short for web scraping operations\n- Recommended: Set `timeout=300.0\n- Adjust based on your use case (crawling operations may need even longer timeouts)\n\n**Tool Filtering:**\n- By default, all 8 tools are exposed to the agent\n- Use `tool_filter` to limit which tools are available:\n  ```python\n  tool_filter=['markdownify', 'smartscraper', 'searchscraper']\n  ```\n\n**API Key Configuration:**\n- Set via environment variable: `export SGAI_API_KEY=your-key`\n- Or pass directly in `env` dict: `'SGAI_API_KEY': 'your-key-here'`\n- Environment variable approach is recommended for security\n\n### Usage Example\n\nOnce configured, your agent can use natural language to interact with web scraping tools:\n\n```python\n# The agent can now handle queries like:\n# - \"Convert https://example.com to markdown\"\n# - \"Extract all product prices from this e-commerce page\"\n# - \"Search for recent AI research papers and summarize them\"\n# - \"Crawl this documentation site and extract all API endpoints\"\n```\nFor more information about Google ADK, visit the [official documentation](https://github.com/google/adk).\n\n## Example Use Cases\n\nThe server enables sophisticated queries across various scraping scenarios:\n\n### Single Page Scraping\n- **Markdownify**: \"Convert the ScrapeGraph documentation page to markdown\"\n- **SmartScraper**: \"Extract all product names, prices, and ratings from this e-commerce page\"\n- **SmartScraper with scrolling**: \"Scrape this infinite scroll page with 5 scrolls and extract all items\"\n- **Basic Scrape**: \"Fetch the HTML content of this JavaScript-heavy page with full rendering\"\n\n### Search and Research\n- **SearchScraper**: \"Research and summarize recent developments in AI-powered web scraping\"\n- **SearchScraper**: \"Search for the top 5 articles about machine learning frameworks and extract key insights\"\n- **SearchScraper**: \"Find recent news about GPT-4 and provide a structured summary\"\n- **SearchScraper with time_range**: \"Search for AI news from the past week only\" (uses `time_range=\"past_week\"`)\n\n### Website Analysis\n- **Sitemap**: \"Extract the complete sitemap structure from the ScrapeGraph website\"\n- **Sitemap**: \"Discover all URLs on this blog site\"\n\n### Multi-Page Crawling\n- **SmartCrawler (AI mode)**: \"Crawl the entire documentation site and extract all API endpoints with descriptions\"\n- **SmartCrawler (Markdown mode)**: \"Convert all pages in the blog to markdown up to 2 levels deep\"\n- **SmartCrawler**: \"Extract all product information from an e-commerce site, maximum 100 pages, same domain only\"\n\n### Advanced Agentic Scraping\n- **Agentic Scraper**: \"Navigate through a multi-step authentication form and extract user dashboard data\"\n- **Agentic Scraper with schema**: \"Follow pagination links and compile a dataset with schema: {title, author, date, content}\"\n- **Agentic Scraper**: \"Execute a complex workflow: login, navigate to reports, download data, and extract summary statistics\"\n\n## Error Handling\n\nThe server implements robust error handling with detailed, actionable error messages for:\n\n- API authentication issues\n- Malformed URL structures\n- Network connectivity failures\n- Rate limiting and quota management\n\n## Common Issues\n\n### Windows-Specific Connection\n\nWhen running on Windows systems, you may need to use the following command to connect to the MCP server:\n\n```bash\nC:\\Windows\\System32\\cmd.exe /c npx -y @smithery/cli@latest run @ScrapeGraphAI/scrapegraph-mcp --config \"{\\\"scrapegraphApiKey\\\":\\\"YOUR-SGAI-API-KEY\\\"}\"\n```\n\nThis ensures proper execution in the Windows environment.\n\n### Other Common Issues\n\n**\"ScrapeGraph client not initialized\"**\n- **Cause**: Missing API key\n- **Solution**: Set `SGAI_API_KEY` environment variable or provide via `--config`\n\n**\"Error 401: Unauthorized\"**\n- **Cause**: Invalid API key\n- **Solution**: Verify your API key at the [ScrapeGraph Dashboard](https://dashboard.scrapegraphai.com)\n\n**\"Error 402: Payment Required\"**\n- **Cause**: Insufficient credits\n- **Solution**: Add credits to your ScrapeGraph account\n\n**SmartCrawler not returning results**\n- **Cause**: Still processing (asynchronous operation)\n- **Solution**: Keep polling `smartcrawler_fetch_results()` until status is \"completed\"\n\n**Tools not appearing in Claude Desktop**\n- **Cause**: Server not starting or configuration error\n- **Solution**: Check Claude logs at `~/Library/Logs/Claude/` (macOS) or `%APPDATA%\\Claude\\Logs\\` (Windows)\n\nFor detailed troubleshooting, see the [.agent documentation](.agent/README.md).\n\n## Development\n\n### Prerequisites\n\n- Python 3.13 or higher\n- pip or uv package manager\n- ScrapeGraph API key\n\n### Installation from Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/ScrapeGraphAI/scrapegraph-mcp\ncd scrapegraph-mcp\n\n# Install dependencies\npip install -e \".[dev]\"\n\n# Set your API key\nexport SGAI_API_KEY=your-api-key\n\n# Run the server\nscrapegraph-mcp\n# or\npython -m scrapegraph_mcp.server\n```\n\n### Testing with MCP Inspector\n\nTest your server locally using the MCP Inspector tool:\n\n```bash\nnpx @modelcontextprotocol/inspector scrapegraph-mcp\n```\n\nThis provides a web interface to test all available tools.\n\n### Code Quality\n\n**Linting:**\n```bash\nruff check src/\n```\n\n**Type Checking:**\n```bash\nmypy src/\n```\n\n**Format Checking:**\n```bash\nruff format --check src/\n```\n\n### Project Structure\n\n```\nscrapegraph-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ scrapegraph_mcp/\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py      # Package initialization\n‚îÇ       ‚îî‚îÄ‚îÄ server.py        # Main MCP server (all code in one file)\n‚îú‚îÄ‚îÄ .agent/                  # Developer documentation\n‚îÇ   ‚îú‚îÄ‚îÄ README.md           # Documentation index\n‚îÇ   ‚îî‚îÄ‚îÄ system/             # System architecture docs\n‚îú‚îÄ‚îÄ assets/                  # Images and badges\n‚îú‚îÄ‚îÄ pyproject.toml          # Project metadata & dependencies\n‚îú‚îÄ‚îÄ smithery.yaml           # Smithery deployment config\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n\n## Contributing\n\nWe welcome contributions! Here's how you can help:\n\n### Adding a New Tool\n\n1. **Add method to `ScapeGraphClient` class** in [server.py](src/scrapegraph_mcp/server.py):\n\n```python\ndef new_tool(self, param: str) -> Dict[str, Any]:\n    \"\"\"Tool description.\"\"\"\n    url = f\"{self.BASE_URL}/new-endpoint\"\n    data = {\"param\": param}\n    response = self.client.post(url, headers=self.headers, json=data)\n    if response.status_code != 200:\n        raise Exception(f\"Error {response.status_code}: {response.text}\")\n    return response.json()\n```\n\n2. **Add MCP tool decorator**:\n\n```python\n@mcp.tool()\ndef new_tool(param: str) -> Dict[str, Any]:\n    \"\"\"\n    Tool description for AI assistants.\n\n    Args:\n        param: Parameter description\n\n    Returns:\n        Dictionary containing results\n    \"\"\"\n    if scrapegraph_client is None:\n        return {\"error\": \"ScrapeGraph client not initialized. Please provide an API key.\"}\n\n    try:\n        return scrapegraph_client.new_tool(param)\n    except Exception as e:\n        return {\"error\": str(e)}\n```\n\n3. **Test with MCP Inspector**:\n```bash\nnpx @modelcontextprotocol/inspector scrapegraph-mcp\n```\n\n4. **Update documentation**:\n   - Add tool to this README\n   - Update [.agent documentation](.agent/README.md)\n\n5. **Submit a pull request**\n\n### Development Workflow\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run linting and type checking\n5. Test with MCP Inspector and Claude Desktop\n6. Update documentation\n7. Commit your changes (`git commit -m 'Add amazing feature'`)\n8. Push to the branch (`git push origin feature/amazing-feature`)\n9. Open a Pull Request\n\n### Code Style\n\n- **Line length**: 100 characters\n- **Type hints**: Required for all functions\n- **Docstrings**: Google-style docstrings\n- **Error handling**: Return error dicts, don't raise exceptions in tools\n- **Python version**: Target 3.13+\n\nFor detailed development guidelines, see the [.agent documentation](.agent/README.md).\n\n## Documentation\n\nFor comprehensive developer documentation, see:\n\n- **[.agent/README.md](.agent/README.md)** - Complete developer documentation index\n- **[.agent/system/project_architecture.md](.agent/system/project_architecture.md)** - System architecture and design\n- **[.agent/system/mcp_protocol.md](.agent/system/mcp_protocol.md)** - MCP protocol integration details\n\n## Technology Stack\n\n### Core Framework\n- **Python 3.13+** - Modern Python with type hints\n- **FastMCP** - Lightweight MCP server framework\n- **httpx 0.24.0+** - Modern async HTTP client\n\n### Development Tools\n- **Ruff** - Fast Python linter and formatter\n- **mypy** - Static type checker\n- **Hatchling** - Modern build backend\n\n### Deployment\n- **Smithery** - Automated MCP server deployment\n- **Docker** - Container support with Alpine Linux\n- **stdio transport** - Standard MCP communication\n\n### API Integration\n- **ScrapeGraph AI API** - Enterprise web scraping service\n- **Base URL**: `https://api.scrapegraphai.com/v1`\n- **Authentication**: API key-based\n\n## License\n\nThis project is distributed under the MIT License. For detailed terms and conditions, please refer to the LICENSE file.\n\n## Acknowledgments\n\nSpecial thanks to [tomekkorbak](https://github.com/tomekkorbak) for his implementation of [oura-mcp-server](https://github.com/tomekkorbak/oura-mcp-server), which served as starting point for this repo.\n\n## Resources\n\n### Official Links\n- [ScrapeGraph AI Homepage](https://scrapegraphai.com)\n- [ScrapeGraph Dashboard](https://dashboard.scrapegraphai.com) - Get your API key\n- [ScrapeGraph API Documentation](https://api.scrapegraphai.com/docs)\n- [GitHub Repository](https://github.com/ScrapeGraphAI/scrapegraph-mcp)\n\n### MCP Resources\n- [Model Context Protocol](https://modelcontextprotocol.io/) - Official MCP specification\n- [FastMCP Framework](https://github.com/jlowin/fastmcp) - Framework used by this server\n- [MCP Inspector](https://github.com/modelcontextprotocol/inspector) - Testing tool\n- [Smithery](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp) - MCP server distribution\n- mcp-name: io.github.ScrapeGraphAI/scrapegraph-mcp\n\n### AI Assistant Integration\n- [Claude Desktop](https://claude.ai/desktop) - Desktop app with MCP support\n- [Cursor](https://cursor.sh/) - AI-powered code editor\n\n### Support\n- [GitHub Issues](https://github.com/ScrapeGraphAI/scrapegraph-mcp/issues) - Report bugs or request features\n- [Developer Documentation](.agent/README.md) - Comprehensive dev docs\n\n---\n\nMade with ‚ù§Ô∏è by [ScrapeGraphAI](https://scrapegraphai.com) Team\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Transform any webpage into clean, structured markdown format",
        "Extract structured data from webpages using AI-powered natural language prompts",
        "Perform AI-powered web searches with structured, actionable results and time filtering",
        "Fetch page content with optional heavy JavaScript rendering support",
        "Extract sitemap URLs and website structure",
        "Initiate and manage asynchronous multi-page web crawling with configurable depth and page limits",
        "Retrieve results from asynchronous crawling operations",
        "Run advanced agentic scraping workflows with customizable steps, output schemas, and persistent sessions"
      ],
      "limitations": [
        "Credits are required per request and vary by tool and usage (e.g., scrolling, pages crawled)",
        "Rate limits and credit costs may restrict extensive or large-scale scraping",
        "Requires valid ScrapeGraph API key for all operations",
        "Local usage requires Python 3.13 or higher and proper environment setup",
        "Some tools have asynchronous operation requiring polling for results"
      ],
      "requirements": [
        "ScrapeGraph API key obtained from ScrapeGraph Dashboard",
        "Python 3.13 or higher for local server usage",
        "Node.js environment for Smithery CLI installation and remote server usage",
        "For Google ADK integration, Google ADK installed and configured",
        "Proper configuration of MCP clients like Claude Desktop or Cursor with API key and server settings"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with usage examples, configuration guidance for multiple clients, explicit limitations related to credits and environment, and integration notes, making it excellent in coverage and clarity.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# ScrapeGraph MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)\n[![smithery badge](https://smithery.ai/badge/@ScrapeGraphAI/scrapegraph-mcp)](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp)\n\n\nA production-ready [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) server that provides seamless integration with the [ScrapeGraph AI](https://scrapegraphai.com) API. This server enables language models to leverage advanced AI-powered web scraping capabilities with enterprise-grade reliability.",
        "start_pos": 0,
        "end_pos": 702,
        "token_count_estimate": 175,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 1,
        "text": "roll Support**: Handle dynamic content loading with configurable scroll counts\n- **JavaScript Rendering**: Full support for JavaScript-heavy websites\n- **Flexible Output Formats**: Get results as markdown, structured JSON, or custom schemas\n- **Easy Integration**: Works seamlessly with Claude Desktop, Cursor, and any MCP-compatible client\n- **Enterprise-Ready**: Robust error handling, timeout management, and production-tested reliability\n- **Simple Deployment**: One-command installation via Smithery or manual setup\n- **Comprehensive Documentation**: Detailed developer docs in `.agent/` folder\n\n## Quick Start\n\n### 1. Get Your API Key\n\nSign up and get your API key from the [ScrapeGraph Dashboard](https://dashboard.scrapegraphai.com)\n\n### 2. Install with Smithery (Recommended)\n\n```bash\nnpx -y @smithery/cli install @ScrapeGraphAI/scrapegraph-mcp --client claude\n```\n\n### 3. Start Using\n\nAsk Claude or Cursor:\n- \"Convert https://scrapegraphai.com to markdown\"\n- \"Extract all product prices from this e-commerce page\"\n- \"Research the latest AI developments and summarize findings\"\n\nThat's it! The server is now available to your AI assistant.\n\n## Available Tools\n\nThe server provides **8 enterprise-ready tools** for AI-powered web scraping:\n\n### Core Scraping Tools\n\n#### 1. `markdownify`\nTransform any webpage into clean, structured markdown format.\n\n```python\nmarkdownify(website_url: str)\n```\n- **Credits**: 2 per request\n- **Use case**: Quick webpage content extraction in markdown\n\n#### 2. `smartscraper`\nLeverage AI to extract structured data from any webpage with support for infinite scrolling.\n\n```python\nsmartscraper(\n    user_prompt: str,\n    website_url: str,\n    number_of_scrolls: int = None,\n    markdown_only: bool = None\n)\n```\n- **Credits**: 10+ (base) + variable based on scrolling\n- **Use case**: AI-powered data extraction with custom prompts\n\n#### 3. `searchscraper`\nExecute AI-powered web searches with structured, actionable results.",
        "start_pos": 1848,
        "end_pos": 3811,
        "token_count_estimate": 490,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 2,
        "text": "*: 10+ (base) + variable based on scrolling\n- **Use case**: AI-powered data extraction with custom prompts\n\n#### 3. `searchscraper`\nExecute AI-powered web searches with structured, actionable results.\n\n```python\nsearchscraper(\n    user_prompt: str,\n    num_results: int = None,\n    number_of_scrolls: int = None,\n    time_range: str = None  # Filter by: past_hour, past_24_hours, past_week, past_month, past_year\n)\n```\n- **Credits**: Variable (3-20 websites √ó 10 credits)\n- **Use case**: Multi-source research and data aggregation\n- **Time filtering**: Use `time_range` to filter results by recency (e.g., `\"past_week\"` for recent results)\n\n### Advanced Scraping Tools\n\n#### 4. `scrape`\nBasic scraping endpoint to fetch page content with optional heavy JavaScript rendering.\n\n```python\nscrape(website_url: str, render_heavy_js: bool = None)\n```\n- **Use case**: Simple page content fetching with JS rendering support\n\n#### 5. `sitemap`\nExtract sitemap URLs and structure for any website.\n\n```python\nsitemap(website_url: str)\n```\n- **Use case**: Website structure analysis and URL discovery\n\n### Multi-Page Crawling\n\n#### 6. `smartcrawler_initiate`\nInitiate intelligent multi-page web crawling (asynchronous operation).\n\n```python\nsmartcrawler_initiate(\n    url: str,\n    prompt: str = None,\n    extraction_mode: str = \"ai\",\n    depth: int = None,\n    max_pages: int = None,\n    same_domain_only: bool = None\n)\n```\n- **AI Extraction Mode**: 10 credits per page - extracts structured data\n- **Markdown Mode**: 2 credits per page - converts to markdown\n- **Returns**: `request_id` for polling\n- **Use case**: Large-scale website crawling and data extraction\n\n#### 7. `smartcrawler_fetch_results`\nRetrieve results from asynchronous crawling operations.\n\n```python\nsmartcrawler_fetch_results(request_id: str)\n```\n- **Returns**: Status and results when crawling is complete\n- **Use case**: Poll for crawl completion and retrieve results\n\n### Intelligent Agent-Based Scraping\n\n#### 8.",
        "start_pos": 3611,
        "end_pos": 5587,
        "token_count_estimate": 494,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 3,
        "text": "h_results(request_id: str)\n```\n- **Returns**: Status and results when crawling is complete\n- **Use case**: Poll for crawl completion and retrieve results\n\n### Intelligent Agent-Based Scraping\n\n#### 8. `agentic_scrapper`\nRun advanced agentic scraping workflows with customizable steps and structured output schemas.\n\n```python\nagentic_scrapper(\n    url: str,\n    user_prompt: str = None,\n    output_schema: dict = None,\n    steps: list = None,\n    ai_extraction: bool = None,\n    persistent_session: bool = None,\n    timeout_seconds: float = None\n)\n```\n- **Use case**: Complex multi-step workflows with custom schemas and persistent sessions\n\n## Setup Instructions\n\nTo utilize this server, you'll need a ScrapeGraph API key. Follow these steps to obtain one:\n\n1. Navigate to the [ScrapeGraph Dashboard](https://dashboard.scrapegraphai.com)\n2.",
        "start_pos": 5387,
        "end_pos": 6228,
        "token_count_estimate": 210,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 4,
        "text": "sktop_config.json`\n- macOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n\n### Cursor Integration\n\nAdd the ScrapeGraphAI MCP server on the settings:\n\n![Cursor MCP Integration](assets/cursor_mcp.png)\n\n## Remote Server Usage\n\nConnect to our hosted MCP server - no local installation required!\n\n### Claude Desktop Configuration (Remote)\n\nAdd this to your Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS):\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapegraph-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote@0.1.25\",\n        \"https://scrapegraph-mcp.onrender.com/mcp\",\n        \"--header\",\n        \"X-API-Key:YOUR_API_KEY\"\n      ]\n    }\n  }\n}\n```\n\n### Cursor Configuration (Remote)\n\nCursor supports native HTTP MCP connections. Add to your Cursor MCP settings (`~/.cursor/mcp.json`):\n\n```json\n{\n  \"mcpServers\": {\n    \"scrapegraph-mcp\": {\n      \"url\": \"https://scrapegraph-mcp.onrender.com/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### Benefits of Remote Server\n\n- **No local setup** - Just configure and start using\n- **Always up-to-date** - Automatically receives latest updates\n- **Cross-platform** - Works on any OS with Node.js\n\n## Local Usage\n\nTo run the MCP server locally for development or testing, follow these steps:\n\n### Prerequisites\n\n- Python 3.13 or higher\n- pip or uv package manager\n- ScrapeGraph API key\n\n### Installation\n\n1. **Clone the repository** (if you haven't already):\n\n```bash\ngit clone https://github.com/ScrapeGraphAI/scrapegraph-mcp\ncd scrapegraph-mcp\n```\n\n2. **Install the package**:\n\n```bash\n# Using pip\npip install -e .\n\n# Or using uv (faster)\nuv pip install -e .\n```\n\n3.",
        "start_pos": 7235,
        "end_pos": 8960,
        "token_count_estimate": 431,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 5,
        "text": "EY=\"your-api-key-here\"\n\n# Windows (CMD)\nset SGAI_API_KEY=your-api-key-here\n```\n\n### Running the Server Locally\n\nYou can run the server directly:\n\n```bash\n# Using the installed command\nscrapegraph-mcp\n\n# Or using Python module\npython -m scrapegraph_mcp.server\n```\n\nThe server will start and communicate via stdio (standard input/output), which is the standard MCP transport method.\n\n### Testing with MCP Inspector\n\nTest your local server using the MCP Inspector tool:\n\n```bash\nnpx @modelcontextprotocol/inspector python -m scrapegraph_mcp.server\n```\n\nThis provides a web interface to test all available tools interactively.\n\n### Configuring Claude Desktop for Local Server\n\nTo use your locally running server with Claude Desktop, update your configuration file:\n\n**macOS/Linux** (`~/Library/Application Support/Claude/claude_desktop_config.json`):\n\n```json\n{\n    \"mcpServers\": {\n        \"scrapegraph-mcp-local\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"-m\",\n                \"scrapegraph_mcp.server\"\n            ],\n            \"env\": {\n                \"SGAI_API_KEY\": \"your-api-key-here\"\n            }\n        }\n    }\n}\n```\n\n**Windows** (`%APPDATA%\\Claude\\claude_desktop_config.json`):\n\n```json\n{\n    \"mcpServers\": {\n        \"scrapegraph-mcp-local\": {\n            \"command\": \"python\",\n            \"args\": [\n                \"-m\",\n                \"scrapegraph_mcp.server\"\n            ],\n            \"env\": {\n                \"SGAI_API_KEY\": \"your-api-key-here\"\n            }\n        }\n    }\n}\n```\n\n**Note**: Make sure Python is in your PATH. You can verify by running `python --version` in your terminal.",
        "start_pos": 9083,
        "end_pos": 10715,
        "token_count_estimate": 408,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 6,
        "text": "-api-key-here\"}`\n\n### Troubleshooting Local Setup\n\n**Server not starting:**\n- Verify Python is installed: `python --version`\n- Check that the package is installed: `pip list | grep scrapegraph-mcp`\n- Ensure API key is set: `echo $SGAI_API_KEY` (macOS/Linux) or `echo %SGAI_API_KEY%` (Windows)\n\n**Tools not appearing:**\n- Check Claude Desktop logs:\n  - macOS: `~/Library/Logs/Claude/`\n  - Windows: `%APPDATA%\\Claude\\Logs\\`\n- Verify the server starts without errors when run directly\n- Check that the configuration JSON is valid\n\n**Import errors:**\n- Reinstall the package: `pip install -e . --force-reinstall`\n- Verify dependencies: `pip install -r requirements.txt` (if available)\n\n## Google ADK Integration\n\nThe ScrapeGraph MCP server can be integrated with [Google ADK (Agent Development Kit)](https://github.com/google/adk) to create AI agents with web scraping capabilities.\n\n### Prerequisites\n\n- Python 3.13 or higher\n- Google ADK installed\n- ScrapeGraph API key\n\n### Installation\n\n1. **Install Google ADK** (if not already installed):\n\n```bash\npip install google-adk\n```\n\n2. **Set your API key**:\n\n```bash\nexport SGAI_API_KEY=your-api-key-here\n```\n\n### Basic Integration Example\n\nCreate an agent file (e.g., `agent.py`) with the following configuration:\n\n```python\nimport os\nfrom google.adk.agents import LlmAgent\nfrom google.adk.tools.mcp_tool.mcp_toolset import MCPToolset\nfrom google.adk.tools.mcp_tool.mcp_session_manager import StdioConnectionParams\nfrom mcp import StdioServerParameters\n\n# Path to the scrapegraph-mcp server directory\nSCRAPEGRAPH_MCP_PATH = \"/path/to/scrapegraph-mcp\"\n\n# Path to the server.py file\nSERVER_SCRIPT_PATH = os.path.join(\n    SCRAPEGRAPH_MCP_PATH, \n    \"src\", \n    \"scrapegraph_mcp\", \n    \"server.py\"\n)\n\nroot_agent = LlmAgent(\n    model='gemini-2.0-flash',\n    name='scrapegraph_assistant_agent',\n    instruction='Help the user with web scraping and data extraction using ScrapeGraph AI.",
        "start_pos": 10931,
        "end_pos": 12858,
        "token_count_estimate": 481,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 7,
        "text": "\"server.py\"\n)\n\nroot_agent = LlmAgent(\n    model='gemini-2.0-flash',\n    name='scrapegraph_assistant_agent',\n    instruction='Help the user with web scraping and data extraction using ScrapeGraph AI.",
        "start_pos": 12658,
        "end_pos": 12858,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 8,
        "text": "le queries like:\n# - \"Convert https://example.com to markdown\"\n# - \"Extract all product prices from this e-commerce page\"\n# - \"Search for recent AI research papers and summarize them\"\n# - \"Crawl this documentation site and extract all API endpoints\"\n```\nFor more information about Google ADK, visit the [official documentation](https://github.com/google/adk).",
        "start_pos": 14506,
        "end_pos": 14865,
        "token_count_estimate": 89,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 9,
        "text": "raper**: \"Navigate through a multi-step authentication form and extract user dashboard data\"\n- **Agentic Scraper with schema**: \"Follow pagination links and compile a dataset with schema: {title, author, date, content}\"\n- **Agentic Scraper**: \"Execute a complex workflow: login, navigate to reports, download data, and extract summary statistics\"\n\n## Error Handling\n\nThe server implements robust error handling with detailed, actionable error messages for:\n\n- API authentication issues\n- Malformed URL structures\n- Network connectivity failures\n- Rate limiting and quota management\n\n## Common Issues\n\n### Windows-Specific Connection\n\nWhen running on Windows systems, you may need to use the following command to connect to the MCP server:\n\n```bash\nC:\\Windows\\System32\\cmd.exe /c npx -y @smithery/cli@latest run @ScrapeGraphAI/scrapegraph-mcp --config \"{\\\"scrapegraphApiKey\\\":\\\"YOUR-SGAI-API-KEY\\\"}\"\n```\n\nThis ensures proper execution in the Windows environment.\n\n### Other Common Issues\n\n**\"ScrapeGraph client not initialized\"**\n- **Cause**: Missing API key\n- **Solution**: Set `SGAI_API_KEY` environment variable or provide via `--config`\n\n**\"Error 401: Unauthorized\"**\n- **Cause**: Invalid API key\n- **Solution**: Verify your API key at the [ScrapeGraph Dashboard](https://dashboard.scrapegraphai.com)\n\n**\"Error 402: Payment Required\"**\n- **Cause**: Insufficient credits\n- **Solution**: Add credits to your ScrapeGraph account\n\n**SmartCrawler not returning results**\n- **Cause**: Still processing (asynchronous operation)\n- **Solution**: Keep polling `smartcrawler_fetch_results()` until status is \"completed\"\n\n**Tools not appearing in Claude Desktop**\n- **Cause**: Server not starting or configuration error\n- **Solution**: Check Claude logs at `~/Library/Logs/Claude/` (macOS) or `%APPDATA%\\Claude\\Logs\\` (Windows)\n\nFor detailed troubleshooting, see the [.agent documentation](.agent/README.md).",
        "start_pos": 16354,
        "end_pos": 18253,
        "token_count_estimate": 474,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 10,
        "text": "ation error\n- **Solution**: Check Claude logs at `~/Library/Logs/Claude/` (macOS) or `%APPDATA%\\Claude\\Logs\\` (Windows)\n\nFor detailed troubleshooting, see the [.agent documentation](.agent/README.md).\n\n## Development\n\n### Prerequisites\n\n- Python 3.13 or higher\n- pip or uv package manager\n- ScrapeGraph API key\n\n### Installation from Source\n\n```bash\n# Clone the repository\ngit clone https://github.com/ScrapeGraphAI/scrapegraph-mcp\ncd scrapegraph-mcp\n\n# Install dependencies\npip install -e \".[dev]\"\n\n# Set your API key\nexport SGAI_API_KEY=your-api-key\n\n# Run the server\nscrapegraph-mcp\n# or\npython -m scrapegraph_mcp.server\n```\n\n### Testing with MCP Inspector\n\nTest your server locally using the MCP Inspector tool:\n\n```bash\nnpx @modelcontextprotocol/inspector scrapegraph-mcp\n```\n\nThis provides a web interface to test all available tools.\n\n### Code Quality\n\n**Linting:**\n```bash\nruff check src/\n```\n\n**Type Checking:**\n```bash\nmypy src/\n```\n\n**Format Checking:**\n```bash\nruff format --check src/\n```\n\n### Project Structure\n\n```\nscrapegraph-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ scrapegraph_mcp/\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py      # Package initialization\n‚îÇ       ‚îî‚îÄ‚îÄ server.py        # Main MCP server (all code in one file)\n‚îú‚îÄ‚îÄ .agent/                  # Developer documentation\n‚îÇ   ‚îú‚îÄ‚îÄ README.md           # Documentation index\n‚îÇ   ‚îî‚îÄ‚îÄ system/             # System architecture docs\n‚îú‚îÄ‚îÄ assets/                  # Images and badges\n‚îú‚îÄ‚îÄ pyproject.toml          # Project metadata & dependencies\n‚îú‚îÄ‚îÄ smithery.yaml           # Smithery deployment config\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n\n## Contributing\n\nWe welcome contributions! Here's how you can help:\n\n### Adding a New Tool\n\n1.",
        "start_pos": 18053,
        "end_pos": 19725,
        "token_count_estimate": 418,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 11,
        "text": "\"\"\"\n    url = f\"{self.BASE_URL}/new-endpoint\"\n    data = {\"param\": param}\n    response = self.client.post(url, headers=self.headers, json=data)\n    if response.status_code != 200:\n        raise Exception(f\"Error {response.status_code}: {response.text}\")\n    return response.json()\n```\n\n2. **Add MCP tool decorator**:\n\n```python\n@mcp.tool()\ndef new_tool(param: str) -> Dict[str, Any]:\n    \"\"\"\n    Tool description for AI assistants.\n\n    Args:\n        param: Parameter description\n\n    Returns:\n        Dictionary containing results\n    \"\"\"\n    if scrapegraph_client is None:\n        return {\"error\": \"ScrapeGraph client not initialized. Please provide an API key.\"}\n\n    try:\n        return scrapegraph_client.new_tool(param)\n    except Exception as e:\n        return {\"error\": str(e)}\n```\n\n3. **Test with MCP Inspector**:\n```bash\nnpx @modelcontextprotocol/inspector scrapegraph-mcp\n```\n\n4. **Update documentation**:\n   - Add tool to this README\n   - Update [.agent documentation](.agent/README.md)\n\n5. **Submit a pull request**\n\n### Development Workflow\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Run linting and type checking\n5. Test with MCP Inspector and Claude Desktop\n6. Update documentation\n7. Commit your changes (`git commit -m 'Add amazing feature'`)\n8. Push to the branch (`git push origin feature/amazing-feature`)\n9. Open a Pull Request\n\n### Code Style\n\n- **Line length**: 100 characters\n- **Type hints**: Required for all functions\n- **Docstrings**: Google-style docstrings\n- **Error handling**: Return error dicts, don't raise exceptions in tools\n- **Python version**: Target 3.13+\n\nFor detailed development guidelines, see the [.agent documentation](.agent/README.md).",
        "start_pos": 19901,
        "end_pos": 21661,
        "token_count_estimate": 440,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 12,
        "text": "md](.agent/README.md)** - Complete developer documentation index\n- **[.agent/system/project_architecture.md](.agent/system/project_architecture.md)** - System architecture and design\n- **[.agent/system/mcp_protocol.md](.agent/system/mcp_protocol.md)** - MCP protocol integration details\n\n## Technology Stack\n\n### Core Framework\n- **Python 3.13+** - Modern Python with type hints\n- **FastMCP** - Lightweight MCP server framework\n- **httpx 0.24.0+** - Modern async HTTP client\n\n### Development Tools\n- **Ruff** - Fast Python linter and formatter\n- **mypy** - Static type checker\n- **Hatchling** - Modern build backend\n\n### Deployment\n- **Smithery** - Automated MCP server deployment\n- **Docker** - Container support with Alpine Linux\n- **stdio transport** - Standard MCP communication\n\n### API Integration\n- **ScrapeGraph AI API** - Enterprise web scraping service\n- **Base URL**: `https://api.scrapegraphai.com/v1`\n- **Authentication**: API key-based\n\n## License\n\nThis project is distributed under the MIT License. For detailed terms and conditions, please refer to the LICENSE file.\n\n## Acknowledgments\n\nSpecial thanks to [tomekkorbak](https://github.com/tomekkorbak) for his implementation of [oura-mcp-server](https://github.com/tomekkorbak/oura-mcp-server), which served as starting point for this repo.",
        "start_pos": 21749,
        "end_pos": 23055,
        "token_count_estimate": 326,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      },
      {
        "chunk_id": 13,
        "text": "ub.com/modelcontextprotocol/inspector) - Testing tool\n- [Smithery](https://smithery.ai/server/@ScrapeGraphAI/scrapegraph-mcp) - MCP server distribution\n- mcp-name: io.github.ScrapeGraphAI/scrapegraph-mcp\n\n### AI Assistant Integration\n- [Claude Desktop](https://claude.ai/desktop) - Desktop app with MCP support\n- [Cursor](https://cursor.sh/) - AI-powered code editor\n\n### Support\n- [GitHub Issues](https://github.com/ScrapeGraphAI/scrapegraph-mcp/issues) - Report bugs or request features\n- [Developer Documentation](.agent/README.md) - Comprehensive dev docs\n\n---\n\nMade with ‚ù§Ô∏è by [ScrapeGraphAI](https://scrapegraphai.com) Team",
        "start_pos": 23597,
        "end_pos": 24227,
        "token_count_estimate": 157,
        "source_type": "readme",
        "agent_id": "af98ba11fe5f2ea6"
      }
    ]
  },
  {
    "agent_id": "52be631344acb0a1",
    "name": "ai.smithery/TakoData-tako-mcp",
    "source": "mcp",
    "source_url": "https://github.com/TakoData/tako-mcp",
    "description": "Provide real-time data querying and visualization by integrating Tako with your agents. Generate o‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T20:40:52.702786Z",
    "indexed_at": "2026-02-18T04:04:32.148664",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Tako MCP Server\n\nAn MCP (Model Context Protocol) server that provides access to Tako's knowledge base and interactive data visualizations.\n\n## What is this?\n\nThis MCP server enables AI agents to:\n\n- **Search** Tako's knowledge base for charts and data visualizations\n- **Fetch** chart preview images and AI-generated insights\n- **Render** fully interactive Tako charts via MCP-UI\n\n## Installation\n\n```bash\npip install tako-mcp\n```\n\nOr install from source:\n\n```bash\ngit clone https://github.com/anthropics/tako-mcp.git\ncd tako-mcp\npip install -e .\n```\n\n## Quick Start\n\n### Get an API Token\n\nSign up at [trytako.com](https://trytako.com) and create an API token in your account settings.\n\n### Run the Server\n\n```bash\ntako-mcp\n```\n\nOr with Docker:\n\n```bash\ndocker build -t tako-mcp .\ndocker run -p 8001:8001 tako-mcp\n```\n\n### Connect Your Agent\n\nPoint your MCP client to `http://localhost:8001`.\n\n## Available Tools\n\n### `knowledge_search`\n\nSearch Tako's knowledge base for charts and data visualizations.\n\n```json\n{\n  \"query\": \"Intel vs Nvidia headcount\",\n  \"api_token\": \"your-api-token\",\n  \"count\": 5,\n  \"search_effort\": \"deep\"\n}\n```\n\nReturns matching charts with IDs, titles, descriptions, and URLs.\n\n### `get_chart_image`\n\nGet a preview image URL for a chart.\n\n```json\n{\n  \"pub_id\": \"chart-id\",\n  \"api_token\": \"your-api-token\",\n  \"dark_mode\": true\n}\n```\n\n### `get_card_insights`\n\nGet AI-generated insights for a chart.\n\n```json\n{\n  \"pub_id\": \"chart-id\",\n  \"api_token\": \"your-api-token\",\n  \"effort\": \"medium\"\n}\n```\n\nReturns bullet-point insights and a natural language description.\n\n### `explore_knowledge_graph`\n\nDiscover available entities, metrics, and cohorts.\n\n```json\n{\n  \"query\": \"tech companies\",\n  \"api_token\": \"your-api-token\",\n  \"limit\": 20\n}\n```\n\n## ThinViz API - Create Custom Charts\n\nThinViz lets you create charts with your own data using pre-configured templates.\n\n### `list_chart_schemas`\n\nList available chart templates.\n\n```json\n{\n  \"api_token\": \"your-api-token\"\n}\n```\n\nReturns schemas like `stock_card`, `bar_chart`, `grouped_bar_chart`.\n\n### `get_chart_schema`\n\nGet detailed info about a schema including required components.\n\n```json\n{\n  \"schema_name\": \"bar_chart\",\n  \"api_token\": \"your-api-token\"\n}\n```\n\n### `create_chart`\n\nCreate a chart from a template with your data.\n\n```json\n{\n  \"schema_name\": \"bar_chart\",\n  \"api_token\": \"your-api-token\",\n  \"source\": \"Company Reports\",\n  \"components\": [\n    {\n      \"component_type\": \"header\",\n      \"config\": {\n        \"title\": \"Revenue by Region\",\n        \"subtitle\": \"Q4 2024\"\n      }\n    },\n    {\n      \"component_type\": \"categorical_bar\",\n      \"config\": {\n        \"datasets\": [{\n          \"label\": \"Revenue\",\n          \"data\": [\n            {\"x\": \"North America\", \"y\": 120},\n            {\"x\": \"Europe\", \"y\": 98},\n            {\"x\": \"Asia\", \"y\": 156}\n          ],\n          \"units\": \"$M\"\n        }],\n        \"title\": \"Revenue by Region\"\n      }\n    }\n  ]\n}\n```\n\nReturns the new chart's `card_id`, `embed_url`, and `image_url`.\n\n## MCP-UI - Interactive Charts\n\n### `open_chart_ui`\n\nOpen an interactive chart in the UI (MCP-UI).\n\n```json\n{\n  \"pub_id\": \"chart-id\",\n  \"dark_mode\": true,\n  \"width\": 900,\n  \"height\": 600\n}\n```\n\nReturns a UIResource for rendering an interactive iframe.\n\n## Configuration\n\nEnvironment variables:\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `TAKO_API_URL` | Tako API endpoint | `https://api.trytako.com` |\n| `PUBLIC_BASE_URL` | Public URL for chart embeds | `https://trytako.com` |\n| `PORT` | Server port | `8001` |\n| `HOST` | Server host | `0.0.0.0` |\n| `MCP_ALLOWED_HOSTS` | Additional allowed hosts (comma-separated) | |\n| `MCP_ENABLE_DNS_REBINDING` | Enable DNS rebinding protection | `true` |\n\n## Testing\n\nRun the test client:\n\n```bash\npython -m tests.test_client --api-token YOUR_API_TOKEN\n```\n\nThis verifies:\n- MCP handshake and initialization\n- Tool discovery\n- Search, images, and insights\n- MCP-UI resource generation\n\n## Example Flow\n\n1. User asks: \"Show me a chart about Intel vs Nvidia headcount\"\n2. Agent calls `knowledge_search` with the query\n3. Agent receives chart results with IDs\n4. Agent can:\n   - Call `get_card_insights` to summarize the data\n   - Call `get_chart_image` for a preview\n   - Call `open_chart_ui` to render an interactive chart\n\n## Health Checks\n\n- `GET /health` - Simple \"ok\" response\n- `GET /health/detailed` - JSON with status and timestamp\n\n## Architecture\n\n```\nAI Agent (LangGraph, CopilotKit, etc.)\n    ‚Üì\n  MCP Protocol (SSE)\n    ‚Üì\nTako MCP Server\n    ‚Üì\nTako API\n```\n\nThe server acts as a thin proxy that:\n1. Authenticates requests with your API token\n2. Translates MCP tool calls to Tako API requests\n3. Returns formatted results and UI resources\n\n## MCP-UI Support\n\nThe `open_chart_ui` tool returns an MCP-UI resource that clients can render as an interactive iframe. The embedded chart supports:\n\n- Zooming and panning\n- Hover interactions\n- Responsive resizing via `postMessage`\n- Light and dark themes\n\nClients that support MCP-UI (like CopilotKit) will automatically render these resources.\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n## Links\n\n- [Tako](https://trytako.com) - Data visualization platform\n- [MCP Specification](https://spec.modelcontextprotocol.io/) - Model Context Protocol\n- [MCP-UI](https://mcpui.dev/) - MCP UI rendering standard\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search Tako's knowledge base for charts and data visualizations",
        "Fetch chart preview images",
        "Retrieve AI-generated insights for charts",
        "Discover entities, metrics, and cohorts via knowledge graph exploration",
        "Create custom charts using pre-configured templates with ThinViz API",
        "Open and render fully interactive Tako charts via MCP-UI",
        "Authenticate and proxy MCP tool calls to Tako API",
        "Provide health check endpoints for server status"
      ],
      "limitations": [
        "Requires valid API token for all operations",
        "Acts as a thin proxy and does not store data locally",
        "Interactive chart rendering depends on client support for MCP-UI",
        "No explicit rate limits mentioned in documentation"
      ],
      "requirements": [
        "Python environment with pip for installation",
        "API token obtained from trytako.com user account",
        "Network access to Tako API endpoint (default https://api.trytako.com)",
        "Optional Docker environment for containerized deployment"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for all tools, configuration options, testing guidance, and clearly states requirements and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Tako MCP Server\n\nAn MCP (Model Context Protocol) server that provides access to Tako's knowledge base and interactive data visualizations.\n\n## What is this?\n\nThis MCP server enables AI agents to:\n\n- **Search** Tako's knowledge base for charts and data visualizations\n- **Fetch** chart preview images and AI-generated insights\n- **Render** fully interactive Tako charts via MCP-UI\n\n## Installation\n\n```bash\npip install tako-mcp\n```\n\nOr install from source:\n\n```bash\ngit clone https://github.com/anthropics/tako-mcp.git\ncd tako-mcp\npip install -e .\n```\n\n## Quick Start\n\n### Get an API Token\n\nSign up at [trytako.com](https://trytako.com) and create an API token in your account settings.\n\n### Run the Server\n\n```bash\ntako-mcp\n```\n\nOr with Docker:\n\n```bash\ndocker build -t tako-mcp .\ndocker run -p 8001:8001 tako-mcp\n```\n\n### Connect Your Agent\n\nPoint your MCP client to `http://localhost:8001`.\n\n## Available Tools\n\n### `knowledge_search`\n\nSearch Tako's knowledge base for charts and data visualizations.\n\n```json\n{\n  \"query\": \"Intel vs Nvidia headcount\",\n  \"api_token\": \"your-api-token\",\n  \"count\": 5,\n  \"search_effort\": \"deep\"\n}\n```\n\nReturns matching charts with IDs, titles, descriptions, and URLs.\n\n### `get_chart_image`\n\nGet a preview image URL for a chart.\n\n```json\n{\n  \"pub_id\": \"chart-id\",\n  \"api_token\": \"your-api-token\",\n  \"dark_mode\": true\n}\n```\n\n### `get_card_insights`\n\nGet AI-generated insights for a chart.\n\n```json\n{\n  \"pub_id\": \"chart-id\",\n  \"api_token\": \"your-api-token\",\n  \"effort\": \"medium\"\n}\n```\n\nReturns bullet-point insights and a natural language description.\n\n### `explore_knowledge_graph`\n\nDiscover available entities, metrics, and cohorts.\n\n```json\n{\n  \"query\": \"tech companies\",\n  \"api_token\": \"your-api-token\",\n  \"limit\": 20\n}\n```\n\n## ThinViz API - Create Custom Charts\n\nThinViz lets you create charts with your own data using pre-configured templates.\n\n### `list_chart_schemas`\n\nList available chart templates.",
        "start_pos": 0,
        "end_pos": 1940,
        "token_count_estimate": 485,
        "source_type": "readme",
        "agent_id": "52be631344acb0a1"
      },
      {
        "chunk_id": 1,
        "text": "\"limit\": 20\n}\n```\n\n## ThinViz API - Create Custom Charts\n\nThinViz lets you create charts with your own data using pre-configured templates.\n\n### `list_chart_schemas`\n\nList available chart templates.\n\n```json\n{\n  \"api_token\": \"your-api-token\"\n}\n```\n\nReturns schemas like `stock_card`, `bar_chart`, `grouped_bar_chart`.\n\n### `get_chart_schema`\n\nGet detailed info about a schema including required components.\n\n```json\n{\n  \"schema_name\": \"bar_chart\",\n  \"api_token\": \"your-api-token\"\n}\n```\n\n### `create_chart`\n\nCreate a chart from a template with your data.\n\n```json\n{\n  \"schema_name\": \"bar_chart\",\n  \"api_token\": \"your-api-token\",\n  \"source\": \"Company Reports\",\n  \"components\": [\n    {\n      \"component_type\": \"header\",\n      \"config\": {\n        \"title\": \"Revenue by Region\",\n        \"subtitle\": \"Q4 2024\"\n      }\n    },\n    {\n      \"component_type\": \"categorical_bar\",\n      \"config\": {\n        \"datasets\": [{\n          \"label\": \"Revenue\",\n          \"data\": [\n            {\"x\": \"North America\", \"y\": 120},\n            {\"x\": \"Europe\", \"y\": 98},\n            {\"x\": \"Asia\", \"y\": 156}\n          ],\n          \"units\": \"$M\"\n        }],\n        \"title\": \"Revenue by Region\"\n      }\n    }\n  ]\n}\n```\n\nReturns the new chart's `card_id`, `embed_url`, and `image_url`.\n\n## MCP-UI - Interactive Charts\n\n### `open_chart_ui`\n\nOpen an interactive chart in the UI (MCP-UI).\n\n```json\n{\n  \"pub_id\": \"chart-id\",\n  \"dark_mode\": true,\n  \"width\": 900,\n  \"height\": 600\n}\n```\n\nReturns a UIResource for rendering an interactive iframe.",
        "start_pos": 1740,
        "end_pos": 3248,
        "token_count_estimate": 376,
        "source_type": "readme",
        "agent_id": "52be631344acb0a1"
      },
      {
        "chunk_id": 2,
        "text": "LLOWED_HOSTS` | Additional allowed hosts (comma-separated) | |\n| `MCP_ENABLE_DNS_REBINDING` | Enable DNS rebinding protection | `true` |\n\n## Testing\n\nRun the test client:\n\n```bash\npython -m tests.test_client --api-token YOUR_API_TOKEN\n```\n\nThis verifies:\n- MCP handshake and initialization\n- Tool discovery\n- Search, images, and insights\n- MCP-UI resource generation\n\n## Example Flow\n\n1. User asks: \"Show me a chart about Intel vs Nvidia headcount\"\n2. Agent calls `knowledge_search` with the query\n3. Agent receives chart results with IDs\n4. Agent can:\n   - Call `get_card_insights` to summarize the data\n   - Call `get_chart_image` for a preview\n   - Call `open_chart_ui` to render an interactive chart\n\n## Health Checks\n\n- `GET /health` - Simple \"ok\" response\n- `GET /health/detailed` - JSON with status and timestamp\n\n## Architecture\n\n```\nAI Agent (LangGraph, CopilotKit, etc.)\n    ‚Üì\n  MCP Protocol (SSE)\n    ‚Üì\nTako MCP Server\n    ‚Üì\nTako API\n```\n\nThe server acts as a thin proxy that:\n1. Authenticates requests with your API token\n2. Translates MCP tool calls to Tako API requests\n3. Returns formatted results and UI resources\n\n## MCP-UI Support\n\nThe `open_chart_ui` tool returns an MCP-UI resource that clients can render as an interactive iframe. The embedded chart supports:\n\n- Zooming and panning\n- Hover interactions\n- Responsive resizing via `postMessage`\n- Light and dark themes\n\nClients that support MCP-UI (like CopilotKit) will automatically render these resources.\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n\n## Links\n\n- [Tako](https://trytako.com) - Data visualization platform\n- [MCP Specification](https://spec.modelcontextprotocol.io/) - Model Context Protocol\n- [MCP-UI](https://mcpui.dev/) - MCP UI rendering standard",
        "start_pos": 3588,
        "end_pos": 5346,
        "token_count_estimate": 439,
        "source_type": "readme",
        "agent_id": "52be631344acb0a1"
      }
    ]
  },
  {
    "agent_id": "fb874ea9ac6304d4",
    "name": "ai.smithery/a-ariff-canvas-instant-mcp",
    "source": "mcp",
    "source_url": "https://github.com/a-ariff/canvas-instant-mcp",
    "description": "Manage your Canvas coursework with quick access to courses, assignments, and grades. Track upcomin‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-05T00:50:43.815585Z",
    "indexed_at": "2026-02-18T04:04:34.249542",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage Canvas coursework",
        "Access courses quickly",
        "Access assignments quickly",
        "Access grades quickly",
        "Track upcoming coursework"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functionality but lacks detailed examples, structure, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "00e86a533d09d371",
    "name": "ai.smithery/aamangeldi-dad-jokes-mcp",
    "source": "mcp",
    "source_url": "https://github.com/aamangeldi/dad-jokes-mcp",
    "description": "Get a random dad joke or search by keyword to fit any moment. Retrieve specific jokes by ID for re‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T00:21:20.72031Z",
    "indexed_at": "2026-02-18T04:04:39.453287",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Dad Jokes MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@aamangeldi/dad-jokes-mcp)](https://smithery.ai/server/@aamangeldi/dad-jokes-mcp)\n\nA lightweight Model Context Protocol (MCP) server that provides dad jokes from [icanhazdadjoke.com](https://icanhazdadjoke.com).\n\n## Features\n\n- üé≠ Get random dad jokes\n- üîç Search jokes by keyword\n- üÜî Retrieve specific jokes by ID\n- ‚ö° Fast and lightweight\n- üöÄ Ready for Smithery deployment\n\n## Tools\n\n### `get_random_joke_tool`\nGet a random dad joke.\n\n**Example:**\n```json\n{}\n```\n\n### `search_jokes_tool`\nSearch for dad jokes containing a specific term.\n\n**Parameters:**\n- `term` (string, required): Search term to find jokes\n- `limit` (integer, optional): Number of jokes to return (default: 5, max: 30)\n\n**Example:**\n```json\n{\n  \"term\": \"pizza\",\n  \"limit\": 3\n}\n```\n\n### `get_joke_by_id_tool`\nRetrieve a specific joke by its ID.\n\n**Parameters:**\n- `joke_id` (string, required): The ID of the joke to retrieve\n\n**Example:**\n```json\n{\n  \"joke_id\": \"R7UfaahVfFd\"\n}\n```\n\n## Local Development\n\n### Prerequisites\n- Python 3.11+\n- pip\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/aamangeldi/dad-jokes-mcp.git\ncd dad-jokes-mcp\n```\n\n2. Create a virtual environment and install dependencies:\n```bash\npython3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install fastmcp smithery httpx\n```\n\n3. Run the server locally:\n```bash\nfastmcp run server.py\n```\n\n## Deployment to Smithery\n\n1. Push your code to GitHub\n2. Connect your repository to [Smithery](https://smithery.ai)\n3. Smithery will automatically detect the configuration and deploy your server\n\nThe server uses:\n- `runtime: python` in `smithery.yaml`\n- FastMCP for the server implementation\n- `@smithery.server()` decorator for configuration\n\n## Configuration\n\nThe server requires no authentication or configuration. It uses the free icanhazdadjoke.com API with the following defaults:\n- API: `https://icanhazdadjoke.com`\n- No API key required\n- Rate limiting follows icanhazdadjoke.com policies\n\n## Credits\n\nDad jokes provided by [icanhazdadjoke.com](https://icanhazdadjoke.com)\n\n## License\n\nMIT License - see LICENSE file for details\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Get random dad jokes",
        "Search dad jokes by keyword",
        "Retrieve specific dad jokes by ID",
        "Deploy the server easily on Smithery platform",
        "Run the server locally using FastMCP"
      ],
      "limitations": [
        "Rate limiting is subject to icanhazdadjoke.com API policies",
        "No authentication or API key support",
        "Maximum of 30 jokes can be returned in a search query"
      ],
      "requirements": [
        "Python 3.11 or higher",
        "pip package manager",
        "Dependencies: fastmcp, smithery, httpx",
        "No API key or authentication needed"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides installation instructions, usage examples for all tools, deployment guidance, configuration details, and mentions limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Dad Jokes MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@aamangeldi/dad-jokes-mcp)](https://smithery.ai/server/@aamangeldi/dad-jokes-mcp)\n\nA lightweight Model Context Protocol (MCP) server that provides dad jokes from [icanhazdadjoke.com](https://icanhazdadjoke.com).\n\n## Features\n\n- üé≠ Get random dad jokes\n- üîç Search jokes by keyword\n- üÜî Retrieve specific jokes by ID\n- ‚ö° Fast and lightweight\n- üöÄ Ready for Smithery deployment\n\n## Tools\n\n### `get_random_joke_tool`\nGet a random dad joke.\n\n**Example:**\n```json\n{}\n```\n\n### `search_jokes_tool`\nSearch for dad jokes containing a specific term.\n\n**Parameters:**\n- `term` (string, required): Search term to find jokes\n- `limit` (integer, optional): Number of jokes to return (default: 5, max: 30)\n\n**Example:**\n```json\n{\n  \"term\": \"pizza\",\n  \"limit\": 3\n}\n```\n\n### `get_joke_by_id_tool`\nRetrieve a specific joke by its ID.\n\n**Parameters:**\n- `joke_id` (string, required): The ID of the joke to retrieve\n\n**Example:**\n```json\n{\n  \"joke_id\": \"R7UfaahVfFd\"\n}\n```\n\n## Local Development\n\n### Prerequisites\n- Python 3.11+\n- pip\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/aamangeldi/dad-jokes-mcp.git\ncd dad-jokes-mcp\n```\n\n2. Create a virtual environment and install dependencies:\n```bash\npython3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install fastmcp smithery httpx\n```\n\n3. Run the server locally:\n```bash\nfastmcp run server.py\n```\n\n## Deployment to Smithery\n\n1. Push your code to GitHub\n2. Connect your repository to [Smithery](https://smithery.ai)\n3. Smithery will automatically detect the configuration and deploy your server\n\nThe server uses:\n- `runtime: python` in `smithery.yaml`\n- FastMCP for the server implementation\n- `@smithery.server()` decorator for configuration\n\n## Configuration\n\nThe server requires no authentication or configuration.",
        "start_pos": 0,
        "end_pos": 1874,
        "token_count_estimate": 468,
        "source_type": "readme",
        "agent_id": "00e86a533d09d371"
      },
      {
        "chunk_id": 1,
        "text": "untime: python` in `smithery.yaml`\n- FastMCP for the server implementation\n- `@smithery.server()` decorator for configuration\n\n## Configuration\n\nThe server requires no authentication or configuration. It uses the free icanhazdadjoke.com API with the following defaults:\n- API: `https://icanhazdadjoke.com`\n- No API key required\n- Rate limiting follows icanhazdadjoke.com policies\n\n## Credits\n\nDad jokes provided by [icanhazdadjoke.com](https://icanhazdadjoke.com)\n\n## License\n\nMIT License - see LICENSE file for details",
        "start_pos": 1674,
        "end_pos": 2194,
        "token_count_estimate": 129,
        "source_type": "readme",
        "agent_id": "00e86a533d09d371"
      }
    ]
  },
  {
    "agent_id": "53ece59500a6e947",
    "name": "ai.smithery/adamamer20-paper-search-mcp-openai",
    "source": "mcp",
    "source_url": "https://github.com/adamamer20/paper-search-mcp-openai",
    "description": "Search and download academic papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, Semantic‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T10:21:35.776286Z",
    "indexed_at": "2026-02-18T04:04:41.277464",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Paper Search MCP\n\nA Model Context Protocol (MCP) server for searching and downloading academic papers from multiple sources, including arXiv, PubMed, bioRxiv, and Sci-Hub (optional). Designed for seamless integration with large language models like Claude Desktop.\n\n![PyPI](https://img.shields.io/pypi/v/paper-search-mcp.svg) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![smithery badge](https://smithery.ai/badge/@openags/paper-search-mcp)](https://smithery.ai/server/@openags/paper-search-mcp)\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n    - [Install Package](#install-package)\n    - [Configure Claude Desktop](#configure-claude-desktop)\n  - [For Development](#for-development)\n    - [Setup Environment](#setup-environment)\n    - [Install Dependencies](#install-dependencies)\n- [Contributing](#contributing)\n- [Demo](#demo)\n- [License](#license)\n- [TODO](#todo)\n\n---\n\n## Overview\n\n`paper-search-mcp` is a Python-based MCP server that enables users to search and download academic papers from various platforms. It provides tools for searching papers (e.g., `search_arxiv`) and downloading PDFs (e.g., `download_arxiv`), making it ideal for researchers and AI-driven workflows. Built with the MCP Python SDK, it integrates seamlessly with LLM clients like Claude Desktop.\n\n---\n\n## Features\n\n- **Multi-Source Support**: Search and download papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, Semantic Scholar.\n- **Deep Research Ready**: Provides the standardized `search` and `fetch` tools required by OpenAI Deep Research and ChatGPT connectors.\n- **Standardized Output**: Papers are returned in a consistent dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.\n\n---\n\n## Installation\n\n`paper-search-mcp` can be installed using `uv` or `pip`. Below are two approaches: a quick start for immediate use and a detailed setup for development.\n\n### Installing via Smithery\n\nTo install paper-search-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openags/paper-search-mcp):\n\n```bash\nnpx -y @smithery/cli install @openags/paper-search-mcp --client claude\n```\n\n### Quick Start\n\nFor users who want to quickly run the server:\n\n1. **Install Package**:\n\n   ```bash\n   uv add paper-search-mcp\n   ```\n\n2. **Configure Claude Desktop**:\n   Add this configuration to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n   ```json\n   {\n     \"mcpServers\": {\n       \"paper_search_server\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--directory\",\n           \"/path/to/your/paper-search-mcp\",\n           \"-m\",\n           \"paper_search_mcp.server\"\n         ],\n         \"env\": {\n           \"SEMANTIC_SCHOLAR_API_KEY\": \"\" // Optional: For enhanced Semantic Scholar features\n         }\n       }\n     }\n   }\n   ```\n   > Note: Replace `/path/to/your/paper-search-mcp` with your actual installation path.\n\n### For Development\n\nFor developers who want to modify the code or contribute:\n\n1. **Setup Environment**:\n\n   ```bash\n   # Install uv if not installed\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Clone repository\n   git clone https://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .\n\n   # Add development dependencies (optional)\n   uv add pytest flake8\n   ```\n\n---\n\n## Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. **Fork the Repository**:\n   Click \"Fork\" on GitHub.\n\n2. **Clone and Set Up**:\n\n   ```bash\n   git clone https://github.com/yourusername/paper-search-mcp.git\n   cd paper-search-mcp\n   pip install -e \".[dev]\"  # Install dev dependencies (if added to pyproject.toml)\n   ```\n\n3. **Make Changes**:\n\n   - Add new platforms in `academic_platforms/`.\n   - Update tests in `tests/`.\n\n4. **Submit a Pull Request**:\n   Push changes and create a PR on GitHub.\n\n---\n\n## Demo\n\n<img src=\"docs\\images\\demo.png\" alt=\"Demo\" width=\"800\">\n\n## TODO\n\n### Planned Academic Platforms\n\n- [‚àö] arXiv\n- [‚àö] PubMed\n- [‚àö] bioRxiv\n- [‚àö] medRxiv\n- [‚àö] Google Scholar\n- [‚àö] IACR ePrint Archive\n- [‚àö] Semantic Scholar\n- [ ] PubMed Central (PMC)\n- [ ] Science Direct\n- [ ] Springer Link\n- [ ] IEEE Xplore\n- [ ] ACM Digital Library\n- [ ] Web of Science\n- [ ] Scopus\n- [ ] JSTOR\n- [ ] ResearchGate\n- [ ] CORE\n- [ ] Microsoft Academic\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n---\n\nHappy researching with `paper-search-mcp`! If you encounter issues, open a GitHub issue.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search academic papers from multiple sources including arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, and Semantic Scholar",
        "Download academic paper PDFs from supported platforms",
        "Provide standardized output of paper metadata using a consistent Paper class dictionary format",
        "Support asynchronous network requests for efficient querying",
        "Integrate seamlessly with MCP clients and large language models like Claude Desktop",
        "Allow extensibility by adding new academic platforms via the academic_platforms module",
        "Provide standardized search and fetch tools compatible with OpenAI Deep Research and ChatGPT connectors"
      ],
      "limitations": [
        "Does not currently support some major academic platforms such as PubMed Central, Science Direct, Springer Link, IEEE Xplore, ACM Digital Library, Web of Science, Scopus, JSTOR, ResearchGate, CORE, and Microsoft Academic",
        "Semantic Scholar enhanced features require an optional API key",
        "No explicit mention of rate limits or usage quotas",
        "Requires manual configuration for integration with Claude Desktop",
        "Sci-Hub support is optional and may have legal or ethical constraints"
      ],
      "requirements": [
        "Python 3.10 or higher",
        "Installation of the paper-search-mcp package via uv or pip",
        "Optional Semantic Scholar API key for enhanced features",
        "Claude Desktop client for MCP integration",
        "Environment configured with appropriate MCP server settings for Claude Desktop",
        "For development: git, virtual environment setup, and dependencies like pytest and flake8 for testing and linting"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed feature descriptions, contributing guidelines, and clearly states limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Paper Search MCP\n\nA Model Context Protocol (MCP) server for searching and downloading academic papers from multiple sources, including arXiv, PubMed, bioRxiv, and Sci-Hub (optional). Designed for seamless integration with large language models like Claude Desktop.\n\n![PyPI](https://img.shields.io/pypi/v/paper-search-mcp.svg) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![smithery badge](https://smithery.ai/badge/@openags/paper-search-mcp)](https://smithery.ai/server/@openags/paper-search-mcp)\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n    - [Install Package](#install-package)\n    - [Configure Claude Desktop](#configure-claude-desktop)\n  - [For Development](#for-development)\n    - [Setup Environment](#setup-environment)\n    - [Install Dependencies](#install-dependencies)\n- [Contributing](#contributing)\n- [Demo](#demo)\n- [License](#license)\n- [TODO](#todo)\n\n---\n\n## Overview\n\n`paper-search-mcp` is a Python-based MCP server that enables users to search and download academic papers from various platforms. It provides tools for searching papers (e.g., `search_arxiv`) and downloading PDFs (e.g., `download_arxiv`), making it ideal for researchers and AI-driven workflows. Built with the MCP Python SDK, it integrates seamlessly with LLM clients like Claude Desktop.\n\n---\n\n## Features\n\n- **Multi-Source Support**: Search and download papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, Semantic Scholar.\n- **Deep Research Ready**: Provides the standardized `search` and `fetch` tools required by OpenAI Deep Research and ChatGPT connectors.\n- **Standardized Output**: Papers are returned in a consistent dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.",
        "start_pos": 0,
        "end_pos": 2019,
        "token_count_estimate": 504,
        "source_type": "readme",
        "agent_id": "53ece59500a6e947"
      },
      {
        "chunk_id": 1,
        "text": "t dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.\n\n---\n\n## Installation\n\n`paper-search-mcp` can be installed using `uv` or `pip`. Below are two approaches: a quick start for immediate use and a detailed setup for development.\n\n### Installing via Smithery\n\nTo install paper-search-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openags/paper-search-mcp):\n\n```bash\nnpx -y @smithery/cli install @openags/paper-search-mcp --client claude\n```\n\n### Quick Start\n\nFor users who want to quickly run the server:\n\n1. **Install Package**:\n\n   ```bash\n   uv add paper-search-mcp\n   ```\n\n2. **Configure Claude Desktop**:\n   Add this configuration to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n   ```json\n   {\n     \"mcpServers\": {\n       \"paper_search_server\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--directory\",\n           \"/path/to/your/paper-search-mcp\",\n           \"-m\",\n           \"paper_search_mcp.server\"\n         ],\n         \"env\": {\n           \"SEMANTIC_SCHOLAR_API_KEY\": \"\" // Optional: For enhanced Semantic Scholar features\n         }\n       }\n     }\n   }\n   ```\n   > Note: Replace `/path/to/your/paper-search-mcp` with your actual installation path.\n\n### For Development\n\nFor developers who want to modify the code or contribute:\n\n1. **Setup Environment**:\n\n   ```bash\n   # Install uv if not installed\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Clone repository\n   git clone https://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2.",
        "start_pos": 1819,
        "end_pos": 3828,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "53ece59500a6e947"
      },
      {
        "chunk_id": 2,
        "text": "://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .\n\n   # Add development dependencies (optional)\n   uv add pytest flake8\n   ```\n\n---\n\n## Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. **Fork the Repository**:\n   Click \"Fork\" on GitHub.\n\n2. **Clone and Set Up**:\n\n   ```bash\n   git clone https://github.com/yourusername/paper-search-mcp.git\n   cd paper-search-mcp\n   pip install -e \".[dev]\"  # Install dev dependencies (if added to pyproject.toml)\n   ```\n\n3. **Make Changes**:\n\n   - Add new platforms in `academic_platforms/`.\n   - Update tests in `tests/`.\n\n4. **Submit a Pull Request**:\n   Push changes and create a PR on GitHub.\n\n---\n\n## Demo\n\n<img src=\"docs\\images\\demo.png\" alt=\"Demo\" width=\"800\">\n\n## TODO\n\n### Planned Academic Platforms\n\n- [‚àö] arXiv\n- [‚àö] PubMed\n- [‚àö] bioRxiv\n- [‚àö] medRxiv\n- [‚àö] Google Scholar\n- [‚àö] IACR ePrint Archive\n- [‚àö] Semantic Scholar\n- [ ] PubMed Central (PMC)\n- [ ] Science Direct\n- [ ] Springer Link\n- [ ] IEEE Xplore\n- [ ] ACM Digital Library\n- [ ] Web of Science\n- [ ] Scopus\n- [ ] JSTOR\n- [ ] ResearchGate\n- [ ] CORE\n- [ ] Microsoft Academic\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n---\n\nHappy researching with `paper-search-mcp`! If you encounter issues, open a GitHub issue.",
        "start_pos": 3628,
        "end_pos": 5168,
        "token_count_estimate": 384,
        "source_type": "readme",
        "agent_id": "53ece59500a6e947"
      }
    ]
  },
  {
    "agent_id": "aa1799eda14dfcf0",
    "name": "ai.smithery/afgong-sqlite-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/afgong/sqlite-mcp-server",
    "description": "Explore your Messages SQLite database to browse tables and inspect schemas with ease. Run flexible‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-10-02T21:47:24.293477Z",
    "indexed_at": "2026-02-18T04:04:42.640437",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "### Prerequisites\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Install FastMCP globally (if not already installed)\npip install fastmcp\n```\n\n### COMMAND CHEATSHEET\n```bash\n# Run FastMCP directly for testing\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp run sqlite_explorer.py\n\n# Test with inspector (if available)\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp inspect sqlite_explorer.py\n\n# To install SQLite Explorer\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp install sqlite_explorer.py --name \"SQLite Explorer\"\n\n# To launch SQLite Explorer via a web-based testing interface. Run with `--transport sse` for HTTP-based communication  \nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp dev sqlite_explorer.py\n\n# To set up the MCP server with Claude Desktop\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp claude-desktop add sqlite_explorer.py --name \"SQLite Explorer\"\n\n# Need to define the SQLITE_DB_PATH variable before running smithery playground \nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db smithery playground\n```\n\nAfter launching Smithery playground, we can now talk to the MCP server using this URL: https://smithery.ai/playground?mcp=https%3A%2F%2Fee09cd8f.ngrok.smithery.ai%2Fmcp\n\n#### For VSCode with Cline\n```bash\n# Add this configuration to Cline MCP settings:\n{\n  \"sqlite-explorer\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"run\",\n      \"--with\",\n      \"fastmcp\",\n      \"--with\",\n      \"uvicorn\",\n      \"fastmcp\",\n      \"run\",\n      \"/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/sqlite_explorer.py\"\n    ],\n    \"env\": {\n      \"SQLITE_DB_PATH\": \"/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db\"\n    }\n  }\n}\n```\n\n#### Example output. MCP server provides four components. SQLite Explorer provides those tools.\nServer\n  Name:         SQLite Explorer\n  Generation:   2\n\nComponents\n  Tools:        3\n  Prompts:      0\n  Resources:    0\n  Templates:    0\n\nEnvironment\n  FastMCP:      2.12.4\n  MCP:          1.15.0\n\n  This will open an interactive inspector where you can test the MCP tools:\n  - list_tables - to see what tables are in your database\n  - describe_table - to see the structure of a specific table\n  - read_query - to run SELECT queries on your data\n\n\n### Notes\nEven though we're running the MCP locally, still have a web interface\nFor locally deployed MCP server SQLite Explorer, this is the MCP server URL that we can access as a client: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=e01acfa4461c1af5227669c2c0051e7cf9c280fb61a43a146a672463441f4c1b#tools\nIn the future, once deployed, we'll have the same thing - HTTP or HTTPS, but instead of a local host, it'll be my server and the port. The rest will be something else. \nLocal host is the server name, so the server is a local host. But if deployed somewhere, then it'll be a different host name. The port serves as a channel to talk to multiple MCP servers. HTTPS is a secure HTTP. \n\n#### Sample Questions to Ask Your AI Assistant\n\nOnce the MCP server is connected, you can ask questions like:\n\n- \"What tables are in my database?\"\n- \"Show me the structure of my financial data\"\n- \"What are the first 10 rows from [table_name]?\"\n- \"How many records are in each table?\"\n- \"Show me all transactions over $1000\"\n- \"What's the average amount in the transactions table?\"\n\nNOTE: The tables starting with SQLite are called system tables. We never created those tables.\n\n### Troubleshooting\n\n```bash\n# Check FastMCP version and available commands\nfastmcp --help\nfastmcp version\n\n# Verify your database file exists\nls -la /Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db\n\n# Test database directly with sqlite3\nsqlite3 /Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db \".tables\"\n```\n\n### HW 3 Documentation\n\n#### Step 1\nCloned this GitHub repository: https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server\n\n#### Step 2\n\nAsked Claude Code: I was asked to evaluate the following deployment platforms: AWS, Vercel, \nRailway, and Smithery for my MCP server. I want the simplest and cheapest \ndeployment. Which option should I select? \n\n‚è∫ Based on your MCP server and requirements, I recommend Smithery.\n\n  Here's why:\n\n  Simplest: Smithery is specifically designed for MCP servers. It has the\n  most straightforward deployment process for this exact use case.\n\n  Cheapest: Smithery offers a free tier for MCP server hosting, making it\n  the most cost-effective option for your SQLite Explorer MCP server.\n\n  Other options comparison:\n  - AWS: Most complex setup, requires managing infrastructure, not\n  cost-effective for simple MCP servers\n  - Vercel: Designed for web apps/APIs, not optimized for MCP servers,\n  serverless model may not fit well\n  - Railway: Good platform but more expensive than Smithery's free tier,\n  more general-purpose\n\n  For a read-only SQLite MCP server like yours, Smithery's MCP-specific\n  infrastructure is the best match for \"simplest and cheapest.\"\n\n#### Step 3\n\nTested this locally via Cline. In VS Code, we configured the system settings to launch a web-based interface with the following JSON file. See URL: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=a164e503687338cb23938baf05ae738ebe5cd0eaefa629e419cea7ef6ef51563#tools\n\n#### Step 4\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server for SQLite database exploration",
        "List tables in the SQLite database",
        "Describe the structure of specific database tables",
        "Execute SELECT queries on the SQLite database",
        "Provide a web-based interactive testing interface for MCP tools",
        "Integrate with Claude Desktop and VSCode Cline for development",
        "Support HTTP and HTTPS communication protocols for deployed servers",
        "Allow installation and launching of the SQLite Explorer MCP server via FastMCP"
      ],
      "limitations": [
        "Read-only access to SQLite database (no mention of write or update capabilities)",
        "Requires local or deployed environment with SQLite database file path defined",
        "System tables starting with 'SQLite' are not user-created and presumably not modifiable"
      ],
      "requirements": [
        "Python environment with dependencies installed via pip requirements.txt",
        "FastMCP installed globally or in environment",
        "SQLite database file accessible and path set in environment variable SQLITE_DB_PATH",
        "Optional: Smithery account or platform for deployment and hosting",
        "Optional: VSCode with Cline extension configured for local development"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, tool descriptions, environment setup, troubleshooting tips, and deployment recommendations, covering capabilities, limitations, and requirements clearly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "### Prerequisites\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Install FastMCP globally (if not already installed)\npip install fastmcp\n```\n\n### COMMAND CHEATSHEET\n```bash\n# Run FastMCP directly for testing\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp run sqlite_explorer.py\n\n# Test with inspector (if available)\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp inspect sqlite_explorer.py\n\n# To install SQLite Explorer\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp install sqlite_explorer.py --name \"SQLite Explorer\"\n\n# To launch SQLite Explorer via a web-based testing interface.",
        "start_pos": 0,
        "end_pos": 838,
        "token_count_estimate": 209,
        "source_type": "readme",
        "agent_id": "aa1799eda14dfcf0"
      },
      {
        "chunk_id": 1,
        "text": ",\n      \"fastmcp\",\n      \"--with\",\n      \"uvicorn\",\n      \"fastmcp\",\n      \"run\",\n      \"/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/sqlite_explorer.py\"\n    ],\n    \"env\": {\n      \"SQLITE_DB_PATH\": \"/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db\"\n    }\n  }\n}\n```\n\n#### Example output. MCP server provides four components. SQLite Explorer provides those tools.\nServer\n  Name:         SQLite Explorer\n  Generation:   2\n\nComponents\n  Tools:        3\n  Prompts:      0\n  Resources:    0\n  Templates:    0\n\nEnvironment\n  FastMCP:      2.12.4\n  MCP:          1.15.0\n\n  This will open an interactive inspector where you can test the MCP tools:\n  - list_tables - to see what tables are in your database\n  - describe_table - to see the structure of a specific table\n  - read_query - to run SELECT queries on your data\n\n\n### Notes\nEven though we're running the MCP locally, still have a web interface\nFor locally deployed MCP server SQLite Explorer, this is the MCP server URL that we can access as a client: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=e01acfa4461c1af5227669c2c0051e7cf9c280fb61a43a146a672463441f4c1b#tools\nIn the future, once deployed, we'll have the same thing - HTTP or HTTPS, but instead of a local host, it'll be my server and the port. The rest will be something else. \nLocal host is the server name, so the server is a local host. But if deployed somewhere, then it'll be a different host name. The port serves as a channel to talk to multiple MCP servers. HTTPS is a secure HTTP. \n\n#### Sample Questions to Ask Your AI Assistant\n\nOnce the MCP server is connected, you can ask questions like:\n\n- \"What tables are in my database?\"\n- \"Show me the structure of my financial data\"\n- \"What are the first 10 rows from [table_name]?\"\n- \"How many records are in each table?\"\n- \"Show me all transactions over $1000\"\n- \"What's the average amount in the transactions table?\"\n\nNOTE: The tables starting with SQLite are called system tables.",
        "start_pos": 1848,
        "end_pos": 3886,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "aa1799eda14dfcf0"
      },
      {
        "chunk_id": 2,
        "text": "\"How many records are in each table?\"\n- \"Show me all transactions over $1000\"\n- \"What's the average amount in the transactions table?\"\n\nNOTE: The tables starting with SQLite are called system tables. We never created those tables.\n\n### Troubleshooting\n\n```bash\n# Check FastMCP version and available commands\nfastmcp --help\nfastmcp version\n\n# Verify your database file exists\nls -la /Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db\n\n# Test database directly with sqlite3\nsqlite3 /Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db \".tables\"\n```\n\n### HW 3 Documentation\n\n#### Step 1\nCloned this GitHub repository: https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server\n\n#### Step 2\n\nAsked Claude Code: I was asked to evaluate the following deployment platforms: AWS, Vercel, \nRailway, and Smithery for my MCP server. I want the simplest and cheapest \ndeployment. Which option should I select? \n\n‚è∫ Based on your MCP server and requirements, I recommend Smithery.\n\n  Here's why:\n\n  Simplest: Smithery is specifically designed for MCP servers. It has the\n  most straightforward deployment process for this exact use case.\n\n  Cheapest: Smithery offers a free tier for MCP server hosting, making it\n  the most cost-effective option for your SQLite Explorer MCP server.\n\n  Other options comparison:\n  - AWS: Most complex setup, requires managing infrastructure, not\n  cost-effective for simple MCP servers\n  - Vercel: Designed for web apps/APIs, not optimized for MCP servers,\n  serverless model may not fit well\n  - Railway: Good platform but more expensive than Smithery's free tier,\n  more general-purpose\n\n  For a read-only SQLite MCP server like yours, Smithery's MCP-specific\n  infrastructure is the best match for \"simplest and cheapest.\"\n\n#### Step 3\n\nTested this locally via Cline. In VS Code, we configured the system settings to launch a web-based interface with the following JSON file.",
        "start_pos": 3686,
        "end_pos": 5695,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "aa1799eda14dfcf0"
      },
      {
        "chunk_id": 3,
        "text": "is the best match for \"simplest and cheapest.\"\n\n#### Step 3\n\nTested this locally via Cline. In VS Code, we configured the system settings to launch a web-based interface with the following JSON file. See URL: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=a164e503687338cb23938baf05ae738ebe5cd0eaefa629e419cea7ef6ef51563#tools\n\n#### Step 4",
        "start_pos": 5495,
        "end_pos": 5833,
        "token_count_estimate": 84,
        "source_type": "readme",
        "agent_id": "aa1799eda14dfcf0"
      }
    ]
  },
  {
    "agent_id": "5861e6e9f085f017",
    "name": "ai.smithery/agentmail",
    "source": "mcp",
    "source_url": "https://github.com/agentmail-to/agentmail-smithery-mcp",
    "description": "AgentMail is the email inbox API for AI agents. It gives agents their own email inboxes, like Gmail",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-01-15T09:49:30.909872Z",
    "indexed_at": "2026-02-18T04:04:44.167289",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Basic MCP Server\n\nA minimal Model Context Protocol (MCP) server demonstrating tools, resources, and prompts.\n\nBuilt with [Smithery SDK](https://smithery.ai/docs)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Install dependencies:\n\n    ```bash\n    npm install\n    ```\n\n2. Start development server:\n    ```bash\n    npm run dev\n    ```\n\nTry the `hello` tool, `history://hello-world` resource, or `greet` prompt.\n\n## Development\n\nYour code is organized as:\n\n- `src/index.ts` - MCP server with tools, resources, and prompts\n- `smithery.yaml` - Runtime specification\n\nEdit `src/index.ts` to add your own tools, resources, and prompts.\n\n## Build\n\n```bash\nnpm run build\n```\n\nCreates bundled server in `.smithery/`\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n\n    ```bash\n    git add .\n    git commit -m \"Initial commit\"\n    git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n    git push -u origin main\n    ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n\n## Learn More\n\n- [Smithery Docs](https://smithery.ai/docs)\n- [MCP Protocol](https://modelcontextprotocol.io)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Demonstrate tools within an MCP server",
        "Provide resources accessible via MCP protocol",
        "Support prompts for interaction",
        "Allow development and customization of tools, resources, and prompts",
        "Build and bundle the MCP server for deployment",
        "Deploy the MCP server to Smithery platform"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key",
        "Node.js environment to run npm commands",
        "GitHub account for repository hosting and deployment",
        "Smithery account for deployment"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, usage examples, development guidance, build and deployment steps, and prerequisite requirements, providing a comprehensive overview.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Basic MCP Server\n\nA minimal Model Context Protocol (MCP) server demonstrating tools, resources, and prompts.\n\nBuilt with [Smithery SDK](https://smithery.ai/docs)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Install dependencies:\n\n    ```bash\n    npm install\n    ```\n\n2. Start development server:\n    ```bash\n    npm run dev\n    ```\n\nTry the `hello` tool, `history://hello-world` resource, or `greet` prompt.\n\n## Development\n\nYour code is organized as:\n\n- `src/index.ts` - MCP server with tools, resources, and prompts\n- `smithery.yaml` - Runtime specification\n\nEdit `src/index.ts` to add your own tools, resources, and prompts.\n\n## Build\n\n```bash\nnpm run build\n```\n\nCreates bundled server in `.smithery/`\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n\n    ```bash\n    git add .\n    git commit -m \"Initial commit\"\n    git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n    git push -u origin main\n    ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n\n## Learn More\n\n- [Smithery Docs](https://smithery.ai/docs)\n- [MCP Protocol](https://modelcontextprotocol.io)",
        "start_pos": 0,
        "end_pos": 1362,
        "token_count_estimate": 340,
        "source_type": "readme",
        "agent_id": "5861e6e9f085f017"
      }
    ]
  },
  {
    "agent_id": "6ddac8ed84bdaca7",
    "name": "ai.smithery/aicastle-school-openai-api-agent-project",
    "source": "mcp",
    "source_url": "https://github.com/aicastle-school/openai-api-agent-project",
    "description": "Fetch current stock prices and key data for symbols across global markets. Look up companies like‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-04T03:03:54.256827Z",
    "indexed_at": "2026-02-18T04:04:46.136273",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# OpenAI Agent School\n\n[(Ï£º)ÏóêÏù¥ÏïÑÏù¥Ï∫êÏä¨](https://aicastle.com)ÏóêÏÑú ÎßåÎì† [**OpenAI Agent School** ](https://openai-agent.aicastle.school/)(OpenAIÎ•º ÌôúÏö©Ìïú ÎÇòÎßåÏùò AI ÏóêÏù¥Ï†ÑÌä∏ ÎßåÎì§Í∏∞) Í∞ïÏùò ÏûêÎ£åÏûÖÎãàÎã§.\n\n### [OpenAI Agent School (Ebook) Î∞îÎ°úÍ∞ÄÍ∏∞](https://openai-agent.aicastle.school/)\n\n### [Render Chat (Branch) Î∞îÎ°úÍ∞ÄÍ∏∞](https://github.com/aicastle-school/openai-agent-school/tree/render-chat)\n\n### [Smithery Stock Price (Branch) Î∞îÎ°úÍ∞ÄÍ∏∞](https://github.com/aicastle-school/openai-agent-school/tree/smithery-stock-price)\n\n### [Fine Tuning (Branch) Î∞îÎ°úÍ∞ÄÍ∏∞](https://github.com/aicastle-school/openai-agent-school/tree/fine-tuning)\n"
    },
    "llm_extracted": {
      "capabilities": [],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.1,
    "quality_rationale": "The documentation provides only a brief description and links to external resources without any installation instructions, usage examples, or detailed tool descriptions.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# OpenAI Agent School\n\n[(Ï£º)ÏóêÏù¥ÏïÑÏù¥Ï∫êÏä¨](https://aicastle.com)ÏóêÏÑú ÎßåÎì† [**OpenAI Agent School** ](https://openai-agent.aicastle.school/)(OpenAIÎ•º ÌôúÏö©Ìïú ÎÇòÎßåÏùò AI ÏóêÏù¥Ï†ÑÌä∏ ÎßåÎì§Í∏∞) Í∞ïÏùò ÏûêÎ£åÏûÖÎãàÎã§.\n\n### [OpenAI Agent School (Ebook) Î∞îÎ°úÍ∞ÄÍ∏∞](https://openai-agent.aicastle.school/)\n\n### [Render Chat (Branch) Î∞îÎ°úÍ∞ÄÍ∏∞](https://github.com/aicastle-school/openai-agent-school/tree/render-chat)\n\n### [Smithery Stock Price (Branch) Î∞îÎ°úÍ∞ÄÍ∏∞](https://github.com/aicastle-school/openai-agent-school/tree/smithery-stock-price)\n\n### [Fine Tuning (Branch) Î∞îÎ°úÍ∞ÄÍ∏∞](https://github.com/aicastle-school/openai-agent-school/tree/fine-tuning)",
        "start_pos": 0,
        "end_pos": 582,
        "token_count_estimate": 145,
        "source_type": "readme",
        "agent_id": "6ddac8ed84bdaca7"
      }
    ]
  },
  {
    "agent_id": "bc97a05ae003ab36",
    "name": "ai.smithery/airmang-hwpx-mcp",
    "source": "mcp",
    "source_url": "https://github.com/airmang/hwpx-mcp",
    "description": "ÏûêÎèôÌôîÌïòÏó¨ HWPX Î¨∏ÏÑúÏùò Î°úÎî©, ÌÉêÏÉâ, Ìé∏Ïßë, Í≤ÄÏ¶ùÏùÑ Ìïú Î≤àÏóê Ï≤òÎ¶¨Ìï©ÎãàÎã§. Î¨∏Îã®¬∑Ìëú¬∑Ï£ºÏÑù Ï∂îÍ∞Ä, ÌÖçÏä§Ìä∏ ÏùºÍ¥Ñ ÏπòÌôò, Î®∏Î¶¨Îßê¬∑Íº¨Î¶¨Îßê ÏÑ§Ï†ï Îì± Î∞òÎ≥µ ÏûëÏóÖÏùÑ Ïã†ÏÜçÌûà ÏàòÌñâÌï©ÎãàÎã§. Í∏∞‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T07:51:38.712981Z",
    "indexed_at": "2026-02-18T04:04:48.274063",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# ÌïúÍ∏Ä MCP (HWPX) ÏÑúÎ≤Ñ - ÌïúÍ∏Ä ÏûêÎèôÌôî HWPX Î¨∏ÏÑú ÏûêÎèô ÏÉùÏÑ±¬∑Ìé∏Ïßë¬∑Í≤ÄÏ¶ùmcp #\n\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî **ÌïúÍ∏Ä MCP(HWPX) ÏÑúÎ≤Ñ**Î°ú, HWPX Î¨∏ÏÑúÎ•º ÌïúÍ∏Ä ÏõåÎìúÌîÑÎ°úÏÑ∏ÏÑú ÏóÜÏù¥ ÏßÅÏ†ë Ïó¥Í≥† ÏûêÎèôÌôîÌï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏóàÏäµÎãàÎã§.  \nGemini CLI, Claude DesktopÍ≥º Í∞ôÏùÄ MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Ïó∞Í≤∞ÌïòÏó¨ Î¨∏ÏÑú ÏÉùÏÑ±¬∑Ìé∏Ïßë¬∑ÌÉêÏÉâ Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n\n[](https://www.google.com/search?q=https://pypi.org/project/hwpx-mcp-server/)\n[](https://opensource.org/licenses/MIT)\n[](https://www.google.com/search?q=https://github.com/your-repo/hwpx-mcp-server/actions/workflows/ci.yml)\n\n**ÏàúÏàò ÌååÏù¥Ïç¨ÏúºÎ°ú HWPX Î¨∏ÏÑúÎ•º ÏûêÏú†Î°≠Í≤å Îã§Î£®Îäî Í∞ÄÏû• Í∞ïÎ†•Ìïú Î∞©Î≤ï.**\n\n\n`hwpx-mcp-server`Îäî [Model Context Protocol](https://github.com/modelcontextprotocol/specification) ÌëúÏ§ÄÏùÑ Îî∞Î•¥Îäî ÏÑúÎ≤ÑÎ°ú, Í∞ïÎ†•Ìïú [`python-hwpx`](https://www.google.com/search?q=%5Bhttps://github.com/airmang/python-hwpx%5D\\(https://github.com/airmang/python-hwpx\\)) ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º Í∏∞Î∞òÏúºÎ°ú Ìï©ÎãàÎã§. Gemini, ClaudeÏôÄ Í∞ôÏùÄ ÏµúÏã† AI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏôÄ ÏôÑÎ≤ΩÌïòÍ≤å Ïó∞ÎèôÌïòÏó¨ ÌïúÍ∏Ä ÏõåÎìú ÌîÑÎ°úÏÑ∏ÏÑú Î°úÏª¨ HWPX Î¨∏ÏÑúÎ•º Ïó¥Îûå, Í≤ÄÏÉâ, Ìé∏Ïßë, Ï†ÄÏû•ÌïòÎäî ÌíçÎ∂ÄÌïú Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n\n-----\n\n## ‚ú® Ï£ºÏöî Í∏∞Îä•\n\n  * **‚úÖ ÌëúÏ§Ä MCP ÏÑúÎ≤Ñ Íµ¨ÌòÑ**: Í≥µÏãù `mcp` SDKÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏïàÏ†ïÏ†ÅÏù∏ ÌëúÏ§Ä ÏûÖ/Ï∂úÎ†• Í∏∞Î∞ò ÏÑúÎ≤ÑÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.\n  * **üìÇ Ï†úÎ°ú ÏÑ§Ï†ï**: Î≥ÑÎèÑ ÏÑ§Ï†ï ÏóÜÏù¥ ÌòÑÏû¨ ÏûëÏóÖ ÎîîÎ†âÌÑ∞Î¶¨Î•º Í∏∞Ï§ÄÏúºÎ°ú Ï¶âÏãú Í≤ΩÎ°úÎ•º Ï≤òÎ¶¨Ìï©ÎãàÎã§.\n  * **üìÑ Í∞ïÎ†•Ìïú Î¨∏ÏÑú Ìé∏Ïßë**: ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú, ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖòÎ∂ÄÌÑ∞ Ïä§ÌÉÄÏùº, Ìëú, Î©îÎ™®, Í∞úÏ≤¥ Ìé∏ÏßëÍπåÏßÄ Î™®Îëê Í∞ÄÎä•Ìï©ÎãàÎã§.\n  * **üß© HWP Ìò∏Ìôò + ÏûêÎèô Î≥ÄÌôò**: `.hwp` Î∞îÏù¥ÎÑàÎ¶¨ Î¨∏ÏÑúÎ•º ÏùΩÍ∏∞ Ï†ÑÏö©ÏúºÎ°ú Ï°∞Ìöå/Í≤ÄÏÉâÌï† Ïàò ÏûàÍ≥†, `convert_hwp_to_hwpx` ÎèÑÍµ¨Î°ú `.hwpx`Î°ú ÏûêÎèô Î≥ÄÌôòÌï¥ Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏Ïóê Î∞îÎ°ú Ïó∞Í≤∞Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n  * **üõ°Ô∏è ÏïàÏ†ÑÌïú Ï†ÄÏû•**: ÏûêÎèô Î∞±ÏóÖ(`*.bak`) ÏòµÏÖòÏúºÎ°ú ÏòàÍ∏∞Ïπò ÏïäÏùÄ Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ Î∞©ÏßÄÌï©ÎãàÎã§.\n  * **üöÄ Ï¶âÏãú Ïã§Ìñâ**: `uv`Îßå ÏûàÏúºÎ©¥ `uvx hwpx-mcp-server` Ìïú Ï§ÑÎ°ú Î∞îÎ°ú ÏãúÏûëÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## üöÄ Îπ†Î•∏ ÏãúÏûë\n\n### 1\\. `uv` ÏÑ§Ïπò\n\nÍ∞ÄÏû• Î®ºÏ†Ä ÌååÏù¥Ïç¨ Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÎèÑÍµ¨Ïù∏ `uv`Î•º ÏÑ§ÏπòÌïòÏÑ∏Ïöî.\n[üëâ Astral uv ÏÑ§Ïπò Í∞ÄÏù¥Îìú](https://docs.astral.sh/uv/getting-started/installation/)\n\n\n### 2\\. MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï\n\nÏÇ¨Ïö© Ï§ëÏù∏ MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ïÏóê ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÏÑúÎ≤Ñ Ï†ïÎ≥¥Î•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.\n\n```json\n{\n  \"mcpServers\": {\n    \"hwpx\": {\n      \"command\": \"uvx\",\n      \"args\": [\"hwpx-mcp-server\"],\n      \"env\": {\n        \"HWPX_MCP_PAGING_PARA_LIMIT\": \"200\",\n        \"HWPX_MCP_AUTOBACKUP\": \"1\",\n        \"LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n### 3\\. ÏÑúÎ≤Ñ Ïã§Ìñâ (Î°úÏª¨ ÌôòÍ≤ΩÏóêÏÑú ÏÇ¨Ïö©Ìï† Í≤ΩÏö∞)\n\nÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏïÑÎûò Î™ÖÎ†πÏñ¥Î•º Ïã§ÌñâÌïòÎ©¥ ÏÑúÎ≤ÑÍ∞Ä Î∞îÎ°ú ÏãúÏûëÎê©ÎãàÎã§.\n\n```bash\nuvx hwpx-mcp-server\n```\n\n> `uvx` Î™ÖÎ†πÏùÄ Ï≤´ Ïã§Ìñâ Ïãú ÌïÑÏöîÌïú Ï¢ÖÏÜçÏÑ±ÏùÑ ÏûêÎèôÏúºÎ°ú ÏÑ§ÏπòÌïòÎ©∞, Î∞òÎìúÏãú `python-hwpx 1.9` Ïù¥ÏÉÅÏùò Î≤ÑÏ†ÑÏù¥ Ï§ÄÎπÑÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n\n> ÏÑúÎ≤ÑÎäî Ïã§ÌñâÎêú ÌòÑÏû¨ ÎîîÎ†âÌÑ∞Î¶¨Î•º Í∏∞Ï§ÄÏúºÎ°ú Í≤ΩÎ°úÎ•º Ìï¥ÏÑùÌïòÎØÄÎ°ú, Î≥ÑÎèÑÏùò ÏûëÏóÖ ÎîîÎ†âÌÑ∞Î¶¨ ÏÑ§Ï†ï ÏóÜÏù¥ Î∞îÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## ‚öôÔ∏è ÌôòÍ≤Ω Î≥ÄÏàò\n\n| Î≥ÄÏàò | ÏÑ§Î™Ö | Í∏∞Î≥∏Í∞í |\n| --- | --- | --- |\n| `HWPX_MCP_PAGING_PARA_LIMIT` | ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò ÎèÑÍµ¨Í∞Ä Î∞òÌôòÌï† ÏµúÎåÄ Î¨∏Îã® Ïàò | `200` |\n| `HWPX_MCP_AUTOBACKUP` | `1`Ïù¥Î©¥ Ï†ÄÏû• Ï†Ñ `<file>.bak` Î∞±ÏóÖ ÏÉùÏÑ± | `0` |\n| `LOG_LEVEL` | stderrÏóê JSONL ÌòïÏãùÏúºÎ°ú Ï∂úÎ†•Ìï† Î°úÍ∑∏ Î†àÎ≤® | `INFO` |\n| `HWPX_MCP_HARDENING` | `1`Î°ú ÏÑ§Ï†ï Ïãú ÌïòÎìúÎãù Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏Í≥º Í≤ÄÏÉâ/Ïª®ÌÖçÏä§Ìä∏ ÎèÑÍµ¨ ÌôúÏÑ±Ìôî | `0` |\n\n> ‚ÑπÔ∏è `read_text` ÎèÑÍµ¨Îäî Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏµúÎåÄ 200Í∞úÏùò Î¨∏Îã®ÏùÑ Î∞òÌôòÌï©ÎãàÎã§. Îçî ÌÅ∞ Îç§ÌîÑÍ∞Ä ÌïÑÏöîÌïòÎ©¥ ÎèÑÍµ¨ Ìò∏Ï∂ú Ïãú `limit` Ïù∏ÏàòÎ•º ÏßÅÏ†ë ÏßÄÏ†ïÌïòÍ±∞ÎÇò `HWPX_MCP_PAGING_PARA_LIMIT` ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÌôïÏû•ÌïòÏÑ∏Ïöî. Ïù¥Îäî Microsoft Office WordÏóêÏÑú ÌïÑÏöîÌïú Î≤îÏúÑÎßå ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏùΩÎäî ÏõåÌÅ¨ÌîåÎ°úÏôÄ ÎèôÏùºÌï©ÎãàÎã§.\n\n> üîê `HWPX_MCP_HARDENING=1`Î°ú Ïã§ÌñâÌïòÎ©¥ ÏÉà Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏(`plan ‚Üí preview ‚Üí apply`)Í≥º Í≤ÄÏÉâ/Ïª®ÌÖçÏä§Ìä∏ ÎèÑÍµ¨Í∞Ä Ìï®Íªò ÎÖ∏Ï∂úÎê©ÎãàÎã§. Í∞íÏù¥ `0` ÎòêÎäî ÎØ∏ÏÑ§Ï†ïÏù¥Î©¥ Í∏∞Ï°¥ ÎèÑÍµ¨ ÌëúÎ©¥Îßå Ïú†ÏßÄÎê©ÎãàÎã§.\n\n### üìÅ Î¨∏ÏÑú Î°úÏºÄÏù¥ÌÑ∞(Document Locator)\n\nÎ™®Îì† ÎèÑÍµ¨Ïùò ÏûÖÎ†•ÏùÄ Ïù¥Ï†ú Î¨∏ÏÑúÎ•º Í∞ÄÎ¶¨ÌÇ§Îäî **discriminated union** Î°úÏºÄÏù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. Í∏∞Î≥∏Í∞íÏùÄ Í∏∞Ï°¥Í≥º ÎèôÏùºÌïòÍ≤å ÏÉÅÏúÑ ÏàòÏ§ÄÏùò `path` ÌïÑÎìúÏù¥Î©∞, Î≥ÑÎèÑ ÏÑ†Ïñ∏ ÏóÜÏù¥ÎèÑ Í≥ÑÏÜç ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÌïÑÏöîÏóê Îî∞Îùº Î™ÖÏãúÏ†ÅÏúºÎ°ú `type`ÏùÑ ÏßÄÏ†ïÌï¥ HTTP Î∞±ÏóîÎìúÎÇò ÏÇ¨Ï†Ñ Îì±Î°ùÎêú Ìï∏Îì§ÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n- **Î°úÏª¨ ÌååÏùº (Í∏∞Ï°¥ Ïä§ÌÇ§ÎßàÏôÄ ÎèôÏùº)**\n\n  ```jsonc\n  {\n    \"name\": \"open_info\",\n    \"arguments\": {\n      \"path\": \"sample.hwpx\"\n    }\n  }\n  ```\n\n- **HTTP Î∞±ÏóîÎìúÏôÄ Ïó∞Í≥Ñ** ‚Äî ÏÑúÎ≤ÑÎ•º HTTP Ïä§ÌÜ†Î¶¨ÏßÄ Î™®ÎìúÎ°ú Ïã§ÌñâÌïú Í≤ΩÏö∞, ÏõêÍ≤© Í≤ΩÎ°úÎ•º `uri` ÌïÑÎìúÎ°ú ÏßÄÏ†ïÌïòÍ≥† ÌïÑÏöî Ïãú `backend` ÌûåÌä∏Î•º Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n  ```jsonc\n  {\n    \"name\": \"open_info\",\n    \"arguments\": {\n      \"type\": \"uri\",\n      \"uri\": \"reports/weekly.hwpx\",\n      \"backend\": \"http\"\n    }\n  }\n  ```\n\n- **ÏÇ¨Ï†Ñ Îì±Î°ùÎêú Ìï∏Îì§ ÏÇ¨Ïö©** ‚Äî ÌïòÎìúÎãù ÌååÏù¥ÌîÑÎùºÏù∏ÏóêÏÑú Ïù¥ÎØ∏ Î°úÎìúÎêú Î¨∏ÏÑúÏóê ÎåÄÌï¥ ÌõÑÏÜç Í≤ÄÏÉâ/Ïª®ÌÖçÏä§Ìä∏/Ìé∏ÏßëÏùÑ ÏàòÌñâÌï† ÎïåÎäî `handleId`Î•º Ï†ÑÎã¨ÌïòÎ©¥ Îê©ÎãàÎã§.\n\n  ```jsonc\n  {\n    \"name\": \"hwpx.plan_edit\",\n    \"arguments\": {\n      \"type\": \"handle\",\n      \"handleId\": \"doc-1234\",\n      \"operations\": [\n        {\n          \"target\": {\"nodeId\": \"n_deadbeef\"},\n          \"match\": \"needle\",\n          \"replacement\": \"haystack\"\n        }\n      ]\n    }\n  }\n  ```\n\nÍ∞Å Î≥ÄÌòïÏùÄ ÌïÑÏöîÏóê Îî∞Îùº `backend` ÌïÑÎìúÎ•º Ï∂îÍ∞ÄÎ°ú Í∞ÄÏßà Ïàò ÏûàÏúºÎ©∞, Î™ÖÏãúÏ†ÅÏúºÎ°ú `document` Í∞ùÏ≤¥Î•º Ï§ëÏ≤©ÌïòÏó¨ Ï†ÑÎã¨ÌïòÎäî Í≤ÉÎèÑ ÌóàÏö©Îê©ÎãàÎã§. Ïä§ÌÇ§ÎßàÎäî SanitizerÎ•º Í±∞Ï≥ê `$ref` ÏóÜÏù¥ ÌèâÌÉÑÌôîÎêú ÌòïÌÉúÎ°ú Ï†úÍ≥µÎê©ÎãàÎã§.\n\n### üîê ÌïòÎìúÎãù Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏ (ÏòµÏÖò)\n\nÌïòÎìúÎãù ÌîåÎûòÍ∑∏Î•º ÏºúÎ©¥ Î™®Îì† Ìé∏Ïßë ÏöîÏ≤≠Ïù¥ **Í≥ÑÌöç(Plan) ‚Üí Í≤ÄÌÜ†(Preview) ‚Üí Ï†ÅÏö©(Apply)**Ïùò 3Îã®Í≥ÑÎ•º Í±∞ÏπòÎèÑÎ°ù ÏÑ§Í≥ÑÎêú Ïã†Í∑ú ÎèÑÍµ¨Í∞Ä Ìï®Íªò ÎÖ∏Ï∂úÎê©ÎãàÎã§. ÌïòÎìúÎãù ÌîåÎûòÍ∑∏Îäî Ï†ÄÎ†¥Ìïú LLM Î™®Îç∏Ïùò ÏöîÏ≤≠ÏóêÎèÑ ÏÑ±Í≥µÏ†ÅÏù∏ ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÍ∏∞ ÏúÑÌï¥ÏÑú ÎèÑÏûÖÌïú ÌÖåÏä§Ìä∏Ï§ëÏù∏ Í∏∞Îä•ÏûÖÎãàÎã§. \n\n1. **`hwpx.plan_edit`**: Î≥ÄÍ≤Ω ÎåÄÏÉÅÍ≥º ÏùòÎèÑÌïú ÏûëÏóÖÏùÑ ÏÑ§Î™ÖÌïòÎ©¥ ÏÑúÎ≤ÑÍ∞Ä ÏïàÏ†ïÏ†ÅÏù∏ `planId`ÏôÄ ÏòàÏÉÅ ÏûëÏóÖ ÏöîÏïΩÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n2. **`hwpx.preview_edit`**: Î∞úÍ∏âÎêú `planId`Î°ú ÎØ∏Î¶¨Î≥¥Í∏∞Î•º ÏöîÏ≤≠ÌïòÎ©¥ Ïã§Ï†ú diff, Î™®Ìò∏ÏÑ± Í≤ΩÍ≥†, ÏïàÏ†Ñ Ï†êÏàò Îì±ÏùÑ Ìè¨Ìï®Ìïú Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞Î•º Î∞òÌôòÌï©ÎãàÎã§. Ïù¥ Îã®Í≥ÑÍ∞Ä Í∏∞Î°ùÎêòÏßÄ ÏïäÏúºÎ©¥ Ï†ÅÏö© Îã®Í≥ÑÎ°ú ÎÑòÏñ¥Í∞à Ïàò ÏóÜÏäµÎãàÎã§.\n3. **`hwpx.apply_edit`**: `preview`Î•º Í±∞Ïπú ÎèôÏùºÌïú `planId`Ïóê `confirm: true`Î•º Î™ÖÏãúÌï¥Ïïº Ïã§Ï†ú Î¨∏ÏÑú Î≥ÄÍ≤ΩÏù¥ Ïù¥Î£®Ïñ¥ÏßëÎãàÎã§. `idempotencyKey`Î•º ÏßÄÏ†ïÌïòÎ©¥ ÎèôÏùº ÏöîÏ≤≠Ïù¥ Î∞òÎ≥µÎêòÎçîÎùºÎèÑ ÏïàÏ†ÑÌïòÍ≤å Î¨¥ÏãúÎê©ÎãàÎã§.\n\nÍ∞Å Îã®Í≥ÑÎäî ÌëúÏ§Ä `ServerResponse` ÎûòÌçºÎ•º ÏÇ¨Ïö©ÌïòÎ©∞, Ïò§Î•ò Î∞úÏÉù Ïãú `PREVIEW_REQUIRED`, `AMBIGUOUS_TARGET`, `UNSAFE_WILDCARD`, `IDEMPOTENT_REPLAY` Îì±Ïùò ÏΩîÎìúÏôÄ Ìï®Íªò ÌõÑÏÜç ÌñâÎèô ÏòàÏãú(`next_actions`)Î•º Î∞òÌôòÌï©ÎãàÎã§. Î™®Îì† Ïä§ÌÇ§ÎßàÎäî draft-07 Ìò∏Ìôò SanitizerÎ•º ÌÜµÌï¥ `$ref`, `anyOf` ÏóÜÏù¥ ÌèâÌÉÑÌôîÎêòÏñ¥ ÎÖ∏Ï∂úÎê©ÎãàÎã§.\n\nÎòêÌïú ÌïòÎìúÎãù Î™®ÎìúÏóêÏÑúÎäî ÏßÄÏõê ÎèÑÍµ¨Í∞Ä ÌôïÏû•Îê©ÎãàÎã§.\n\n- **`hwpx.search`**: Ï†ïÍ∑úÏãù ÎòêÎäî ÌÇ§ÏõåÎìú Í∏∞Î∞òÏúºÎ°ú Î¨∏ÏÑú Ï†ÑÎ∞òÏùÑ Í≤ÄÏÉâÌïòÏó¨ ÎÖ∏Ï∂ú Í∞ÄÎä•Ìïú Î¨∏Îß•Îßå Ìè¨Ìï®Ìïú ÏùºÏπò Í≤∞Í≥ºÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.\n- **`hwpx.get_context`**: ÌäπÏ†ï Î¨∏Îã® Ï£ºÎ≥ÄÏùò Ï†úÌïúÎêú Ï∞Ω(window)Îßå Ï∂îÏ∂úÌïòÏó¨ ÌîÑÎùºÏù¥Î≤ÑÏãúÎ•º Ïú†ÏßÄÌïú Ï±Ñ Î¶¨Î∑∞Ïóê ÌôúÏö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## üõ†Ô∏è Ï†úÍ≥µ ÎèÑÍµ¨\n\nÎã§ÏñëÌïú Î¨∏ÏÑú Ìé∏Ïßë Î∞è Í¥ÄÎ¶¨ ÎèÑÍµ¨Î•º Ï†úÍ≥µÌï©ÎãàÎã§. Í∞Å ÎèÑÍµ¨Ïùò ÏÉÅÏÑ∏Ìïú ÏûÖÏ∂úÎ†• ÌòïÏãùÏùÄ `ListTools` ÏùëÎãµÏóê Ìè¨Ìï®Îêú JSON Ïä§ÌÇ§ÎßàÎ•º ÌÜµÌï¥ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n\\<details\\>\n\\<summary\\>\\<b\\>Ï†ÑÏ≤¥ ÎèÑÍµ¨ Î™©Î°ù ÌéºÏ≥êÎ≥¥Í∏∞...\\</b\\>\\</summary\\>\n\n  - **Î¨∏ÏÑú Ï†ïÎ≥¥ Î∞è ÌÉêÏÉâ**\n      - `open_info`: Î¨∏ÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∞è Îã®ÎùΩ¬∑Ìó§Îçî Í∞úÏàò ÏöîÏïΩ\n      - `list_sections`, `list_headers`: ÏÑπÏÖò/Ìó§Îçî Íµ¨Ï°∞ ÌÉêÏÉâ\n      - `list_master_pages_histories_versions`: ÎßàÏä§ÌÑ∞ ÌéòÏù¥ÏßÄ/ÌûàÏä§ÌÜ†Î¶¨/Î≤ÑÏ†Ñ ÏöîÏïΩ\n  - **ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú Î∞è Í≤ÄÏÉâ**\n      - `read_text`, `read_paragraphs`, `text_extract_report`: ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò, ÏÑ†ÌÉù Î¨∏Îã®, Ï£ºÏÑù Ìè¨Ìï® ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú\n      - `analyze_template_structure`: ÏñëÏãù Î¨∏ÏÑúÎ•º Ìó§Îçî/Î≥∏Î¨∏/Ìë∏ÌÑ∞(Ìú¥Î¶¨Ïä§Ìã±)Î°ú ÏöîÏïΩÌïòÍ≥† ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî ÌõÑÎ≥¥Î•º ÌÉêÏßÄ\n      - `find`, `find_runs_by_style`: ÌÖçÏä§Ìä∏ Í≤ÄÏÉâ Î∞è Ïä§ÌÉÄÏùº Í∏∞Î∞ò Í≤ÄÏÉâ\n      - `hwpx.search` *(ÌîåÎûòÍ∑∏ ÌôúÏÑ± Ïãú)*: Ï†ïÍ∑úÏãù/ÌÇ§ÏõåÎìú Í≤ÄÏÉâÍ≥º ÏïàÏ†ïÏ†ÅÏù∏ ÎÖ∏Îìú ÏãùÎ≥ÑÏûê Î∞òÌôò\n      - `hwpx.get_context` *(ÌîåÎûòÍ∑∏ ÌôúÏÑ± Ïãú)*: Î¨∏Îã® Ï†ÑÌõÑ Î¨∏Îß•Îßå Ï†úÌïúÏ†ÅÏúºÎ°ú Ï°∞Ìöå\n  - **Î¨∏ÏÑú Ìé∏Ïßë**\n      - `replace_text_in_runs`: Ïä§ÌÉÄÏùºÏùÑ Î≥¥Ï°¥ÌïòÎ©∞ ÌÖçÏä§Ìä∏ ÏπòÌôò (Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Î¨∏ÏÑúÎ•º Ï†ÄÏû•ÌïòÎØÄÎ°ú,\n        ÎØ∏Î¶¨Î≥¥Í∏∞Îßå ÏõêÌïòÎ©¥ `dryRun: true`Î•º ÏßÄÏ†ïÌïòÏÑ∏Ïöî.)\n      - `add_paragraph`, `insert_paragraphs_bulk`: Î¨∏Îã® Ï∂îÍ∞Ä\n      - `add_table`, `get_table_cell_map`, `set_table_cell_text`, `replace_table_region`, `split_table_cell`: Ìëú ÏÉùÏÑ±¬∑Ìé∏Ïßë Î∞è Î≥ëÌï© Ìï¥Ï†ú\n      - `add_shape`, `add_control`: Í∞úÏ≤¥ Ï∂îÍ∞Ä\n      - `add_memo`, `attach_memo_field`, `add_memo_with_anchor`, `remove_memo`: Î©îÎ™® Í¥ÄÎ¶¨\n      - `hwpx.plan_edit`, `hwpx.preview_edit`, `hwpx.apply_edit` *(ÌîåÎûòÍ∑∏ ÌôúÏÑ± Ïãú)*: Í≤ÄÏ¶ùÎêú 3Îã®Í≥Ñ Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏\n  - **Ïä§ÌÉÄÏùºÎßÅ**\n      - `ensure_run_style`, `list_styles_and_bullets`: Ïä§ÌÉÄÏùº Î∞è Í∏ÄÎ®∏Î¶¨Ìëú Î™©Î°ù ÌôïÏù∏/ÏÉùÏÑ±\n      - `apply_style_to_text_ranges`, `apply_style_to_paragraphs`: Îã®Ïñ¥/Î¨∏Îã® Îã®ÏúÑ Ïä§ÌÉÄÏùº Ï†ÅÏö©\n  - **ÌååÏùº Í¥ÄÎ¶¨**\n      - `save`, `save_as`: Î¨∏ÏÑú Ï†ÄÏû•\n      - `fill_template`: ÌÖúÌîåÎ¶ø ÏÇ¨Î≥∏ ÏÉùÏÑ± + Îã§Ï§ë ÏπòÌôòÏùÑ 1Ìöå Ìò∏Ï∂úÎ°ú ÏàòÌñâ\n      - `make_blank`: ÏÉà Îπà Î¨∏ÏÑú ÏÉùÏÑ±\n      - `convert_hwp_to_hwpx`: HWP Î∞îÏù¥ÎÑàÎ¶¨Î•º HWPXÎ°ú Î≥ÄÌôò(Í∏∞Î≥∏ ÌÖçÏä§Ìä∏/Ìëú Ï§ëÏã¨)\n  - **Íµ¨Ï°∞ Í≤ÄÏ¶ù Î∞è Í≥†Í∏â Í≤ÄÏÉâ**\n      - `object_find_by_tag`, `object_find_by_attr`: XML ÏöîÏÜå Í≤ÄÏÉâ\n      - `validate_structure`, `lint_text_conventions`: Î¨∏ÏÑú Íµ¨Ï°∞ Í≤ÄÏ¶ù Î∞è ÌÖçÏä§Ìä∏ Î¶∞Ìä∏\n\n\\</details\\>\n\n### üéØ ÌïÑÏöîÌïú Î¨∏Îã®Îßå Îπ†Î•¥Í≤å ÏùΩÍ∏∞\n\nÎåÄÏö©Îüâ Î¨∏ÏÑúÎ•º ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÌôïÏù∏Ìï† ÎïåÎäî `read_text` ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖòÏù¥ Ìé∏Î¶¨ÌïòÏßÄÎßå, ÌäπÏ†ï Î¨∏Îã®Îßå Î∞îÎ°ú ÌôïÏù∏ÌïòÍ≥† Ïã∂ÏùÑ ÎïåÎäî `read_paragraphs` ÎèÑÍµ¨Í∞Ä Îçî Ï†ÅÌï©Ìï©ÎãàÎã§. `paragraphIndexes` Î∞∞Ïó¥Ïóê ÏõêÌïòÎäî Î¨∏Îã® Î≤àÌò∏Îßå Ï†ÑÎã¨ÌïòÎ©¥, ÏöîÏ≤≠Ìïú Î¨∏Îã®Îßå ÏàúÏÑúÎåÄÎ°ú Î∞òÌôòÌï©ÎãàÎã§. Í∞Å Ìï≠Î™©ÏóêÎäî ÏõêÎ≥∏ Î¨∏Îã® Ïù∏Îç±Ïä§(`paragraphIndex`)ÏôÄ Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏Í∞Ä Ìï®Íªò Ìè¨Ìï®ÎêòÎØÄÎ°ú, Ïù¥Ï†Ñ Ìò∏Ï∂úÏóêÏÑú Í∏∞ÏñµÌïú Î¨∏Îã®ÏùÑ Ï†ïÌôïÌûà Îã§Ïãú Î∂àÎü¨Ïò¨ Ïàò ÏûàÏäµÎãàÎã§.\n\n```jsonc\n{\n  \"name\": \"read_paragraphs\",\n  \"arguments\": {\n    \"path\": \"sample.hwpx\",\n    \"paragraphIndexes\": [1, 4, 9],\n    \"withHighlights\": false,\n    \"withFootnotes\": false\n  }\n}\n```\n\nÏÑ†ÌÉùÎêú Î¨∏Îã®Îßå Ï≤òÎ¶¨ÌïòÎØÄÎ°ú ÌÅ∞ Î¨∏ÏÑúÎ•º Î∞òÎ≥µÌï¥ÏÑú ÌÉêÏÉâÌï† Îïå Î∂àÌïÑÏöîÌïú ÌÖçÏä§Ìä∏ Î≥µÏÇ¨Î•º Ï§ÑÏù¥Í≥†, ÌïòÏù¥ÎùºÏù¥Ìä∏/Í∞ÅÏ£º ÏòµÏÖòÎèÑ `read_text`ÏôÄ ÎèôÏùºÌïòÍ≤å ÌôúÏö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Ïù∏Îç±Ïä§Î•º ÏöîÏ≤≠ÌïòÎ©¥ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÎØÄÎ°ú, Ïù¥Ï†ÑÏóê Î∞õÏùÄ Î¨∏Îã® Í∞úÏàò Ï†ïÎ≥¥Î•º ÌôúÏö©Ìï¥ ÏïàÏ†ÑÌïòÍ≤å ÏöîÏ≤≠ÌïòÏÑ∏Ïöî.\n\n### üîç Í≤ÄÏÉâ Î¨∏Îß• Í∏∏Ïù¥ Ï°∞Ï†à\n\n`find` ÎèÑÍµ¨Îäî Í∞Å ÏùºÏπò Ìï≠Î™© Ï£ºÎ≥ÄÏùò Ï†ÑÌõÑ 80ÏûêÎ•º Í∏∞Î≥∏ÏúºÎ°ú ÏûòÎùº `context` Ïä§ÎãàÌé´ÏùÑ Î∞òÌôòÌïòÎ©∞, ÏûòÎ¶∞ Í≤ΩÏö∞ Î¨∏ÏûêÏó¥ ÏïûÎí§Ïóê `...`Ïù¥ Î∂ôÏäµÎãàÎã§. Îçî ÎÑìÏùÄ Î≤îÏúÑÍ∞Ä ÌïÑÏöîÌïòÎ©¥ `contextRadius` Ïù∏ÏàòÎ•º ÏÇ¨Ïö©Ìï¥ Ïú†ÏßÄÌï† Î¨∏Ïûê ÏàòÎ•º Ï°∞Ï†ïÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n```jsonc\n{\n  \"name\": \"find\",\n  \"arguments\": {\n    \"path\": \"sample.hwpx\",\n    \"query\": \"HWPX\",\n    \"contextRadius\": 200\n  }\n}\n```\n\n`contextRadius` Í∞íÏùÄ ÏùºÏπò Íµ¨Í∞Ñ ÏïûÎí§ Í∞ÅÍ∞ÅÏóê Ìè¨Ìï®Ìï† Î¨∏Ïûê ÏàòÎ•º ÏùòÎØ∏Ìï©ÎãàÎã§.\n\n### üìê Ìëú Ìé∏Ïßë Í≥†Í∏â ÏòµÏÖò\n\n`get_table_cell_map` ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÎ©¥ ÌëúÏùò Ï†ÑÏ≤¥ Í≤©ÏûêÎ•º Í∑∏ÎåÄÎ°ú ÏßÅÎ†¨ÌôîÌïòÏó¨ Í∞Å ÏúÑÏπòÍ∞Ä Ïñ¥Îäê ÏïµÏª§ ÏÖÄ(`anchor`)Ïóê ÏÜçÌïòÎäîÏßÄ, Î≥ëÌï© Î≤îÏúÑ(`rowSpan`, `colSpan`)Îäî ÏñºÎßàÏù∏ÏßÄ ÌïúÎààÏóê ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÏùëÎãµÏùÄ Ìï≠ÏÉÅ Ìñâ√óÏó¥ Ï†ÑÏ≤¥Î•º Ï±ÑÏö∞Î©∞, Í∞Å ÏúÑÏπòÏóê ÎåÄÌï¥ `row`/`column` Ï¢åÌëúÏôÄ Î≥ëÌï©Îêú ÏïµÏª§ ÏÖÄÏùò ÌÖçÏä§Ìä∏Î•º ÏïåÎ†§ Ï§çÎãàÎã§.\n\n```jsonc\n{\n  \"name\": \"get_table_cell_map\",\n  \"arguments\": {\"path\": \"sample.hwpx\", \"tableIndex\": 0},\n  \"result\": {\n    \"rowCount\": 3,\n    \"columnCount\": 3,\n    \"grid\": [\n      [\n        {\"row\": 0, \"column\": 0, \"anchor\": {\"row\": 0, \"column\": 0}, \"rowSpan\": 2, \"colSpan\": 2, \"text\": \"Ï†úÎ™©\"},\n        {\"row\": 0, \"column\": 1, \"anchor\": {\"row\": 0, \"column\": 0}, \"rowSpan\": 2, \"colSpan\": 2, \"text\": \"Ï†úÎ™©\"},\n        {\"row\": 0, \"column\": 2, \"anchor\": {\"row\": 0, \"column\": 2}, \"rowSpan\": 3, \"colSpan\": 1, \"text\": \"ÏöîÏïΩ\"}\n      ],\n      [\n        {\"row\": 1, \"column\": 0, \"anchor\": {\"row\": 0, \"column\": 0}, \"rowSpan\": 2, \"colSpan\": 2, \"text\": \"Ï†úÎ™©\"},\n        {\"row\": 1, \"column\": 1, \"anchor\": {\"row\": 0, \"column\": 0}, \"rowSpan\": 2, \"colSpan\": 2, \"text\": \"Ï†úÎ™©\"},\n        {\"row\": 1, \"column\": 2, \"anchor\": {\"row\": 0, \"column\": 2}, \"rowSpan\": 3, \"colSpan\": 1, \"text\": \"ÏöîÏïΩ\"}\n      ],\n      \"... ÏÉùÎûµ ...\"\n    ]\n  }\n}\n```\n\n`set_table_cell_text`ÏôÄ `replace_table_region`ÏùÄ ÏÑ†ÌÉùÏ†ÅÏù∏ `logical`/`splitMerged` ÌîåÎûòÍ∑∏Î•º ÏßÄÏõêÌï©ÎãàÎã§. `logical: true`Î°ú ÏßÄÏ†ïÌïòÎ©¥ Î∞©Í∏à ÌôïÏù∏Ìïú ÎÖºÎ¶¨ Ï¢åÌëúÍ≥ÑÎ•º Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÍ≥†, `splitMerged: true`Î•º Ìï®Íªò Ï†ÑÎã¨ÌïòÎ©¥ Ïì∞Í∏∞ Ï†ÑÏóê ÏûêÎèôÏúºÎ°ú Ìï¥Îãπ Î≥ëÌï© ÏòÅÏó≠ÏùÑ Î∂ÑÌï†Ìï©ÎãàÎã§. Í∏¥ ÌÖçÏä§Ìä∏Î•º Ï±ÑÏö∏ ÎïåÎäî `autoFit: true`Î•º Ï∂îÍ∞ÄÎ°ú ÏßÄÏ†ïÌïòÎ©¥ Í∞Å Ïó¥ ÎÑàÎπÑÍ∞Ä ÏÖÄ ÎÇ¥Ïö© Í∏∏Ïù¥Ïóê ÎßûÏ∂∞ Îã§Ïãú Í≥ÑÏÇ∞ÎêòÏñ¥ Ìëú Ï†ÑÏ≤¥ Ìè≠(`hp:sz`)Í≥º ÏÖÄ ÌÅ¨Í∏∞(`hp:cellSz`)Í∞Ä Ìï®Íªò ÏóÖÎç∞Ïù¥Ìä∏Îê©ÎãàÎã§. Î≥ëÌï©ÏùÑ ÏßÅÏ†ë Ìï¥Ï†úÌï¥Ïïº Ìï† ÎïåÎäî `split_table_cell` ÎèÑÍµ¨Í∞Ä ÏõêÎûò Î≤îÏúÑÎ•º ÏïåÎ†§Ï£ºÎ©¥ÏÑú ÏÖÄÏùÑ Î∂ÑÌï†Ìï©ÎãàÎã§.\n\n```jsonc\n{\n  \"name\": \"set_table_cell_text\",\n  \"arguments\": {\n    \"path\": \"sample.hwpx\",\n    \"tableIndex\": 0,\n    \"row\": 1,\n    \"col\": 1,\n    \"text\": \"ÎÖºÎ¶¨ Ï¢åÌëú Ìé∏Ïßë\",\n    \"logical\": true,\n    \"splitMerged\": true,\n    \"autoFit\": true,\n    \"dryRun\": false\n  }\n}\n```\n\nÏúÑ ÏòàÏãúÎäî 2√ó2Î°ú Î≥ëÌï©Îêú ÏÖÄÏóê ÎÖºÎ¶¨ Ï¢åÌëú `(1, 1)`ÏùÑ ÏßÄÏ†ïÌïòÏó¨ ÏûêÎèô Î∂ÑÌï† ÌõÑ ÌÖçÏä§Ìä∏Î•º Í∏∞Î°ùÌï©ÎãàÎã§. Î∂ÑÌï† Ïó¨Î∂ÄÏôÄ ÏõêÎûò Î≤îÏúÑÎ•º ÌôïÏù∏ÌïòÎ†§Î©¥ `split_table_cell`ÏùÑ Ìò∏Ï∂úÌïòÏÑ∏Ïöî.\n\n```jsonc\n{\n  \"name\": \"split_table_cell\",\n  \"arguments\": {\"path\": \"sample.hwpx\", \"tableIndex\": 0, \"row\": 0, \"col\": 0},\n  \"result\": {\"startRow\": 0, \"startCol\": 0, \"rowSpan\": 2, \"colSpan\": 2}\n}\n```\n\nÏùëÎãµÏùò `rowSpan`/`colSpan` Í∞íÏùÄ Î∂ÑÌï†ÎêòÍ∏∞ Ï†Ñ Î≥ëÌï© Î≤îÏúÑÎ•º ÏïåÎ†§Ï£ºÎØÄÎ°ú, ÌîÑÎü∞Ìä∏ÏóîÎìú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä UI ÏÉÅÌÉúÎ•º Ï¶âÏãú Í∞±Ïã†Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## ‚ò¢Ô∏è Í≥†Í∏â Í∏∞Îä•: OPC Ìå®ÌÇ§ÏßÄ ÎÇ¥Î∂Ä ÏÇ¥Ìé¥Î≥¥Í∏∞\n\n> **‚ö†Ô∏è Í≤ΩÍ≥†:** ÏïÑÎûò ÎèÑÍµ¨Îì§ÏùÄ HWPX Î¨∏ÏÑúÏùò ÎÇ¥Î∂Ä OPC ÌååÌä∏Î•º Í∑∏ÎåÄÎ°ú ÎÖ∏Ï∂úÌï©ÎãàÎã§. Íµ¨Ï°∞Î•º ÏûòÎ™ª Ìï¥ÏÑùÌïòÎ©¥ Î¨∏ÏÑúÎ•º Ïò§Ìï¥Ìï† Ïàò ÏûàÏúºÎãà, Ïä§ÌÇ§ÎßàÏôÄ Í¥ÄÍ≥ÑÎ•º Ï∂©Î∂ÑÌûà Ïù¥Ìï¥Ìïú ÏÉÅÌÉúÏóêÏÑú ÌôúÏö©ÌïòÏÑ∏Ïöî. ÌòÑÏû¨ MCP ÏÑúÎ≤ÑÎäî ÏùòÎèÑÏπò ÏïäÏùÄ ÏÜêÏÉÅÏùÑ ÎßâÍ∏∞ ÏúÑÌï¥ **ÏùΩÍ∏∞ Ï†ÑÏö© ÎèÑÍµ¨Îßå** Ï†úÍ≥µÌï©ÎãàÎã§.\n\n  * `package_parts`: Ìå®ÌÇ§ÏßÄÏóê Ìè¨Ìï®Îêú Î™®Îì† OPC ÌååÌä∏Ïùò Í≤ΩÎ°ú Î™©Î°ùÏùÑ ÌôïÏù∏Ìï©ÎãàÎã§.\n  * `package_get_text`: ÏßÄÏ†ïÌïú ÌååÌä∏Î•º ÌÖçÏä§Ìä∏Î°ú ÏùΩÏñ¥ÏòµÎãàÎã§ (Ïù∏ÏΩîÎî© ÏßÄÏ†ï Í∞ÄÎä•).\n  * `package_get_xml`: ÏßÄÏ†ïÌïú ÌååÌä∏Î•º XML Î¨∏ÏûêÏó¥Î°ú Î∞òÌôòÌï©ÎãàÎã§.\n\n#### ÏãúÎÇòÎ¶¨Ïò§ ÏòàÏãú\n\nÏä§ÌÉÄÏùº Ï†ïÏùò XML ÌååÏùº(`Styles.xml`)Ïùò ÎÇ¥Ïö©ÏùÑ ÌôïÏù∏ÌïòÍ≥† Ïã∂Îã§Î©¥:\n\n1.  `package_parts` ÎèÑÍµ¨Ïóê `{\"path\": \"sample.hwpx\"}`Î•º Ï†ÑÎã¨ÌïòÏó¨ `Contents/Styles.xml`Í≥º Í∞ôÏùÄ ÌååÌä∏ Ïù¥Î¶ÑÏùÑ Ï∞æÏäµÎãàÎã§.\n2.  `package_get_xml` ÎèÑÍµ¨Ïóê `{\"path\": \"sample.hwpx\", \"partName\": \"Contents/Styles.xml\"}`ÏùÑ Ï†ÑÎã¨ÌïòÏó¨ Ìï¥Îãπ ÌååÌä∏Ïùò ÏõêÎ≥∏ XMLÏùÑ ÏïàÏ†ÑÌïòÍ≤å Í≤ÄÌÜ†Ìï©ÎãàÎã§.\n\n\n## üîÅ HWP ‚Üí HWPX ÏûêÎèô Î≥ÄÌôò\n\n`convert_hwp_to_hwpx` ÎèÑÍµ¨Îäî ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú `hwp5proc xml` Í≤∞Í≥ºÎ•º Îß§ÌïëÌï¥ `.hwp` Î¨∏ÏÑúÎ•º `.hwpx`Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.\n\n- ÏûÖÎ†•: `source`(ÌïÑÏàò, `.hwp` Í≤ΩÎ°ú), `output`(ÏÑ†ÌÉù, ÎØ∏ÏßÄÏ†ï Ïãú Í∞ôÏùÄ Í≤ΩÎ°úÏóê `.hwpx`)\n- Ï∂úÎ†•: Î≥ÄÌôò ÏÑ±Í≥µ Ïó¨Î∂Ä, Î≥ÄÌôòÎêú Î¨∏Îã®/Ìëú Í∞úÏàò, Î≥ÄÌôò Ï†úÏô∏ ÏöîÏÜå Î™©Î°ù, Í≤ΩÍ≥† Î©îÏãúÏßÄ\n\nÏòàÏãú:\n\n```json\n{\n  \"name\": \"convert_hwp_to_hwpx\",\n  \"arguments\": {\n    \"source\": \"legacy/report.hwp\",\n    \"output\": \"legacy/report.hwpx\"\n  }\n}\n```\n\n### ÏßÄÏõê Î≤îÏúÑ\n\n- **P0**: ÏùºÎ∞ò Î¨∏Îã® ÌÖçÏä§Ìä∏\n- **P1(Î∂ÄÎ∂Ñ ÏßÄÏõê)**: ÌëúÏùò Ìñâ/Ïó¥Í≥º ÏÖÄ ÌÖçÏä§Ìä∏\n- **P2/P3**: OLE, Í∞ÅÏ£º/ÎØ∏Ï£º, Î≥ÄÍ≤Ω Ï∂îÏ†Å, ÏñëÏãù Ïª®Ìä∏Î°§ Îì±ÏùÄ Í≤ΩÍ≥†ÏôÄ Ìï®Íªò Ïä§ÌÇµÎê† Ïàò ÏûàÏùå\n\n### ÏïåÎ†§ÏßÑ Ï†úÌïúÏÇ¨Ìï≠\n\n- Î≥ÄÌôò Î™©ÌëúÎäî 100% ÏãúÍ∞Å Ïû¨ÌòÑÏù¥ ÏïÑÎãàÎùº **ÌÖçÏä§Ìä∏ Î≥¥Ï°¥ + Í∏∞Î≥∏ Íµ¨Ï°∞ Ïù¥Í¥Ä**ÏûÖÎãàÎã§.\n- Î≥µÏû°Ìïú ÏÑúÏãù(ÏÑ∏Î∞ÄÌïú Ïä§ÌÉÄÏùº, Í≥†Í∏â Í∞úÏ≤¥, ÏùºÎ∂Ä Î≥ëÌï© Ìëú)ÏùÄ Í≤∞Í≥º Î¨∏ÏÑúÏóêÏÑú ÏàòÎèô Î≥¥Ï†ïÏù¥ ÌïÑÏöîÌï† Ïàò ÏûàÏäµÎãàÎã§.\n- `hwp5proc` Ïã§Ìñâ ÌôòÍ≤ΩÏù¥ ÏóÜÏúºÎ©¥ Î≥ÄÌôò ÎèÑÍµ¨Îäî Ïã§Ìå®ÌïòÎ©∞ ÏÑ§Ïπò ÏïàÎÇ¥ Ïò§Î•òÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n\n## üß™ ÌÖåÏä§Ìä∏\n\nÌïµÏã¨ Í∏∞Îä•Î∂ÄÌÑ∞ Î™®Îì† MCP ÎèÑÍµ¨Ïùò Ïã§Ï†ú Ìò∏Ï∂úÍπåÏßÄ Í≤ÄÏ¶ùÌïòÎäî ÏóîÎìúÌà¨ÏóîÎìú ÌÖåÏä§Ìä∏ Ïä§ÏúÑÌä∏Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n\n```bash\n# 1. ÌÖåÏä§Ìä∏ ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\npython -m pip install -e .[test]\n\n# 2. ÌÖåÏä§Ìä∏ Ïã§Ìñâ\npython -m pytest\n```\n\n`tests/test_mcp_end_to_end.py`Îäî ÏÑúÎ≤ÑÍ∞Ä ÎÖ∏Ï∂úÌïòÎäî ÎåÄÎ∂ÄÎ∂ÑÏùò ÎèÑÍµ¨Î•º Ïã§Ï†úÎ°ú Ìò∏Ï∂úÌïòÏó¨ ÌÖçÏä§Ìä∏, Ìëú, Î©îÎ™® Ìé∏Ïßë, OPC Ìå®ÌÇ§ÏßÄ ÏùΩÍ∏∞, ÏûêÎèô Î∞±ÏóÖ ÏÉùÏÑ± Îì± ÌïµÏã¨ ÎèôÏûëÏùÑ ÏôÑÎ≤ΩÌïòÍ≤å Í≤ÄÏ¶ùÌï©ÎãàÎã§.\n\n## üßë‚Äçüíª Í∞úÎ∞ú Ï∞∏Í≥†\n\n  * Ïù¥ ÏÑúÎ≤ÑÎäî `python-hwpx>=1.9`, `mcp`, `anyio`, `pydantic` Îì± ÏàúÏàò ÌååÏù¥Ïç¨ ÎùºÏù¥Î∏åÎü¨Î¶¨Î°úÎßå Íµ¨ÏÑ±Îê©ÎãàÎã§.\n  * Î™®Îì† ÎèÑÍµ¨ Ìï∏Îì§Îü¨Îäî `HwpxOps`Ïùò Í≤ΩÎ°ú Ìó¨ÌçºÏôÄ `HwpxDocument` APIÎ•º ÌÜµÌï¥ Î¨∏ÏÑúÎ•º ÏïàÏ†ÑÌïòÍ≤å Ï°∞ÏûëÌï©ÎãàÎã§.\n  * ÌååÍ¥¥Ï†Å ÏûëÏóÖ(ÏàòÏ†ï/Ï†ÄÏû•)ÏóêÎäî `dryRun` ÌîåÎûòÍ∑∏Î•º Ïö∞ÏÑ† Ï†úÍ≥µÌïòÎ©∞, ÏûêÎèô Î∞±ÏóÖ ÏòµÏÖòÏù¥ ÌôúÏÑ±ÌôîÎêòÏñ¥ ÏûàÏúºÎ©¥ `.bak` ÌååÏùºÏùÑ ÏÉùÏÑ±ÌïòÏó¨ ÏïàÏ†ïÏÑ±ÏùÑ ÎÜíÏûÖÎãàÎã§.\n  * JSON Ïä§ÌÇ§ÎßàÎäî ÎÇ¥Î∂Ä `schema.builder` Í≤ΩÎ°úÎ•º ÌÜµÌï¥ draft-07 Ìò∏Ìôò SanitizerÎ•º Í±∞Ïπú ÌõÑ ÎÖ∏Ï∂úÎêòÎØÄÎ°ú `$ref`/`anyOf`Í∞Ä Ï†úÍ±∞Îêú ÌèâÌÉÑÌïú Íµ¨Ï°∞Î•º Í∏∞ÎåÄÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n### üîí ÏÑúÎ≤Ñ ÌïòÎìúÎãù & JSON Ïä§ÌÇ§Îßà (draft-07) ‚Äî ÏÑ†ÌÉù ÏÇ¨Ïö©\n\n- `HWPX_MCP_HARDENING=1`ÏùÑ ÏÑ§Ï†ïÌïòÎ©¥ plan/preview/apply ÌååÏù¥ÌîÑÎùºÏù∏, `hwpx.search`, `hwpx.get_context`Í∞Ä ÌôúÏÑ±ÌôîÎê©ÎãàÎã§.\n- ÌîåÎûòÍ∑∏Î•º ÎÅÑÎ©¥ (`0` ÎòêÎäî ÎØ∏ÏÑ§Ï†ï) Í∏∞Ï°¥ ÎèÑÍµ¨Îßå Ïú†ÏßÄÌïòÎ©¥ÏÑúÎèÑ Í∞ïÌôîÎêú Ïä§ÌÇ§Îßà SanitizerÎäî Í≥ÑÏÜç Ï†ÅÏö©Îê©ÎãàÎã§.\n- `pytest -q`Î•º Ïã§ÌñâÌïòÎ©¥ Ïä§ÌÇ§Îßà ÌöåÍ∑Ä, ÌååÏù¥ÌîÑÎùºÏù∏ Í≤åÏù¥Ìä∏, Î©±Îì±ÏÑ± Í≤ÄÏ¶ù ÌÖåÏä§Ìä∏Í∞Ä Ìï®Íªò ÏàòÌñâÎêòÏñ¥ Î∞∞Ìè¨ Ï†Ñ ÏïàÏ†ÑÏÑ±ÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## üìú ÎùºÏù¥ÏÑ†Ïä§\n\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî [MIT ÎùºÏù¥ÏÑ†Ïä§](https://www.google.com/search?q=LICENSE)Î°ú Î∞∞Ìè¨Îê©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ ÎùºÏù¥ÏÑ†Ïä§ ÌååÏùºÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî.\n\n## Ïù¥Î©îÏùº\n\nÍ¥ëÍµêÍ≥†Îì±ÌïôÍµê ÍµêÏÇ¨ Í≥†Í∑úÌòÑ : kokyuhyun@hotmail.com\n\n\n## üß© ÏñëÏãù(ÌÖúÌîåÎ¶ø) Î¨∏ÏÑú ÏûëÏóÖ\n\n### 1) Íµ¨Ï°∞ ÌååÏïÖ: `analyze_template_structure`\n\nÏñëÏãù Î¨∏ÏÑúÎ•º Ïó¥ÏûêÎßàÏûê ÏàòÏ†ï Í∞ÄÎä•/Î∂àÍ∞Ä ÏòÅÏó≠Í≥º ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî ÌõÑÎ≥¥Î•º ÌååÏïÖÌïòÎ†§Î©¥ ÏïÑÎûò ÎèÑÍµ¨Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n\n```json\n{\n  \"name\": \"analyze_template_structure\",\n  \"arguments\": {\n    \"path\": \"sample.hwpx\",\n    \"placeholderPatterns\": [\"\\\\{\\\\{[^{}]+\\\\}\\\\}\", \"Î≥∏Î¨∏ ÏòÅÏó≠\"],\n    \"lockKeywords\": [\"ÌïôÍµêÏû•\", \"ÏßÅÏù∏\", \"Î°úÍ≥†\"]\n  }\n}\n```\n\nÏùëÎãµÏóêÎäî `summary`(Î¨∏Îã® Ïàò/ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî Ïàò), `regions`(header/body/footer), `placeholders`(ÌÜ†ÌÅ∞/Î¨∏Îã® Ïù∏Îç±Ïä§/ÏàòÏ†ï Í∞ÄÎä• Ïó¨Î∂Ä)Í∞Ä Ìè¨Ìï®Îê©ÎãàÎã§.\n\n### 2) Ìïú Î≤àÏóê Ï±ÑÏö∞Í∏∞: `fill_template`\n\nÍ∏∞Ï°¥Ïùò `save_as -> find -> replace...` Îã§Îã®Í≥Ñ ÎåÄÏã†, `fill_template` ÌïòÎÇòÎ°ú ÌÖúÌîåÎ¶ø Î≥µÏÇ¨ÏôÄ Îã§Ï§ë ÏπòÌôòÏùÑ Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n```json\n{\n  \"name\": \"fill_template\",\n  \"arguments\": {\n    \"source\": \"forms/notice_template.hwpx\",\n    \"output\": \"out/notice_2026.hwpx\",\n    \"replacements\": {\n      \"Î≥∏Î¨∏ ÏòÅÏó≠\": \"Ïã§Ï†ú ÏïàÎÇ¥Î¨∏ Î≥∏Î¨∏\",\n      \"Ï†ú2025ÎÖÑ\": \"Ï†ú2026ÎÖÑ\",\n      \"2025. 1. 1.\": \"2026. 3. 5.\"\n    }\n  }\n}\n```\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Open and automate HWPX documents without Hancom Word Processor",
        "Connect with MCP clients like Gemini CLI and Claude Desktop for document creation, editing, and navigation",
        "Extract text, paginate, and edit styles, tables, memos, and objects within HWPX documents",
        "Read-only access and search of legacy .hwp binary documents with automatic conversion to .hwpx format",
        "Automatically backup documents before saving to prevent data loss",
        "Run as a zero-configuration MCP server interpreting paths relative to the current working directory",
        "Perform advanced table editing including merged cell splitting and auto-fitting column widths",
        "Use a hardened 3-step editing pipeline (plan, preview, apply) with validation and idempotency support",
        "Search documents with regex or keyword queries and retrieve limited context windows for privacy",
        "Inspect internal OPC package parts of HWPX documents in read-only mode"
      ],
      "limitations": [
        "Cannot fully preserve complex formatting or advanced objects during HWP to HWPX conversion",
        "HWP to HWPX conversion requires external hwp5proc environment and may fail without it",
        "Editing pipeline with plan/preview/apply is experimental and requires enabling a hardening flag",
        "Read-only access only for OPC package internal parts; no write support",
        "Requests for non-existent paragraph indexes result in errors",
        "Default text extraction limits to 200 paragraphs unless overridden",
        "Some advanced features require environment variables or flags to be enabled explicitly"
      ],
      "requirements": [
        "Python environment with python-hwpx version 1.9 or higher",
        "Installation of 'uv' Python package manager for easy server startup",
        "MCP client configured to launch the server with specified command and environment variables",
        "Optional environment variables to control paging limits, auto backup, log level, and hardening mode",
        "For HWP to HWPX conversion, hwp5proc executable must be installed and accessible"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, extensive tool descriptions, environment variable explanations, limitations, and testing guidance, making it excellent in quality.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# ÌïúÍ∏Ä MCP (HWPX) ÏÑúÎ≤Ñ - ÌïúÍ∏Ä ÏûêÎèôÌôî HWPX Î¨∏ÏÑú ÏûêÎèô ÏÉùÏÑ±¬∑Ìé∏Ïßë¬∑Í≤ÄÏ¶ùmcp #\n\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî **ÌïúÍ∏Ä MCP(HWPX) ÏÑúÎ≤Ñ**Î°ú, HWPX Î¨∏ÏÑúÎ•º ÌïúÍ∏Ä ÏõåÎìúÌîÑÎ°úÏÑ∏ÏÑú ÏóÜÏù¥ ÏßÅÏ†ë Ïó¥Í≥† ÏûêÎèôÌôîÌï† Ïàò ÏûàÎèÑÎ°ù ÏÑ§Í≥ÑÎêòÏóàÏäµÎãàÎã§.  \nGemini CLI, Claude DesktopÍ≥º Í∞ôÏùÄ MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïóê Ïó∞Í≤∞ÌïòÏó¨ Î¨∏ÏÑú ÏÉùÏÑ±¬∑Ìé∏Ïßë¬∑ÌÉêÏÉâ Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n\n[](https://www.google.com/search?q=https://pypi.org/project/hwpx-mcp-server/)\n[](https://opensource.org/licenses/MIT)\n[](https://www.google.com/search?q=https://github.com/your-repo/hwpx-mcp-server/actions/workflows/ci.yml)\n\n**ÏàúÏàò ÌååÏù¥Ïç¨ÏúºÎ°ú HWPX Î¨∏ÏÑúÎ•º ÏûêÏú†Î°≠Í≤å Îã§Î£®Îäî Í∞ÄÏû• Í∞ïÎ†•Ìïú Î∞©Î≤ï.**\n\n\n`hwpx-mcp-server`Îäî [Model Context Protocol](https://github.com/modelcontextprotocol/specification) ÌëúÏ§ÄÏùÑ Îî∞Î•¥Îäî ÏÑúÎ≤ÑÎ°ú, Í∞ïÎ†•Ìïú [`python-hwpx`](https://www.google.com/search?q=%5Bhttps://github.com/airmang/python-hwpx%5D\\(https://github.com/airmang/python-hwpx\\)) ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º Í∏∞Î∞òÏúºÎ°ú Ìï©ÎãàÎã§. Gemini, ClaudeÏôÄ Í∞ôÏùÄ ÏµúÏã† AI ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ÏôÄ ÏôÑÎ≤ΩÌïòÍ≤å Ïó∞ÎèôÌïòÏó¨ ÌïúÍ∏Ä ÏõåÎìú ÌîÑÎ°úÏÑ∏ÏÑú Î°úÏª¨ HWPX Î¨∏ÏÑúÎ•º Ïó¥Îûå, Í≤ÄÏÉâ, Ìé∏Ïßë, Ï†ÄÏû•ÌïòÎäî ÌíçÎ∂ÄÌïú Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n\n-----\n\n## ‚ú® Ï£ºÏöî Í∏∞Îä•\n\n  * **‚úÖ ÌëúÏ§Ä MCP ÏÑúÎ≤Ñ Íµ¨ÌòÑ**: Í≥µÏãù `mcp` SDKÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏïàÏ†ïÏ†ÅÏù∏ ÌëúÏ§Ä ÏûÖ/Ï∂úÎ†• Í∏∞Î∞ò ÏÑúÎ≤ÑÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.\n  * **üìÇ Ï†úÎ°ú ÏÑ§Ï†ï**: Î≥ÑÎèÑ ÏÑ§Ï†ï ÏóÜÏù¥ ÌòÑÏû¨ ÏûëÏóÖ ÎîîÎ†âÌÑ∞Î¶¨Î•º Í∏∞Ï§ÄÏúºÎ°ú Ï¶âÏãú Í≤ΩÎ°úÎ•º Ï≤òÎ¶¨Ìï©ÎãàÎã§.\n  * **üìÑ Í∞ïÎ†•Ìïú Î¨∏ÏÑú Ìé∏Ïßë**: ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú, ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖòÎ∂ÄÌÑ∞ Ïä§ÌÉÄÏùº, Ìëú, Î©îÎ™®, Í∞úÏ≤¥ Ìé∏ÏßëÍπåÏßÄ Î™®Îëê Í∞ÄÎä•Ìï©ÎãàÎã§.\n  * **üß© HWP Ìò∏Ìôò + ÏûêÎèô Î≥ÄÌôò**: `.hwp` Î∞îÏù¥ÎÑàÎ¶¨ Î¨∏ÏÑúÎ•º ÏùΩÍ∏∞ Ï†ÑÏö©ÏúºÎ°ú Ï°∞Ìöå/Í≤ÄÏÉâÌï† Ïàò ÏûàÍ≥†, `convert_hwp_to_hwpx` ÎèÑÍµ¨Î°ú `.hwpx`Î°ú ÏûêÎèô Î≥ÄÌôòÌï¥ Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏Ïóê Î∞îÎ°ú Ïó∞Í≤∞Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n  * **üõ°Ô∏è ÏïàÏ†ÑÌïú Ï†ÄÏû•**: ÏûêÎèô Î∞±ÏóÖ(`*.bak`) ÏòµÏÖòÏúºÎ°ú ÏòàÍ∏∞Ïπò ÏïäÏùÄ Îç∞Ïù¥ÌÑ∞ ÏÜêÏã§ÏùÑ Î∞©ÏßÄÌï©ÎãàÎã§.\n  * **üöÄ Ï¶âÏãú Ïã§Ìñâ**: `uv`Îßå ÏûàÏúºÎ©¥ `uvx hwpx-mcp-server` Ìïú Ï§ÑÎ°ú Î∞îÎ°ú ÏãúÏûëÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## üöÄ Îπ†Î•∏ ÏãúÏûë\n\n### 1\\. `uv` ÏÑ§Ïπò\n\nÍ∞ÄÏû• Î®ºÏ†Ä ÌååÏù¥Ïç¨ Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÎèÑÍµ¨Ïù∏ `uv`Î•º ÏÑ§ÏπòÌïòÏÑ∏Ïöî.\n[üëâ Astral uv ÏÑ§Ïπò Í∞ÄÏù¥Îìú](https://docs.astral.sh/uv/getting-started/installation/)\n\n\n### 2\\. MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï\n\nÏÇ¨Ïö© Ï§ëÏù∏ MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ïÏóê ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÏÑúÎ≤Ñ Ï†ïÎ≥¥Î•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.\n\n```json\n{\n  \"mcpServers\": {\n    \"hwpx\": {\n      \"command\": \"uvx\",\n      \"args\": [\"hwpx-mcp-server\"],\n      \"env\": {\n        \"HWPX_MCP_PAGING_PARA_LIMIT\": \"200\",\n        \"HWPX_MCP_AUTOBACKUP\": \"1\",\n        \"LOG_LEVEL\": \"INFO\"\n      }\n    }\n  }\n}\n```\n### 3\\. ÏÑúÎ≤Ñ Ïã§Ìñâ (Î°úÏª¨ ÌôòÍ≤ΩÏóêÏÑú ÏÇ¨Ïö©Ìï† Í≤ΩÏö∞)\n\nÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏïÑÎûò Î™ÖÎ†πÏñ¥Î•º Ïã§ÌñâÌïòÎ©¥ ÏÑúÎ≤ÑÍ∞Ä Î∞îÎ°ú ÏãúÏûëÎê©ÎãàÎã§.\n\n```bash\nuvx hwpx-mcp-server\n```\n\n> `uvx` Î™ÖÎ†πÏùÄ Ï≤´ Ïã§Ìñâ Ïãú ÌïÑÏöîÌïú Ï¢ÖÏÜçÏÑ±ÏùÑ ÏûêÎèôÏúºÎ°ú ÏÑ§ÏπòÌïòÎ©∞, Î∞òÎìúÏãú `python-hwpx 1.9` Ïù¥ÏÉÅÏùò Î≤ÑÏ†ÑÏù¥ Ï§ÄÎπÑÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n\n> ÏÑúÎ≤ÑÎäî Ïã§ÌñâÎêú ÌòÑÏû¨ ÎîîÎ†âÌÑ∞Î¶¨Î•º Í∏∞Ï§ÄÏúºÎ°ú Í≤ΩÎ°úÎ•º Ìï¥ÏÑùÌïòÎØÄÎ°ú, Î≥ÑÎèÑÏùò ÏûëÏóÖ ÎîîÎ†âÌÑ∞Î¶¨ ÏÑ§Ï†ï ÏóÜÏù¥ Î∞îÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.",
        "start_pos": 0,
        "end_pos": 2020,
        "token_count_estimate": 505,
        "source_type": "readme",
        "agent_id": "bc97a05ae003ab36"
      },
      {
        "chunk_id": 1,
        "text": "ÌïòÎ©¥ ÏÑúÎ≤ÑÍ∞Ä Î∞îÎ°ú ÏãúÏûëÎê©ÎãàÎã§.\n\n```bash\nuvx hwpx-mcp-server\n```\n\n> `uvx` Î™ÖÎ†πÏùÄ Ï≤´ Ïã§Ìñâ Ïãú ÌïÑÏöîÌïú Ï¢ÖÏÜçÏÑ±ÏùÑ ÏûêÎèôÏúºÎ°ú ÏÑ§ÏπòÌïòÎ©∞, Î∞òÎìúÏãú `python-hwpx 1.9` Ïù¥ÏÉÅÏùò Î≤ÑÏ†ÑÏù¥ Ï§ÄÎπÑÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n\n> ÏÑúÎ≤ÑÎäî Ïã§ÌñâÎêú ÌòÑÏû¨ ÎîîÎ†âÌÑ∞Î¶¨Î•º Í∏∞Ï§ÄÏúºÎ°ú Í≤ΩÎ°úÎ•º Ìï¥ÏÑùÌïòÎØÄÎ°ú, Î≥ÑÎèÑÏùò ÏûëÏóÖ ÎîîÎ†âÌÑ∞Î¶¨ ÏÑ§Ï†ï ÏóÜÏù¥ Î∞îÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## ‚öôÔ∏è ÌôòÍ≤Ω Î≥ÄÏàò\n\n| Î≥ÄÏàò | ÏÑ§Î™Ö | Í∏∞Î≥∏Í∞í |\n| --- | --- | --- |\n| `HWPX_MCP_PAGING_PARA_LIMIT` | ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò ÎèÑÍµ¨Í∞Ä Î∞òÌôòÌï† ÏµúÎåÄ Î¨∏Îã® Ïàò | `200` |\n| `HWPX_MCP_AUTOBACKUP` | `1`Ïù¥Î©¥ Ï†ÄÏû• Ï†Ñ `<file>.bak` Î∞±ÏóÖ ÏÉùÏÑ± | `0` |\n| `LOG_LEVEL` | stderrÏóê JSONL ÌòïÏãùÏúºÎ°ú Ï∂úÎ†•Ìï† Î°úÍ∑∏ Î†àÎ≤® | `INFO` |\n| `HWPX_MCP_HARDENING` | `1`Î°ú ÏÑ§Ï†ï Ïãú ÌïòÎìúÎãù Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏Í≥º Í≤ÄÏÉâ/Ïª®ÌÖçÏä§Ìä∏ ÎèÑÍµ¨ ÌôúÏÑ±Ìôî | `0` |\n\n> ‚ÑπÔ∏è `read_text` ÎèÑÍµ¨Îäî Í∏∞Î≥∏Ï†ÅÏúºÎ°ú ÏµúÎåÄ 200Í∞úÏùò Î¨∏Îã®ÏùÑ Î∞òÌôòÌï©ÎãàÎã§. Îçî ÌÅ∞ Îç§ÌîÑÍ∞Ä ÌïÑÏöîÌïòÎ©¥ ÎèÑÍµ¨ Ìò∏Ï∂ú Ïãú `limit` Ïù∏ÏàòÎ•º ÏßÅÏ†ë ÏßÄÏ†ïÌïòÍ±∞ÎÇò `HWPX_MCP_PAGING_PARA_LIMIT` ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÌôïÏû•ÌïòÏÑ∏Ïöî. Ïù¥Îäî Microsoft Office WordÏóêÏÑú ÌïÑÏöîÌïú Î≤îÏúÑÎßå ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏùΩÎäî ÏõåÌÅ¨ÌîåÎ°úÏôÄ ÎèôÏùºÌï©ÎãàÎã§.\n\n> üîê `HWPX_MCP_HARDENING=1`Î°ú Ïã§ÌñâÌïòÎ©¥ ÏÉà Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏(`plan ‚Üí preview ‚Üí apply`)Í≥º Í≤ÄÏÉâ/Ïª®ÌÖçÏä§Ìä∏ ÎèÑÍµ¨Í∞Ä Ìï®Íªò ÎÖ∏Ï∂úÎê©ÎãàÎã§. Í∞íÏù¥ `0` ÎòêÎäî ÎØ∏ÏÑ§Ï†ïÏù¥Î©¥ Í∏∞Ï°¥ ÎèÑÍµ¨ ÌëúÎ©¥Îßå Ïú†ÏßÄÎê©ÎãàÎã§.\n\n### üìÅ Î¨∏ÏÑú Î°úÏºÄÏù¥ÌÑ∞(Document Locator)\n\nÎ™®Îì† ÎèÑÍµ¨Ïùò ÏûÖÎ†•ÏùÄ Ïù¥Ï†ú Î¨∏ÏÑúÎ•º Í∞ÄÎ¶¨ÌÇ§Îäî **discriminated union** Î°úÏºÄÏù¥ÌÑ∞Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§. Í∏∞Î≥∏Í∞íÏùÄ Í∏∞Ï°¥Í≥º ÎèôÏùºÌïòÍ≤å ÏÉÅÏúÑ ÏàòÏ§ÄÏùò `path` ÌïÑÎìúÏù¥Î©∞, Î≥ÑÎèÑ ÏÑ†Ïñ∏ ÏóÜÏù¥ÎèÑ Í≥ÑÏÜç ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÌïÑÏöîÏóê Îî∞Îùº Î™ÖÏãúÏ†ÅÏúºÎ°ú `type`ÏùÑ ÏßÄÏ†ïÌï¥ HTTP Î∞±ÏóîÎìúÎÇò ÏÇ¨Ï†Ñ Îì±Î°ùÎêú Ìï∏Îì§ÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n- **Î°úÏª¨ ÌååÏùº (Í∏∞Ï°¥ Ïä§ÌÇ§ÎßàÏôÄ ÎèôÏùº)**\n\n  ```jsonc\n  {\n    \"name\": \"open_info\",\n    \"arguments\": {\n      \"path\": \"sample.hwpx\"\n    }\n  }\n  ```\n\n- **HTTP Î∞±ÏóîÎìúÏôÄ Ïó∞Í≥Ñ** ‚Äî ÏÑúÎ≤ÑÎ•º HTTP Ïä§ÌÜ†Î¶¨ÏßÄ Î™®ÎìúÎ°ú Ïã§ÌñâÌïú Í≤ΩÏö∞, ÏõêÍ≤© Í≤ΩÎ°úÎ•º `uri` ÌïÑÎìúÎ°ú ÏßÄÏ†ïÌïòÍ≥† ÌïÑÏöî Ïãú `backend` ÌûåÌä∏Î•º Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n  ```jsonc\n  {\n    \"name\": \"open_info\",\n    \"arguments\": {\n      \"type\": \"uri\",\n      \"uri\": \"reports/weekly.hwpx\",\n      \"backend\": \"http\"\n    }\n  }\n  ```\n\n- **ÏÇ¨Ï†Ñ Îì±Î°ùÎêú Ìï∏Îì§ ÏÇ¨Ïö©** ‚Äî ÌïòÎìúÎãù ÌååÏù¥ÌîÑÎùºÏù∏ÏóêÏÑú Ïù¥ÎØ∏ Î°úÎìúÎêú Î¨∏ÏÑúÏóê ÎåÄÌï¥ ÌõÑÏÜç Í≤ÄÏÉâ/Ïª®ÌÖçÏä§Ìä∏/Ìé∏ÏßëÏùÑ ÏàòÌñâÌï† ÎïåÎäî `handleId`Î•º Ï†ÑÎã¨ÌïòÎ©¥ Îê©ÎãàÎã§.\n\n  ```jsonc\n  {\n    \"name\": \"hwpx.plan_edit\",\n    \"arguments\": {\n      \"type\": \"handle\",\n      \"handleId\": \"doc-1234\",\n      \"operations\": [\n        {\n          \"target\": {\"nodeId\": \"n_deadbeef\"},\n          \"match\": \"needle\",\n          \"replacement\": \"haystack\"\n        }\n      ]\n    }\n  }\n  ```\n\nÍ∞Å Î≥ÄÌòïÏùÄ ÌïÑÏöîÏóê Îî∞Îùº `backend` ÌïÑÎìúÎ•º Ï∂îÍ∞ÄÎ°ú Í∞ÄÏßà Ïàò ÏûàÏúºÎ©∞, Î™ÖÏãúÏ†ÅÏúºÎ°ú `document` Í∞ùÏ≤¥Î•º Ï§ëÏ≤©ÌïòÏó¨ Ï†ÑÎã¨ÌïòÎäî Í≤ÉÎèÑ ÌóàÏö©Îê©ÎãàÎã§. Ïä§ÌÇ§ÎßàÎäî SanitizerÎ•º Í±∞Ï≥ê `$ref` ÏóÜÏù¥ ÌèâÌÉÑÌôîÎêú ÌòïÌÉúÎ°ú Ï†úÍ≥µÎê©ÎãàÎã§.",
        "start_pos": 1820,
        "end_pos": 3762,
        "token_count_estimate": 485,
        "source_type": "readme",
        "agent_id": "bc97a05ae003ab36"
      },
      {
        "chunk_id": 2,
        "text": "\",\n          \"replacement\": \"haystack\"\n        }\n      ]\n    }\n  }\n  ```\n\nÍ∞Å Î≥ÄÌòïÏùÄ ÌïÑÏöîÏóê Îî∞Îùº `backend` ÌïÑÎìúÎ•º Ï∂îÍ∞ÄÎ°ú Í∞ÄÏßà Ïàò ÏûàÏúºÎ©∞, Î™ÖÏãúÏ†ÅÏúºÎ°ú `document` Í∞ùÏ≤¥Î•º Ï§ëÏ≤©ÌïòÏó¨ Ï†ÑÎã¨ÌïòÎäî Í≤ÉÎèÑ ÌóàÏö©Îê©ÎãàÎã§. Ïä§ÌÇ§ÎßàÎäî SanitizerÎ•º Í±∞Ï≥ê `$ref` ÏóÜÏù¥ ÌèâÌÉÑÌôîÎêú ÌòïÌÉúÎ°ú Ï†úÍ≥µÎê©ÎãàÎã§.\n\n### üîê ÌïòÎìúÎãù Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏ (ÏòµÏÖò)\n\nÌïòÎìúÎãù ÌîåÎûòÍ∑∏Î•º ÏºúÎ©¥ Î™®Îì† Ìé∏Ïßë ÏöîÏ≤≠Ïù¥ **Í≥ÑÌöç(Plan) ‚Üí Í≤ÄÌÜ†(Preview) ‚Üí Ï†ÅÏö©(Apply)**Ïùò 3Îã®Í≥ÑÎ•º Í±∞ÏπòÎèÑÎ°ù ÏÑ§Í≥ÑÎêú Ïã†Í∑ú ÎèÑÍµ¨Í∞Ä Ìï®Íªò ÎÖ∏Ï∂úÎê©ÎãàÎã§. ÌïòÎìúÎãù ÌîåÎûòÍ∑∏Îäî Ï†ÄÎ†¥Ìïú LLM Î™®Îç∏Ïùò ÏöîÏ≤≠ÏóêÎèÑ ÏÑ±Í≥µÏ†ÅÏù∏ ÏûëÏóÖÏùÑ ÏàòÌñâÌïòÍ∏∞ ÏúÑÌï¥ÏÑú ÎèÑÏûÖÌïú ÌÖåÏä§Ìä∏Ï§ëÏù∏ Í∏∞Îä•ÏûÖÎãàÎã§. \n\n1. **`hwpx.plan_edit`**: Î≥ÄÍ≤Ω ÎåÄÏÉÅÍ≥º ÏùòÎèÑÌïú ÏûëÏóÖÏùÑ ÏÑ§Î™ÖÌïòÎ©¥ ÏÑúÎ≤ÑÍ∞Ä ÏïàÏ†ïÏ†ÅÏù∏ `planId`ÏôÄ ÏòàÏÉÅ ÏûëÏóÖ ÏöîÏïΩÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n2. **`hwpx.preview_edit`**: Î∞úÍ∏âÎêú `planId`Î°ú ÎØ∏Î¶¨Î≥¥Í∏∞Î•º ÏöîÏ≤≠ÌïòÎ©¥ Ïã§Ï†ú diff, Î™®Ìò∏ÏÑ± Í≤ΩÍ≥†, ÏïàÏ†Ñ Ï†êÏàò Îì±ÏùÑ Ìè¨Ìï®Ìïú Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞Î•º Î∞òÌôòÌï©ÎãàÎã§. Ïù¥ Îã®Í≥ÑÍ∞Ä Í∏∞Î°ùÎêòÏßÄ ÏïäÏúºÎ©¥ Ï†ÅÏö© Îã®Í≥ÑÎ°ú ÎÑòÏñ¥Í∞à Ïàò ÏóÜÏäµÎãàÎã§.\n3. **`hwpx.apply_edit`**: `preview`Î•º Í±∞Ïπú ÎèôÏùºÌïú `planId`Ïóê `confirm: true`Î•º Î™ÖÏãúÌï¥Ïïº Ïã§Ï†ú Î¨∏ÏÑú Î≥ÄÍ≤ΩÏù¥ Ïù¥Î£®Ïñ¥ÏßëÎãàÎã§. `idempotencyKey`Î•º ÏßÄÏ†ïÌïòÎ©¥ ÎèôÏùº ÏöîÏ≤≠Ïù¥ Î∞òÎ≥µÎêòÎçîÎùºÎèÑ ÏïàÏ†ÑÌïòÍ≤å Î¨¥ÏãúÎê©ÎãàÎã§.\n\nÍ∞Å Îã®Í≥ÑÎäî ÌëúÏ§Ä `ServerResponse` ÎûòÌçºÎ•º ÏÇ¨Ïö©ÌïòÎ©∞, Ïò§Î•ò Î∞úÏÉù Ïãú `PREVIEW_REQUIRED`, `AMBIGUOUS_TARGET`, `UNSAFE_WILDCARD`, `IDEMPOTENT_REPLAY` Îì±Ïùò ÏΩîÎìúÏôÄ Ìï®Íªò ÌõÑÏÜç ÌñâÎèô ÏòàÏãú(`next_actions`)Î•º Î∞òÌôòÌï©ÎãàÎã§. Î™®Îì† Ïä§ÌÇ§ÎßàÎäî draft-07 Ìò∏Ìôò SanitizerÎ•º ÌÜµÌï¥ `$ref`, `anyOf` ÏóÜÏù¥ ÌèâÌÉÑÌôîÎêòÏñ¥ ÎÖ∏Ï∂úÎê©ÎãàÎã§.\n\nÎòêÌïú ÌïòÎìúÎãù Î™®ÎìúÏóêÏÑúÎäî ÏßÄÏõê ÎèÑÍµ¨Í∞Ä ÌôïÏû•Îê©ÎãàÎã§.\n\n- **`hwpx.search`**: Ï†ïÍ∑úÏãù ÎòêÎäî ÌÇ§ÏõåÎìú Í∏∞Î∞òÏúºÎ°ú Î¨∏ÏÑú Ï†ÑÎ∞òÏùÑ Í≤ÄÏÉâÌïòÏó¨ ÎÖ∏Ï∂ú Í∞ÄÎä•Ìïú Î¨∏Îß•Îßå Ìè¨Ìï®Ìïú ÏùºÏπò Í≤∞Í≥ºÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.\n- **`hwpx.get_context`**: ÌäπÏ†ï Î¨∏Îã® Ï£ºÎ≥ÄÏùò Ï†úÌïúÎêú Ï∞Ω(window)Îßå Ï∂îÏ∂úÌïòÏó¨ ÌîÑÎùºÏù¥Î≤ÑÏãúÎ•º Ïú†ÏßÄÌïú Ï±Ñ Î¶¨Î∑∞Ïóê ÌôúÏö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## üõ†Ô∏è Ï†úÍ≥µ ÎèÑÍµ¨\n\nÎã§ÏñëÌïú Î¨∏ÏÑú Ìé∏Ïßë Î∞è Í¥ÄÎ¶¨ ÎèÑÍµ¨Î•º Ï†úÍ≥µÌï©ÎãàÎã§. Í∞Å ÎèÑÍµ¨Ïùò ÏÉÅÏÑ∏Ìïú ÏûÖÏ∂úÎ†• ÌòïÏãùÏùÄ `ListTools` ÏùëÎãµÏóê Ìè¨Ìï®Îêú JSON Ïä§ÌÇ§ÎßàÎ•º ÌÜµÌï¥ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.",
        "start_pos": 3562,
        "end_pos": 4828,
        "token_count_estimate": 316,
        "source_type": "readme",
        "agent_id": "bc97a05ae003ab36"
      },
      {
        "chunk_id": 3,
        "text": "ext` *(ÌîåÎûòÍ∑∏ ÌôúÏÑ± Ïãú)*: Î¨∏Îã® Ï†ÑÌõÑ Î¨∏Îß•Îßå Ï†úÌïúÏ†ÅÏúºÎ°ú Ï°∞Ìöå\n  - **Î¨∏ÏÑú Ìé∏Ïßë**\n      - `replace_text_in_runs`: Ïä§ÌÉÄÏùºÏùÑ Î≥¥Ï°¥ÌïòÎ©∞ ÌÖçÏä§Ìä∏ ÏπòÌôò (Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Î¨∏ÏÑúÎ•º Ï†ÄÏû•ÌïòÎØÄÎ°ú,\n        ÎØ∏Î¶¨Î≥¥Í∏∞Îßå ÏõêÌïòÎ©¥ `dryRun: true`Î•º ÏßÄÏ†ïÌïòÏÑ∏Ïöî.)\n      - `add_paragraph`, `insert_paragraphs_bulk`: Î¨∏Îã® Ï∂îÍ∞Ä\n      - `add_table`, `get_table_cell_map`, `set_table_cell_text`, `replace_table_region`, `split_table_cell`: Ìëú ÏÉùÏÑ±¬∑Ìé∏Ïßë Î∞è Î≥ëÌï© Ìï¥Ï†ú\n      - `add_shape`, `add_control`: Í∞úÏ≤¥ Ï∂îÍ∞Ä\n      - `add_memo`, `attach_memo_field`, `add_memo_with_anchor`, `remove_memo`: Î©îÎ™® Í¥ÄÎ¶¨\n      - `hwpx.plan_edit`, `hwpx.preview_edit`, `hwpx.apply_edit` *(ÌîåÎûòÍ∑∏ ÌôúÏÑ± Ïãú)*: Í≤ÄÏ¶ùÎêú 3Îã®Í≥Ñ Ìé∏Ïßë ÌååÏù¥ÌîÑÎùºÏù∏\n  - **Ïä§ÌÉÄÏùºÎßÅ**\n      - `ensure_run_style`, `list_styles_and_bullets`: Ïä§ÌÉÄÏùº Î∞è Í∏ÄÎ®∏Î¶¨Ìëú Î™©Î°ù ÌôïÏù∏/ÏÉùÏÑ±\n      - `apply_style_to_text_ranges`, `apply_style_to_paragraphs`: Îã®Ïñ¥/Î¨∏Îã® Îã®ÏúÑ Ïä§ÌÉÄÏùº Ï†ÅÏö©\n  - **ÌååÏùº Í¥ÄÎ¶¨**\n      - `save`, `save_as`: Î¨∏ÏÑú Ï†ÄÏû•\n      - `fill_template`: ÌÖúÌîåÎ¶ø ÏÇ¨Î≥∏ ÏÉùÏÑ± + Îã§Ï§ë ÏπòÌôòÏùÑ 1Ìöå Ìò∏Ï∂úÎ°ú ÏàòÌñâ\n      - `make_blank`: ÏÉà Îπà Î¨∏ÏÑú ÏÉùÏÑ±\n      - `convert_hwp_to_hwpx`: HWP Î∞îÏù¥ÎÑàÎ¶¨Î•º HWPXÎ°ú Î≥ÄÌôò(Í∏∞Î≥∏ ÌÖçÏä§Ìä∏/Ìëú Ï§ëÏã¨)\n  - **Íµ¨Ï°∞ Í≤ÄÏ¶ù Î∞è Í≥†Í∏â Í≤ÄÏÉâ**\n      - `object_find_by_tag`, `object_find_by_attr`: XML ÏöîÏÜå Í≤ÄÏÉâ\n      - `validate_structure`, `lint_text_conventions`: Î¨∏ÏÑú Íµ¨Ï°∞ Í≤ÄÏ¶ù Î∞è ÌÖçÏä§Ìä∏ Î¶∞Ìä∏\n\n\\</details\\>\n\n### üéØ ÌïÑÏöîÌïú Î¨∏Îã®Îßå Îπ†Î•¥Í≤å ÏùΩÍ∏∞\n\nÎåÄÏö©Îüâ Î¨∏ÏÑúÎ•º ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÌôïÏù∏Ìï† ÎïåÎäî `read_text` ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖòÏù¥ Ìé∏Î¶¨ÌïòÏßÄÎßå, ÌäπÏ†ï Î¨∏Îã®Îßå Î∞îÎ°ú ÌôïÏù∏ÌïòÍ≥† Ïã∂ÏùÑ ÎïåÎäî `read_paragraphs` ÎèÑÍµ¨Í∞Ä Îçî Ï†ÅÌï©Ìï©ÎãàÎã§. `paragraphIndexes` Î∞∞Ïó¥Ïóê ÏõêÌïòÎäî Î¨∏Îã® Î≤àÌò∏Îßå Ï†ÑÎã¨ÌïòÎ©¥, ÏöîÏ≤≠Ìïú Î¨∏Îã®Îßå ÏàúÏÑúÎåÄÎ°ú Î∞òÌôòÌï©ÎãàÎã§. Í∞Å Ìï≠Î™©ÏóêÎäî ÏõêÎ≥∏ Î¨∏Îã® Ïù∏Îç±Ïä§(`paragraphIndex`)ÏôÄ Ï∂îÏ∂úÎêú ÌÖçÏä§Ìä∏Í∞Ä Ìï®Íªò Ìè¨Ìï®ÎêòÎØÄÎ°ú, Ïù¥Ï†Ñ Ìò∏Ï∂úÏóêÏÑú Í∏∞ÏñµÌïú Î¨∏Îã®ÏùÑ Ï†ïÌôïÌûà Îã§Ïãú Î∂àÎü¨Ïò¨ Ïàò ÏûàÏäµÎãàÎã§.\n\n```jsonc\n{\n  \"name\": \"read_paragraphs\",\n  \"arguments\": {\n    \"path\": \"sample.hwpx\",\n    \"paragraphIndexes\": [1, 4, 9],\n    \"withHighlights\": false,\n    \"withFootnotes\": false\n  }\n}\n```\n\nÏÑ†ÌÉùÎêú Î¨∏Îã®Îßå Ï≤òÎ¶¨ÌïòÎØÄÎ°ú ÌÅ∞ Î¨∏ÏÑúÎ•º Î∞òÎ≥µÌï¥ÏÑú ÌÉêÏÉâÌï† Îïå Î∂àÌïÑÏöîÌïú ÌÖçÏä§Ìä∏ Î≥µÏÇ¨Î•º Ï§ÑÏù¥Í≥†, ÌïòÏù¥ÎùºÏù¥Ìä∏/Í∞ÅÏ£º ÏòµÏÖòÎèÑ `read_text`ÏôÄ ÎèôÏùºÌïòÍ≤å ÌôúÏö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ï°¥Ïû¨ÌïòÏßÄ ÏïäÎäî Ïù∏Îç±Ïä§Î•º ÏöîÏ≤≠ÌïòÎ©¥ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌïòÎØÄÎ°ú, Ïù¥Ï†ÑÏóê Î∞õÏùÄ Î¨∏Îã® Í∞úÏàò Ï†ïÎ≥¥Î•º ÌôúÏö©Ìï¥ ÏïàÏ†ÑÌïòÍ≤å ÏöîÏ≤≠ÌïòÏÑ∏Ïöî.\n\n### üîç Í≤ÄÏÉâ Î¨∏Îß• Í∏∏Ïù¥ Ï°∞Ï†à\n\n`find` ÎèÑÍµ¨Îäî Í∞Å ÏùºÏπò Ìï≠Î™© Ï£ºÎ≥ÄÏùò Ï†ÑÌõÑ 80ÏûêÎ•º Í∏∞Î≥∏ÏúºÎ°ú ÏûòÎùº `context` Ïä§ÎãàÌé´ÏùÑ Î∞òÌôòÌïòÎ©∞, ÏûòÎ¶∞ Í≤ΩÏö∞ Î¨∏ÏûêÏó¥ ÏïûÎí§Ïóê `...`Ïù¥ Î∂ôÏäµÎãàÎã§. Îçî ÎÑìÏùÄ Î≤îÏúÑÍ∞Ä ÌïÑÏöîÌïòÎ©¥ `contextRadius` Ïù∏ÏàòÎ•º ÏÇ¨Ïö©Ìï¥ Ïú†ÏßÄÌï† Î¨∏Ïûê ÏàòÎ•º Ï°∞Ï†ïÌï† Ïàò ÏûàÏäµÎãàÎã§.",
        "start_pos": 5410,
        "end_pos": 7297,
        "token_count_estimate": 471,
        "source_type": "readme",
        "agent_id": "bc97a05ae003ab36"
      },
      {
        "chunk_id": 4,
        "text": "Ïù¥Ï†ÑÏóê Î∞õÏùÄ Î¨∏Îã® Í∞úÏàò Ï†ïÎ≥¥Î•º ÌôúÏö©Ìï¥ ÏïàÏ†ÑÌïòÍ≤å ÏöîÏ≤≠ÌïòÏÑ∏Ïöî.\n\n### üîç Í≤ÄÏÉâ Î¨∏Îß• Í∏∏Ïù¥ Ï°∞Ï†à\n\n`find` ÎèÑÍµ¨Îäî Í∞Å ÏùºÏπò Ìï≠Î™© Ï£ºÎ≥ÄÏùò Ï†ÑÌõÑ 80ÏûêÎ•º Í∏∞Î≥∏ÏúºÎ°ú ÏûòÎùº `context` Ïä§ÎãàÌé´ÏùÑ Î∞òÌôòÌïòÎ©∞, ÏûòÎ¶∞ Í≤ΩÏö∞ Î¨∏ÏûêÏó¥ ÏïûÎí§Ïóê `...`Ïù¥ Î∂ôÏäµÎãàÎã§. Îçî ÎÑìÏùÄ Î≤îÏúÑÍ∞Ä ÌïÑÏöîÌïòÎ©¥ `contextRadius` Ïù∏ÏàòÎ•º ÏÇ¨Ïö©Ìï¥ Ïú†ÏßÄÌï† Î¨∏Ïûê ÏàòÎ•º Ï°∞Ï†ïÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n```jsonc\n{\n  \"name\": \"find\",\n  \"arguments\": {\n    \"path\": \"sample.hwpx\",\n    \"query\": \"HWPX\",\n    \"contextRadius\": 200\n  }\n}\n```\n\n`contextRadius` Í∞íÏùÄ ÏùºÏπò Íµ¨Í∞Ñ ÏïûÎí§ Í∞ÅÍ∞ÅÏóê Ìè¨Ìï®Ìï† Î¨∏Ïûê ÏàòÎ•º ÏùòÎØ∏Ìï©ÎãàÎã§.\n\n### üìê Ìëú Ìé∏Ïßë Í≥†Í∏â ÏòµÏÖò\n\n`get_table_cell_map` ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÎ©¥ ÌëúÏùò Ï†ÑÏ≤¥ Í≤©ÏûêÎ•º Í∑∏ÎåÄÎ°ú ÏßÅÎ†¨ÌôîÌïòÏó¨ Í∞Å ÏúÑÏπòÍ∞Ä Ïñ¥Îäê ÏïµÏª§ ÏÖÄ(`anchor`)Ïóê ÏÜçÌïòÎäîÏßÄ, Î≥ëÌï© Î≤îÏúÑ(`rowSpan`, `colSpan`)Îäî ÏñºÎßàÏù∏ÏßÄ ÌïúÎààÏóê ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÏùëÎãµÏùÄ Ìï≠ÏÉÅ Ìñâ√óÏó¥ Ï†ÑÏ≤¥Î•º Ï±ÑÏö∞Î©∞, Í∞Å ÏúÑÏπòÏóê ÎåÄÌï¥ `row`/`column` Ï¢åÌëúÏôÄ Î≥ëÌï©Îêú ÏïµÏª§ ÏÖÄÏùò ÌÖçÏä§Ìä∏Î•º ÏïåÎ†§ Ï§çÎãàÎã§.\n\n```jsonc\n{\n  \"name\": \"get_table_cell_map\",\n  \"arguments\": {\"path\": \"sample.hwpx\", \"tableIndex\": 0},\n  \"result\": {\n    \"rowCount\": 3,\n    \"columnCount\": 3,\n    \"grid\": [\n      [\n        {\"row\": 0, \"column\": 0, \"anchor\": {\"row\": 0, \"column\": 0}, \"rowSpan\": 2, \"colSpan\": 2, \"text\": \"Ï†úÎ™©\"},\n        {\"row\": 0, \"column\": 1, \"anchor\": {\"row\": 0, \"column\": 0}, \"rowSpan\": 2, \"colSpan\": 2, \"text\": \"Ï†úÎ™©\"},\n        {\"row\": 0, \"column\": 2, \"anchor\": {\"row\": 0, \"column\": 2}, \"rowSpan\": 3, \"colSpan\": 1, \"text\": \"ÏöîÏïΩ\"}\n      ],\n      [\n        {\"row\": 1, \"column\": 0, \"anchor\": {\"row\": 0, \"column\": 0}, \"rowSpan\": 2, \"colSpan\": 2, \"text\": \"Ï†úÎ™©\"},\n        {\"row\": 1, \"column\": 1, \"anchor\": {\"row\": 0, \"column\": 0}, \"rowSpan\": 2, \"colSpan\": 2, \"text\": \"Ï†úÎ™©\"},\n        {\"row\": 1, \"column\": 2, \"anchor\": {\"row\": 0, \"column\": 2}, \"rowSpan\": 3, \"colSpan\": 1, \"text\": \"ÏöîÏïΩ\"}\n      ],\n      \"... ÏÉùÎûµ ...\"\n    ]\n  }\n}\n```\n\n`set_table_cell_text`ÏôÄ `replace_table_region`ÏùÄ ÏÑ†ÌÉùÏ†ÅÏù∏ `logical`/`splitMerged` ÌîåÎûòÍ∑∏Î•º ÏßÄÏõêÌï©ÎãàÎã§. `logical: true`Î°ú ÏßÄÏ†ïÌïòÎ©¥ Î∞©Í∏à ÌôïÏù∏Ìïú ÎÖºÎ¶¨ Ï¢åÌëúÍ≥ÑÎ•º Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÍ≥†, `splitMerged: true`Î•º Ìï®Íªò Ï†ÑÎã¨ÌïòÎ©¥ Ïì∞Í∏∞ Ï†ÑÏóê ÏûêÎèôÏúºÎ°ú Ìï¥Îãπ Î≥ëÌï© ÏòÅÏó≠ÏùÑ Î∂ÑÌï†Ìï©ÎãàÎã§. Í∏¥ ÌÖçÏä§Ìä∏Î•º Ï±ÑÏö∏ ÎïåÎäî `autoFit: true`Î•º Ï∂îÍ∞ÄÎ°ú ÏßÄÏ†ïÌïòÎ©¥ Í∞Å Ïó¥ ÎÑàÎπÑÍ∞Ä ÏÖÄ ÎÇ¥Ïö© Í∏∏Ïù¥Ïóê ÎßûÏ∂∞ Îã§Ïãú Í≥ÑÏÇ∞ÎêòÏñ¥ Ìëú Ï†ÑÏ≤¥ Ìè≠(`hp:sz`)Í≥º ÏÖÄ ÌÅ¨Í∏∞(`hp:cellSz`)Í∞Ä Ìï®Íªò ÏóÖÎç∞Ïù¥Ìä∏Îê©ÎãàÎã§. Î≥ëÌï©ÏùÑ ÏßÅÏ†ë Ìï¥Ï†úÌï¥Ïïº Ìï† ÎïåÎäî `split_table_cell` ÎèÑÍµ¨Í∞Ä ÏõêÎûò Î≤îÏúÑÎ•º ÏïåÎ†§Ï£ºÎ©¥ÏÑú ÏÖÄÏùÑ Î∂ÑÌï†Ìï©ÎãàÎã§.",
        "start_pos": 7097,
        "end_pos": 8970,
        "token_count_estimate": 468,
        "source_type": "readme",
        "agent_id": "bc97a05ae003ab36"
      },
      {
        "chunk_id": 5,
        "text": "ÏûêÎèôÏúºÎ°ú Ìï¥Îãπ Î≥ëÌï© ÏòÅÏó≠ÏùÑ Î∂ÑÌï†Ìï©ÎãàÎã§. Í∏¥ ÌÖçÏä§Ìä∏Î•º Ï±ÑÏö∏ ÎïåÎäî `autoFit: true`Î•º Ï∂îÍ∞ÄÎ°ú ÏßÄÏ†ïÌïòÎ©¥ Í∞Å Ïó¥ ÎÑàÎπÑÍ∞Ä ÏÖÄ ÎÇ¥Ïö© Í∏∏Ïù¥Ïóê ÎßûÏ∂∞ Îã§Ïãú Í≥ÑÏÇ∞ÎêòÏñ¥ Ìëú Ï†ÑÏ≤¥ Ìè≠(`hp:sz`)Í≥º ÏÖÄ ÌÅ¨Í∏∞(`hp:cellSz`)Í∞Ä Ìï®Íªò ÏóÖÎç∞Ïù¥Ìä∏Îê©ÎãàÎã§. Î≥ëÌï©ÏùÑ ÏßÅÏ†ë Ìï¥Ï†úÌï¥Ïïº Ìï† ÎïåÎäî `split_table_cell` ÎèÑÍµ¨Í∞Ä ÏõêÎûò Î≤îÏúÑÎ•º ÏïåÎ†§Ï£ºÎ©¥ÏÑú ÏÖÄÏùÑ Î∂ÑÌï†Ìï©ÎãàÎã§.\n\n```jsonc\n{\n  \"name\": \"set_table_cell_text\",\n  \"arguments\": {\n    \"path\": \"sample.hwpx\",\n    \"tableIndex\": 0,\n    \"row\": 1,\n    \"col\": 1,\n    \"text\": \"ÎÖºÎ¶¨ Ï¢åÌëú Ìé∏Ïßë\",\n    \"logical\": true,\n    \"splitMerged\": true,\n    \"autoFit\": true,\n    \"dryRun\": false\n  }\n}\n```\n\nÏúÑ ÏòàÏãúÎäî 2√ó2Î°ú Î≥ëÌï©Îêú ÏÖÄÏóê ÎÖºÎ¶¨ Ï¢åÌëú `(1, 1)`ÏùÑ ÏßÄÏ†ïÌïòÏó¨ ÏûêÎèô Î∂ÑÌï† ÌõÑ ÌÖçÏä§Ìä∏Î•º Í∏∞Î°ùÌï©ÎãàÎã§. Î∂ÑÌï† Ïó¨Î∂ÄÏôÄ ÏõêÎûò Î≤îÏúÑÎ•º ÌôïÏù∏ÌïòÎ†§Î©¥ `split_table_cell`ÏùÑ Ìò∏Ï∂úÌïòÏÑ∏Ïöî.\n\n```jsonc\n{\n  \"name\": \"split_table_cell\",\n  \"arguments\": {\"path\": \"sample.hwpx\", \"tableIndex\": 0, \"row\": 0, \"col\": 0},\n  \"result\": {\"startRow\": 0, \"startCol\": 0, \"rowSpan\": 2, \"colSpan\": 2}\n}\n```\n\nÏùëÎãµÏùò `rowSpan`/`colSpan` Í∞íÏùÄ Î∂ÑÌï†ÎêòÍ∏∞ Ï†Ñ Î≥ëÌï© Î≤îÏúÑÎ•º ÏïåÎ†§Ï£ºÎØÄÎ°ú, ÌîÑÎü∞Ìä∏ÏóîÎìú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä UI ÏÉÅÌÉúÎ•º Ï¶âÏãú Í∞±Ïã†Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## ‚ò¢Ô∏è Í≥†Í∏â Í∏∞Îä•: OPC Ìå®ÌÇ§ÏßÄ ÎÇ¥Î∂Ä ÏÇ¥Ìé¥Î≥¥Í∏∞\n\n> **‚ö†Ô∏è Í≤ΩÍ≥†:** ÏïÑÎûò ÎèÑÍµ¨Îì§ÏùÄ HWPX Î¨∏ÏÑúÏùò ÎÇ¥Î∂Ä OPC ÌååÌä∏Î•º Í∑∏ÎåÄÎ°ú ÎÖ∏Ï∂úÌï©ÎãàÎã§. Íµ¨Ï°∞Î•º ÏûòÎ™ª Ìï¥ÏÑùÌïòÎ©¥ Î¨∏ÏÑúÎ•º Ïò§Ìï¥Ìï† Ïàò ÏûàÏúºÎãà, Ïä§ÌÇ§ÎßàÏôÄ Í¥ÄÍ≥ÑÎ•º Ï∂©Î∂ÑÌûà Ïù¥Ìï¥Ìïú ÏÉÅÌÉúÏóêÏÑú ÌôúÏö©ÌïòÏÑ∏Ïöî. ÌòÑÏû¨ MCP ÏÑúÎ≤ÑÎäî ÏùòÎèÑÏπò ÏïäÏùÄ ÏÜêÏÉÅÏùÑ ÎßâÍ∏∞ ÏúÑÌï¥ **ÏùΩÍ∏∞ Ï†ÑÏö© ÎèÑÍµ¨Îßå** Ï†úÍ≥µÌï©ÎãàÎã§.\n\n  * `package_parts`: Ìå®ÌÇ§ÏßÄÏóê Ìè¨Ìï®Îêú Î™®Îì† OPC ÌååÌä∏Ïùò Í≤ΩÎ°ú Î™©Î°ùÏùÑ ÌôïÏù∏Ìï©ÎãàÎã§.\n  * `package_get_text`: ÏßÄÏ†ïÌïú ÌååÌä∏Î•º ÌÖçÏä§Ìä∏Î°ú ÏùΩÏñ¥ÏòµÎãàÎã§ (Ïù∏ÏΩîÎî© ÏßÄÏ†ï Í∞ÄÎä•).\n  * `package_get_xml`: ÏßÄÏ†ïÌïú ÌååÌä∏Î•º XML Î¨∏ÏûêÏó¥Î°ú Î∞òÌôòÌï©ÎãàÎã§.\n\n#### ÏãúÎÇòÎ¶¨Ïò§ ÏòàÏãú\n\nÏä§ÌÉÄÏùº Ï†ïÏùò XML ÌååÏùº(`Styles.xml`)Ïùò ÎÇ¥Ïö©ÏùÑ ÌôïÏù∏ÌïòÍ≥† Ïã∂Îã§Î©¥:\n\n1.  `package_parts` ÎèÑÍµ¨Ïóê `{\"path\": \"sample.hwpx\"}`Î•º Ï†ÑÎã¨ÌïòÏó¨ `Contents/Styles.xml`Í≥º Í∞ôÏùÄ ÌååÌä∏ Ïù¥Î¶ÑÏùÑ Ï∞æÏäµÎãàÎã§.\n2.  `package_get_xml` ÎèÑÍµ¨Ïóê `{\"path\": \"sample.hwpx\", \"partName\": \"Contents/Styles.xml\"}`ÏùÑ Ï†ÑÎã¨ÌïòÏó¨ Ìï¥Îãπ ÌååÌä∏Ïùò ÏõêÎ≥∏ XMLÏùÑ ÏïàÏ†ÑÌïòÍ≤å Í≤ÄÌÜ†Ìï©ÎãàÎã§.\n\n\n## üîÅ HWP ‚Üí HWPX ÏûêÎèô Î≥ÄÌôò\n\n`convert_hwp_to_hwpx` ÎèÑÍµ¨Îäî ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú `hwp5proc xml` Í≤∞Í≥ºÎ•º Îß§ÌïëÌï¥ `.hwp` Î¨∏ÏÑúÎ•º `.hwpx`Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.\n\n- ÏûÖÎ†•: `source`(ÌïÑÏàò, `.hwp` Í≤ΩÎ°ú), `output`(ÏÑ†ÌÉù, ÎØ∏ÏßÄÏ†ï Ïãú Í∞ôÏùÄ Í≤ΩÎ°úÏóê `.hwpx`)\n- Ï∂úÎ†•: Î≥ÄÌôò ÏÑ±Í≥µ Ïó¨Î∂Ä, Î≥ÄÌôòÎêú Î¨∏Îã®/Ìëú Í∞úÏàò, Î≥ÄÌôò Ï†úÏô∏ ÏöîÏÜå Î™©Î°ù, Í≤ΩÍ≥† Î©îÏãúÏßÄ\n\nÏòàÏãú:\n\n```json\n{\n  \"name\": \"convert_hwp_to_hwpx\",\n  \"arguments\": {\n    \"source\": \"legacy/report.hwp\",\n    \"output\": \"legacy/report.hwpx\"\n  }\n}\n```\n\n### ÏßÄÏõê Î≤îÏúÑ\n\n- **P0**: ÏùºÎ∞ò Î¨∏Îã® ÌÖçÏä§Ìä∏\n- **P1(Î∂ÄÎ∂Ñ ÏßÄÏõê)**: ÌëúÏùò Ìñâ/Ïó¥Í≥º ÏÖÄ ÌÖçÏä§Ìä∏\n- **P2/P3**: OLE, Í∞ÅÏ£º/ÎØ∏Ï£º, Î≥ÄÍ≤Ω Ï∂îÏ†Å, ÏñëÏãù Ïª®Ìä∏Î°§ Îì±ÏùÄ Í≤ΩÍ≥†ÏôÄ Ìï®Íªò Ïä§ÌÇµÎê† Ïàò ÏûàÏùå\n\n### ÏïåÎ†§ÏßÑ Ï†úÌïúÏÇ¨Ìï≠\n\n- Î≥ÄÌôò Î™©ÌëúÎäî 100% ÏãúÍ∞Å Ïû¨ÌòÑÏù¥ ÏïÑÎãàÎùº **ÌÖçÏä§Ìä∏ Î≥¥Ï°¥ + Í∏∞Î≥∏ Íµ¨Ï°∞ Ïù¥Í¥Ä**ÏûÖÎãàÎã§.",
        "start_pos": 8770,
        "end_pos": 10793,
        "token_count_estimate": 505,
        "source_type": "readme",
        "agent_id": "bc97a05ae003ab36"
      },
      {
        "chunk_id": 6,
        "text": "px\"\n  }\n}\n```\n\n### ÏßÄÏõê Î≤îÏúÑ\n\n- **P0**: ÏùºÎ∞ò Î¨∏Îã® ÌÖçÏä§Ìä∏\n- **P1(Î∂ÄÎ∂Ñ ÏßÄÏõê)**: ÌëúÏùò Ìñâ/Ïó¥Í≥º ÏÖÄ ÌÖçÏä§Ìä∏\n- **P2/P3**: OLE, Í∞ÅÏ£º/ÎØ∏Ï£º, Î≥ÄÍ≤Ω Ï∂îÏ†Å, ÏñëÏãù Ïª®Ìä∏Î°§ Îì±ÏùÄ Í≤ΩÍ≥†ÏôÄ Ìï®Íªò Ïä§ÌÇµÎê† Ïàò ÏûàÏùå\n\n### ÏïåÎ†§ÏßÑ Ï†úÌïúÏÇ¨Ìï≠\n\n- Î≥ÄÌôò Î™©ÌëúÎäî 100% ÏãúÍ∞Å Ïû¨ÌòÑÏù¥ ÏïÑÎãàÎùº **ÌÖçÏä§Ìä∏ Î≥¥Ï°¥ + Í∏∞Î≥∏ Íµ¨Ï°∞ Ïù¥Í¥Ä**ÏûÖÎãàÎã§.\n- Î≥µÏû°Ìïú ÏÑúÏãù(ÏÑ∏Î∞ÄÌïú Ïä§ÌÉÄÏùº, Í≥†Í∏â Í∞úÏ≤¥, ÏùºÎ∂Ä Î≥ëÌï© Ìëú)ÏùÄ Í≤∞Í≥º Î¨∏ÏÑúÏóêÏÑú ÏàòÎèô Î≥¥Ï†ïÏù¥ ÌïÑÏöîÌï† Ïàò ÏûàÏäµÎãàÎã§.\n- `hwp5proc` Ïã§Ìñâ ÌôòÍ≤ΩÏù¥ ÏóÜÏúºÎ©¥ Î≥ÄÌôò ÎèÑÍµ¨Îäî Ïã§Ìå®ÌïòÎ©∞ ÏÑ§Ïπò ÏïàÎÇ¥ Ïò§Î•òÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n\n## üß™ ÌÖåÏä§Ìä∏\n\nÌïµÏã¨ Í∏∞Îä•Î∂ÄÌÑ∞ Î™®Îì† MCP ÎèÑÍµ¨Ïùò Ïã§Ï†ú Ìò∏Ï∂úÍπåÏßÄ Í≤ÄÏ¶ùÌïòÎäî ÏóîÎìúÌà¨ÏóîÎìú ÌÖåÏä§Ìä∏ Ïä§ÏúÑÌä∏Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n\n```bash\n# 1. ÌÖåÏä§Ìä∏ ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\npython -m pip install -e .[test]\n\n# 2. ÌÖåÏä§Ìä∏ Ïã§Ìñâ\npython -m pytest\n```\n\n`tests/test_mcp_end_to_end.py`Îäî ÏÑúÎ≤ÑÍ∞Ä ÎÖ∏Ï∂úÌïòÎäî ÎåÄÎ∂ÄÎ∂ÑÏùò ÎèÑÍµ¨Î•º Ïã§Ï†úÎ°ú Ìò∏Ï∂úÌïòÏó¨ ÌÖçÏä§Ìä∏, Ìëú, Î©îÎ™® Ìé∏Ïßë, OPC Ìå®ÌÇ§ÏßÄ ÏùΩÍ∏∞, ÏûêÎèô Î∞±ÏóÖ ÏÉùÏÑ± Îì± ÌïµÏã¨ ÎèôÏûëÏùÑ ÏôÑÎ≤ΩÌïòÍ≤å Í≤ÄÏ¶ùÌï©ÎãàÎã§.\n\n## üßë‚Äçüíª Í∞úÎ∞ú Ï∞∏Í≥†\n\n  * Ïù¥ ÏÑúÎ≤ÑÎäî `python-hwpx>=1.9`, `mcp`, `anyio`, `pydantic` Îì± ÏàúÏàò ÌååÏù¥Ïç¨ ÎùºÏù¥Î∏åÎü¨Î¶¨Î°úÎßå Íµ¨ÏÑ±Îê©ÎãàÎã§.\n  * Î™®Îì† ÎèÑÍµ¨ Ìï∏Îì§Îü¨Îäî `HwpxOps`Ïùò Í≤ΩÎ°ú Ìó¨ÌçºÏôÄ `HwpxDocument` APIÎ•º ÌÜµÌï¥ Î¨∏ÏÑúÎ•º ÏïàÏ†ÑÌïòÍ≤å Ï°∞ÏûëÌï©ÎãàÎã§.\n  * ÌååÍ¥¥Ï†Å ÏûëÏóÖ(ÏàòÏ†ï/Ï†ÄÏû•)ÏóêÎäî `dryRun` ÌîåÎûòÍ∑∏Î•º Ïö∞ÏÑ† Ï†úÍ≥µÌïòÎ©∞, ÏûêÎèô Î∞±ÏóÖ ÏòµÏÖòÏù¥ ÌôúÏÑ±ÌôîÎêòÏñ¥ ÏûàÏúºÎ©¥ `.bak` ÌååÏùºÏùÑ ÏÉùÏÑ±ÌïòÏó¨ ÏïàÏ†ïÏÑ±ÏùÑ ÎÜíÏûÖÎãàÎã§.\n  * JSON Ïä§ÌÇ§ÎßàÎäî ÎÇ¥Î∂Ä `schema.builder` Í≤ΩÎ°úÎ•º ÌÜµÌï¥ draft-07 Ìò∏Ìôò SanitizerÎ•º Í±∞Ïπú ÌõÑ ÎÖ∏Ï∂úÎêòÎØÄÎ°ú `$ref`/`anyOf`Í∞Ä Ï†úÍ±∞Îêú ÌèâÌÉÑÌïú Íµ¨Ï°∞Î•º Í∏∞ÎåÄÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n### üîí ÏÑúÎ≤Ñ ÌïòÎìúÎãù & JSON Ïä§ÌÇ§Îßà (draft-07) ‚Äî ÏÑ†ÌÉù ÏÇ¨Ïö©\n\n- `HWPX_MCP_HARDENING=1`ÏùÑ ÏÑ§Ï†ïÌïòÎ©¥ plan/preview/apply ÌååÏù¥ÌîÑÎùºÏù∏, `hwpx.search`, `hwpx.get_context`Í∞Ä ÌôúÏÑ±ÌôîÎê©ÎãàÎã§.\n- ÌîåÎûòÍ∑∏Î•º ÎÅÑÎ©¥ (`0` ÎòêÎäî ÎØ∏ÏÑ§Ï†ï) Í∏∞Ï°¥ ÎèÑÍµ¨Îßå Ïú†ÏßÄÌïòÎ©¥ÏÑúÎèÑ Í∞ïÌôîÎêú Ïä§ÌÇ§Îßà SanitizerÎäî Í≥ÑÏÜç Ï†ÅÏö©Îê©ÎãàÎã§.\n- `pytest -q`Î•º Ïã§ÌñâÌïòÎ©¥ Ïä§ÌÇ§Îßà ÌöåÍ∑Ä, ÌååÏù¥ÌîÑÎùºÏù∏ Í≤åÏù¥Ìä∏, Î©±Îì±ÏÑ± Í≤ÄÏ¶ù ÌÖåÏä§Ìä∏Í∞Ä Ìï®Íªò ÏàòÌñâÎêòÏñ¥ Î∞∞Ìè¨ Ï†Ñ ÏïàÏ†ÑÏÑ±ÏùÑ ÌôïÏù∏Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## üìú ÎùºÏù¥ÏÑ†Ïä§\n\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî [MIT ÎùºÏù¥ÏÑ†Ïä§](https://www.google.com/search?q=LICENSE)Î°ú Î∞∞Ìè¨Îê©ÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ ÎùºÏù¥ÏÑ†Ïä§ ÌååÏùºÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî.\n\n## Ïù¥Î©îÏùº\n\nÍ¥ëÍµêÍ≥†Îì±ÌïôÍµê ÍµêÏÇ¨ Í≥†Í∑úÌòÑ : kokyuhyun@hotmail.com\n\n\n## üß© ÏñëÏãù(ÌÖúÌîåÎ¶ø) Î¨∏ÏÑú ÏûëÏóÖ\n\n### 1) Íµ¨Ï°∞ ÌååÏïÖ: `analyze_template_structure`\n\nÏñëÏãù Î¨∏ÏÑúÎ•º Ïó¥ÏûêÎßàÏûê ÏàòÏ†ï Í∞ÄÎä•/Î∂àÍ∞Ä ÏòÅÏó≠Í≥º ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî ÌõÑÎ≥¥Î•º ÌååÏïÖÌïòÎ†§Î©¥ ÏïÑÎûò ÎèÑÍµ¨Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n\n```json\n{\n  \"name\": \"analyze_template_structure\",\n  \"arguments\": {\n    \"path\": \"sample.hwpx\",\n    \"placeholderPatterns\": [\"\\\\{\\\\{[^{}]+\\\\}\\\\}\", \"Î≥∏Î¨∏ ÏòÅÏó≠\"],\n    \"lockKeywords\": [\"ÌïôÍµêÏû•\", \"ÏßÅÏù∏\", \"Î°úÍ≥†\"]\n  }\n}\n```\n\nÏùëÎãµÏóêÎäî `summary`(Î¨∏Îã® Ïàò/ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî Ïàò), `regions`(header/body/footer), `placeholders`(ÌÜ†ÌÅ∞/Î¨∏Îã® Ïù∏Îç±Ïä§/ÏàòÏ†ï Í∞ÄÎä• Ïó¨Î∂Ä)Í∞Ä Ìè¨Ìï®Îê©ÎãàÎã§.\n\n### 2) Ìïú Î≤àÏóê Ï±ÑÏö∞Í∏∞: `fill_template`\n\nÍ∏∞Ï°¥Ïùò `save_as -> find -> replace...` Îã§Îã®Í≥Ñ ÎåÄÏã†, `fill_template` ÌïòÎÇòÎ°ú ÌÖúÌîåÎ¶ø Î≥µÏÇ¨ÏôÄ Îã§Ï§ë ÏπòÌôòÏùÑ Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.",
        "start_pos": 10593,
        "end_pos": 12561,
        "token_count_estimate": 492,
        "source_type": "readme",
        "agent_id": "bc97a05ae003ab36"
      },
      {
        "chunk_id": 7,
        "text": "`regions`(header/body/footer), `placeholders`(ÌÜ†ÌÅ∞/Î¨∏Îã® Ïù∏Îç±Ïä§/ÏàòÏ†ï Í∞ÄÎä• Ïó¨Î∂Ä)Í∞Ä Ìè¨Ìï®Îê©ÎãàÎã§.\n\n### 2) Ìïú Î≤àÏóê Ï±ÑÏö∞Í∏∞: `fill_template`\n\nÍ∏∞Ï°¥Ïùò `save_as -> find -> replace...` Îã§Îã®Í≥Ñ ÎåÄÏã†, `fill_template` ÌïòÎÇòÎ°ú ÌÖúÌîåÎ¶ø Î≥µÏÇ¨ÏôÄ Îã§Ï§ë ÏπòÌôòÏùÑ Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n\n```json\n{\n  \"name\": \"fill_template\",\n  \"arguments\": {\n    \"source\": \"forms/notice_template.hwpx\",\n    \"output\": \"out/notice_2026.hwpx\",\n    \"replacements\": {\n      \"Î≥∏Î¨∏ ÏòÅÏó≠\": \"Ïã§Ï†ú ÏïàÎÇ¥Î¨∏ Î≥∏Î¨∏\",\n      \"Ï†ú2025ÎÖÑ\": \"Ï†ú2026ÎÖÑ\",\n      \"2025. 1. 1.\": \"2026. 3. 5.\"\n    }\n  }\n}\n```",
        "start_pos": 12361,
        "end_pos": 12826,
        "token_count_estimate": 115,
        "source_type": "readme",
        "agent_id": "bc97a05ae003ab36"
      }
    ]
  },
  {
    "agent_id": "599fc4739751c0fb",
    "name": "ai.smithery/akilat-spec-leave-manager-mcp",
    "source": "mcp",
    "source_url": "https://github.com/akilat-spec/leave-manager-mcp",
    "description": "Track and manage employee time off with quick balance lookups and streamlined applications. Find t‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-06T10:13:37.506388Z",
    "indexed_at": "2026-02-18T04:04:49.915940",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": ""
    },
    "llm_extracted": {
      "capabilities": [
        "Track employee time off",
        "Manage employee leave requests",
        "Provide quick balance lookups for time off",
        "Streamline leave application processes"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of core functionalities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "a8f8cb4a82772b21",
    "name": "ai.smithery/alex-llm-attack-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/alex-llm/attAck-mcp-server",
    "description": "Query and retrieve information about various adversarial tactics and techniques used in cyber atta‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T17:11:31.060313Z",
    "indexed_at": "2026-02-18T04:04:52.009134",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# attAck-mcp-server\n\nThis project is an MCP (Model Context Protocol) server for querying ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) techniques and tactics. It provides a way to access and retrieve information about various attack techniques and tactics used by adversaries.\n\n## Tools\n\nThe server provides the following tools:\n\n*   **query\\_technique:**  This tool allows you to query ATT&CK techniques by ID or name.\n    *   **Arguments:**\n        *   `technique_id` (string, optional): The ID of the technique to query.\n        *   `tech_name` (string, optional): The name (or partial name) of the technique to query. ÊîØÊåÅÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢„ÄÇ\n    *   **Example:**\n        - ÊåâIDÊü•ËØ¢Ôºö\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n        - ÊåâÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢Ôºö\n        ```json\n        {\n          \"tech_name\": \"phishing\"\n        }\n        ```\n*   **search\\_technique\\_full:**  ÈÄöËøáÊäÄÊúØ ID ÊàñÂêçÁß∞Êü•ËØ¢ÊîªÂáªÊäÄÊúØÁöÑÊâÄÊúâËØ¶ÁªÜ‰ø°ÊÅØÔºåËøîÂõûÁöÑÊï∞ÊçÆÂåÖÂê´ ID„ÄÅÂêçÁß∞„ÄÅÊèèËø∞„ÄÅÈÄÇÁî®Âπ≥Âè∞„ÄÅKill Chain Èò∂ÊÆµ„ÄÅÂèÇËÄÉËµÑÊñô„ÄÅÂ≠êÊäÄÊúØÂèäÁºìËß£Êé™ÊñΩ„ÄÇÂêçÁß∞ÊêúÁ¥¢ËøîÂõûÊ†ºÂºè‰∏∫ `{ \"results\": [...], \"count\": N }` ÁöÑÂ≠óÂÖ∏ÔºåÂÖ∂‰∏≠ `results` ‰∏∫ÂåπÈÖçÊäÄÊúØÂÆåÊï¥Êï∞ÊçÆÂàóË°®„ÄÇ\n    *   **Arguments:**\n        *   `technique_id` (string, optional): Ë¶ÅÊü•ËØ¢ÁöÑÊäÄÊúØID„ÄÇ\n        *   `tech_name` (string, optional): ÊäÄÊúØÂêçÁß∞ÂÖ≥ÈîÆÂ≠óÔºåÊîØÊåÅÊ®°Á≥äÂåπÈÖç„ÄÇ\n    *   **Example:**\n        - ÊåâIDÊü•ËØ¢Ôºö\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n        - ÊåâÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢Ôºö\n        ```json\n        {\n          \"tech_name\": \"phishing\"\n        }\n        ```\n*   **query\\_mitigations:** Êü•ËØ¢ÊäÄÊúØÁöÑÁºìËß£Êé™ÊñΩ\n    *   **Arguments:**\n        *   `technique_id` (string, required): Ë¶ÅÊü•ËØ¢ÁöÑÊäÄÊúØID\n    *   **Example:**\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n*   **query\\_detections:** Êü•ËØ¢ÊäÄÊúØÁöÑÊ£ÄÊµãÊñπÊ≥ï\n    *   **Arguments:**\n        *   `technique_id` (string, required): Ë¶ÅÊü•ËØ¢ÁöÑÊäÄÊúØID\n    *   **Example:**\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n*   **list\\_tactics:** This tool allows you to retrieve a list of all ATT&CK tactics.\n    *   **Arguments:** None\n*   **server_info:** ËøîÂõûÊúçÂä°‰∏éÊï∞ÊçÆÈõÜÁöÑÁâàÊú¨„ÄÅÁª¥Êä§ËÄÖÂíåGit‰ø°ÊÅØ„ÄÇ\n    *   **Arguments:** None\n    *   **Example:**\n        ```json\n        {}\n        ```\n\n## Usage\n\nTo use this MCP server, you need to have an MCP client configured to connect to it. Once connected, you can use the provided tools to query ATT&CK techniques and tactics.\n\n## MCP Client ÈÖçÁΩÆËØ¥Êòé\n\n### 1. Êú¨Âú∞ stdio ÊñπÂºèÔºàÊé®Ëçê Smithery/Êú¨Âú∞ÈõÜÊàêÔºâ\n\n- Áõ¥Êé•ËøêË°åÔºö\n  ```bash\n  python main.py\n  ```\n- Á®ãÂ∫è‰ºöËá™Âä®ÈÄâÊã© stdio Ê®°ÂºèÔºàÈªòËÆ§Êàñ `ATTACK_MCP_MODE=stdio`ÔºâÔºåÈÄÇÁî®‰∫é Smithery„ÄÅCursor Á≠âÊîØÊåÅÊú¨Âú∞ MCP stdio ÁöÑÂÆ¢Êà∑Á´Ø„ÄÇ\n- MCP ÂÆ¢Êà∑Á´ØÈÖçÁΩÆÊúçÂä°Á±ªÂûã‰∏∫\"local/stdio\"ÔºåÊó†ÈúÄÊåáÂÆöÁ´ØÂè£„ÄÇ\n- ÈÄÇÁî®Âú∫ÊôØÔºöSmithery Ëá™Âä®Âåñ„ÄÅCI/CD„ÄÅÊú¨Âú∞ AI Agent ÈõÜÊàê„ÄÇ\n\n### 2. HTTP/Streamable ÊñπÂºèÔºàËøúÁ®ã/ÂºÄÂèë/Ë∞ÉËØïÔºâ\n\n- ‰ΩøÁî® CLI ÂèÇÊï∞ÂàáÊç¢Ê®°ÂºèÔºö\n  ```bash\n  python main.py --mode http --host 0.0.0.0 --port 8081 --log-level info\n  ```\n- ÊàñÈÄöËøáÁéØÂ¢ÉÂèòÈáèÊéßÂà∂Ôºö\n  ```bash\n  export ATTACK_MCP_MODE=http\n  export ATTACK_MCP_HOST=0.0.0.0   # ÂèØÈÄâÔºåÈªòËÆ§ 0.0.0.0 Êàñ $HOST\n  export ATTACK_MCP_PORT=8081      # ÂèØÈÄâÔºåÈªòËÆ§ 8081 Êàñ $PORT\n  export ATTACK_MCP_LOG_LEVEL=info # ÂèØÈÄâÔºåÈªòËÆ§ info\n  python main.py\n  ```\n- ËøêË°åÂêéÊúçÂä°‰ª• streamable HTTP ÊñπÂºèÊö¥Èú≤ÔºåÂèØÂú®ÂÆ¢Êà∑Á´ØÈÖçÁΩÆÊúçÂä°Á±ªÂûã‰∏∫ \"http\"ÔºåÂú∞ÂùÄÂ¶Ç `http://127.0.0.1:8081/mcp`„ÄÇ\n- ËøúÁ®ãÈÉ®ÁΩ≤ÔºàÂ¶Ç Smithery CloudÔºâÈÄöÂ∏∏‰ºöÊèê‰æõ `PORT` Êàñ `MCP_TRANSPORT` ÁéØÂ¢ÉÂèòÈáèÔºåÂèØÁõ¥Êé•ËøêË°å `python main.py` Âç≥‰ΩøÁî® HTTP„ÄÇÂØπ‰∫éÂÄº‰∏∫ `streaming`„ÄÅ`streamable`„ÄÅ`streamable-http`„ÄÅ`streamable HTTP transport` Êàñ `stdioNotSupported` Á≠âÊñ∞Êûö‰∏æÁöÑËøêË°åÁéØÂ¢ÉÔºåÁ®ãÂ∫è‰ºöËá™Âä®ÂõûÈÄÄÂà∞ HTTP Ê®°ÂºèÔºåÊó†ÈúÄÈ¢ùÂ§ñÈÖçÁΩÆ„ÄÇ\n- Smithery Á≠âÂÆπÂô®Âπ≥Âè∞‰ºöÈÄöËøá `PORT`ÔºàÈªòËÆ§‰∏∫ 8081ÔºâÂëäÁü•ÁõëÂê¨Á´ØÂè£ÔºõÁ®ãÂ∫è‰ºöËá™Âä®ËØªÂèñËØ•ÂÄºÂπ∂ÁõëÂê¨Âú® `0.0.0.0:$PORT`„ÄÇ\n\n- **Â∑•ÂÖ∑ÂêçÁß∞**Ôºö`query_technique`„ÄÅ`search_technique_full`„ÄÅ`query_mitigations`„ÄÅ`query_detections`„ÄÅ`list_tactics`„ÄÅ`server_info`\n- **ÂèÇÊï∞Á§∫‰æã**Ôºö\n  - ÊåâIDÊü•ËØ¢ÊäÄÊúØÔºö\n    ```json\n    {\n      \"technique_id\": \"T1059.001\"\n    }\n    ```\n  - ÊåâÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢ÊäÄÊúØÔºö\n    ```json\n    {\n      \"tech_name\": \"phishing\"\n    }\n    ```\n  - ‰ΩøÁî® `search_technique_full` Ëé∑ÂèñÊäÄÊúØÁöÑÂÆåÊï¥ËØ¶ÁªÜ‰ø°ÊÅØÔºö\n    ```json\n    {\n      \"tech_name\": \"phishing\"\n    }\n    ```\n  - Êü•ËØ¢ÊäÄÊúØÁºìËß£Êé™ÊñΩÔºö\n    ```json\n    {\n      \"technique_id\": \"T1059.001\"\n    }\n    ```\n  - Êü•ËØ¢ÊäÄÊúØÊ£ÄÊµãÊñπÊ≥ïÔºö\n    ```json\n    {\n      \"technique_id\": \"T1059.001\"\n    }\n    ```\n  - Êü•ËØ¢ÊàòÊúØÂàóË°®Ôºö\n    ```json\n    {}\n    ```\n  - Êü•ËØ¢ÊúçÂä°‰∏éÊï∞ÊçÆÈõÜ‰ø°ÊÅØÔºö\n    ```json\n    {}\n    ```\n\n> ÂÖ∑‰ΩìÁöÑÂÆ¢Êà∑Á´ØÈÖçÁΩÆÊñπÂºèËØ∑ÂèÇËÄÉÊÇ®ÁöÑ MCP ÂÆ¢Êà∑Á´ØÊñáÊ°£ÔºåÂ∞Ü‰∏äËø∞ÊúçÂä°Âú∞ÂùÄÂíåÂ∑•ÂÖ∑ÂêçÁß∞Â°´ÂÖ•ÂØπÂ∫î‰ΩçÁΩÆÂç≥ÂèØ„ÄÇ\n\n## Installation\n\n1.  Clone this repository.\n2.  Install the required dependencies using `pip install -r requirements.txt`.\n3.  Configure the MCP server in your MCP client.\n\n## ATT&CK\n\nATT&CK is a curated knowledge base and model for cyber adversary behavior, reflecting the various phases of an adversary's attack lifecycle and the platforms they are known to target. ATT&CK is useful for understanding security risks against any specific technology or organization.\n\n## Âø´ÈÄüÂêØÂä®\n\n### ÊñπÂºè‰∏ÄÔºöÁõ¥Êé•Áî® Python ËÑöÊú¨ËøêË°åÔºàÂºÄÂèë/Ë∞ÉËØïÊé®ËçêÔºâ\n\n1. ÂÆâË£Ö‰æùËµñÔºàÂª∫ËÆÆÂú®ËôöÊãüÁéØÂ¢É‰∏≠ÔºâÔºö\n   ```bash\n   pip install -r requirements.txt\n   ```\n2. Á°Æ‰øù enterprise-attack.json Êï∞ÊçÆÈõÜÂú®È°πÁõÆÊ†πÁõÆÂΩï„ÄÇ\n3. ÂêØÂä®ÊúçÂä°ÔºàÈªòËÆ§ stdio Ê®°ÂºèÔºåÈÄÇÁî®‰∫éÊú¨Âú∞ÂÆ¢Êà∑Á´ØÈõÜÊàêÔºâÔºö\n   ```bash\n   python main.py\n   ```\n4. Â¶ÇÊûúÈúÄË¶Å‰ª• HTTP ÊñπÂºèÊèê‰æõÊúçÂä°ÔºåËØ∑ÊòæÂºèÈÄâÊã©Ê®°ÂºèÔºö\n   ```bash\n   python main.py --mode http --host 127.0.0.1 --port 8081\n   ```\n\n### ÊñπÂºè‰∫åÔºöÁîü‰∫ßÁéØÂ¢ÉÊé®ËçêÔºàDocker ÈÉ®ÁΩ≤Ôºâ\n\n#### Docker\n1. ÊûÑÂª∫ÈïúÂÉèÔºö\n   ```bash\n   docker build -t attack-mcp-server .\n   ```\n2. ËøêË°åÂÆπÂô®Ôºö\n   ```bash\n   docker run -p 8081:8081 attack-mcp-server\n   ```\n\n---\n\n## API ËØ¥Êòé\n- /query_technique ÈÄöËøáIDÊàñÂêçÁß∞Êü•ËØ¢ÊîªÂáªÊäÄÊúØËØ¶ÊÉÖÔºàÊîØÊåÅÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢Ôºâ\n- /search_technique_full ÈÄöËøáIDÊàñÂêçÁß∞Êü•ËØ¢ÊîªÂáªÊäÄÊúØÁöÑÂÆåÊï¥ËØ¶ÁªÜ‰ø°ÊÅØÔºàÂêçÁß∞ÊêúÁ¥¢ËøîÂõûÂåπÈÖçÊäÄÊúØÂàóË°®ÔºåÂåÖÂê´Â≠êÊäÄÊúØ‰∏éÁºìËß£Êé™ÊñΩÔºâ\n- /query_mitigations Êü•ËØ¢ÊåáÂÆöÊäÄÊúØÁöÑÁºìËß£Êé™ÊñΩ\n- /query_detections Êü•ËØ¢ÊåáÂÆöÊäÄÊúØÁöÑÊ£ÄÊµãÊñπÊ≥ï\n- /list_tactics Ëé∑ÂèñÊâÄÊúâATT&CKÊàòÊúØÂàÜÁ±ª\n- /server_info ËøîÂõûÊúçÂä°ÁâàÊú¨„ÄÅÊï∞ÊçÆÈõÜÁâàÊú¨ÂíåGit‰ø°ÊÅØ\n\n---\n\nÂ¶ÇÊúâÈóÆÈ¢òËØ∑ËÅîÁ≥ªÁª¥Êä§ËÄÖ„ÄÇ\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Query ATT&CK techniques by ID or name with fuzzy search support",
        "Retrieve full detailed information of ATT&CK techniques including ID, name, description, platforms, kill chain phases, references, sub-techniques, and mitigations",
        "Query mitigations for a specified ATT&CK technique",
        "Query detection methods for a specified ATT&CK technique",
        "List all ATT&CK tactics",
        "Provide server and dataset version information along with maintainer and Git details",
        "Operate as an MCP server supporting both local stdio and HTTP/streamable modes",
        "Support integration with MCP clients such as Smithery and Cursor"
      ],
      "limitations": [
        "Requires the enterprise-attack.json dataset to be present in the project root directory",
        "No explicit mention of rate limits or concurrency handling",
        "Does not provide write or update capabilities to the ATT&CK dataset",
        "Fuzzy search is supported only for technique names, not for other fields"
      ],
      "requirements": [
        "Python environment with dependencies installed via pip from requirements.txt",
        "Presence of enterprise-attack.json dataset file in the project root directory",
        "MCP client configured to connect via stdio or HTTP mode",
        "Optional environment variables for HTTP mode configuration (ATTACK_MCP_MODE, ATTACK_MCP_HOST, ATTACK_MCP_PORT, ATTACK_MCP_LOG_LEVEL)",
        "Docker for containerized deployment if used"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with arguments and examples, usage modes, environment configuration, and limitations, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# attAck-mcp-server\n\nThis project is an MCP (Model Context Protocol) server for querying ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) techniques and tactics. It provides a way to access and retrieve information about various attack techniques and tactics used by adversaries.\n\n## Tools\n\nThe server provides the following tools:\n\n*   **query\\_technique:**  This tool allows you to query ATT&CK techniques by ID or name.\n    *   **Arguments:**\n        *   `technique_id` (string, optional): The ID of the technique to query.\n        *   `tech_name` (string, optional): The name (or partial name) of the technique to query. ÊîØÊåÅÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢„ÄÇ\n    *   **Example:**\n        - ÊåâIDÊü•ËØ¢Ôºö\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n        - ÊåâÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢Ôºö\n        ```json\n        {\n          \"tech_name\": \"phishing\"\n        }\n        ```\n*   **search\\_technique\\_full:**  ÈÄöËøáÊäÄÊúØ ID ÊàñÂêçÁß∞Êü•ËØ¢ÊîªÂáªÊäÄÊúØÁöÑÊâÄÊúâËØ¶ÁªÜ‰ø°ÊÅØÔºåËøîÂõûÁöÑÊï∞ÊçÆÂåÖÂê´ ID„ÄÅÂêçÁß∞„ÄÅÊèèËø∞„ÄÅÈÄÇÁî®Âπ≥Âè∞„ÄÅKill Chain Èò∂ÊÆµ„ÄÅÂèÇËÄÉËµÑÊñô„ÄÅÂ≠êÊäÄÊúØÂèäÁºìËß£Êé™ÊñΩ„ÄÇÂêçÁß∞ÊêúÁ¥¢ËøîÂõûÊ†ºÂºè‰∏∫ `{ \"results\": [...], \"count\": N }` ÁöÑÂ≠óÂÖ∏ÔºåÂÖ∂‰∏≠ `results` ‰∏∫ÂåπÈÖçÊäÄÊúØÂÆåÊï¥Êï∞ÊçÆÂàóË°®„ÄÇ\n    *   **Arguments:**\n        *   `technique_id` (string, optional): Ë¶ÅÊü•ËØ¢ÁöÑÊäÄÊúØID„ÄÇ\n        *   `tech_name` (string, optional): ÊäÄÊúØÂêçÁß∞ÂÖ≥ÈîÆÂ≠óÔºåÊîØÊåÅÊ®°Á≥äÂåπÈÖç„ÄÇ\n    *   **Example:**\n        - ÊåâIDÊü•ËØ¢Ôºö\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n        - ÊåâÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢Ôºö\n        ```json\n        {\n          \"tech_name\": \"phishing\"\n        }\n        ```\n*   **query\\_mitigations:** Êü•ËØ¢ÊäÄÊúØÁöÑÁºìËß£Êé™ÊñΩ\n    *   **Arguments:**\n        *   `technique_id` (string, required): Ë¶ÅÊü•ËØ¢ÁöÑÊäÄÊúØID\n    *   **Example:**\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n*   **query\\_detections:** Êü•ËØ¢ÊäÄÊúØÁöÑÊ£ÄÊµãÊñπÊ≥ï\n    *   **Arguments:**\n        *   `technique_id` (string, required): Ë¶ÅÊü•ËØ¢ÁöÑÊäÄÊúØID\n    *   **Example:**\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n*   **list\\_tactics:** This tool allows you to retrieve a list of all ATT&CK tactics.",
        "start_pos": 0,
        "end_pos": 1955,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "a8f8cb4a82772b21"
      },
      {
        "chunk_id": 1,
        "text": "Êü•ËØ¢ÁöÑÊäÄÊúØID\n    *   **Example:**\n        ```json\n        {\n          \"technique_id\": \"T1059.001\"\n        }\n        ```\n*   **list\\_tactics:** This tool allows you to retrieve a list of all ATT&CK tactics.\n    *   **Arguments:** None\n*   **server_info:** ËøîÂõûÊúçÂä°‰∏éÊï∞ÊçÆÈõÜÁöÑÁâàÊú¨„ÄÅÁª¥Êä§ËÄÖÂíåGit‰ø°ÊÅØ„ÄÇ\n    *   **Arguments:** None\n    *   **Example:**\n        ```json\n        {}\n        ```\n\n## Usage\n\nTo use this MCP server, you need to have an MCP client configured to connect to it. Once connected, you can use the provided tools to query ATT&CK techniques and tactics.\n\n## MCP Client ÈÖçÁΩÆËØ¥Êòé\n\n### 1. Êú¨Âú∞ stdio ÊñπÂºèÔºàÊé®Ëçê Smithery/Êú¨Âú∞ÈõÜÊàêÔºâ\n\n- Áõ¥Êé•ËøêË°åÔºö\n  ```bash\n  python main.py\n  ```\n- Á®ãÂ∫è‰ºöËá™Âä®ÈÄâÊã© stdio Ê®°ÂºèÔºàÈªòËÆ§Êàñ `ATTACK_MCP_MODE=stdio`ÔºâÔºåÈÄÇÁî®‰∫é Smithery„ÄÅCursor Á≠âÊîØÊåÅÊú¨Âú∞ MCP stdio ÁöÑÂÆ¢Êà∑Á´Ø„ÄÇ\n- MCP ÂÆ¢Êà∑Á´ØÈÖçÁΩÆÊúçÂä°Á±ªÂûã‰∏∫\"local/stdio\"ÔºåÊó†ÈúÄÊåáÂÆöÁ´ØÂè£„ÄÇ\n- ÈÄÇÁî®Âú∫ÊôØÔºöSmithery Ëá™Âä®Âåñ„ÄÅCI/CD„ÄÅÊú¨Âú∞ AI Agent ÈõÜÊàê„ÄÇ\n\n### 2.",
        "start_pos": 1755,
        "end_pos": 2575,
        "token_count_estimate": 205,
        "source_type": "readme",
        "agent_id": "a8f8cb4a82772b21"
      },
      {
        "chunk_id": 2,
        "text": "hing\"\n    }\n    ```\n  - ‰ΩøÁî® `search_technique_full` Ëé∑ÂèñÊäÄÊúØÁöÑÂÆåÊï¥ËØ¶ÁªÜ‰ø°ÊÅØÔºö\n    ```json\n    {\n      \"tech_name\": \"phishing\"\n    }\n    ```\n  - Êü•ËØ¢ÊäÄÊúØÁºìËß£Êé™ÊñΩÔºö\n    ```json\n    {\n      \"technique_id\": \"T1059.001\"\n    }\n    ```\n  - Êü•ËØ¢ÊäÄÊúØÊ£ÄÊµãÊñπÊ≥ïÔºö\n    ```json\n    {\n      \"technique_id\": \"T1059.001\"\n    }\n    ```\n  - Êü•ËØ¢ÊàòÊúØÂàóË°®Ôºö\n    ```json\n    {}\n    ```\n  - Êü•ËØ¢ÊúçÂä°‰∏éÊï∞ÊçÆÈõÜ‰ø°ÊÅØÔºö\n    ```json\n    {}\n    ```\n\n> ÂÖ∑‰ΩìÁöÑÂÆ¢Êà∑Á´ØÈÖçÁΩÆÊñπÂºèËØ∑ÂèÇËÄÉÊÇ®ÁöÑ MCP ÂÆ¢Êà∑Á´ØÊñáÊ°£ÔºåÂ∞Ü‰∏äËø∞ÊúçÂä°Âú∞ÂùÄÂíåÂ∑•ÂÖ∑ÂêçÁß∞Â°´ÂÖ•ÂØπÂ∫î‰ΩçÁΩÆÂç≥ÂèØ„ÄÇ\n\n## Installation\n\n1.  Clone this repository.\n2.  Install the required dependencies using `pip install -r requirements.txt`.\n3.  Configure the MCP server in your MCP client.\n\n## ATT&CK\n\nATT&CK is a curated knowledge base and model for cyber adversary behavior, reflecting the various phases of an adversary's attack lifecycle and the platforms they are known to target. ATT&CK is useful for understanding security risks against any specific technology or organization.\n\n## Âø´ÈÄüÂêØÂä®\n\n### ÊñπÂºè‰∏ÄÔºöÁõ¥Êé•Áî® Python ËÑöÊú¨ËøêË°åÔºàÂºÄÂèë/Ë∞ÉËØïÊé®ËçêÔºâ\n\n1. ÂÆâË£Ö‰æùËµñÔºàÂª∫ËÆÆÂú®ËôöÊãüÁéØÂ¢É‰∏≠ÔºâÔºö\n   ```bash\n   pip install -r requirements.txt\n   ```\n2. Á°Æ‰øù enterprise-attack.json Êï∞ÊçÆÈõÜÂú®È°πÁõÆÊ†πÁõÆÂΩï„ÄÇ\n3. ÂêØÂä®ÊúçÂä°ÔºàÈªòËÆ§ stdio Ê®°ÂºèÔºåÈÄÇÁî®‰∫éÊú¨Âú∞ÂÆ¢Êà∑Á´ØÈõÜÊàêÔºâÔºö\n   ```bash\n   python main.py\n   ```\n4. Â¶ÇÊûúÈúÄË¶Å‰ª• HTTP ÊñπÂºèÊèê‰æõÊúçÂä°ÔºåËØ∑ÊòæÂºèÈÄâÊã©Ê®°ÂºèÔºö\n   ```bash\n   python main.py --mode http --host 127.0.0.1 --port 8081\n   ```\n\n### ÊñπÂºè‰∫åÔºöÁîü‰∫ßÁéØÂ¢ÉÊé®ËçêÔºàDocker ÈÉ®ÁΩ≤Ôºâ\n\n#### Docker\n1. ÊûÑÂª∫ÈïúÂÉèÔºö\n   ```bash\n   docker build -t attack-mcp-server .\n   ```\n2. ËøêË°åÂÆπÂô®Ôºö\n   ```bash\n   docker run -p 8081:8081 attack-mcp-server\n   ```\n\n---\n\n## API ËØ¥Êòé\n- /query_technique ÈÄöËøáIDÊàñÂêçÁß∞Êü•ËØ¢ÊîªÂáªÊäÄÊúØËØ¶ÊÉÖÔºàÊîØÊåÅÂêçÁß∞Ê®°Á≥äÊêúÁ¥¢Ôºâ\n- /search_technique_full ÈÄöËøáIDÊàñÂêçÁß∞Êü•ËØ¢ÊîªÂáªÊäÄÊúØÁöÑÂÆåÊï¥ËØ¶ÁªÜ‰ø°ÊÅØÔºàÂêçÁß∞ÊêúÁ¥¢ËøîÂõûÂåπÈÖçÊäÄÊúØÂàóË°®ÔºåÂåÖÂê´Â≠êÊäÄÊúØ‰∏éÁºìËß£Êé™ÊñΩÔºâ\n- /query_mitigations Êü•ËØ¢ÊåáÂÆöÊäÄÊúØÁöÑÁºìËß£Êé™ÊñΩ\n- /query_detections Êü•ËØ¢ÊåáÂÆöÊäÄÊúØÁöÑÊ£ÄÊµãÊñπÊ≥ï\n- /list_tactics Ëé∑ÂèñÊâÄÊúâATT&CKÊàòÊúØÂàÜÁ±ª\n- /server_info ËøîÂõûÊúçÂä°ÁâàÊú¨„ÄÅÊï∞ÊçÆÈõÜÁâàÊú¨ÂíåGit‰ø°ÊÅØ\n\n---\n\nÂ¶ÇÊúâÈóÆÈ¢òËØ∑ËÅîÁ≥ªÁª¥Êä§ËÄÖ„ÄÇ",
        "start_pos": 3603,
        "end_pos": 5280,
        "token_count_estimate": 419,
        "source_type": "readme",
        "agent_id": "a8f8cb4a82772b21"
      }
    ]
  },
  {
    "agent_id": "3dea47d2add13c3e",
    "name": "ai.smithery/alphago2580-naramarketmcp",
    "source": "mcp",
    "source_url": "https://github.com/alphago2580/naramarketmcp",
    "description": "Access Korea‚Äôs G2B procurement and Nara Market data for bid notices, awards, contracts, statistics‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-19T02:36:55.167644Z",
    "indexed_at": "2026-02-18T04:04:53.390218",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# ÎÇòÎùºÏû•ÌÑ∞ MCP ÏÑúÎ≤Ñ\n\nÌïúÍµ≠ Í≥µÍ≥µÏ°∞Îã¨(G2B) Îç∞Ïù¥ÌÑ∞Î•º MCP(Model Context Protocol)Î°ú Ï†úÍ≥µÌïòÎäî FastMCP 2.0 ÏÑúÎ≤ÑÏûÖÎãàÎã§.\n\nÏûÖÏ∞∞Í≥µÍ≥†, ÎÇôÏ∞∞Ï†ïÎ≥¥, Í≥ÑÏïΩÏ†ïÎ≥¥, Ï°∞Îã¨ÌÜµÍ≥Ñ, Î¨ºÌíàÎ™©Î°ù, Ï¢ÖÌï©ÏáºÌïëÎ™∞ Îì± ÎÇòÎùºÏû•ÌÑ∞Ïùò Ï£ºÏöî APIÎ•º AI ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Î∞îÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù ÌÜµÌï©ÌñàÏäµÎãàÎã§.\n\n## ÏïÑÌÇ§ÌÖçÏ≤ò\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     MCP Protocol      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   AI Agent      ‚îÇ ‚óÑ‚îÄ‚îÄ(stdio/http/sse)‚îÄ‚îÄ‚ñ∫ ‚îÇ  naramarket-mcp      ‚îÇ\n‚îÇ (Claude, etc.)  ‚îÇ                        ‚îÇ                      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n                                           ‚îÇ  ‚îÇ MCP Tools (16) ‚îÇ  ‚îÇ\n                                           ‚îÇ  ‚îÇ Resources (3)  ‚îÇ  ‚îÇ\n                                           ‚îÇ  ‚îÇ Prompts (3)    ‚îÇ  ‚îÇ\n                                           ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n                                           ‚îÇ          ‚îÇ           ‚îÇ\n                                           ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n                                           ‚îÇ  ‚îÇ  API Clients   ‚îÇ  ‚îÇ\n                                           ‚îÇ  ‚îÇ  (sync/async)  ‚îÇ  ‚îÇ\n                                           ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n                                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                                      ‚îÇ\n                                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                                           ‚îÇ  data.go.kr APIs     ‚îÇ\n                                           ‚îÇ  (Í≥µÍ≥µÎç∞Ïù¥ÌÑ∞Ìè¨ÌÑ∏)      ‚îÇ\n                                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## MCP ÎèÑÍµ¨\n\n### Í∏∞Î≥∏ ÎèÑÍµ¨ (3Í∞ú)\n\n| ÎèÑÍµ¨ | ÏÑ§Î™Ö |\n|------|------|\n| `crawl_list` | ÎÇòÎùºÏû•ÌÑ∞ ÏÉÅÌíà Î™©Î°ù Ï°∞Ìöå |\n| `get_detailed_attributes` | ÏÉÅÌíà ÏÉÅÏÑ∏ ÏÜçÏÑ± Ï°∞Ìöå |\n| `server_info` | ÏÑúÎ≤Ñ Ï†ïÎ≥¥ Î∞è ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨ Î™©Î°ù |\n\n### Ï†ïÎ∂ÄÏ°∞Îã¨ API (4Í∞ú)\n\n| ÎèÑÍµ¨ | ÏÑ§Î™Ö | ÏßÄÏõê API |\n|------|------|---------|\n| `call_public_data_standard_api` | Í≥µÍ≥µÎç∞Ïù¥ÌÑ∞Í∞úÎ∞©ÌëúÏ§Ä API | ÏûÖÏ∞∞Í≥µÍ≥†, ÎÇôÏ∞∞Ï†ïÎ≥¥, Í≥ÑÏïΩÏ†ïÎ≥¥ |\n| `call_procurement_statistics_api` | Ï°∞Îã¨ÌÜµÍ≥Ñ API | Ï†ÑÏ≤¥/Í∏∞Í¥ÄÎ≥Ñ/Í∏∞ÏóÖÎ≥Ñ Ï°∞Îã¨ ÌÜµÍ≥Ñ (14Í∞ú Ïò§ÌçºÎ†àÏù¥ÏÖò) |\n| `call_product_list_api` | Î¨ºÌíàÎ™©Î°ù API | Î¨ºÌíàÎ∂ÑÎ•ò, ÌíàÎ™© Ï°∞Ìöå (12Í∞ú Ïò§ÌçºÎ†àÏù¥ÏÖò) |\n| `call_shopping_mall_api` | Ï¢ÖÌï©ÏáºÌïëÎ™∞ API | MAS Í≥ÑÏïΩ, ÎÇ©ÌíàÏöîÍµ¨, Î≤§Ï≤òÎÇòÎùº (9Í∞ú Ïò§ÌçºÎ†àÏù¥ÏÖò) |\n\n### AI ÏπúÌôî Í∞ÑÌé∏ ÎèÑÍµ¨ (4Í∞ú)\n\n| ÎèÑÍµ¨ | ÏÑ§Î™Ö |\n|------|------|\n| `get_recent_bid_announcements` | ÏµúÍ∑º ÏûÖÏ∞∞Í≥µÍ≥† Ï°∞Ìöå (Í∏∞Í∞Ñ ÏûêÎèô Í≥ÑÏÇ∞) |\n| `get_successful_bids_by_business_type` | ÏóÖÎ¨¥Íµ¨Î∂ÑÎ≥Ñ ÎÇôÏ∞∞Ï†ïÎ≥¥ (ÌïúÍ∏Ä ‚Üí ÏΩîÎìú ÏûêÎèô Î≥ÄÌôò) |\n| `get_procurement_statistics_by_year` | Ïó∞ÎèÑÎ≥Ñ Ï°∞Îã¨ÌÜµÍ≥Ñ |\n| `search_shopping_mall_products` | ÏáºÌïëÎ™∞ Ï†úÌíà Í≤ÄÏÉâ (Ï†úÌíàÎ™Ö/ÏóÖÏ≤¥Î™Ö) |\n\n### ÌÉêÏÉâ ÎèÑÍµ¨ (4Í∞ú)\n\n| ÎèÑÍµ¨ | ÏÑ§Î™Ö |\n|------|------|\n| `get_all_api_services_info` | Ï†ÑÏ≤¥ ÏÑúÎπÑÏä§ Î∞è Ïò§ÌçºÎ†àÏù¥ÏÖò Î™©Î°ù |\n| `get_api_operations` | ÏÑúÎπÑÏä§Î≥Ñ ÏÑ∏Î∂Ä Ïò§ÌçºÎ†àÏù¥ÏÖò Ï°∞Ìöå |\n| `call_api_with_pagination_support` | ÌéòÏù¥Ïßï ÏßÄÏõê ÎåÄÎüâ Îç∞Ïù¥ÌÑ∞ ÌÉêÏÉâ |\n| `get_data_exploration_guide` | Îç∞Ïù¥ÌÑ∞ ÌÉêÏÉâ Ï†ÑÎûµ Í∞ÄÏù¥Îìú |\n\n### MCP Î¶¨ÏÜåÏä§ & ÌîÑÎ°¨ÌîÑÌä∏\n\n- **Î¶¨ÏÜåÏä§**: API ÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏù¥Îìú, Í∞í ÏòàÏãú, Í≤ÄÏÉâ Ìå®ÌÑ¥\n- **ÌîÑÎ°¨ÌîÑÌä∏**: ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í∞ÄÏù¥Îìú, ÌååÎùºÎØ∏ÌÑ∞ ÏÑ†ÌÉù Í∞ÄÏù¥Îìú, Ïã§Ï†Ñ ÏøºÎ¶¨ ÏòàÏ†ú\n\n## ÏãúÏûëÌïòÍ∏∞\n\n### ÌïÑÏàò ÏöîÍµ¨ÏÇ¨Ìï≠\n\n- Python 3.10+\n- [Í≥µÍ≥µÎç∞Ïù¥ÌÑ∞Ìè¨ÌÑ∏](https://www.data.go.kr/) API ÏÑúÎπÑÏä§ ÌÇ§\n\n### ÏÑ§Ïπò\n\n```bash\ngit clone https://github.com/alphago2580/naramarketmcp.git\ncd naramarketmcp\n\n# ÌôòÍ≤Ω ÏÑ§Ï†ï\ncp .env.example .env\n# .envÏóêÏÑú NARAMARKET_SERVICE_KEY ÏÑ§Ï†ï\n\n# ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\npip install -r requirements.txt\n```\n\n### Ïã§Ìñâ\n\n```bash\n# STDIO Î™®Îìú (MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Îèô)\npython -m src.main\n\n# HTTP Î™®Îìú (Ïõπ ÏÑúÎπÑÏä§)\nFASTMCP_TRANSPORT=http FASTMCP_PORT=8000 python -m src.main\n\n# Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÌõÑ Ïã§Ìñâ\npip install .\nnaramarket-mcp\n```\n\n### Docker\n\n```bash\ndocker build -t naramarket-mcp .\ndocker run --rm -e NARAMARKET_SERVICE_KEY=your-key -p 8000:8000 naramarket-mcp\n```\n\n## ÌôòÍ≤Ω Î≥ÄÏàò\n\n| Î≥ÄÏàò | ÌïÑÏàò | Í∏∞Î≥∏Í∞í | ÏÑ§Î™Ö |\n|------|:---:|--------|------|\n| `NARAMARKET_SERVICE_KEY` | ‚úÖ | - | Í≥µÍ≥µÎç∞Ïù¥ÌÑ∞Ìè¨ÌÑ∏ API ÌÇ§ |\n| `FASTMCP_TRANSPORT` | - | `stdio` | Ï†ÑÏÜ° Î™®Îìú (`stdio`, `http`, `sse`) |\n| `FASTMCP_HOST` | - | `127.0.0.1` | HTTP/SSE Î∞îÏù∏Îî© Ìò∏Ïä§Ìä∏ |\n| `FASTMCP_PORT` | - | `8081` | HTTP/SSE Ìè¨Ìä∏ |\n| `LOG_LEVEL` | - | `INFO` | Î°úÍπÖ Î†àÎ≤® |\n\n## ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞\n\n```\nnaramarketmcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastMCP ÏÑúÎ≤Ñ ÏßÑÏûÖÏ†ê (ÎèÑÍµ¨/Î¶¨ÏÜåÏä§/ÌîÑÎ°¨ÌîÑÌä∏ Îì±Î°ù)\n‚îÇ   ‚îú‚îÄ‚îÄ core/                # ÌïµÏã¨ Î™®Îìà\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py        # ÏÑ§Ï†ï Í¥ÄÎ¶¨\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py        # Îç∞Ïù¥ÌÑ∞ Î™®Îç∏\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.py        # API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ async_client.py  # ÎπÑÎèôÍ∏∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py         # Ïú†Ìã∏Î¶¨Ìã∞\n‚îÇ   ‚îú‚îÄ‚îÄ api/                 # HTTP/REST Ïù∏ÌÑ∞ÌéòÏù¥Ïä§\n‚îÇ   ‚îú‚îÄ‚îÄ services/            # ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅ (ÌÅ¨Î°§Îü¨, Ïù∏Ï¶ù)\n‚îÇ   ‚îî‚îÄ‚îÄ tools/               # MCP ÎèÑÍµ¨ Íµ¨ÌòÑ\n‚îÇ       ‚îú‚îÄ‚îÄ naramarket.py    # Í∏∞Î≥∏ ÎÇòÎùºÏû•ÌÑ∞ ÎèÑÍµ¨\n‚îÇ       ‚îú‚îÄ‚îÄ enhanced_tools.py # ÌôïÏû• API ÎèÑÍµ¨\n‚îÇ       ‚îî‚îÄ‚îÄ openapi_tools.py # G2B OpenAPI ÎèÑÍµ¨\n‚îú‚îÄ‚îÄ tests/                   # ÌÖåÏä§Ìä∏\n‚îú‚îÄ‚îÄ deployments/             # Î∞∞Ìè¨ ÏÑ§Ï†ï (docker-compose, nginx)\n‚îú‚îÄ‚îÄ Dockerfile\n‚îú‚îÄ‚îÄ smithery.yaml            # Smithery.ai Î∞∞Ìè¨ ÏÑ§Ï†ï\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îî‚îÄ‚îÄ requirements.txt\n```\n\n## Í∞úÎ∞ú\n\n```bash\n# Í∞úÎ∞ú ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\npip install .[dev]\n\n# ÌÖåÏä§Ìä∏ Ïã§Ìñâ\npytest tests/\n\n# ÌÉÄÏûÖ Ï≤¥ÌÅ¨\nmypy src/\n```\n\n## Í∏∞Ïà† Ïä§ÌÉù\n\n- **FastMCP 2.0** ‚Äî MCP ÏÑúÎ≤Ñ ÌîÑÎ†àÏûÑÏõåÌÅ¨\n- **Requests / Pandas** ‚Äî API Ìò∏Ï∂ú Î∞è Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨\n- **Uvicorn / Starlette** ‚Äî HTTP/SSE ÏÑúÎπô\n- **Docker** ‚Äî Ïª®ÌÖåÏù¥ÎÑà Î∞∞Ìè¨\n- **Smithery.ai** ‚Äî ÌÅ¥ÎùºÏö∞Îìú MCP Ìò∏Ïä§ÌåÖ\n\n## ÎùºÏù¥ÏÑ†Ïä§\n\nApache License 2.0 ‚Äî [LICENSE](LICENSE) Ï∞∏Ï°∞\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide Korean public procurement (G2B) data via MCP protocol",
        "Integrate multiple Nara Market APIs for bid announcements, successful bids, contracts, procurement statistics, product lists, and shopping mall data",
        "Support synchronous and asynchronous API clients for data retrieval",
        "Offer 16 MCP tools including basic tools, government procurement APIs, AI-friendly simplified tools, and exploration tools",
        "Enable AI agents to query recent bid announcements, successful bids by business type, procurement statistics by year, and shopping mall product searches",
        "Support data exploration with pagination and detailed API operation information",
        "Serve MCP protocol over stdio, HTTP, or SSE transport modes",
        "Provide comprehensive resources and prompts for API parameter guidance, search patterns, and query examples",
        "Deploy via Docker and support cloud hosting with Smithery.ai"
      ],
      "limitations": [
        "Requires a valid public data portal API key for operation",
        "Supports only Korean public procurement data from data.go.kr APIs",
        "Limited to the APIs and operations exposed by the Nara Market and related government procurement services",
        "No mention of support for non-Korean or non-government procurement data",
        "No explicit rate limit details provided in the documentation"
      ],
      "requirements": [
        "Python 3.10 or higher",
        "Public Data Portal (data.go.kr) API service key",
        "Environment variables configuration including NARAMARKET_SERVICE_KEY",
        "Dependencies installed via pip from requirements.txt",
        "Optional Docker environment for containerized deployment"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions, usage examples, environment setup, architecture overview, and limitations, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# ÎÇòÎùºÏû•ÌÑ∞ MCP ÏÑúÎ≤Ñ\n\nÌïúÍµ≠ Í≥µÍ≥µÏ°∞Îã¨(G2B) Îç∞Ïù¥ÌÑ∞Î•º MCP(Model Context Protocol)Î°ú Ï†úÍ≥µÌïòÎäî FastMCP 2.0 ÏÑúÎ≤ÑÏûÖÎãàÎã§.\n\nÏûÖÏ∞∞Í≥µÍ≥†, ÎÇôÏ∞∞Ï†ïÎ≥¥, Í≥ÑÏïΩÏ†ïÎ≥¥, Ï°∞Îã¨ÌÜµÍ≥Ñ, Î¨ºÌíàÎ™©Î°ù, Ï¢ÖÌï©ÏáºÌïëÎ™∞ Îì± ÎÇòÎùºÏû•ÌÑ∞Ïùò Ï£ºÏöî APIÎ•º AI ÏóêÏù¥Ï†ÑÌä∏Í∞Ä Î∞îÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù ÌÜµÌï©ÌñàÏäµÎãàÎã§.",
        "start_pos": 0,
        "end_pos": 168,
        "token_count_estimate": 42,
        "source_type": "readme",
        "agent_id": "3dea47d2add13c3e"
      },
      {
        "chunk_id": 1,
        "text": "ÏÖò) |\n| `call_product_list_api` | Î¨ºÌíàÎ™©Î°ù API | Î¨ºÌíàÎ∂ÑÎ•ò, ÌíàÎ™© Ï°∞Ìöå (12Í∞ú Ïò§ÌçºÎ†àÏù¥ÏÖò) |\n| `call_shopping_mall_api` | Ï¢ÖÌï©ÏáºÌïëÎ™∞ API | MAS Í≥ÑÏïΩ, ÎÇ©ÌíàÏöîÍµ¨, Î≤§Ï≤òÎÇòÎùº (9Í∞ú Ïò§ÌçºÎ†àÏù¥ÏÖò) |\n\n### AI ÏπúÌôî Í∞ÑÌé∏ ÎèÑÍµ¨ (4Í∞ú)\n\n| ÎèÑÍµ¨ | ÏÑ§Î™Ö |\n|------|------|\n| `get_recent_bid_announcements` | ÏµúÍ∑º ÏûÖÏ∞∞Í≥µÍ≥† Ï°∞Ìöå (Í∏∞Í∞Ñ ÏûêÎèô Í≥ÑÏÇ∞) |\n| `get_successful_bids_by_business_type` | ÏóÖÎ¨¥Íµ¨Î∂ÑÎ≥Ñ ÎÇôÏ∞∞Ï†ïÎ≥¥ (ÌïúÍ∏Ä ‚Üí ÏΩîÎìú ÏûêÎèô Î≥ÄÌôò) |\n| `get_procurement_statistics_by_year` | Ïó∞ÎèÑÎ≥Ñ Ï°∞Îã¨ÌÜµÍ≥Ñ |\n| `search_shopping_mall_products` | ÏáºÌïëÎ™∞ Ï†úÌíà Í≤ÄÏÉâ (Ï†úÌíàÎ™Ö/ÏóÖÏ≤¥Î™Ö) |\n\n### ÌÉêÏÉâ ÎèÑÍµ¨ (4Í∞ú)\n\n| ÎèÑÍµ¨ | ÏÑ§Î™Ö |\n|------|------|\n| `get_all_api_services_info` | Ï†ÑÏ≤¥ ÏÑúÎπÑÏä§ Î∞è Ïò§ÌçºÎ†àÏù¥ÏÖò Î™©Î°ù |\n| `get_api_operations` | ÏÑúÎπÑÏä§Î≥Ñ ÏÑ∏Î∂Ä Ïò§ÌçºÎ†àÏù¥ÏÖò Ï°∞Ìöå |\n| `call_api_with_pagination_support` | ÌéòÏù¥Ïßï ÏßÄÏõê ÎåÄÎüâ Îç∞Ïù¥ÌÑ∞ ÌÉêÏÉâ |\n| `get_data_exploration_guide` | Îç∞Ïù¥ÌÑ∞ ÌÉêÏÉâ Ï†ÑÎûµ Í∞ÄÏù¥Îìú |\n\n### MCP Î¶¨ÏÜåÏä§ & ÌîÑÎ°¨ÌîÑÌä∏\n\n- **Î¶¨ÏÜåÏä§**: API ÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏù¥Îìú, Í∞í ÏòàÏãú, Í≤ÄÏÉâ Ìå®ÌÑ¥\n- **ÌîÑÎ°¨ÌîÑÌä∏**: ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í∞ÄÏù¥Îìú, ÌååÎùºÎØ∏ÌÑ∞ ÏÑ†ÌÉù Í∞ÄÏù¥Îìú, Ïã§Ï†Ñ ÏøºÎ¶¨ ÏòàÏ†ú\n\n## ÏãúÏûëÌïòÍ∏∞\n\n### ÌïÑÏàò ÏöîÍµ¨ÏÇ¨Ìï≠\n\n- Python 3.10+\n- [Í≥µÍ≥µÎç∞Ïù¥ÌÑ∞Ìè¨ÌÑ∏](https://www.data.go.kr/) API ÏÑúÎπÑÏä§ ÌÇ§\n\n### ÏÑ§Ïπò\n\n```bash\ngit clone https://github.com/alphago2580/naramarketmcp.git\ncd naramarketmcp\n\n# ÌôòÍ≤Ω ÏÑ§Ï†ï\ncp .env.example .env\n# .envÏóêÏÑú NARAMARKET_SERVICE_KEY ÏÑ§Ï†ï\n\n# ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\npip install -r requirements.txt\n```\n\n### Ïã§Ìñâ\n\n```bash\n# STDIO Î™®Îìú (MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Îèô)\npython -m src.main\n\n# HTTP Î™®Îìú (Ïõπ ÏÑúÎπÑÏä§)\nFASTMCP_TRANSPORT=http FASTMCP_PORT=8000 python -m src.main\n\n# Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò ÌõÑ Ïã§Ìñâ\npip install .\nnaramarket-mcp\n```\n\n### Docker\n\n```bash\ndocker build -t naramarket-mcp .",
        "start_pos": 1848,
        "end_pos": 3165,
        "token_count_estimate": 329,
        "source_type": "readme",
        "agent_id": "3dea47d2add13c3e"
      },
      {
        "chunk_id": 2,
        "text": "# ÌïµÏã¨ Î™®Îìà\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py        # ÏÑ§Ï†ï Í¥ÄÎ¶¨\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py        # Îç∞Ïù¥ÌÑ∞ Î™®Îç∏\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.py        # API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ async_client.py  # ÎπÑÎèôÍ∏∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py         # Ïú†Ìã∏Î¶¨Ìã∞\n‚îÇ   ‚îú‚îÄ‚îÄ api/                 # HTTP/REST Ïù∏ÌÑ∞ÌéòÏù¥Ïä§\n‚îÇ   ‚îú‚îÄ‚îÄ services/            # ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅ (ÌÅ¨Î°§Îü¨, Ïù∏Ï¶ù)\n‚îÇ   ‚îî‚îÄ‚îÄ tools/               # MCP ÎèÑÍµ¨ Íµ¨ÌòÑ\n‚îÇ       ‚îú‚îÄ‚îÄ naramarket.py    # Í∏∞Î≥∏ ÎÇòÎùºÏû•ÌÑ∞ ÎèÑÍµ¨\n‚îÇ       ‚îú‚îÄ‚îÄ enhanced_tools.py # ÌôïÏû• API ÎèÑÍµ¨\n‚îÇ       ‚îî‚îÄ‚îÄ openapi_tools.py # G2B OpenAPI ÎèÑÍµ¨\n‚îú‚îÄ‚îÄ tests/                   # ÌÖåÏä§Ìä∏\n‚îú‚îÄ‚îÄ deployments/             # Î∞∞Ìè¨ ÏÑ§Ï†ï (docker-compose, nginx)\n‚îú‚îÄ‚îÄ Dockerfile\n‚îú‚îÄ‚îÄ smithery.yaml            # Smithery.ai Î∞∞Ìè¨ ÏÑ§Ï†ï\n‚îú‚îÄ‚îÄ pyproject.toml\n‚îî‚îÄ‚îÄ requirements.txt\n```\n\n## Í∞úÎ∞ú\n\n```bash\n# Í∞úÎ∞ú ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\npip install .[dev]\n\n# ÌÖåÏä§Ìä∏ Ïã§Ìñâ\npytest tests/\n\n# ÌÉÄÏûÖ Ï≤¥ÌÅ¨\nmypy src/\n```\n\n## Í∏∞Ïà† Ïä§ÌÉù\n\n- **FastMCP 2.0** ‚Äî MCP ÏÑúÎ≤Ñ ÌîÑÎ†àÏûÑÏõåÌÅ¨\n- **Requests / Pandas** ‚Äî API Ìò∏Ï∂ú Î∞è Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨\n- **Uvicorn / Starlette** ‚Äî HTTP/SSE ÏÑúÎπô\n- **Docker** ‚Äî Ïª®ÌÖåÏù¥ÎÑà Î∞∞Ìè¨\n- **Smithery.ai** ‚Äî ÌÅ¥ÎùºÏö∞Îìú MCP Ìò∏Ïä§ÌåÖ\n\n## ÎùºÏù¥ÏÑ†Ïä§\n\nApache License 2.0 ‚Äî [LICENSE](LICENSE) Ï∞∏Ï°∞",
        "start_pos": 3696,
        "end_pos": 4711,
        "token_count_estimate": 250,
        "source_type": "readme",
        "agent_id": "3dea47d2add13c3e"
      }
    ]
  },
  {
    "agent_id": "29b0e77999c32963",
    "name": "ai.smithery/anirbanbasu-frankfurtermcp",
    "source": "mcp",
    "source_url": "https://github.com/anirbanbasu/frankfurtermcp",
    "description": "A MCP server for the Frankfurter API for currency exchange rates.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T11:56:36.099614Z",
    "indexed_at": "2026-02-18T04:04:55.233167",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue?logo=python&logoColor=3776ab&labelColor=e4e4e4)](https://www.python.org/downloads/release/python-3120/) [![pytest](https://github.com/anirbanbasu/frankfurtermcp/actions/workflows/uv-pytest-coverage.yml/badge.svg)](https://github.com/anirbanbasu/frankfurtermcp/actions/workflows/uv-pytest-coverage.yml) ![GitHub commits since latest release](https://img.shields.io/github/commits-since/anirbanbasu/frankfurtermcp/latest)\n [![PyPI](https://img.shields.io/pypi/v/frankfurtermcp?label=pypi%20package)](https://pypi.org/project/frankfurtermcp/#history)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/frankfurtermcp?label=pypi%20downloads)](https://pypi.org/project/frankfurtermcp/)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/c6527bdb-9b60-430d-9ed6-cb3c8b9a2b54) [![smithery badge](https://smithery.ai/badge/@anirbanbasu/frankfurtermcp)](https://smithery.ai/server/@anirbanbasu/frankfurtermcp)\n\n# Frankfurter MCP\n\n[Frankfurter](https://frankfurter.dev/) is a useful API for latest currency exchange rates, historical data, or time series published by sources such as the European Central Bank. Should you have to access the Frankfurter API as tools for language model agents exposed over the Model Context Protocol (MCP), Frankfurter MCP is what you need.\n\n# Installation\n\n_If your objective is to use the tools available on this MCP server, please refer to the usage > client sub-section below_.\n\nThe directory where you clone this repository will be referred to as the _working directory_ or _WD_ hereinafter.\n\nInstall [just](https://just.systems/man/en/) to manage project tasks.\n\nInstall [uv](https://docs.astral.sh/uv/getting-started/installation/). To install the project with its minimal dependencies in a virtual environment, run the `just install` in the _WD_. To install all non-essential dependencies (_which are required for developing and testing_), run `just install-all` instead.\n\n## Environment variables\n\nFollowing is a list of environment variables that can be used to configure the application. A template of environment variables is provided in the file `.env.template`. _Note that the default values listed in the table below are not always the same as those in the `.env.template` file_.\n\nThe following environment variables can be specified, prefixed with `FASTMCP_`: `HOST`, `PORT`, `DEBUG` and `LOG_LEVEL`. See [global configuration options](https://gofastmcp.com/servers/server#global-settings) for FastMCP. Note that `on_duplicate_` prefixed options specified as environment variables _will be ignored_.\n\nThe underlying HTTP client also respects some environment variables, as documented in [the HTTPX library](https://www.python-httpx.org/environment_variables/). In addition, `SSL_CERT_FILE` and `SSL_CERT_DIR` can be configured to use self-signed certificates of hosted API endpoint or intermediate HTTP(S) proxy server(s).\n\nFrankfurter MCP will cache calls to the Frankfurter API to improve performance. The cache happens with two different strategies. For API calls whose responses do not change for certain parameters, e.g., historical rate lookup, a least recently used (LRU) cache is used. For API calls whose responses do change, e.g., latest rate lookup, a time-to-live (TTL) cache is used with a default time-to-live set to 15 minutes. The cache parameters can be adjusted using the environment variables, see below.\n\n| Variable |  [Default value] and description   |\n|--------------|----------------|\n| `LOG_LEVEL` | [INFO] The level for logging. Changing this level also affects the log output of other dependent libraries that may use the same environment variable. See valid values at [Python logging documentation](https://docs.python.org/3/library/logging.html#logging-levels). |\n| `HTTPX_TIMEOUT` | [5.0] The time for the underlying HTTP client to wait, in seconds, for a response from the Frankfurter API. The acceptable range of values is between 5.0 and 60.0. |\n| `HTTPX_VERIFY_SSL` | [True] This variable can be set to False to turn off SSL certificate verification, if, for instance, you are using a proxy server with a self-signed certificate. However, setting this to False _is advised against_: instead, use the `SSL_CERT_FILE` and `SSL_CERT_DIR` variables to properly configure self-signed certificates. |\n| `FAST_MCP_HOST` | [localhost] This variable specifies which host the MCP server must bind to unless the server transport (see below) is set to `stdio`. _Note that running the server to bind to any IP by specifying `0.0.0.0` poses a security threat. Such a setting should only be used in demo environments._|\n| `FAST_MCP_PORT` | [8000] This variable specifies which port the MCP server must listen on unless the server transport (see below) is set to `stdio`. |\n| `CORS_MIDDLEWARE_ALLOW_ORIGINS` | [\"localhost\", \"127.0.0.1\"] This variable specifies [Cross-Origin Resource Sharing (CORS)](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS) allowed origins for the MCP server unless the server transport (see below) is set to `stdio`. You **must** set it to \"*\" explicitly (and you will get a warning by doing so) if you want to test this server over an HTTP transport using [the MCP inspector described below](https://github.com/anirbanbasu/frankfurtermcp?tab=readme-ov-file#the-official-mcp-visual-inspector). |\n| `MCP_SERVER_TRANSPORT` | [stdio] The acceptable options are `stdio`, `sse` or `streamable-http`. However, in the `.env.template`, the default value is set to `stdio`. |\n| `MCP_SERVER_INCLUDE_METADATA_IN_RESPONSE` | [True] This specifies if additional metadata will be included with the MCP  response from each tool call. The additional metadata, for example, will include the API URL of the Frankfurter server, amongst others, that is used to obtain the responses. |\n| `FRANKFURTER_API_URL` | [https://api.frankfurter.dev/v1] If you are [self-hosting the Frankfurter API](https://hub.docker.com/r/lineofflight/frankfurter), you should change this to the API endpoint address of your deployment. |\n| `LRU_CACHE_MAX_SIZE` | [1024] The maximum size of the least recently used (LRU) cache for API calls. The acceptable range of values is between 128 and 65536. |\n| `TTL_CACHE_MAX_SIZE` | [256] The maximum size of the time-to-live (TTL) cache for API calls. The acceptable range of values is between 64 and 16384. |\n| `TTL_CACHE_TTL_SECONDS` | [900] The time limit, in seconds, of the time-to-live (TTL) cache for API calls. The acceptable range of values is between 60 and 3600. |\n| `UVICORN_LIMIT_CONCURRENCY` | [100] The maximum number of concurrent connections the server will accept. This helps prevent resource exhaustion from too many simultaneous connections. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 10 and 10000. |\n<!-- | `UVICORN_LIMIT_MAX_REQUESTS` | [10000] The maximum number of requests a worker will process before being restarted. This helps prevent memory leaks from accumulating over time. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 1000 and 1000000. | -->\n| `UVICORN_TIMEOUT_KEEP_ALIVE` | [60] The timeout in seconds for keeping idle connections alive. Idle connections will be closed after this period to free up resources. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 60 and 300. |\n| `UVICORN_TIMEOUT_GRACEFUL_SHUTDOWN` | [5] The timeout in seconds for graceful shutdown. The server will wait this long for active connections to complete before forcefully shutting down. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 5 and 60. |\n| `RATE_LIMIT_MAX_REQUESTS_PER_SECOND` | [10.0] The maximum number of requests allowed per second using a token bucket algorithm. This implements rate limiting to prevent API abuse and ensure fair resource allocation. The acceptable range of values is between 1.0 and 10000.0. |\n| `RATE_LIMIT_BURST_CAPACITY` | [20] The burst capacity for the rate limiter, allowing short bursts of requests above the per-second limit. This provides flexibility for legitimate usage patterns while still protecting against sustained high request rates. The acceptable range of values is between 2x and 5x the `RATE_LIMIT_MAX_REQUESTS_PER_SECOND` value. |\n| `REQUEST_SIZE_LIMIT_BYTES` | [102400] The maximum size in bytes for HTTP request bodies (default 100KB). Requests exceeding this limit will be rejected with a 413 status code. This prevents memory exhaustion attacks from large payloads. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 10240 (10KB) and 524288 (512KB). |\n| `DOCKER_TMPFS_SIZE_MB` | [100] The size in megabytes for the temporary filesystem (`/tmp`) when running in Docker with read-only root filesystem. This temporary storage is used for runtime file operations. Increase this value if the application requires more temporary storage for caching or processing large datasets. Only relevant when deploying with Docker Compose. |\n\n# Usage\n\nThe following sub-sections illustrate how to run the Frankfurter MCP as a server and how to access it from MCP clients.\n\n## Server\nWhile running the server, you have the choice to use `stdio` transport or HTTP options (`sse` or the newer `streamable-http`).\n\nUsing default settings and `MCP_SERVER_TRANSPORT` set to `sse` or `streamable-http`, the MCP endpoint will be available over HTTP at [http://localhost:8000/sse](http://localhost:8000/sse) for the Server Sent Events (SSE) transport, or [http://localhost:8000/mcp](http://localhost:8000/mcp) for the streamable HTTP transport.\n\nIf you want to run Frankfurter MCP with `stdio` transport and the default parameters, execute the commands below without using the `.env.template` file.\n\n### Server with `uv`\n\n_Optional_: Copy the `.env.template` file to a `.env` file in the _WD_, to modify the aforementioned environment variables, if you want to use anything other than the default settings. Or, on your shell, you can export the environment variables that you wish to modify.\n\nRun the following in the _WD_ to start the MCP server.\n\n```bash\nuv run frankfurtermcp\n```\n\n### Server with `pip` from PyPI package\n\nAdd this package from PyPI using `pip` in a virtual environment (possibly managed by `uv`, `pyenv` or `conda`) and then start the server by running the following.\n\n_Optional_: Add a `.env` file with the contents of the `.env.template` file if you wish to modify the default values of the aforementioned environment variables. Or, on your shell, you can export the environment variables that you wish to modify.\n\n```bash\npip install frankfurtermcp\npython -m frankfurtermcp.server\n```\n\n### Server using Docker\n\nThere are two Dockerfiles provided in this repository.\n\n - `local.dockerfile` for containerising the Frankfurter MCP server.\n - `smithery.dockerfile` for deploying to [Smithery AI](https://smithery.ai/), which you do not have to use. Note that runtime hardening of the container based on this Dockerfile is not provided in this repository through Docker Compose because this is managed by Smithery AI during deployment.\n\nFirst, make a copy of the `.env.template` to a `.env` file. Then, modify the following variables in the `.env` file as needed.\n\n - `FASTMCP_HOST`: Set to `0.0.0.0` to allow external access to the container. _This is only for local testing and is not recommended for production deployments_.\n - `CORS_MIDDLEWARE_ALLOW_ORIGINS`: Set to `*` to allow external access to the MCP server from any origin. _This is needed if you want to test the server using the MCP Inspector over HTTP transport and is not recommended for production deployments_.\n\nTo build the image, create the container and start it using Docker Compose, run the following in _WD_.\n\nIf you change the port to anything other than 8000 in `.env`, _do remember to change the port number in `docker-compose.yml`_. Instead of using the `.env` file, you can also modify `docker-compose.yml` to pass individual environment variables using the `environment` section.\n\n```bash\ndocker compose up --build\n```\n\nTo run in detached mode (background), add the `-d` flag:\n\n```bash\ndocker compose up -d --build\n```\n\nTo stop the container:\n\n```bash\ndocker compose down\n```\n\nThe `docker-compose.yml` file includes security hardening with read-only filesystem, dropped capabilities, seccomp and AppArmor profiles, and resource limits (512MB memory, 1 CPU).\n\nUpon successful build and container start, the MCP server will be available over HTTP at [http://localhost:8000/sse](http://localhost:8000/sse) for the Server Sent Events (SSE) transport, or [http://localhost:8000/mcp](http://localhost:8000/mcp) for the streamable HTTP transport.\n\n### Cloud hosted servers\n\nThe currently available cloud hosted options are as follows.\n\n - FastMCP Cloud: https://frankfurtermcp.fastmcp.app/mcp\n - Glama.AI: https://glama.ai/mcp/servers/@anirbanbasu/frankfurtermcp\n - Smithery.AI: https://smithery.ai/server/@anirbanbasu/frankfurtermcp (_This will be deprecated beyond March 2026._)\n\n\n## Client access\n\nThis sub-section explains ways for a client to connect and test the FrankfurterMCP server.\n\n### The official MCP visual inspector\n\nThe [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is an _official_ Model Context Protocol tool that can be used by developers to test and debug MCP servers. This is the most comprehensive way to explore the MCP server.\n\nTo use it, you must have Node.js installed. The best way to install and manage `node` as well as packages such as the MCP Inspector is to use the [Node Version Manager (or, `nvm`)](https://github.com/nvm-sh/nvm). Once you have `nvm` installed, you can install and use the latest Long Term Release version of `node` by executing the following.\n\n```bash\nnvm install --lts\nnvm use --lts\n```\n\nFollowing that (install and) run the MCP Inspector by executing the following in the _WD_.\n\n```bash\nnpx @modelcontextprotocol/inspector uv run frankfurtermcp\n```\n\nThis will create a local URL at port 6274 with an authentication token, which you can copy and browse to on your browser. Once on the MCP Inspector UI, press _Connect_ to connect to the MCP server. Thereafter, you can explore the tools available on the server.\n\n### Claude Desktop, Visual Studio, and so on\n\nThe server entry to run with `stdio` transport that you can use with systems such as Claude Desktop, Visual Studio Code, and so on is as follows.\n\n```json\n{\n    \"command\": \"uv\",\n    \"args\": [\n        \"run\",\n        \"frankfurtermcp\"\n    ]\n}\n```\n\nInstead of having `frankfurtermcp` as the last item in the list of `args`, you may need to specify the full path to the script, e.g., _WD_`/.venv/bin/frankfurtermcp`. Likewise, instead of using `uv`, you could also have the following JSON configuration with the path properly substituted for `python3.12`, for instance such as _WD_`/.venv/bin/python3.12`.\n\n```json\n{\n    \"command\": \"python3.12\",\n    \"args\": [\n        \"-m\",\n        \"frankfurtermcp.server\"\n    ]\n}\n```\n\n# List of available MCP features\n\nFrankfurterMCP has the following MCP features.\n\n## Tools\n\nThe following table lists the names of the tools as exposed by the FrankfurterMCP server. The descriptions shown here are for documentation purposes, which may differ from the actual descriptions exposed over the model context protocol.\n\n| Name         |  Description   |\n|--------------|----------------|\n| `get_supported_currencies` | Get a list of currencies supported by the Frankfurter API. |\n| `get_latest_exchange_rates` | Get latest exchange rates in specific currencies for a given base currency. |\n| `convert_currency_latest` | Convert an amount from one currency to another using the latest exchange rates. |\n| `get_historical_exchange_rates` | Get historical exchange rates for a specific date or date range in specific currencies for a given base currency. |\n| `convert_currency_specific_date` | Convert an amount from one currency to another using the exchange rates for a specific date. |\n| `greet` | Get a greeting from the FrankfurterMCP server. _This is mostly used for internal testing_. |\n\nThe required and optional arguments for each tool are not listed in the following table for brevity but are available to the MCP client over the protocol.\n\n# Contributing\n\nInstall [`prek`](https://prek.j178.dev/). Then enable `prek` by running the following in the _WD_.\n\n```bash\nprek install\n```\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n# Testing and coverage\n\nTo run the provided test cases, execute the following. Add the flag `--capture=tee-sys` to the command to display further console output.\n\n```bash\nuv run --group test pytest tests/\n```\n\nInvoke `just test-coverage` to run all the tests and generate a coverage report as follows. If all tests are run, the generated coverage report may look like the one below.\n\n```bash\n---------------------------------------------------------------------------------------- benchmark: 2 tests ---------------------------------------------------------------------------------------\nName (time in ms)                         Min               Max              Mean            StdDev            Median               IQR            Outliers       OPS            Rounds  Iterations\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_get_historical_exchange_rates     4.4944 (1.0)      5.1512 (1.0)      4.7919 (1.0)      0.2460 (1.0)      4.7819 (1.0)      0.3249 (1.0)           2;0  208.6840 (1.0)           5           1\ntest_get_latest_exchange_rates         4.7937 (1.07)     5.6976 (1.11)     5.3257 (1.11)     0.3345 (1.36)     5.4182 (1.13)     0.3575 (1.10)          2;0  187.7702 (0.90)          5           1\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nLegend:\n  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n  OPS: Operations Per Second, computed as 1 / Mean\n=============================================================== 15 passed in 4.09s ===============================================================\nName    Stmts   Miss    Cover   Missing\n---------------------------------------\nTOTAL     244      0  100.00%\n\n6 files skipped due to complete coverage.\nTest coverage complete.\n```\n\n# License\n\n[MIT](https://choosealicense.com/licenses/mit/).\n\n# Security considerations\n\nThis section documents security-related findings from vulnerability scans and provides context for deployment decisions.\n\n## Airtable vulnerability scan findings and rationale\n\nCheck for security-related findings from [the Airtable vulnerability scan](https://airtable.com/appXjXF6ejJL028Rl/shrwBNQSIDMCo00jO) (search for `frankfurtermcp`) below, along with rationale and counter-arguments.\n\n| Rule ID | Issue and counter arguments |\n|----------------------|-------------------|\n| MCP-R001 | **Issue**: Tools are registered dynamically at server startup without cryptographic signatures, immutable versioning, or integrity checks. The architecture permits hot-reload scenarios (via `register_features` pattern), but no signature verification or approval flow exists.<br/><br/>**Counter arguments**: Tools are not loaded from external sources or plugins‚Äîthey are defined directly in the application source code. Integrity is ensured through version control and code review processes. Since tools are part of the application binary (not dynamically loaded plugins), cryptographic signing would add complexity without meaningful security benefit. |\n| MCP-R004 | **Issue**: The server warns but accepts wildcard in CORS origins.<br/><br/>**Counter arguments**: This server is not intended to be run directly in a production environment when using HTTP transports. _For deployments with stricter CORS origin control, users should use the `.env.template` defaults (`127.0.0.1`) and deploy the server behind their own reverse proxy with appropriate CORS origin controls at the reverse proxy level_. |\n| MCP-R005 | **Issue**: There is no TLS enforcement when the server is set to listen on `0.0.0.0`, e.g., in the `smithery.dockerfile`.<br/><br/>**Counter arguments**: This configuration is a requirement for deployment on [Smithery](https://smithery.ai/), which functions as an MCP gateway. Smithery provides its own security layer including TLS termination, authentication, and access control. If TLS requirement is enforced in the code, the Smithery deployment will fail. _For local deployments, users should use the `.env.template` defaults (`127.0.0.1`) or deploy the server behind their own reverse proxy with appropriate security controls. The warning in the code serves to alert users when binding to all interfaces_. |\n| MCP-R013 | **Issue**: There is no support for HTTPS when the server binds to any IP other than 127.0.0.1.<br/><br/>**Counter arguments**: This server is not intended to be run directly in a production environment with HTTPS support when using HTTP transports. _For deployments requiring HTTPS support, users should use the `.env.template` defaults (`127.0.0.1`) and deploy the server behind their own reverse proxy with appropriate HTTPS configuration_. |\n| MCP-R018 | **Issue**: There are no authentication or authorisation checks.<br/><br/>**Counter arguments**: This server is not intended to be run directly in a multi-user mode of operation when using HTTP transports. _For deployments with access control, users should use the `.env.template` defaults (`127.0.0.1`) and deploy the server behind their own reverse proxy with appropriate security controls_. |\n\n# Project status\n\nFollowing is a table of some updates regarding the project status. Note that these do not correspond to specific commits or milestones.\n\n| Date     |  Status   |  Notes or observations   |\n|----------|:-------------:|----------------------|\n| January 15, 2025 |  active |  Improved code with rate and size limiting middleware. |\n| December 2, 2025 |  active |  Added a middleware to remove unknown tool arguments, such as [those passed by `n8n`](https://github.com/n8n-io/n8n/issues/21500). |\n| November 26, 2025 |  active |  Using the new [`ToolResult` to package response metadata](https://gofastmcp.com/servers/tools#toolresult-and-metadata). |\n| November 21, 2025 |  active |  New tooling using `prek` (instead of `pre-commit`), `ty` (instead of `mypy`) and `just`. |\n| September 6, 2025 |  active |  Code refactoring and cleanup. |\n| June 27, 2025 |  active |  Successful remote deployments on Glama.AI and Smithery.AI. |\n| June 9, 2025 |  active |  Added containerisation, support for self-signed proxies. |\n| June 7, 2025 |  active |  Project started. Added tools to cover all the functionalities of the Frankfurter API. |\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide latest currency exchange rates",
        "Provide historical currency exchange rate data",
        "Provide time series currency exchange rate data",
        "Expose Frankfurter API tools over the Model Context Protocol (MCP)",
        "Cache API calls using LRU and TTL caching strategies to improve performance",
        "Support multiple server transports including stdio, SSE, and streamable HTTP",
        "Allow configuration of server behavior and caching via environment variables",
        "Support rate limiting and concurrency controls for HTTP transports",
        "Enable deployment via Python package, direct server run, or Docker containers"
      ],
      "limitations": [
        "Binding server to 0.0.0.0 is insecure and recommended only for demo environments",
        "SSL verification can be disabled but is discouraged; proper cert configuration is preferred",
        "Rate limiting restricts requests to a default of 10 requests per second with burst capacity",
        "Request size limited to 100KB by default for HTTP transports",
        "Concurrency limited to 100 simultaneous connections by default for HTTP transports"
      ],
      "requirements": [
        "Python 3.12 or higher",
        "Installation of 'uv' tool for running the server",
        "Optional installation of 'just' for managing project tasks",
        "Environment variables configuration (e.g., FASTMCP_HOST, FASTMCP_PORT, LOG_LEVEL)",
        "Optional Docker environment for containerized deployment",
        "Network access to Frankfurter API endpoint or self-hosted equivalent"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed environment variable configuration, usage examples for multiple deployment methods, and clearly states limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue?logo=python&logoColor=3776ab&labelColor=e4e4e4)](https://www.python.org/downloads/release/python-3120/) [![pytest](https://github.com/anirbanbasu/frankfurtermcp/actions/workflows/uv-pytest-coverage.yml/badge.svg)](https://github.com/anirbanbasu/frankfurtermcp/actions/workflows/uv-pytest-coverage.yml) ![GitHub commits since latest release](https://img.shields.io/github/commits-since/anirbanbasu/frankfurtermcp/latest)\n [![PyPI](https://img.shields.io/pypi/v/frankfurtermcp?label=pypi%20package)](https://pypi.org/project/frankfurtermcp/#history)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/frankfurtermcp?label=pypi%20downloads)](https://pypi.org/project/frankfurtermcp/)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/c6527bdb-9b60-430d-9ed6-cb3c8b9a2b54) [![smithery badge](https://smithery.ai/badge/@anirbanbasu/frankfurtermcp)](https://smithery.ai/server/@anirbanbasu/frankfurtermcp)\n\n# Frankfurter MCP\n\n[Frankfurter](https://frankfurter.dev/) is a useful API for latest currency exchange rates, historical data, or time series published by sources such as the European Central Bank. Should you have to access the Frankfurter API as tools for language model agents exposed over the Model Context Protocol (MCP), Frankfurter MCP is what you need.\n\n# Installation\n\n_If your objective is to use the tools available on this MCP server, please refer to the usage > client sub-section below_.\n\nThe directory where you clone this repository will be referred to as the _working directory_ or _WD_ hereinafter.\n\nInstall [just](https://just.systems/man/en/) to manage project tasks.\n\nInstall [uv](https://docs.astral.sh/uv/getting-started/installation/). To install the project with its minimal dependencies in a virtual environment, run the `just install` in the _WD_. To install all non-essential dependencies (_which are required for developing and testing_), run `just install-all` instead.",
        "start_pos": 0,
        "end_pos": 1987,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 1,
        "text": "dependencies in a virtual environment, run the `just install` in the _WD_. To install all non-essential dependencies (_which are required for developing and testing_), run `just install-all` instead.\n\n## Environment variables\n\nFollowing is a list of environment variables that can be used to configure the application. A template of environment variables is provided in the file `.env.template`. _Note that the default values listed in the table below are not always the same as those in the `.env.template` file_.\n\nThe following environment variables can be specified, prefixed with `FASTMCP_`: `HOST`, `PORT`, `DEBUG` and `LOG_LEVEL`. See [global configuration options](https://gofastmcp.com/servers/server#global-settings) for FastMCP. Note that `on_duplicate_` prefixed options specified as environment variables _will be ignored_.\n\nThe underlying HTTP client also respects some environment variables, as documented in [the HTTPX library](https://www.python-httpx.org/environment_variables/). In addition, `SSL_CERT_FILE` and `SSL_CERT_DIR` can be configured to use self-signed certificates of hosted API endpoint or intermediate HTTP(S) proxy server(s).\n\nFrankfurter MCP will cache calls to the Frankfurter API to improve performance. The cache happens with two different strategies. For API calls whose responses do not change for certain parameters, e.g., historical rate lookup, a least recently used (LRU) cache is used. For API calls whose responses do change, e.g., latest rate lookup, a time-to-live (TTL) cache is used with a default time-to-live set to 15 minutes. The cache parameters can be adjusted using the environment variables, see below.\n\n| Variable |  [Default value] and description   |\n|--------------|----------------|\n| `LOG_LEVEL` | [INFO] The level for logging. Changing this level also affects the log output of other dependent libraries that may use the same environment variable. See valid values at [Python logging documentation](https://docs.python.org/3/library/logging.html#logging-levels).",
        "start_pos": 1787,
        "end_pos": 3814,
        "token_count_estimate": 506,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 2,
        "text": "e log output of other dependent libraries that may use the same environment variable. See valid values at [Python logging documentation](https://docs.python.org/3/library/logging.html#logging-levels). |\n| `HTTPX_TIMEOUT` | [5.0] The time for the underlying HTTP client to wait, in seconds, for a response from the Frankfurter API. The acceptable range of values is between 5.0 and 60.0. |\n| `HTTPX_VERIFY_SSL` | [True] This variable can be set to False to turn off SSL certificate verification, if, for instance, you are using a proxy server with a self-signed certificate. However, setting this to False _is advised against_: instead, use the `SSL_CERT_FILE` and `SSL_CERT_DIR` variables to properly configure self-signed certificates. |\n| `FAST_MCP_HOST` | [localhost] This variable specifies which host the MCP server must bind to unless the server transport (see below) is set to `stdio`. _Note that running the server to bind to any IP by specifying `0.0.0.0` poses a security threat. Such a setting should only be used in demo environments._|\n| `FAST_MCP_PORT` | [8000] This variable specifies which port the MCP server must listen on unless the server transport (see below) is set to `stdio`. |\n| `CORS_MIDDLEWARE_ALLOW_ORIGINS` | [\"localhost\", \"127.0.0.1\"] This variable specifies [Cross-Origin Resource Sharing (CORS)](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS) allowed origins for the MCP server unless the server transport (see below) is set to `stdio`. You **must** set it to \"*\" explicitly (and you will get a warning by doing so) if you want to test this server over an HTTP transport using [the MCP inspector described below](https://github.com/anirbanbasu/frankfurtermcp?tab=readme-ov-file#the-official-mcp-visual-inspector). |\n| `MCP_SERVER_TRANSPORT` | [stdio] The acceptable options are `stdio`, `sse` or `streamable-http`. However, in the `.env.template`, the default value is set to `stdio`.",
        "start_pos": 3614,
        "end_pos": 5546,
        "token_count_estimate": 483,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 3,
        "text": "ficial-mcp-visual-inspector). |\n| `MCP_SERVER_TRANSPORT` | [stdio] The acceptable options are `stdio`, `sse` or `streamable-http`. However, in the `.env.template`, the default value is set to `stdio`. |\n| `MCP_SERVER_INCLUDE_METADATA_IN_RESPONSE` | [True] This specifies if additional metadata will be included with the MCP  response from each tool call. The additional metadata, for example, will include the API URL of the Frankfurter server, amongst others, that is used to obtain the responses. |\n| `FRANKFURTER_API_URL` | [https://api.frankfurter.dev/v1] If you are [self-hosting the Frankfurter API](https://hub.docker.com/r/lineofflight/frankfurter), you should change this to the API endpoint address of your deployment. |\n| `LRU_CACHE_MAX_SIZE` | [1024] The maximum size of the least recently used (LRU) cache for API calls. The acceptable range of values is between 128 and 65536. |\n| `TTL_CACHE_MAX_SIZE` | [256] The maximum size of the time-to-live (TTL) cache for API calls. The acceptable range of values is between 64 and 16384. |\n| `TTL_CACHE_TTL_SECONDS` | [900] The time limit, in seconds, of the time-to-live (TTL) cache for API calls. The acceptable range of values is between 60 and 3600. |\n| `UVICORN_LIMIT_CONCURRENCY` | [100] The maximum number of concurrent connections the server will accept. This helps prevent resource exhaustion from too many simultaneous connections. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 10 and 10000. |\n<!-- | `UVICORN_LIMIT_MAX_REQUESTS` | [10000] The maximum number of requests a worker will process before being restarted. This helps prevent memory leaks from accumulating over time. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 1000 and 1000000. | -->\n| `UVICORN_TIMEOUT_KEEP_ALIVE` | [60] The timeout in seconds for keeping idle connections alive. Idle connections will be closed after this period to free up resources.",
        "start_pos": 5346,
        "end_pos": 7360,
        "token_count_estimate": 503,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 4,
        "text": "between 1000 and 1000000. | -->\n| `UVICORN_TIMEOUT_KEEP_ALIVE` | [60] The timeout in seconds for keeping idle connections alive. Idle connections will be closed after this period to free up resources. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 60 and 300. |\n| `UVICORN_TIMEOUT_GRACEFUL_SHUTDOWN` | [5] The timeout in seconds for graceful shutdown. The server will wait this long for active connections to complete before forcefully shutting down. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 5 and 60. |\n| `RATE_LIMIT_MAX_REQUESTS_PER_SECOND` | [10.0] The maximum number of requests allowed per second using a token bucket algorithm. This implements rate limiting to prevent API abuse and ensure fair resource allocation. The acceptable range of values is between 1.0 and 10000.0. |\n| `RATE_LIMIT_BURST_CAPACITY` | [20] The burst capacity for the rate limiter, allowing short bursts of requests above the per-second limit. This provides flexibility for legitimate usage patterns while still protecting against sustained high request rates. The acceptable range of values is between 2x and 5x the `RATE_LIMIT_MAX_REQUESTS_PER_SECOND` value. |\n| `REQUEST_SIZE_LIMIT_BYTES` | [102400] The maximum size in bytes for HTTP request bodies (default 100KB). Requests exceeding this limit will be rejected with a 413 status code. This prevents memory exhaustion attacks from large payloads. Only applies when using HTTP transports (`sse` or `streamable-http`). The acceptable range of values is between 10240 (10KB) and 524288 (512KB). |\n| `DOCKER_TMPFS_SIZE_MB` | [100] The size in megabytes for the temporary filesystem (`/tmp`) when running in Docker with read-only root filesystem. This temporary storage is used for runtime file operations. Increase this value if the application requires more temporary storage for caching or processing large datasets. Only relevant when deploying with Docker Compose.",
        "start_pos": 7160,
        "end_pos": 9194,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 5,
        "text": "is used for runtime file operations. Increase this value if the application requires more temporary storage for caching or processing large datasets. Only relevant when deploying with Docker Compose. |\n\n# Usage\n\nThe following sub-sections illustrate how to run the Frankfurter MCP as a server and how to access it from MCP clients.\n\n## Server\nWhile running the server, you have the choice to use `stdio` transport or HTTP options (`sse` or the newer `streamable-http`).\n\nUsing default settings and `MCP_SERVER_TRANSPORT` set to `sse` or `streamable-http`, the MCP endpoint will be available over HTTP at [http://localhost:8000/sse](http://localhost:8000/sse) for the Server Sent Events (SSE) transport, or [http://localhost:8000/mcp](http://localhost:8000/mcp) for the streamable HTTP transport.\n\nIf you want to run Frankfurter MCP with `stdio` transport and the default parameters, execute the commands below without using the `.env.template` file.\n\n### Server with `uv`\n\n_Optional_: Copy the `.env.template` file to a `.env` file in the _WD_, to modify the aforementioned environment variables, if you want to use anything other than the default settings. Or, on your shell, you can export the environment variables that you wish to modify.\n\nRun the following in the _WD_ to start the MCP server.\n\n```bash\nuv run frankfurtermcp\n```\n\n### Server with `pip` from PyPI package\n\nAdd this package from PyPI using `pip` in a virtual environment (possibly managed by `uv`, `pyenv` or `conda`) and then start the server by running the following.\n\n_Optional_: Add a `.env` file with the contents of the `.env.template` file if you wish to modify the default values of the aforementioned environment variables. Or, on your shell, you can export the environment variables that you wish to modify.\n\n```bash\npip install frankfurtermcp\npython -m frankfurtermcp.server\n```\n\n### Server using Docker\n\nThere are two Dockerfiles provided in this repository.\n\n - `local.dockerfile` for containerising the Frankfurter MCP server.",
        "start_pos": 8994,
        "end_pos": 11004,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 6,
        "text": "rankfurtermcp\npython -m frankfurtermcp.server\n```\n\n### Server using Docker\n\nThere are two Dockerfiles provided in this repository.\n\n - `local.dockerfile` for containerising the Frankfurter MCP server.\n - `smithery.dockerfile` for deploying to [Smithery AI](https://smithery.ai/), which you do not have to use. Note that runtime hardening of the container based on this Dockerfile is not provided in this repository through Docker Compose because this is managed by Smithery AI during deployment.\n\nFirst, make a copy of the `.env.template` to a `.env` file. Then, modify the following variables in the `.env` file as needed.\n\n - `FASTMCP_HOST`: Set to `0.0.0.0` to allow external access to the container. _This is only for local testing and is not recommended for production deployments_.\n - `CORS_MIDDLEWARE_ALLOW_ORIGINS`: Set to `*` to allow external access to the MCP server from any origin. _This is needed if you want to test the server using the MCP Inspector over HTTP transport and is not recommended for production deployments_.\n\nTo build the image, create the container and start it using Docker Compose, run the following in _WD_.\n\nIf you change the port to anything other than 8000 in `.env`, _do remember to change the port number in `docker-compose.yml`_. Instead of using the `.env` file, you can also modify `docker-compose.yml` to pass individual environment variables using the `environment` section.\n\n```bash\ndocker compose up --build\n```\n\nTo run in detached mode (background), add the `-d` flag:\n\n```bash\ndocker compose up -d --build\n```\n\nTo stop the container:\n\n```bash\ndocker compose down\n```\n\nThe `docker-compose.yml` file includes security hardening with read-only filesystem, dropped capabilities, seccomp and AppArmor profiles, and resource limits (512MB memory, 1 CPU).",
        "start_pos": 10804,
        "end_pos": 12600,
        "token_count_estimate": 449,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 7,
        "text": "server will be available over HTTP at [http://localhost:8000/sse](http://localhost:8000/sse) for the Server Sent Events (SSE) transport, or [http://localhost:8000/mcp](http://localhost:8000/mcp) for the streamable HTTP transport.\n\n### Cloud hosted servers\n\nThe currently available cloud hosted options are as follows.\n\n - FastMCP Cloud: https://frankfurtermcp.fastmcp.app/mcp\n - Glama.AI: https://glama.ai/mcp/servers/@anirbanbasu/frankfurtermcp\n - Smithery.AI: https://smithery.ai/server/@anirbanbasu/frankfurtermcp (_This will be deprecated beyond March 2026._)\n\n\n## Client access\n\nThis sub-section explains ways for a client to connect and test the FrankfurterMCP server.\n\n### The official MCP visual inspector\n\nThe [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is an _official_ Model Context Protocol tool that can be used by developers to test and debug MCP servers. This is the most comprehensive way to explore the MCP server.\n\nTo use it, you must have Node.js installed. The best way to install and manage `node` as well as packages such as the MCP Inspector is to use the [Node Version Manager (or, `nvm`)](https://github.com/nvm-sh/nvm). Once you have `nvm` installed, you can install and use the latest Long Term Release version of `node` by executing the following.\n\n```bash\nnvm install --lts\nnvm use --lts\n```\n\nFollowing that (install and) run the MCP Inspector by executing the following in the _WD_.\n\n```bash\nnpx @modelcontextprotocol/inspector uv run frankfurtermcp\n```\n\nThis will create a local URL at port 6274 with an authentication token, which you can copy and browse to on your browser. Once on the MCP Inspector UI, press _Connect_ to connect to the MCP server. Thereafter, you can explore the tools available on the server.\n\n### Claude Desktop, Visual Studio, and so on\n\nThe server entry to run with `stdio` transport that you can use with systems such as Claude Desktop, Visual Studio Code, and so on is as follows.",
        "start_pos": 12652,
        "end_pos": 14614,
        "token_count_estimate": 490,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 8,
        "text": "server.\n\n### Claude Desktop, Visual Studio, and so on\n\nThe server entry to run with `stdio` transport that you can use with systems such as Claude Desktop, Visual Studio Code, and so on is as follows.\n\n```json\n{\n    \"command\": \"uv\",\n    \"args\": [\n        \"run\",\n        \"frankfurtermcp\"\n    ]\n}\n```\n\nInstead of having `frankfurtermcp` as the last item in the list of `args`, you may need to specify the full path to the script, e.g., _WD_`/.venv/bin/frankfurtermcp`. Likewise, instead of using `uv`, you could also have the following JSON configuration with the path properly substituted for `python3.12`, for instance such as _WD_`/.venv/bin/python3.12`.\n\n```json\n{\n    \"command\": \"python3.12\",\n    \"args\": [\n        \"-m\",\n        \"frankfurtermcp.server\"\n    ]\n}\n```\n\n# List of available MCP features\n\nFrankfurterMCP has the following MCP features.\n\n## Tools\n\nThe following table lists the names of the tools as exposed by the FrankfurterMCP server. The descriptions shown here are for documentation purposes, which may differ from the actual descriptions exposed over the model context protocol.\n\n| Name         |  Description   |\n|--------------|----------------|\n| `get_supported_currencies` | Get a list of currencies supported by the Frankfurter API. |\n| `get_latest_exchange_rates` | Get latest exchange rates in specific currencies for a given base currency. |\n| `convert_currency_latest` | Convert an amount from one currency to another using the latest exchange rates. |\n| `get_historical_exchange_rates` | Get historical exchange rates for a specific date or date range in specific currencies for a given base currency. |\n| `convert_currency_specific_date` | Convert an amount from one currency to another using the exchange rates for a specific date. |\n| `greet` | Get a greeting from the FrankfurterMCP server. _This is mostly used for internal testing_. |\n\nThe required and optional arguments for each tool are not listed in the following table for brevity but are available to the MCP client over the protocol.",
        "start_pos": 14414,
        "end_pos": 16439,
        "token_count_estimate": 506,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 9,
        "text": "This is mostly used for internal testing_. |\n\nThe required and optional arguments for each tool are not listed in the following table for brevity but are available to the MCP client over the protocol.\n\n# Contributing\n\nInstall [`prek`](https://prek.j178.dev/). Then enable `prek` by running the following in the _WD_.\n\n```bash\nprek install\n```\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n# Testing and coverage\n\nTo run the provided test cases, execute the following. Add the flag `--capture=tee-sys` to the command to display further console output.\n\n```bash\nuv run --group test pytest tests/\n```\n\nInvoke `just test-coverage` to run all the tests and generate a coverage report as follows. If all tests are run, the generated coverage report may look like the one below.",
        "start_pos": 16239,
        "end_pos": 17080,
        "token_count_estimate": 210,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 10,
        "text": "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nLegend:\n  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n  OPS: Operations Per Second, computed as 1 / Mean\n=============================================================== 15 passed in 4.09s ===============================================================\nName    Stmts   Miss    Cover   Missing\n---------------------------------------\nTOTAL     244      0  100.00%\n\n6 files skipped due to complete coverage.\nTest coverage complete.\n```\n\n# License\n\n[MIT](https://choosealicense.com/licenses/mit/).\n\n# Security considerations\n\nThis section documents security-related findings from vulnerability scans and provides context for deployment decisions.\n\n## Airtable vulnerability scan findings and rationale\n\nCheck for security-related findings from [the Airtable vulnerability scan](https://airtable.com/appXjXF6ejJL028Rl/shrwBNQSIDMCo00jO) (search for `frankfurtermcp`) below, along with rationale and counter-arguments.\n\n| Rule ID | Issue and counter arguments |\n|----------------------|-------------------|\n| MCP-R001 | **Issue**: Tools are registered dynamically at server startup without cryptographic signatures, immutable versioning, or integrity checks. The architecture permits hot-reload scenarios (via `register_features` pattern), but no signature verification or approval flow exists.<br/><br/>**Counter arguments**: Tools are not loaded from external sources or plugins‚Äîthey are defined directly in the application source code. Integrity is ensured through version control and code review processes. Since tools are part of the application binary (not dynamically loaded plugins), cryptographic signing would add complexity without meaningful security benefit.",
        "start_pos": 18087,
        "end_pos": 19996,
        "token_count_estimate": 477,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 11,
        "text": "sion control and code review processes. Since tools are part of the application binary (not dynamically loaded plugins), cryptographic signing would add complexity without meaningful security benefit. |\n| MCP-R004 | **Issue**: The server warns but accepts wildcard in CORS origins.<br/><br/>**Counter arguments**: This server is not intended to be run directly in a production environment when using HTTP transports. _For deployments with stricter CORS origin control, users should use the `.env.template` defaults (`127.0.0.1`) and deploy the server behind their own reverse proxy with appropriate CORS origin controls at the reverse proxy level_. |\n| MCP-R005 | **Issue**: There is no TLS enforcement when the server is set to listen on `0.0.0.0`, e.g., in the `smithery.dockerfile`.<br/><br/>**Counter arguments**: This configuration is a requirement for deployment on [Smithery](https://smithery.ai/), which functions as an MCP gateway. Smithery provides its own security layer including TLS termination, authentication, and access control. If TLS requirement is enforced in the code, the Smithery deployment will fail. _For local deployments, users should use the `.env.template` defaults (`127.0.0.1`) or deploy the server behind their own reverse proxy with appropriate security controls. The warning in the code serves to alert users when binding to all interfaces_. |\n| MCP-R013 | **Issue**: There is no support for HTTPS when the server binds to any IP other than 127.0.0.1.<br/><br/>**Counter arguments**: This server is not intended to be run directly in a production environment with HTTPS support when using HTTP transports. _For deployments requiring HTTPS support, users should use the `.env.template` defaults (`127.0.0.1`) and deploy the server behind their own reverse proxy with appropriate HTTPS configuration_.",
        "start_pos": 19796,
        "end_pos": 21628,
        "token_count_estimate": 458,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      },
      {
        "chunk_id": 12,
        "text": "**Issue**: There are no authentication or authorisation checks.<br/><br/>**Counter arguments**: This server is not intended to be run directly in a multi-user mode of operation when using HTTP transports. _For deployments with access control, users should use the `.env.template` defaults (`127.0.0.1`) and deploy the server behind their own reverse proxy with appropriate security controls_. |\n\n# Project status\n\nFollowing is a table of some updates regarding the project status. Note that these do not correspond to specific commits or milestones.\n\n| Date     |  Status   |  Notes or observations   |\n|----------|:-------------:|----------------------|\n| January 15, 2025 |  active |  Improved code with rate and size limiting middleware. |\n| December 2, 2025 |  active |  Added a middleware to remove unknown tool arguments, such as [those passed by `n8n`](https://github.com/n8n-io/n8n/issues/21500). |\n| November 26, 2025 |  active |  Using the new [`ToolResult` to package response metadata](https://gofastmcp.com/servers/tools#toolresult-and-metadata). |\n| November 21, 2025 |  active |  New tooling using `prek` (instead of `pre-commit`), `ty` (instead of `mypy`) and `just`. |\n| September 6, 2025 |  active |  Code refactoring and cleanup. |\n| June 27, 2025 |  active |  Successful remote deployments on Glama.AI and Smithery.AI. |\n| June 9, 2025 |  active |  Added containerisation, support for self-signed proxies. |\n| June 7, 2025 |  active |  Project started. Added tools to cover all the functionalities of the Frankfurter API. |",
        "start_pos": 21644,
        "end_pos": 23188,
        "token_count_estimate": 385,
        "source_type": "readme",
        "agent_id": "29b0e77999c32963"
      }
    ]
  },
  {
    "agent_id": "16b022bbfb2167b4",
    "name": "ai.smithery/anirbanbasu-pymcp",
    "source": "mcp",
    "source_url": "https://github.com/anirbanbasu/pymcp",
    "description": "Primarily to be used as a template repository for developing MCP servers with FastMCP in Python, P‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T05:41:08.153835Z",
    "indexed_at": "2026-02-18T04:04:56.850831",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue?logo=python&logoColor=3776ab&labelColor=e4e4e4)](https://www.python.org/downloads/release/python-3120/) [![pytest](https://github.com/anirbanbasu/pymcp/actions/workflows/uv-pytest-coverage.yml/badge.svg)](https://github.com/anirbanbasu/pymcp/actions/workflows/uv-pytest-coverage.yml) [![PyPI](https://img.shields.io/pypi/v/pymcp-template?label=pypi%20package)](https://pypi.org/project/pymcp-template/#history) ![GitHub commits since latest release](https://img.shields.io/github/commits-since/anirbanbasu/pymcp/latest) [![CodeQL Advanced](https://github.com/anirbanbasu/pymcp/actions/workflows/codeql.yml/badge.svg)](https://github.com/anirbanbasu/pymcp/actions/workflows/codeql.yml) [![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/anirbanbasu/pymcp/badge)](https://scorecard.dev/viewer/?uri=github.com/anirbanbasu/pymcp)\n\n\n<p align=\"center\">\n  <img width=\"256\" height=\"84\" src=\"https://raw.githubusercontent.com/anirbanbasu/pymcp/master/resources/logo.svg\" alt=\"pymcp logo\" style=\"filter: invert(1)\">\n</p>\n\nPrimarily to be used as a template repository for developing MCP servers with [FastMCP](http://gofastmcp.com/) in Python, PyMCP is somewhat inspired by the [official everything MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/everything) in Typescript.\n\n# Components\n\nThe following components are available on this MCP server.\n\n## Tools\n\n1. **`greet`**\n  - Greets the caller with a quintessential Hello World message.\n  - Input(s)\n    - `name`: _`string`_ (_optional_): The name to greet. Default value is none.\n  - Output(s)\n    - `TextContent` with a UTC time-stamped greeting.\n2. **`generate_password`**\n  - Generates a random password with specified length, optionally including special characters and conforming to the complexity requirements of at least one lowercase letter, one uppercase letter, and two digits. If special characters are included, it will also contain at least one such character.\n  - Input(s)\n    - `length`: _`integer`_: The length of the generated password. The value must be an integer between 8 and 64, both inclusive.\n    - `use_special_chars`: _`boolean`_ (_optional_): A flag to indicate whether the password should include special characters. Default value is `False`.\n  - Output(s)\n    - `TextContent` with the generated password.\n3. **`text_web_search`**\n  - Searches the web with a text query using the [Dux Distributed Global Search (DDGS)](https://github.com/deedy5/ddgs).\n  - Input(s)\n    - `query`: _`string`_: The search query to fetch results for. It should be a non-empty string.\n    - `region`: _`string`_ (_optional_): Two letter country code followed by a hyphen and then by two letter language code, e.g., `uk-en` or `us-en`. Default value is `uk-en`.\n    - `max_results`: _`integer`_ (_optional_): Optional maximum number of results to be fetched. Default value is 10.\n    - `pages`: _`integer`_ (_optional_): Optional number of pages to spread the results over. Default value is 1.\n  - Environment variable(s)\n    - `DDGS_PROXY`: _`string`_ (_optional_): Optional proxy server to use for egress web search requests.\n  - Output(s)\n    - `TextContent` with a list of dictionaries with search results.\n4. **`permutations`**\n  - Calculates the number of ways to choose $k$ items from $n$ items without repetition and with order. If $k$ is not provided, it defaults to $n$.\n  - Input(s)\n    - `n`: _`integer`_: The number of items to choose from. This should be a non-zero, positive integer.\n    - `k`: _`integer`_ (_optional_): The number of items to choose. Default value is the value of `n`.\n  - Output(s)\n    - `TextContent` with number of ways to choose $k$ items from $n$, essentially ${}^{n}P_{k}$.\n5. **`run_python_code`**\n  - Runs arbitrary Python code in a secure and fast interpreter using [Pydantic Monty](https://github.com/pydantic/monty). Note that Pydantic Monty is experimental and Python language support is partial as of February 8, 2026.\n  - Input(s)\n    - `code`: _`string`_: The Python code to run.\n    - `inputs`: _`dict[str, Any]`_ (_optional_): A dictionary of input values for the Python code. Default value is `None`.\n    - `script_name`: _`str`_ (_optional_): The name of the script used in traceback and error messages. Default value is `main.py`.\n    - `check_types`: _`bool`_ (_optional_): A flag to indicate whether to check types. Default value is `True`.\n    - `type_definitions`: _`str`_ (_optional_): Type definitions to be used for type checking. Default value is `None`.\n  - Output(s)\n    - `TextContent` with the output, if any, of the Python code.\n\n6. **`pirate_summary`**\n  - Summarises the given text in a pirate style. _This tool uses LLM client sampling. Hence, a sampling handler must exist on the client-side._\n  - Input(s)\n    - `text`: _`string`_: The text to summarise.\n  - Output(s)\n    - `TextContent` with the summary of `text` in pirate speak.\n7. **`vonmises_random`**\n  - Generates a random number from the [von Mises distribution](https://reference.wolfram.com/language/ref/VonMisesDistribution.html). _This tool uses client elicitation to obtain the parameter kappa ($\\kappa$). Hence, an elicitation handler must exist on the client-side._\n  - Input(s)\n    - `mu`: _`float`_: The parameter $\\mu$ between 0 and $2\\pi$.\n  - Output(s)\n    - `TextContent` with the a random number from the von Mises distribution.\n\n## Resources\n\n1. **`resource_logo`**\n  - Retrieves the Base64 encoded PNG logo of PyMCP along with its SHA3-512 hash.\n  - URL: `data://logo`\n  - Output(s)\n    - `TextContent` with a `Base64EncodedBinaryDataResponse` Pydantic object with the following fields.\n      - `data`: _`string`_: The Base64 encoded PNG logo of PyMCP.\n      - `hash`: _`string`_: The hexadecimal encoded cryptographic hash of the raw binary data, which is represented by its Base64 encoded string equivalent in `data`. (The hex encoded hash value is expected to be _6414b58d9e44336c2629846172ec5c4008477a9c94fa572d3419c723a8b30eb4c0e2909b151fa13420aaa6a2596555b29834ac9b2baab38919c87dada7a6ef14_.)\n      - `hash_algorithm`: _`string`_: The cryptographic hash algorithm used, e.g., _sha3_512_.\n\n2. **`resource_logo_svg`**\n  - Retrieves the SVG logo of PyMCP.\n  - URL: `data://logo_svg`\n  - Output(s)\n    - `TextContent` with a the SVG data for the PyMCP logo.\n\n3. **`resource_unicode_modulo10`**\n  - Computes the modulus 10 of a given number and returns a Unicode character representing the result. The character is chosen based on whether the modulus is odd or even. For odd modulus, it uses the Unicode characters ‚ù∂ (U+2776), ‚ù∏ (U+2778), ‚ù∫ (U+277A), ‚ùº (U+277C), and ‚ùæ (U+277E). For even modulus, it uses the Unicode characters ‚ì™ (U+24EA), ‚ë° (U+2461), ‚ë£ (U+2463), ‚ë• (U+2465), and ‚ëß (U+2467).\n  - URL: `data://modulo10/{number}`\n  - Input(s)\n    - `number`: _`integer`_: A positive integer between 1 and 1000, both inclusive.\n  - Output(s)\n    - `TextContent` with a string representing the correct Unicode character.\n\n## Prompts\n\n1. **`code_prompt`**\n  - Get a prompt to write a code snippet in Python based on the specified task..\n  - Input(s)\n    - `task`: _`string`_: The description of the task for which a code implementation prompt will be generated.\n  - Output(s)\n    - `str` representing the prompt.\n\n# Installation\n\nThe directory where you clone this repository will be referred to as the _working directory_ or _WD_ hereinafter.\n\nInstall [`uv`](https://docs.astral.sh/uv/getting-started/installation/). Install [`just`](https://github.com/casey/just?tab=readme-ov-file#installation). To install the project with its minimal dependencies in a virtual environment, run the following in the _WD_. To install all non-essential dependencies (_which are required for developing and testing_), replace the `install` taget with the `install-all` target in the following command.\n\n```bash\njust install\n```\n\n# Environment variables\n\nThe following environment variables can be configured.\n\n - `PYMCP_LOG_LEVEL`: Sets the [Python log level](https://docs.python.org/3/library/logging.html#logging-levels) for the PyMCP server. Default is `INFO`.\n - `MCP_SERVER_TRANSPORT`: Sets the [FastMCP server transport](https://gofastmcp.com/deployment/running-server#transport-protocols) type of this MCP server. Default is `stdio`.\n - `RESPONSE_CACHE_TTL`: Sets the time, in seconds, for the time-to-live (TTL) cache that can be activated for caching prompt, resource and tool responses from the server. Default value is 30. Any integer value between 0 and 86400 (i.e., one day), both inclusive, is valid. Setting it to 0 effectively disables response caching.\n - `FASTMCP_HOST`: Sets the host address for the FastMCP server when using network transports (e.g., `streamable-http`, `sse`). Default is `localhost`.\n - `FASTMCP_PORT`: Sets the port number for the FastMCP server when using network transports. Default is `8000`.\n - `ASGI_CORS_ALLOWED_ORIGINS`: Sets the [CORS allowed origins](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS) when the MCP server is started with a transport over HTTP. Default is `[\"*\"]`.\n\n# Standalone usage\nPyMCP can be started standalone as a MCP server with `stdio` transport by running the following. Alternatively, it can be started using `streamable-http` or `sse` transports by specifying the transport type using the `MCP_SERVER_TRANSPORT` environment variable.\n\n```bash\nuv run pymcp\n```\n\n# Test with the MCP Inspector\n\nThe [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is an _official_ Model Context Protocol tool that can be used by developers to test and debug MCP servers. This is the most comprehensive way to explore the MCP server.\n\nTo use it, you must have Node.js installed. The best way to install and manage `node` as well as packages such as the MCP Inspector is to use the [Node Version Manager (or, `nvm`)](https://github.com/nvm-sh/nvm). Once you have `nvm` installed, you can install and use the latest Long Term Release version of `node` by executing the following.\n\n```bash\nnvm install --lts\nnvm use --lts\n```\n\nFollowing that, run the MCP Inspector and PyMCP by executing the following in the _WD_.\n\n```bash\nnpx @modelcontextprotocol/inspector uv run pymcp\n```\n\nThis will create a local URL at port 6274 with an authentication token, which you can copy and browse to on your browser. Once on the MCP Inspector UI, press _Connect_ to connect to the MCP server. Thereafter, you can explore the tools available on the server.\n\nYou can, alternatively, launch the MCP inspector by running `just launch-inspector`.\n\n# Use it with Claude Desktop, Visual Studio, and so on\n\nThe server entry to run with `stdio` transport that you can use with systems such as Claude Desktop, Visual Studio Code, and so on is as follows.\n\n```json\n{\n    \"command\": \"uv\",\n    \"args\": [\n        \"run\",\n        \"pymcp\"\n    ]\n}\n```\n\nInstead of having `pymcp` as the last item in the list of `args`, you may need to specify the full path to the script, e.g., _WD_`/.venv/bin/pymcp`.\n\n# Remotely hosted options\n\nThe currently available remotely hosted options are as follows.\n\n - FastMCP Cloud: https://pymcp-template.fastmcp.app/mcp\n - Glama.AI: https://glama.ai/mcp/servers/@anirbanbasu/pymcp\n\n# Testing and coverage\n\nTo run the provided set of tests using `pytest`, execute the following in _WD_. To get a report on coverage while invoking the tests, run the following in _WD_.\n\n```bash\njust test-coverage\n```\n\nThis will generate something like the following output.\n\n```bash\nName    Stmts   Miss    Cover   Missing\n---------------------------------------\nTOTAL     226      0  100.00%\n```\n\n# Contributing\n\nSee the [Contributing guide](CONTRIBUTING.md).\n\n# License\n\n[MIT](https://choosealicense.com/licenses/mit/).\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Greet the caller with a Hello World message",
        "Generate random passwords with configurable length and complexity",
        "Perform web searches using Dux Distributed Global Search (DDGS)",
        "Calculate permutations for choosing k items from n items",
        "Run arbitrary Python code securely using Pydantic Monty",
        "Summarise text in a pirate style using LLM client sampling",
        "Generate random numbers from the von Mises distribution with client elicitation",
        "Retrieve Base64 encoded PNG and SVG logos of PyMCP",
        "Compute modulus 10 of a number and return a corresponding Unicode character",
        "Generate code writing prompts for Python based on task descriptions"
      ],
      "limitations": [
        "Python code execution support is partial and experimental as of February 8, 2026",
        "Pirate summary tool requires a sampling handler on the client side",
        "Von Mises random number generation requires an elicitation handler on the client side",
        "Password length must be between 8 and 64 characters",
        "Web search results are limited by default to 10 results and 1 page",
        "Response caching TTL can be set between 0 and 86400 seconds only"
      ],
      "requirements": [
        "Python 3.12 or higher",
        "Installation of uv and just tools",
        "Optional environment variables for configuration such as PYMCP_LOG_LEVEL, MCP_SERVER_TRANSPORT, DDGS_PROXY",
        "Node.js and nvm for using MCP Inspector",
        "Client-side handlers for LLM sampling and elicitation for certain tools"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed descriptions of multiple tools with inputs and outputs, environment variable configurations, usage examples including standalone and inspector testing, and clearly states limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue?logo=python&logoColor=3776ab&labelColor=e4e4e4)](https://www.python.org/downloads/release/python-3120/) [![pytest](https://github.com/anirbanbasu/pymcp/actions/workflows/uv-pytest-coverage.yml/badge.svg)](https://github.com/anirbanbasu/pymcp/actions/workflows/uv-pytest-coverage.yml) [![PyPI](https://img.shields.io/pypi/v/pymcp-template?label=pypi%20package)](https://pypi.org/project/pymcp-template/#history) ![GitHub commits since latest release](https://img.shields.io/github/commits-since/anirbanbasu/pymcp/latest) [![CodeQL Advanced](https://github.com/anirbanbasu/pymcp/actions/workflows/codeql.yml/badge.svg)](https://github.com/anirbanbasu/pymcp/actions/workflows/codeql.yml) [![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/anirbanbasu/pymcp/badge)](https://scorecard.dev/viewer/?uri=github.com/anirbanbasu/pymcp)\n\n\n<p align=\"center\">\n  <img width=\"256\" height=\"84\" src=\"https://raw.githubusercontent.com/anirbanbasu/pymcp/master/resources/logo.svg\" alt=\"pymcp logo\" style=\"filter: invert(1)\">\n</p>\n\nPrimarily to be used as a template repository for developing MCP servers with [FastMCP](http://gofastmcp.com/) in Python, PyMCP is somewhat inspired by the [official everything MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/everything) in Typescript.\n\n# Components\n\nThe following components are available on this MCP server.\n\n## Tools\n\n1. **`greet`**\n  - Greets the caller with a quintessential Hello World message.\n  - Input(s)\n    - `name`: _`string`_ (_optional_): The name to greet. Default value is none.\n  - Output(s)\n    - `TextContent` with a UTC time-stamped greeting.\n2. **`generate_password`**\n  - Generates a random password with specified length, optionally including special characters and conforming to the complexity requirements of at least one lowercase letter, one uppercase letter, and two digits. If special characters are included, it will also contain at least one such character.",
        "start_pos": 0,
        "end_pos": 2023,
        "token_count_estimate": 505,
        "source_type": "readme",
        "agent_id": "16b022bbfb2167b4"
      },
      {
        "chunk_id": 1,
        "text": "conforming to the complexity requirements of at least one lowercase letter, one uppercase letter, and two digits. If special characters are included, it will also contain at least one such character.\n  - Input(s)\n    - `length`: _`integer`_: The length of the generated password. The value must be an integer between 8 and 64, both inclusive.\n    - `use_special_chars`: _`boolean`_ (_optional_): A flag to indicate whether the password should include special characters. Default value is `False`.\n  - Output(s)\n    - `TextContent` with the generated password.\n3. **`text_web_search`**\n  - Searches the web with a text query using the [Dux Distributed Global Search (DDGS)](https://github.com/deedy5/ddgs).\n  - Input(s)\n    - `query`: _`string`_: The search query to fetch results for. It should be a non-empty string.\n    - `region`: _`string`_ (_optional_): Two letter country code followed by a hyphen and then by two letter language code, e.g., `uk-en` or `us-en`. Default value is `uk-en`.\n    - `max_results`: _`integer`_ (_optional_): Optional maximum number of results to be fetched. Default value is 10.\n    - `pages`: _`integer`_ (_optional_): Optional number of pages to spread the results over. Default value is 1.\n  - Environment variable(s)\n    - `DDGS_PROXY`: _`string`_ (_optional_): Optional proxy server to use for egress web search requests.\n  - Output(s)\n    - `TextContent` with a list of dictionaries with search results.\n4. **`permutations`**\n  - Calculates the number of ways to choose $k$ items from $n$ items without repetition and with order. If $k$ is not provided, it defaults to $n$.\n  - Input(s)\n    - `n`: _`integer`_: The number of items to choose from. This should be a non-zero, positive integer.\n    - `k`: _`integer`_ (_optional_): The number of items to choose. Default value is the value of `n`.\n  - Output(s)\n    - `TextContent` with number of ways to choose $k$ items from $n$, essentially ${}^{n}P_{k}$.\n5.",
        "start_pos": 1823,
        "end_pos": 3771,
        "token_count_estimate": 486,
        "source_type": "readme",
        "agent_id": "16b022bbfb2167b4"
      },
      {
        "chunk_id": 2,
        "text": "eger`_ (_optional_): The number of items to choose. Default value is the value of `n`.\n  - Output(s)\n    - `TextContent` with number of ways to choose $k$ items from $n$, essentially ${}^{n}P_{k}$.\n5. **`run_python_code`**\n  - Runs arbitrary Python code in a secure and fast interpreter using [Pydantic Monty](https://github.com/pydantic/monty). Note that Pydantic Monty is experimental and Python language support is partial as of February 8, 2026.\n  - Input(s)\n    - `code`: _`string`_: The Python code to run.\n    - `inputs`: _`dict[str, Any]`_ (_optional_): A dictionary of input values for the Python code. Default value is `None`.\n    - `script_name`: _`str`_ (_optional_): The name of the script used in traceback and error messages. Default value is `main.py`.\n    - `check_types`: _`bool`_ (_optional_): A flag to indicate whether to check types. Default value is `True`.\n    - `type_definitions`: _`str`_ (_optional_): Type definitions to be used for type checking. Default value is `None`.\n  - Output(s)\n    - `TextContent` with the output, if any, of the Python code.\n\n6. **`pirate_summary`**\n  - Summarises the given text in a pirate style. _This tool uses LLM client sampling. Hence, a sampling handler must exist on the client-side._\n  - Input(s)\n    - `text`: _`string`_: The text to summarise.\n  - Output(s)\n    - `TextContent` with the summary of `text` in pirate speak.\n7. **`vonmises_random`**\n  - Generates a random number from the [von Mises distribution](https://reference.wolfram.com/language/ref/VonMisesDistribution.html). _This tool uses client elicitation to obtain the parameter kappa ($\\kappa$). Hence, an elicitation handler must exist on the client-side._\n  - Input(s)\n    - `mu`: _`float`_: The parameter $\\mu$ between 0 and $2\\pi$.\n  - Output(s)\n    - `TextContent` with the a random number from the von Mises distribution.\n\n## Resources\n\n1. **`resource_logo`**\n  - Retrieves the Base64 encoded PNG logo of PyMCP along with its SHA3-512 hash.",
        "start_pos": 3571,
        "end_pos": 5547,
        "token_count_estimate": 494,
        "source_type": "readme",
        "agent_id": "16b022bbfb2167b4"
      },
      {
        "chunk_id": 3,
        "text": "(s)\n    - `TextContent` with the a random number from the von Mises distribution.\n\n## Resources\n\n1. **`resource_logo`**\n  - Retrieves the Base64 encoded PNG logo of PyMCP along with its SHA3-512 hash.\n  - URL: `data://logo`\n  - Output(s)\n    - `TextContent` with a `Base64EncodedBinaryDataResponse` Pydantic object with the following fields.\n      - `data`: _`string`_: The Base64 encoded PNG logo of PyMCP.\n      - `hash`: _`string`_: The hexadecimal encoded cryptographic hash of the raw binary data, which is represented by its Base64 encoded string equivalent in `data`. (The hex encoded hash value is expected to be _6414b58d9e44336c2629846172ec5c4008477a9c94fa572d3419c723a8b30eb4c0e2909b151fa13420aaa6a2596555b29834ac9b2baab38919c87dada7a6ef14_.)\n      - `hash_algorithm`: _`string`_: The cryptographic hash algorithm used, e.g., _sha3_512_.\n\n2. **`resource_logo_svg`**\n  - Retrieves the SVG logo of PyMCP.\n  - URL: `data://logo_svg`\n  - Output(s)\n    - `TextContent` with a the SVG data for the PyMCP logo.\n\n3. **`resource_unicode_modulo10`**\n  - Computes the modulus 10 of a given number and returns a Unicode character representing the result. The character is chosen based on whether the modulus is odd or even. For odd modulus, it uses the Unicode characters ‚ù∂ (U+2776), ‚ù∏ (U+2778), ‚ù∫ (U+277A), ‚ùº (U+277C), and ‚ùæ (U+277E). For even modulus, it uses the Unicode characters ‚ì™ (U+24EA), ‚ë° (U+2461), ‚ë£ (U+2463), ‚ë• (U+2465), and ‚ëß (U+2467).\n  - URL: `data://modulo10/{number}`\n  - Input(s)\n    - `number`: _`integer`_: A positive integer between 1 and 1000, both inclusive.\n  - Output(s)\n    - `TextContent` with a string representing the correct Unicode character.\n\n## Prompts\n\n1. **`code_prompt`**\n  - Get a prompt to write a code snippet in Python based on the specified task..\n  - Input(s)\n    - `task`: _`string`_: The description of the task for which a code implementation prompt will be generated.\n  - Output(s)\n    - `str` representing the prompt.",
        "start_pos": 5347,
        "end_pos": 7310,
        "token_count_estimate": 490,
        "source_type": "readme",
        "agent_id": "16b022bbfb2167b4"
      },
      {
        "chunk_id": 4,
        "text": "on the specified task..\n  - Input(s)\n    - `task`: _`string`_: The description of the task for which a code implementation prompt will be generated.\n  - Output(s)\n    - `str` representing the prompt.\n\n# Installation\n\nThe directory where you clone this repository will be referred to as the _working directory_ or _WD_ hereinafter.\n\nInstall [`uv`](https://docs.astral.sh/uv/getting-started/installation/). Install [`just`](https://github.com/casey/just?tab=readme-ov-file#installation). To install the project with its minimal dependencies in a virtual environment, run the following in the _WD_. To install all non-essential dependencies (_which are required for developing and testing_), replace the `install` taget with the `install-all` target in the following command.\n\n```bash\njust install\n```\n\n# Environment variables\n\nThe following environment variables can be configured.\n\n - `PYMCP_LOG_LEVEL`: Sets the [Python log level](https://docs.python.org/3/library/logging.html#logging-levels) for the PyMCP server. Default is `INFO`.\n - `MCP_SERVER_TRANSPORT`: Sets the [FastMCP server transport](https://gofastmcp.com/deployment/running-server#transport-protocols) type of this MCP server. Default is `stdio`.\n - `RESPONSE_CACHE_TTL`: Sets the time, in seconds, for the time-to-live (TTL) cache that can be activated for caching prompt, resource and tool responses from the server. Default value is 30. Any integer value between 0 and 86400 (i.e., one day), both inclusive, is valid. Setting it to 0 effectively disables response caching.\n - `FASTMCP_HOST`: Sets the host address for the FastMCP server when using network transports (e.g., `streamable-http`, `sse`). Default is `localhost`.\n - `FASTMCP_PORT`: Sets the port number for the FastMCP server when using network transports. Default is `8000`.\n - `ASGI_CORS_ALLOWED_ORIGINS`: Sets the [CORS allowed origins](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS) when the MCP server is started with a transport over HTTP. Default is `[\"*\"]`.",
        "start_pos": 7110,
        "end_pos": 9122,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "16b022bbfb2167b4"
      },
      {
        "chunk_id": 5,
        "text": "SGI_CORS_ALLOWED_ORIGINS`: Sets the [CORS allowed origins](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/CORS) when the MCP server is started with a transport over HTTP. Default is `[\"*\"]`.\n\n# Standalone usage\nPyMCP can be started standalone as a MCP server with `stdio` transport by running the following. Alternatively, it can be started using `streamable-http` or `sse` transports by specifying the transport type using the `MCP_SERVER_TRANSPORT` environment variable.\n\n```bash\nuv run pymcp\n```\n\n# Test with the MCP Inspector\n\nThe [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is an _official_ Model Context Protocol tool that can be used by developers to test and debug MCP servers. This is the most comprehensive way to explore the MCP server.\n\nTo use it, you must have Node.js installed. The best way to install and manage `node` as well as packages such as the MCP Inspector is to use the [Node Version Manager (or, `nvm`)](https://github.com/nvm-sh/nvm). Once you have `nvm` installed, you can install and use the latest Long Term Release version of `node` by executing the following.\n\n```bash\nnvm install --lts\nnvm use --lts\n```\n\nFollowing that, run the MCP Inspector and PyMCP by executing the following in the _WD_.\n\n```bash\nnpx @modelcontextprotocol/inspector uv run pymcp\n```\n\nThis will create a local URL at port 6274 with an authentication token, which you can copy and browse to on your browser. Once on the MCP Inspector UI, press _Connect_ to connect to the MCP server. Thereafter, you can explore the tools available on the server.\n\nYou can, alternatively, launch the MCP inspector by running `just launch-inspector`.\n\n# Use it with Claude Desktop, Visual Studio, and so on\n\nThe server entry to run with `stdio` transport that you can use with systems such as Claude Desktop, Visual Studio Code, and so on is as follows.",
        "start_pos": 8922,
        "end_pos": 10793,
        "token_count_estimate": 467,
        "source_type": "readme",
        "agent_id": "16b022bbfb2167b4"
      },
      {
        "chunk_id": 6,
        "text": "Use it with Claude Desktop, Visual Studio, and so on\n\nThe server entry to run with `stdio` transport that you can use with systems such as Claude Desktop, Visual Studio Code, and so on is as follows.\n\n```json\n{\n    \"command\": \"uv\",\n    \"args\": [\n        \"run\",\n        \"pymcp\"\n    ]\n}\n```\n\nInstead of having `pymcp` as the last item in the list of `args`, you may need to specify the full path to the script, e.g., _WD_`/.venv/bin/pymcp`.\n\n# Remotely hosted options\n\nThe currently available remotely hosted options are as follows.\n\n - FastMCP Cloud: https://pymcp-template.fastmcp.app/mcp\n - Glama.AI: https://glama.ai/mcp/servers/@anirbanbasu/pymcp\n\n# Testing and coverage\n\nTo run the provided set of tests using `pytest`, execute the following in _WD_. To get a report on coverage while invoking the tests, run the following in _WD_.\n\n```bash\njust test-coverage\n```\n\nThis will generate something like the following output.\n\n```bash\nName    Stmts   Miss    Cover   Missing\n---------------------------------------\nTOTAL     226      0  100.00%\n```\n\n# Contributing\n\nSee the [Contributing guide](CONTRIBUTING.md).\n\n# License\n\n[MIT](https://choosealicense.com/licenses/mit/).",
        "start_pos": 10593,
        "end_pos": 11767,
        "token_count_estimate": 293,
        "source_type": "readme",
        "agent_id": "16b022bbfb2167b4"
      }
    ]
  },
  {
    "agent_id": "c5f8411cb78867cc",
    "name": "ai.smithery/arjunkmrm-ahoy",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/ahoy",
    "description": "Send friendly, personalized greetings by name. Switch to a playful pirate voice for themed salutat‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T03:11:31.224722Z",
    "indexed_at": "2026-02-18T04:04:58.839821",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# ahoy\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server built with Smithery",
        "Test server interactions interactively via playground",
        "Customize server capabilities by modifying server code"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai/account/api-keys",
        "GitHub account for repository creation and deployment",
        "Smithery account for server deployment"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation steps, usage examples, and deployment instructions but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# ahoy\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 911,
        "token_count_estimate": 227,
        "source_type": "readme",
        "agent_id": "c5f8411cb78867cc"
      }
    ]
  },
  {
    "agent_id": "231b7bd89a424b5e",
    "name": "ai.smithery/arjunkmrm-boba-tea",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@arjunkmrm/boba-tea/mcp",
    "description": "Send friendly greetings to people by name. Discover the origin story behind 'Hello, World' for qui‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-04T01:51:32.373165Z",
    "indexed_at": "2026-02-18T04:05:00.242718",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Send friendly greetings to people by name",
        "Provide the origin story behind 'Hello, World'"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of two capabilities but lacks detail, examples, or structure.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "8a58f8d683d1be34",
    "name": "ai.smithery/arjunkmrm-bobo",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@arjunkmrm/bobo/mcp",
    "description": "Send friendly, personalized greetings on command. Explore the origin of 'Hello, World' for quick c‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-19T03:28:29.606731Z",
    "indexed_at": "2026-02-18T04:05:02.407112",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Send friendly, personalized greetings on command",
        "Explore the origin of 'Hello, World' quickly"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of two capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "335132e9b412d7fd",
    "name": "ai.smithery/arjunkmrm-brave-search-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/brave-search-mcp-server",
    "description": "Search the web, images, videos, news, and local businesses with robust filters, freshness controls‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-10-02T06:39:58.418517Z",
    "indexed_at": "2026-02-18T04:05:04.279414",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing comprehensive search capabilities including web search, local business search, image search, video search, news search, and AI-powered summarization. This project supports both STDIO and HTTP transports, with STDIO as the default mode.\n\n## Migration\n\n### 1.x to 2.x\n\n#### Default transport now STDIO\n\nTo follow established MCP conventions, the server now defaults to STDIO. If you would like to continue using HTTP, you will need to set the `BRAVE_MCP_TRANSPORT` environment variable to `http`, or provide the runtime argument `--transport http` when launching the server.\n\n#### Response structure of `brave_image_search`\n\nVersion 1.x of the MCP server would return base64-encoded image data along with image URLs. This dramatically slowed down the response, as well as consumed unnecessarily context in the session. Version 2.x removes the base64-encoded data, and returns a response object that more closely reflects the original Brave Search API response. The updated output schema is defined in [`src/tools/images/schemas/output.ts`](https://github.com/brave/brave-search-mcp-server/blob/main/src/tools/images/schemas/output.ts).\n\n## Tools\n\n### Web Search (`brave_web_search`)\nPerforms comprehensive web searches with rich result types and advanced filtering options.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-20, default: 10)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n- `text_decorations` (boolean, optional): Include highlighting markers (default: true)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `result_filter` (array, optional): Filter result types (default: [\"web\", \"query\"])\n- `goggles` (array, optional): Custom re-ranking definitions\n- `units` (string, optional): Measurement units (\"metric\" or \"imperial\")\n- `extra_snippets` (boolean, optional): Get additional excerpts (Pro plans only)\n- `summary` (boolean, optional): Enable summary key generation for AI summarization\n\n### Local Search (`brave_local_search`)\nSearches for local businesses and places with detailed information including ratings, hours, and AI-generated descriptions.\n\n**Parameters:**\n- Same as `brave_web_search` with automatic location filtering\n- Automatically includes \"web\" and \"locations\" in result_filter\n\n**Note:** Requires Pro plan for full local search capabilities. Falls back to web search otherwise.\n\n### Video Search (`brave_video_search`)\nSearches for videos with comprehensive metadata and thumbnail information.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-50, default: 20)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n\n### Image Search (`brave_image_search`)\nSearches for images with automatic fetching and base64 encoding for direct display.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `count` (number, optional): Results per page (1-200, default: 50)\n- `safesearch` (string, optional): Content filtering (\"off\", \"strict\", default: \"strict\")\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n\n### News Search (`brave_news_search`)\nSearches for current news articles with freshness controls and breaking news indicators.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-50, default: 20)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (default: \"pd\" for last 24 hours)\n- `extra_snippets` (boolean, optional): Get additional excerpts (Pro plans only)\n- `goggles` (array, optional): Custom re-ranking definitions\n\n### Summarizer Search (`brave_summarizer`)\nGenerates AI-powered summaries from web search results using Brave's summarization API.\n\n**Parameters:**\n- `key` (string, required): Summary key from web search results (use `summary: true` in web search)\n- `entity_info` (boolean, optional): Include entity information (default: false)\n- `inline_references` (boolean, optional): Add source URL references (default: false)\n\n**Usage:** First perform a web search with `summary: true`, then use the returned summary key with this tool.\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan:\n   - **Free**: 2,000 queries/month, basic web search\n   - **Pro**: Enhanced features including local search, AI summaries, extra snippets\n3. Generate your API key from the [developer dashboard](https://api-dashboard.search.brave.com/app/keys)\n\n### Environment Variables\n\nThe server supports the following environment variables:\n\n- `BRAVE_API_KEY`: Your Brave Search API key (required)\n- `BRAVE_MCP_TRANSPORT`: Transport mode (\"http\" or \"stdio\", default: \"stdio\")\n- `BRAVE_MCP_PORT`: HTTP server port (default: 8080)\n- `BRAVE_MCP_HOST`: HTTP server host (default: \"0.0.0.0\")\n- `BRAVE_MCP_LOG_LEVEL`: Desired logging level(\"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", or \"emergency\", default: \"info\")\n- `BRAVE_MCP_ENABLED_TOOLS`: When used, specifies a whitelist for supported tools\n- `BRAVE_MCP_DISABLED_TOOLS`: When used, specifies a blacklist for supported tools\n\n### Command Line Options\n\n```bash\nnode dist/index.js [options]\n\nOptions:\n  --brave-api-key <string>    Brave API key\n  --transport <stdio|http>    Transport type (default: stdio)\n  --port <number>             HTTP server port (default: 8080)\n  --host <string>             HTTP server host (default: 0.0.0.0)\n  --logging-level <string>    Desired logging level (one of _debug_, _info_, _notice_, _warning_, _error_, _critical_, _alert_, or _emergency_)\n  --enabled-tools             Tools whitelist (only the specified tools will be enabled)\n  --disabled-tools            Tools blacklist (included tools will be disabled)\n```\n\n## Installation\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n#### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"BRAVE_API_KEY\", \"docker.io/mcp/brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@brave/brave-search-mcp-server\", \"--transport\", \"http\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation, use the one-click installation buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40brave%2Fbrave-search-mcp-server%22%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40brave%2Fbrave-search-mcp-server%22%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)  \n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following to your User Settings (JSON) or `.vscode/mcp.json`:\n\n#### Docker\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key\",\n    }\n  ],\n  \"servers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"BRAVE_API_KEY\", \"mcp/brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key\",\n    }\n  ],\n  \"servers\": {\n    \"brave-search-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@brave/brave-search-mcp-server\", \"--transport\", \"stdio\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n## Build\n\n### Docker\n\n```bash\ndocker build -t mcp/brave-search:latest .\n```\n\n### Local Build\n\n```bash\nnpm install\nnpm run build\n```\n\n## Development\n\n### Prerequisites\n\n- Node.js 22.x or higher\n- npm\n- Brave Search API key\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/brave/brave-search-mcp-server.git\ncd brave-search-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n### Testing via Claude Desktop\n\nAdd a reference to your local build in `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search-dev\": {\n      \"command\": \"node\",\n      \"args\": [\"C:\\\\GitHub\\\\brave-search-mcp-server\\\\dist\\\\index.js\"], // Verify your path\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Testing via MCP Inspector\n\n1. Build and start the server:\n```bash\nnpm run build\nnode dist/index.js\n```\n\n2. In another terminal, start the MCP Inspector:\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\nSTDIO is the default mode. For HTTP mode testing, add `--transport http` to the arguments in the Inspector UI.\n\n### Testing via Smithery.AI\n\n1. Establish and acquire a smithery.ai account and API key\n2. Run `npm run install`, `npm run smithery:build`, and lastly `npm run smithery:dev` to begin testing\n\n### Available Scripts\n\n- `npm run build`: Build the TypeScript project\n- `npm run watch`: Watch for changes and rebuild\n- `npm run format`: Format code with Prettier\n- `npm run format:check`: Check code formatting\n- `npm run prepare`: Format and build (runs automatically on npm install)\n\n- `npm run inspector`: Launch an instance of MCP Inspector\n- `npm run inspector:stdio`: Launch a instance of MCP Inspector, configured for STDIO\n- `npm run smithery:build`: Build the project for smithery.ai\n- `npm run smithery:dev`: Launch the development environment for smithery.ai\n\n### Docker Compose\n\nFor local development with Docker:\n\n```bash\ndocker-compose up --build\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Perform comprehensive web searches with advanced filtering and rich result types",
        "Search for local businesses and places with detailed information and AI-generated descriptions",
        "Search for videos with metadata and thumbnail information",
        "Search for images with automatic fetching and base64 encoding",
        "Search for current news articles with freshness controls and breaking news indicators",
        "Generate AI-powered summaries from web search results using Brave's summarization API",
        "Support both STDIO and HTTP transports for communication",
        "Allow customization of search parameters including language, country, safesearch, pagination, and spellcheck",
        "Enable custom re-ranking definitions via goggles parameter"
      ],
      "limitations": [
        "Local search full capabilities require a Pro plan; otherwise, it falls back to web search",
        "Extra snippets and AI summarization features require a Pro plan",
        "Pagination offset is limited to a maximum of 9",
        "Web search query length limited to 400 characters and 50 words",
        "Image search base64 encoding removed in version 2.x to improve performance",
        "Rate limits depend on the Brave Search API plan (Free: 2,000 queries/month)"
      ],
      "requirements": [
        "Brave Search API key (required)",
        "Node.js version 22.x or higher for local development",
        "npm for dependency management and build",
        "Environment variables setup for API key and optional transport configuration",
        "Optional Docker or NPX for installation and running the server"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides detailed installation instructions, comprehensive tool descriptions with parameters, usage examples, configuration options, environment requirements, and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing comprehensive search capabilities including web search, local business search, image search, video search, news search, and AI-powered summarization. This project supports both STDIO and HTTP transports, with STDIO as the default mode.\n\n## Migration\n\n### 1.x to 2.x\n\n#### Default transport now STDIO\n\nTo follow established MCP conventions, the server now defaults to STDIO. If you would like to continue using HTTP, you will need to set the `BRAVE_MCP_TRANSPORT` environment variable to `http`, or provide the runtime argument `--transport http` when launching the server.\n\n#### Response structure of `brave_image_search`\n\nVersion 1.x of the MCP server would return base64-encoded image data along with image URLs. This dramatically slowed down the response, as well as consumed unnecessarily context in the session. Version 2.x removes the base64-encoded data, and returns a response object that more closely reflects the original Brave Search API response. The updated output schema is defined in [`src/tools/images/schemas/output.ts`](https://github.com/brave/brave-search-mcp-server/blob/main/src/tools/images/schemas/output.ts).\n\n## Tools\n\n### Web Search (`brave_web_search`)\nPerforms comprehensive web searches with rich result types and advanced filtering options.",
        "start_pos": 0,
        "end_pos": 1375,
        "token_count_estimate": 343,
        "source_type": "readme",
        "agent_id": "335132e9b412d7fd"
      },
      {
        "chunk_id": 1,
        "text": "\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n- `text_decorations` (boolean, optional): Include highlighting markers (default: true)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `result_filter` (array, optional): Filter result types (default: [\"web\", \"query\"])\n- `goggles` (array, optional): Custom re-ranking definitions\n- `units` (string, optional): Measurement units (\"metric\" or \"imperial\")\n- `extra_snippets` (boolean, optional): Get additional excerpts (Pro plans only)\n- `summary` (boolean, optional): Enable summary key generation for AI summarization\n\n### Local Search (`brave_local_search`)\nSearches for local businesses and places with detailed information including ratings, hours, and AI-generated descriptions.\n\n**Parameters:**\n- Same as `brave_web_search` with automatic location filtering\n- Automatically includes \"web\" and \"locations\" in result_filter\n\n**Note:** Requires Pro plan for full local search capabilities. Falls back to web search otherwise.\n\n### Video Search (`brave_video_search`)\nSearches for videos with comprehensive metadata and thumbnail information.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-50, default: 20)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n\n### Image Search (`brave_image_search`)\nSearches for images with automatic fetching and base64 encoding for direct display.",
        "start_pos": 1848,
        "end_pos": 3853,
        "token_count_estimate": 501,
        "source_type": "readme",
        "agent_id": "335132e9b412d7fd"
      },
      {
        "chunk_id": 2,
        "text": "ss` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n\n### Image Search (`brave_image_search`)\nSearches for images with automatic fetching and base64 encoding for direct display.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `count` (number, optional): Results per page (1-200, default: 50)\n- `safesearch` (string, optional): Content filtering (\"off\", \"strict\", default: \"strict\")\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n\n### News Search (`brave_news_search`)\nSearches for current news articles with freshness controls and breaking news indicators.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-50, default: 20)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (default: \"pd\" for last 24 hours)\n- `extra_snippets` (boolean, optional): Get additional excerpts (Pro plans only)\n- `goggles` (array, optional): Custom re-ranking definitions\n\n### Summarizer Search (`brave_summarizer`)\nGenerates AI-powered summaries from web search results using Brave's summarization API.",
        "start_pos": 3653,
        "end_pos": 5374,
        "token_count_estimate": 430,
        "source_type": "readme",
        "agent_id": "335132e9b412d7fd"
      },
      {
        "chunk_id": 3,
        "text": "_info` (boolean, optional): Include entity information (default: false)\n- `inline_references` (boolean, optional): Add source URL references (default: false)\n\n**Usage:** First perform a web search with `summary: true`, then use the returned summary key with this tool.\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan:\n   - **Free**: 2,000 queries/month, basic web search\n   - **Pro**: Enhanced features including local search, AI summaries, extra snippets\n3.",
        "start_pos": 5501,
        "end_pos": 6046,
        "token_count_estimate": 136,
        "source_type": "readme",
        "agent_id": "335132e9b412d7fd"
      },
      {
        "chunk_id": 4,
        "text": "ools            Tools blacklist (included tools will be disabled)\n```\n\n## Installation\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n#### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"BRAVE_API_KEY\", \"docker.io/mcp/brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@brave/brave-search-mcp-server\", \"--transport\", \"http\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation, use the one-click installation buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40brave%2Fbrave-search-mcp-server%22%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40brave%2Fbrave-search-mcp-server%22%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)  \n[![Install with Docker in VS Code](https://img.shields.io/ba",
        "start_pos": 7349,
        "end_pos": 9397,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "335132e9b412d7fd"
      },
      {
        "chunk_id": 5,
        "text": "%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)  \n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following to your User Settings (JSON) or `.vscode/mcp.json`:\n\n#### Docker\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key\",\n    }\n  ],\n  \"servers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"BRAVE_API_KEY\", \"mcp/brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave",
        "start_pos": 9197,
        "end_pos": 11245,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "335132e9b412d7fd"
      },
      {
        "chunk_id": 6,
        "text": "\"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key\",\n    }\n  ],\n  \"servers\": {\n    \"brave-search-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@brave/brave-search-mcp-server\", \"--transport\", \"stdio\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n## Build\n\n### Docker\n\n```bash\ndocker build -t mcp/brave-search:latest .\n```\n\n### Local Build\n\n```bash\nnpm install\nnpm run build\n```\n\n## Development\n\n### Prerequisites\n\n- Node.js 22.x or higher\n- npm\n- Brave Search API key\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/brave/brave-search-mcp-server.git\ncd brave-search-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n### Testing via Claude Desktop\n\nAdd a reference to your local build in `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search-dev\": {\n      \"command\": \"node\",\n      \"args\": [\"C:\\\\GitHub\\\\brave-search-mcp-server\\\\dist\\\\index.js\"], // Verify your path\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Testing via MCP Inspector\n\n1. Build and start the server:\n```bash\nnpm run build\nnode dist/index.js\n```\n\n2. In another terminal, start the MCP Inspector:\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\nSTDIO is the default mode. For HTTP mode testing, add `--transport http` to the arguments in the Inspector UI.\n\n### Testing via Smithery.AI\n\n1. Establish and acquire a smithery.ai account and API key\n2.",
        "start_pos": 11045,
        "end_pos": 12761,
        "token_count_estimate": 429,
        "source_type": "readme",
        "agent_id": "335132e9b412d7fd"
      },
      {
        "chunk_id": 7,
        "text": "run build`: Build the TypeScript project\n- `npm run watch`: Watch for changes and rebuild\n- `npm run format`: Format code with Prettier\n- `npm run format:check`: Check code formatting\n- `npm run prepare`: Format and build (runs automatically on npm install)\n\n- `npm run inspector`: Launch an instance of MCP Inspector\n- `npm run inspector:stdio`: Launch a instance of MCP Inspector, configured for STDIO\n- `npm run smithery:build`: Build the project for smithery.ai\n- `npm run smithery:dev`: Launch the development environment for smithery.ai\n\n### Docker Compose\n\nFor local development with Docker:\n\n```bash\ndocker-compose up --build\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
        "start_pos": 12893,
        "end_pos": 13793,
        "token_count_estimate": 224,
        "source_type": "readme",
        "agent_id": "335132e9b412d7fd"
      }
    ]
  },
  {
    "agent_id": "6560690b79dbacc5",
    "name": "ai.smithery/arjunkmrm-clock",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/clock",
    "description": "Check the current time instantly and explore world timezones by region. Browse available continent‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-19T08:00:25.675646Z",
    "indexed_at": "2026-02-18T04:05:06.066333",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# time\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server built with Smithery",
        "Test server interactively using a playground",
        "Respond to simple commands such as greeting a user"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key",
        "Python environment capable of running uvicorn (implied by 'uv run dev')",
        "GitHub account for deployment"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation steps, usage examples, and deployment instructions but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# time\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 911,
        "token_count_estimate": 227,
        "source_type": "readme",
        "agent_id": "6560690b79dbacc5"
      }
    ]
  },
  {
    "agent_id": "dc725a1ed8148565",
    "name": "ai.smithery/arjunkmrm-fetch",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/fetch",
    "description": "Fetch web pages and extract exactly the content you need. Select elements with CSS and retrieve co‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-08T02:32:40.481877Z",
    "indexed_at": "2026-02-18T04:05:07.384920",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Fetch MCP Server\n\nA Model Context Protocol (MCP) server for making HTTP requests and extracting data from web pages.\n\n## Features\n\n- **Fetch URL**: Make HTTP requests and get basic page information\n- **Extract Elements**: Extract HTML elements using CSS selectors\n- **Get Page Metadata**: Extract comprehensive metadata including Open Graph tags, Twitter cards, and more\n\n## Tools\n\n### `fetch_url`\nFetch a URL and return basic information about the page.\n\n**Parameters:**\n- `url` (string): The URL to fetch\n\n**Returns:** Status code, headers, content type, title, and description\n\n### `extract_elements`\nExtract specific elements from a web page using CSS selectors.\n\n**Parameters:**\n- `url` (string): The URL to fetch\n- `selector` (string): CSS selector (e.g., 'img', '.class', '#id')\n- `attribute` (optional string): Specific attribute to extract (e.g., 'href', 'src')\n- `limit` (number, default: 10): Maximum number of elements to return\n\n**Returns:** Array of extracted elements with their attributes\n\n### `get_page_metadata`\nExtract comprehensive metadata from a web page.\n\n**Parameters:**\n- `url` (string): The URL to analyze\n\n**Returns:** Title, description, Open Graph tags, Twitter card data, canonical URL, and more\n\n## Configuration\n\n- `userAgent` (string): Custom User-Agent header (default: \"Fetch-MCP-Server/1.0\")\n- `timeout` (number): Request timeout in milliseconds (default: 10000)\n- `followRedirects` (boolean): Follow HTTP redirects (default: true)\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build for production\nnpm run build\n```\n\n## Deployment\n\nThis server is designed to be deployed on [Smithery](https://smithery.ai) as a remote MCP server.\n\n## License\n\nMIT\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Fetch URLs and return basic page information including status code, headers, content type, title, and description",
        "Extract HTML elements from web pages using CSS selectors",
        "Extract specific attributes from HTML elements such as href or src",
        "Limit the number of extracted elements returned",
        "Extract comprehensive page metadata including Open Graph tags, Twitter card data, and canonical URLs",
        "Customize HTTP request headers such as User-Agent",
        "Configure request timeout and follow HTTP redirects"
      ],
      "limitations": [],
      "requirements": [
        "Node.js environment to install dependencies and run the server",
        "Deployment on Smithery platform as a remote MCP server"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, detailed tool descriptions with parameters and return values, configuration options, and deployment guidance, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Fetch MCP Server\n\nA Model Context Protocol (MCP) server for making HTTP requests and extracting data from web pages.\n\n## Features\n\n- **Fetch URL**: Make HTTP requests and get basic page information\n- **Extract Elements**: Extract HTML elements using CSS selectors\n- **Get Page Metadata**: Extract comprehensive metadata including Open Graph tags, Twitter cards, and more\n\n## Tools\n\n### `fetch_url`\nFetch a URL and return basic information about the page.\n\n**Parameters:**\n- `url` (string): The URL to fetch\n\n**Returns:** Status code, headers, content type, title, and description\n\n### `extract_elements`\nExtract specific elements from a web page using CSS selectors.\n\n**Parameters:**\n- `url` (string): The URL to fetch\n- `selector` (string): CSS selector (e.g., 'img', '.class', '#id')\n- `attribute` (optional string): Specific attribute to extract (e.g., 'href', 'src')\n- `limit` (number, default: 10): Maximum number of elements to return\n\n**Returns:** Array of extracted elements with their attributes\n\n### `get_page_metadata`\nExtract comprehensive metadata from a web page.\n\n**Parameters:**\n- `url` (string): The URL to analyze\n\n**Returns:** Title, description, Open Graph tags, Twitter card data, canonical URL, and more\n\n## Configuration\n\n- `userAgent` (string): Custom User-Agent header (default: \"Fetch-MCP-Server/1.0\")\n- `timeout` (number): Request timeout in milliseconds (default: 10000)\n- `followRedirects` (boolean): Follow HTTP redirects (default: true)\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Build for production\nnpm run build\n```\n\n## Deployment\n\nThis server is designed to be deployed on [Smithery](https://smithery.ai) as a remote MCP server.\n\n## License\n\nMIT",
        "start_pos": 0,
        "end_pos": 1743,
        "token_count_estimate": 435,
        "source_type": "readme",
        "agent_id": "dc725a1ed8148565"
      }
    ]
  },
  {
    "agent_id": "403ebc014debd7fb",
    "name": "ai.smithery/arjunkmrm-http-test",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/arjunkmrm/http-test",
    "description": "Kickstart your setup with ready-to-run greetings and the 'Hello, World' origin story. Learn the inte",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-08T07:14:18.81443Z",
    "indexed_at": "2026-02-18T04:05:09.127245",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide ready-to-run greeting messages",
        "Offer a 'Hello, World' example for initial setup"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single incomplete sentence with minimal information about capabilities and no details on usage, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "444b36722b9850f1",
    "name": "ai.smithery/arjunkmrm-local-test2",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@arjunkmrm/local-test2/mcp",
    "description": "Send friendly greetings instantly. Learn the origin of 'Hello, World' to add a fun fact to your me‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-29T08:52:53.0348Z",
    "indexed_at": "2026-02-18T04:05:11.075099",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Send friendly greetings instantly",
        "Provide the origin of 'Hello, World' as a fun fact"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of two capabilities but lacks detail, examples, or structure.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "11ea82bed37039fd",
    "name": "ai.smithery/arjunkmrm-local001",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/time",
    "description": "Get the current time in your preferred timezone or any region you specify. Browse concise informat‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T12:44:39.189348Z",
    "indexed_at": "2026-02-18T04:05:12.896528",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# time\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server using Smithery CLI",
        "Test server interactions interactively via playground",
        "Add or update server capabilities in Python code"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai/account/api-keys",
        "Python environment capable of running uv commands",
        "GitHub account for deployment"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation steps, usage examples, and deployment instructions but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# time\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 982,
        "token_count_estimate": 245,
        "source_type": "readme",
        "agent_id": "11ea82bed37039fd"
      }
    ]
  },
  {
    "agent_id": "d6122b68a0147e38",
    "name": "ai.smithery/arjunkmrm-local01",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@arjunkmrm/local01/mcp",
    "description": "Greet people warmly or roast them with playful banter. Explore the origin of 'Hello, World' for qu‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-02T04:20:15.678667Z",
    "indexed_at": "2026-02-18T04:05:14.410143",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Greet people warmly",
        "Roast people with playful banter",
        "Explore the origin of 'Hello, World'"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functions but lacks detailed information, examples, or structure.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "e908e849d7da8f6d",
    "name": "ai.smithery/arjunkmrm-lta-mcp",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/lta-mcp",
    "description": "Provide real-time transportation data including bus arrivals, train service alerts, carpark availa‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T16:52:48.059122Z",
    "indexed_at": "2026-02-18T04:05:16.566127",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Singapore LTA MCP Server\n[![smithery badge](https://smithery.ai/badge/@arjunkmrm/mcp-sg-lta)](https://smithery.ai/server/@arjunkmrm/mcp-sg-lta)\n\nAn MCP server for Singapore's Land Transport Authority (LTA) DataMall API, providing real-time access to transportation information including bus arrivals, traffic conditions, and train service updates.\n\n### Installing via Smithery\n\nTo install Singapore LTA MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@arjunkmrm/mcp-sg-lta):\n\n```bash\nnpx -y @smithery/cli install @arjunkmrm/mcp-sg-lta --client claude\n```\n\n## Tools\n\n### bus_arrival\n\nGet real-time bus arrival information for specific bus stops.\n\n**Inputs:**\n- `busStopCode` (string, required): The unique 5-digit bus stop code\n- `serviceNo` (string, optional): Specific bus service number to filter results\n\n### station_crowding\n\nGet real-time crowding levels at MRT/LRT stations (Updates every 10 minutes).\n\n**Inputs:**\n- `trainLine` (string, required): Code of train network line\n  - Supported values: CCL, CEL, CGL, DTL, EWL, NEL, NSL, BPL, SLRT, PLRT, TEL\n\n### train_alerts\n\nGet real-time train service alerts including disruptions and shuttle services.\n\n**Inputs:** None required\n\n### carpark_availability\n\nGet real-time availability of parking lots for HDB, LTA, and URA carparks (Updates every minute).\n\n**Inputs:** None required\n\n### travel_times\n\nGet estimated travel times on expressway segments (Updates every 5 minutes).\n\n**Inputs:** None required\n\n### traffic_incidents\n\nGet current road incidents including accidents, roadworks, and heavy traffic (Updates every 2 minutes).\n\n**Inputs:** None required\n\n### station_crowd_forecast\n\nGet forecasted MRT/LRT station crowdedness levels in 30-minute intervals.\n\n**Inputs:**\n- `trainLine` (string, required): Code of train network line\n  - Supported values: CCL, CEL, CGL, DTL, EWL, NEL, NSL, BPL, SLRT, PLRT, TEL\n\n## Configuration\n\n### Getting an API Key\n\n1. Register for an account on [LTA DataMall](https://datamall.lta.gov.sg)\n2. Subscribe to the API services\n3. Obtain your API key from the account dashboard\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"lta\": {\n        \"command\": \"npx\", \n        \"args\": [\n          \"-y\", \n          \"arjunkmrm/mcp-sg-lta\"\n        ],\n        \"env\": {\n          \"LTA_API_KEY\": \"YOUR-API-KEY\"\n        }\n    }\n  }\n}\n```\n\ntest webhook - test, another, test\ntest test test\ntest test test\ntest test test\ntest test test\ntest test test\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide real-time bus arrival information for specific bus stops",
        "Provide real-time crowding levels at MRT/LRT stations",
        "Provide real-time train service alerts including disruptions and shuttle services",
        "Provide real-time availability of parking lots for HDB, LTA, and URA carparks",
        "Provide estimated travel times on expressway segments",
        "Provide current road incidents including accidents, roadworks, and heavy traffic",
        "Provide forecasted MRT/LRT station crowdedness levels in 30-minute intervals"
      ],
      "limitations": [
        "Station crowding data updates every 10 minutes",
        "Carpark availability data updates every minute",
        "Travel times data updates every 5 minutes",
        "Traffic incidents data updates every 2 minutes",
        "Station crowd forecast limited to specific train lines only"
      ],
      "requirements": [
        "Register for an account on LTA DataMall",
        "Subscribe to the LTA DataMall API services",
        "Obtain and provide an LTA API key for authentication",
        "Use with Claude Desktop configured with the MCP server and environment variable LTA_API_KEY"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, detailed tool descriptions with inputs, usage configuration, and explicit requirements, but has minimal limitations stated.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Singapore LTA MCP Server\n[![smithery badge](https://smithery.ai/badge/@arjunkmrm/mcp-sg-lta)](https://smithery.ai/server/@arjunkmrm/mcp-sg-lta)\n\nAn MCP server for Singapore's Land Transport Authority (LTA) DataMall API, providing real-time access to transportation information including bus arrivals, traffic conditions, and train service updates.\n\n### Installing via Smithery\n\nTo install Singapore LTA MCP Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@arjunkmrm/mcp-sg-lta):\n\n```bash\nnpx -y @smithery/cli install @arjunkmrm/mcp-sg-lta --client claude\n```\n\n## Tools\n\n### bus_arrival\n\nGet real-time bus arrival information for specific bus stops.\n\n**Inputs:**\n- `busStopCode` (string, required): The unique 5-digit bus stop code\n- `serviceNo` (string, optional): Specific bus service number to filter results\n\n### station_crowding\n\nGet real-time crowding levels at MRT/LRT stations (Updates every 10 minutes).\n\n**Inputs:**\n- `trainLine` (string, required): Code of train network line\n  - Supported values: CCL, CEL, CGL, DTL, EWL, NEL, NSL, BPL, SLRT, PLRT, TEL\n\n### train_alerts\n\nGet real-time train service alerts including disruptions and shuttle services.\n\n**Inputs:** None required\n\n### carpark_availability\n\nGet real-time availability of parking lots for HDB, LTA, and URA carparks (Updates every minute).\n\n**Inputs:** None required\n\n### travel_times\n\nGet estimated travel times on expressway segments (Updates every 5 minutes).\n\n**Inputs:** None required\n\n### traffic_incidents\n\nGet current road incidents including accidents, roadworks, and heavy traffic (Updates every 2 minutes).\n\n**Inputs:** None required\n\n### station_crowd_forecast\n\nGet forecasted MRT/LRT station crowdedness levels in 30-minute intervals.\n\n**Inputs:**\n- `trainLine` (string, required): Code of train network line\n  - Supported values: CCL, CEL, CGL, DTL, EWL, NEL, NSL, BPL, SLRT, PLRT, TEL\n\n## Configuration\n\n### Getting an API Key\n\n1. Register for an account on [LTA DataMall](https://datamall.lta.gov.sg)\n2.",
        "start_pos": 0,
        "end_pos": 2029,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "e908e849d7da8f6d"
      },
      {
        "chunk_id": 1,
        "text": "ne\n  - Supported values: CCL, CEL, CGL, DTL, EWL, NEL, NSL, BPL, SLRT, PLRT, TEL\n\n## Configuration\n\n### Getting an API Key\n\n1. Register for an account on [LTA DataMall](https://datamall.lta.gov.sg)\n2. Subscribe to the API services\n3. Obtain your API key from the account dashboard\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"lta\": {\n        \"command\": \"npx\", \n        \"args\": [\n          \"-y\", \n          \"arjunkmrm/mcp-sg-lta\"\n        ],\n        \"env\": {\n          \"LTA_API_KEY\": \"YOUR-API-KEY\"\n        }\n    }\n  }\n}\n```\n\ntest webhook - test, another, test\ntest test test\ntest test test\ntest test test\ntest test test\ntest test test",
        "start_pos": 1829,
        "end_pos": 2531,
        "token_count_estimate": 175,
        "source_type": "readme",
        "agent_id": "e908e849d7da8f6d"
      }
    ]
  },
  {
    "agent_id": "7a21f162cb6cac67",
    "name": "ai.smithery/arjunkmrm-mango-sago",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/mango-sago",
    "description": "Create cheerful, personalized greetings in seconds. Switch to playful pirate-speak for extra flair‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-04T02:20:25.785458Z",
    "indexed_at": "2026-02-18T04:05:18.576616",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# mango-sago\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server using Smithery CLI",
        "Test server interactions interactively via playground",
        "Add or update server capabilities in Python code"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai",
        "Python environment with Smithery CLI installed",
        "GitHub account for deployment"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "The documentation provides installation steps, usage examples, development guidance, and deployment instructions but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# mango-sago\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 988,
        "token_count_estimate": 246,
        "source_type": "readme",
        "agent_id": "7a21f162cb6cac67"
      }
    ]
  },
  {
    "agent_id": "340ad0028dde296a",
    "name": "ai.smithery/arjunkmrm-perplexity-search",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/perplexity-search",
    "description": "Enable AI assistants to perform web searches using Perplexity's Sonar Pro.",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T13:59:26.758557Z",
    "indexed_at": "2026-02-18T04:05:20.148136",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Perplexity Search MCP\n\n[![smithery badge](https://smithery.ai/badge/@arjunkmrm/perplexity-search)](https://smithery.ai/server/@arjunkmrm/perplexity-search)\n\nA simple Model Context Protocol (MCP) server for Perplexity's web search with sonar or sonar-pro.\n\n## Features\n\n- Provides a `search` tool for AI assistants to perform web searches\n- Uses Perplexity's chat completions API with the sonar/sonar-pro models\n\n## Tool: search\n\nThe server provides a `search` tool with the following input parameters:\n\n- `query` (required): The search query to perform\n- `search_recency_filter` (optional): Filter search results by recency (options: month, week, day, hour). If not specified, no time filtering is applied.\n\n## Configuration\n\n### Environment Variables\n\n- `PERPLEXITY_API_KEY`: Your Perplexity API key (required)\n\n## Response Format\n\nThe response from the `search` tool includes:\n\n- `content`: The search results content\n- `citations`: Array of citations for the information\n\n## License\n\nMIT "
    },
    "llm_extracted": {
      "capabilities": [
        "Perform web searches using Perplexity's chat completions API",
        "Filter search results by recency (month, week, day, hour)",
        "Provide search results content",
        "Provide citations for search results"
      ],
      "limitations": [],
      "requirements": [
        "Perplexity API key via PERPLEXITY_API_KEY environment variable"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "The documentation includes installation requirements, tool descriptions, input parameters, and response format but lacks usage examples and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Perplexity Search MCP\n\n[![smithery badge](https://smithery.ai/badge/@arjunkmrm/perplexity-search)](https://smithery.ai/server/@arjunkmrm/perplexity-search)\n\nA simple Model Context Protocol (MCP) server for Perplexity's web search with sonar or sonar-pro.\n\n## Features\n\n- Provides a `search` tool for AI assistants to perform web searches\n- Uses Perplexity's chat completions API with the sonar/sonar-pro models\n\n## Tool: search\n\nThe server provides a `search` tool with the following input parameters:\n\n- `query` (required): The search query to perform\n- `search_recency_filter` (optional): Filter search results by recency (options: month, week, day, hour). If not specified, no time filtering is applied.\n\n## Configuration\n\n### Environment Variables\n\n- `PERPLEXITY_API_KEY`: Your Perplexity API key (required)\n\n## Response Format\n\nThe response from the `search` tool includes:\n\n- `content`: The search results content\n- `citations`: Array of citations for the information\n\n## License\n\nMIT",
        "start_pos": 0,
        "end_pos": 993,
        "token_count_estimate": 248,
        "source_type": "readme",
        "agent_id": "340ad0028dde296a"
      }
    ]
  },
  {
    "agent_id": "9e0cebb1ff2d3565",
    "name": "ai.smithery/arjunkmrm-py-test-0",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@arjunkmrm/py-test-0/mcp",
    "description": "Send personalized greetings by name, with an optional pirate tone. Generate greeting prompts and e‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-04T02:06:51.365096Z",
    "indexed_at": "2026-02-18T04:05:21.712231",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Send personalized greetings by name",
        "Generate greeting prompts",
        "Optionally apply a pirate tone to greetings"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of greeting generation with a style option but lacks detailed structure, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "94d6410eaf867227",
    "name": "ai.smithery/arjunkmrm-scrapermcp_el",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/ScraperMcp_el",
    "description": "Extract and parse web pages into clean HTML, links, or Markdown. Handle dynamic, complex, or block‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T12:13:12.824688Z",
    "indexed_at": "2026-02-18T04:05:23.744235",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<p align=\"center\">\n  <img src=\"images/product.jpg\" alt=\"Thordata\">\n</p>\n<h1 align=\"center\" style=\"border-bottom: none;\">\n  Thordata MCP Server\n</h1>\n\n<p align=\"center\">\n  <em>Built on a 195+ country proxy network, Thordata MCP breaks through web data barriers, delivering pure, structured, globally unlimited real-time information streams to AI models</em>\n</p>\n\n<div align=\"center\">\n\n\n[![Licence](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\n<br/>\n\n\n</div>\n\n---\n\n## üìñ Overview\n\nScraperMCP server seamlessly bridges AI and web ecosystems, providing one-click access to any website worldwide, real-time JavaScript rendering, intelligent anti-crawling mechanism bypass, and outputting AI-ready structured data content.\n\n## üõ†Ô∏è MCP Tools\n\nThordata MCP supports dual-channel data acquisition through unlocker and regular proxies, fully compatible with multiple data formats including MarkDown, HTML, and Links.\n\n### Web Scraper API Tool\n\nThordata MCP provides the parse_with_ai_selectors tool, leveraging Thordata Web Scraper API to implement intelligent scraping of any website.\n\n## ‚úÖ Prerequisites\n\nBefore deployment, please ensure you have:\n\n- **Thordata Web Scraper API Account**: Visit [thordata](https://www.thordata.com/) to obtain your exclusive username and password;\n\n## üì¶ Configuration\n\n### Environment Variables\n\nThordata MCP server supports the following environment variable configurations:\n\n| Name                       | Description                                   | Default Value |\n|----------------------------|-----------------------------------------------|---------------|\n| `UNLOCKER_PROXY_LOGIN`     | Unlocker username                             |               |\n| `UNLOCKER_PROXY_PASSWORD`  | Unlocker password                             |               |\n| `UNLOCKER_PROXY_URL`       | Unlocker proxy address                        |               |\n| `DEFAULT_PROXY_LOGIN`      | Regular proxy username                        |               |\n| `DEFAULT_PROXY_PASSWORD`   | Regular proxy password                        |               |\n| `DEFAULT_PROXY_URL`        | Regular proxy address                         |               |\n\n### Using uv Configuration\n\n- Install uv package manager:\n  ```bash\n  # macOS and Linux\n  curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n  Or:\n  ```bash\n  # Windows\n  powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n\n- Use the following configuration:\n  ```json\n  {\n  \"mcpServers\": {\n    \"Scraper\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"<absolute folder path>\", # e.g., D:\\\\ScraperMcp\n        \"run\",\n        \"Scraper.py\"\n      ]\n    }\n  }\n}\n  ```\n\n### Startup Command\nfastmcp run Scraper.py:mcp\n\n### üñ•Ô∏è Manual Setup Guide\n\n#### Claude Desktop Configuration\n1. Open Claude application\n2. Navigate to **Settings ‚Üí Developer ‚Üí Edit Configuration**\n3. Add the above configuration to the `claude_desktop_config.json` file\n\n#### Cursor AI Configuration  \n1. Open Cursor editor\n2. Navigate to **Settings ‚Üí Cursor Settings ‚Üí MCP**\n3. Click **Add New Global MCP Server**\n4. Configure corresponding parameters\n\n#### Cline Configuration\n1. Open Cline settings\n2. Navigate to **MCP Server Settings ‚Üí Installed**\n3. Click **Configure MCP Server**\n4. Configure corresponding parameters\n\n### Manual Setup: Cline Settings ‚Üí MCP Server Settings ‚Üí Installed ‚Üí Click Configure MCP Server and configure corresponding parameters\n\n## üõ°Ô∏è License\n\nOpen source distribution under MIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\n## About Thordata\n\nThordata, as a market-leading web intelligence collection platform, adheres to the highest business ethics and compliance standards, empowering global enterprises to uncover data-driven business insights.\n\n<div align=\"center\">\n<sub>\n  Made by <a href=\"https://www.thordata.com/\">Thordata</a>, if MCP saves you valuable time, we invite you to give ‚≠ê support.\n</sub>\n</div>\n\n## ‚ú® Core Features\n\n<details>\n<summary><strong>Global Website Content Scraping</strong></summary>\n<br>\n\n- Supports data extraction from any URL, including complex single-page applications\n- Complete JavaScript rendering capability, ensuring perfect presentation of dynamic content\n- Flexible rendering mode selection: full JS rendering, pure HTML, or no rendering\n\n</details>\n\n<details>\n<summary><strong>Intelligent AI Data Preprocessing</strong></summary>\n<br>\n\n- Automated HTML cleaning and conversion to highly readable Markdown\n- Intelligent extraction of valid and usable links, optimizing data structure\n- Native HTML format support, maintaining data integrity\n\n</details>\n\n<details>\n<summary><strong>Global Network Barrier-Free Access</strong></summary>\n<br>\n\n- Efficiently bypasses complex anti-crawling protection systems\n- Stable scraping of high-difficulty website content\n- 195+ country IP pool automatic rotation, breaking geographical restrictions\n\n</details>\n\n<details>\n<summary><strong>Cross-Platform Flexible Deployment</strong></summary>\n<br>\n\n- Customizable rendering and parsing parameter configuration\n- Seamless integration with AI models and analysis tools\n- Full support for macOS, Windows, and Linux systems\n\n</details>\n\n---\n\n## Why Choose Thordata MCP?&nbsp;üï∏Ô∏è ‚ûú üì¶ ‚ûú ü§ñ\n\nJust tell the LLM *\"Summarize the latest discussions about MCP on Hacker News\"* and get precise answers immediately.  \nMCP (Multi-Client Protocol) handles all the tedious steps for you:\n\n| Thordata MCP Core Value                                           | Benefits for You                           |\n|-------------------------------------------------------------------|-------------------------------------------|\n| **Thordata global proxy network intelligently bypasses anti-bot detection** | Ensures access availability and identity anonymity |\n| **One-click data acquisition solution**                           | Easily handles complex single-page applications |\n| **Multi-format output support (MarkDown/HTML/Links)**             | Precisely matches your data requirements |\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Scrape content from any website worldwide including complex single-page applications",
        "Render JavaScript in real-time for accurate dynamic content extraction",
        "Bypass intelligent anti-crawling and anti-bot protection mechanisms",
        "Output structured data in multiple formats including Markdown, HTML, and Links",
        "Automatically rotate IP addresses using a 195+ country proxy network",
        "Preprocess HTML data by cleaning and converting it to readable Markdown",
        "Extract valid and usable links intelligently to optimize data structure",
        "Support flexible rendering modes: full JS rendering, pure HTML, or no rendering",
        "Integrate seamlessly with AI models and analysis tools across macOS, Windows, and Linux"
      ],
      "limitations": [
        "Requires a Thordata Web Scraper API account for operation",
        "Dependent on proxy credentials and configuration for unlocking and regular proxies",
        "No explicit mention of rate limits or maximum request volumes",
        "No stated support for non-web data sources or offline data processing"
      ],
      "requirements": [
        "Thordata Web Scraper API account with username and password",
        "Environment variables for unlocker and regular proxy credentials and URLs",
        "Installation of uv package manager for server execution",
        "Compatible operating system: macOS, Windows, or Linux",
        "Configuration within supported AI clients such as Claude Desktop, Cursor AI, or Cline"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear descriptions of tools and features, explicit prerequisites, and some limitations, making it an excellent resource.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<p align=\"center\">\n  <img src=\"images/product.jpg\" alt=\"Thordata\">\n</p>\n<h1 align=\"center\" style=\"border-bottom: none;\">\n  Thordata MCP Server\n</h1>\n\n<p align=\"center\">\n  <em>Built on a 195+ country proxy network, Thordata MCP breaks through web data barriers, delivering pure, structured, globally unlimited real-time information streams to AI models</em>\n</p>\n\n<div align=\"center\">\n\n\n[![Licence](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n\n<br/>\n\n\n</div>\n\n---\n\n## üìñ Overview\n\nScraperMCP server seamlessly bridges AI and web ecosystems, providing one-click access to any website worldwide, real-time JavaScript rendering, intelligent anti-crawling mechanism bypass, and outputting AI-ready structured data content.\n\n## üõ†Ô∏è MCP Tools\n\nThordata MCP supports dual-channel data acquisition through unlocker and regular proxies, fully compatible with multiple data formats including MarkDown, HTML, and Links.\n\n### Web Scraper API Tool\n\nThordata MCP provides the parse_with_ai_selectors tool, leveraging Thordata Web Scraper API to implement intelligent scraping of any website.",
        "start_pos": 0,
        "end_pos": 1091,
        "token_count_estimate": 272,
        "source_type": "readme",
        "agent_id": "94d6410eaf867227"
      },
      {
        "chunk_id": 1,
        "text": "ess                        |               |\n| `DEFAULT_PROXY_LOGIN`      | Regular proxy username                        |               |\n| `DEFAULT_PROXY_PASSWORD`   | Regular proxy password                        |               |\n| `DEFAULT_PROXY_URL`        | Regular proxy address                         |               |\n\n### Using uv Configuration\n\n- Install uv package manager:\n  ```bash\n  # macOS and Linux\n  curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n  Or:\n  ```bash\n  # Windows\n  powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n\n- Use the following configuration:\n  ```json\n  {\n  \"mcpServers\": {\n    \"Scraper\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"<absolute folder path>\", # e.g., D:\\\\ScraperMcp\n        \"run\",\n        \"Scraper.py\"\n      ]\n    }\n  }\n}\n  ```\n\n### Startup Command\nfastmcp run Scraper.py:mcp\n\n### üñ•Ô∏è Manual Setup Guide\n\n#### Claude Desktop Configuration\n1. Open Claude application\n2. Navigate to **Settings ‚Üí Developer ‚Üí Edit Configuration**\n3. Add the above configuration to the `claude_desktop_config.json` file\n\n#### Cursor AI Configuration  \n1. Open Cursor editor\n2. Navigate to **Settings ‚Üí Cursor Settings ‚Üí MCP**\n3. Click **Add New Global MCP Server**\n4. Configure corresponding parameters\n\n#### Cline Configuration\n1. Open Cline settings\n2. Navigate to **MCP Server Settings ‚Üí Installed**\n3. Click **Configure MCP Server**\n4. Configure corresponding parameters\n\n### Manual Setup: Cline Settings ‚Üí MCP Server Settings ‚Üí Installed ‚Üí Click Configure MCP Server and configure corresponding parameters\n\n## üõ°Ô∏è License\n\nOpen source distribution under MIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\n## About Thordata\n\nThordata, as a market-leading web intelligence collection platform, adheres to the highest business ethics and compliance standards, empowering global enterprises to uncover data-driven business insights.",
        "start_pos": 1848,
        "end_pos": 3800,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "94d6410eaf867227"
      },
      {
        "chunk_id": 2,
        "text": "data, as a market-leading web intelligence collection platform, adheres to the highest business ethics and compliance standards, empowering global enterprises to uncover data-driven business insights.\n\n<div align=\"center\">\n<sub>\n  Made by <a href=\"https://www.thordata.com/\">Thordata</a>, if MCP saves you valuable time, we invite you to give ‚≠ê support.\n</sub>\n</div>\n\n## ‚ú® Core Features\n\n<details>\n<summary><strong>Global Website Content Scraping</strong></summary>\n<br>\n\n- Supports data extraction from any URL, including complex single-page applications\n- Complete JavaScript rendering capability, ensuring perfect presentation of dynamic content\n- Flexible rendering mode selection: full JS rendering, pure HTML, or no rendering\n\n</details>\n\n<details>\n<summary><strong>Intelligent AI Data Preprocessing</strong></summary>\n<br>\n\n- Automated HTML cleaning and conversion to highly readable Markdown\n- Intelligent extraction of valid and usable links, optimizing data structure\n- Native HTML format support, maintaining data integrity\n\n</details>\n\n<details>\n<summary><strong>Global Network Barrier-Free Access</strong></summary>\n<br>\n\n- Efficiently bypasses complex anti-crawling protection systems\n- Stable scraping of high-difficulty website content\n- 195+ country IP pool automatic rotation, breaking geographical restrictions\n\n</details>\n\n<details>\n<summary><strong>Cross-Platform Flexible Deployment</strong></summary>\n<br>\n\n- Customizable rendering and parsing parameter configuration\n- Seamless integration with AI models and analysis tools\n- Full support for macOS, Windows, and Linux systems\n\n</details>\n\n---\n\n## Why Choose Thordata MCP?&nbsp;üï∏Ô∏è ‚ûú üì¶ ‚ûú ü§ñ\n\nJust tell the LLM *\"Summarize the latest discussions about MCP on Hacker News\"* and get precise answers immediately.",
        "start_pos": 3600,
        "end_pos": 5381,
        "token_count_estimate": 445,
        "source_type": "readme",
        "agent_id": "94d6410eaf867227"
      },
      {
        "chunk_id": 3,
        "text": "u:\n\n| Thordata MCP Core Value                                           | Benefits for You                           |\n|-------------------------------------------------------------------|-------------------------------------------|\n| **Thordata global proxy network intelligently bypasses anti-bot detection** | Ensures access availability and identity anonymity |\n| **One-click data acquisition solution**                           | Easily handles complex single-page applications |\n| **Multi-format output support (MarkDown/HTML/Links)**             | Precisely matches your data requirements |",
        "start_pos": 5448,
        "end_pos": 6047,
        "token_count_estimate": 149,
        "source_type": "readme",
        "agent_id": "94d6410eaf867227"
      }
    ]
  },
  {
    "agent_id": "199fada6c3aeba16",
    "name": "ai.smithery/arjunkmrm-sg-bus-test",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/sg-bus-test",
    "description": "Get real-time bus arrival times for any Singapore bus stop by code, with optional service filterin‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T08:54:49.020099Z",
    "indexed_at": "2026-02-18T04:05:25.047751",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Singapore Bus Arrival MCP Server\n\nAn MCP (Model Context Protocol) server that provides real-time bus arrival information for Singapore using the LTA DataMall API.\n\n## Features\n\n- Get real-time bus arrival times for any Singapore bus stop\n- Filter by specific bus service number (optional)\n- Shows next 3 buses with:\n  - Estimated arrival time in minutes\n  - Bus capacity status (Seats/Standing Available/Limited Standing)\n  - Bus type (Single/Double Deck/Bendy)\n  - Wheelchair accessibility\n  - Operator information\n\n## Prerequisites\n\n1. **LTA DataMall API Key**: You need to register for a free API key from LTA DataMall:\n   - Go to [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html)\n   - Create an account and request for API access\n   - Copy your API key (AccountKey)\n\n2. **Node.js**: Version 18 or higher\n\n## Installation\n\n```bash\n# Install dependencies\nnpm install\n```\n\n## Usage\n\n### Development Mode\n\n```bash\nnpm run dev\n```\n\nThe server will start on `http://localhost:3000`\n\n### Testing the Tool\n\nYou can test the bus arrival tool using curl:\n\n```bash\n# Initialize the connection with your API key\ncurl -X POST \"http://127.0.0.1:3000/mcp?ltaApiKey=YOUR_API_KEY_HERE&debug=false\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"2024-11-05\",\"capabilities\":{\"tools\":{}},\"clientInfo\":{\"name\":\"test-client\",\"version\":\"1.0.0\"}}}'\n\n# Send initialized notification\ncurl -X POST \"http://127.0.0.1:3000/mcp?ltaApiKey=YOUR_API_KEY_HERE\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"method\":\"notifications/initialized\"}'\n\n# Get bus arrival for a specific bus stop (e.g., 83139)\ncurl -X POST \"http://127.0.0.1:3000/mcp?ltaApiKey=YOUR_API_KEY_HERE\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":5,\"method\":\"tools/call\",\"params\":{\"name\":\"get-bus-arrival\",\"arguments\":{\"busStopCode\":\"83139\"}}}'\n\n# Get arrival for a specific bus service at a bus stop\ncurl -X POST \"http://127.0.0.1:3000/mcp?ltaApiKey=YOUR_API_KEY_HERE\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":6,\"method\":\"tools/call\",\"params\":{\"name\":\"get-bus-arrival\",\"arguments\":{\"busStopCode\":\"83139\",\"serviceNo\":\"15\"}}}'\n```\n\nReplace `YOUR_API_KEY_HERE` with your actual LTA DataMall API key.\n\n## Configuration\n\nThe server requires the following configuration:\n\n- `ltaApiKey` (required): Your LTA DataMall API key (AccountKey)\n- `debug` (optional, default: false): Enable debug logging\n\n## API Reference\n\n### Tool: `get-bus-arrival`\n\nGet real-time bus arrival information for a specific bus stop.\n\n**Parameters:**\n- `busStopCode` (required): Bus stop reference code (e.g., \"83139\")\n- `serviceNo` (optional): Specific bus service number to filter (e.g., \"15\")\n\n**Example Response:**\n```\nüìç Bus Stop: 83139\n‚è∞ Updated: 10/7/2025, 2:30:00 PM\n\nüöå Service 15 (GAS)\nNext: 3 min üü¢ (SEA) | Single Deck ‚ôø\n2nd: 8 min üü° (SDA) | Double Deck\n3rd: 15 min üü¢ (SEA) | Single Deck\n\nüöå Service 175 (SMRT)\nNext: 5 min üü¢ (SEA) | Double Deck\n2nd: 12 min üü¢ (SEA) | Double Deck\n3rd: No data\n\nüü¢ Seats Available | üü° Standing Available | üî¥ Limited Standing | ‚ôø Wheelchair Accessible\n```\n\n## Finding Bus Stop Codes\n\nTo find bus stop codes:\n1. Use the LTA DataMall Bus Stops API: `https://datamall2.mytransport.sg/ltaodataservice/BusStops`\n2. Check physical bus stop signs - the code is usually displayed\n3. Use third-party apps like SG BusLeh or transit apps\n\n## Deployment\n\n### Deploy to Smithery\n\n1. Push your code to GitHub\n2. Go to [smithery.ai/new](https://smithery.ai/new)\n3. Connect your repository\n4. Smithery will handle the deployment\n\n### Build for Production\n\n```bash\nnpm run build\n```\n\n## Data Update Frequency\n\nThe LTA DataMall Bus Arrival API updates every 20 seconds with real-time information.\n\n## Operators\n\n- **SBST**: SBS Transit\n- **SMRT**: SMRT Corporation\n- **TTS**: Tower Transit Singapore\n- **GAS**: Go-Ahead Singapore\n\n## License\n\nISC\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide real-time bus arrival times for any Singapore bus stop",
        "Filter bus arrival information by specific bus service number",
        "Display next three buses with estimated arrival times in minutes",
        "Show bus capacity status including Seats Available, Standing Available, and Limited Standing",
        "Indicate bus type such as Single Deck, Double Deck, or Bendy",
        "Report wheelchair accessibility status of buses",
        "Provide operator information for each bus service"
      ],
      "limitations": [
        "Requires a valid LTA DataMall API key for operation",
        "Only supports bus stops and services within Singapore",
        "Data updates are limited to the frequency of the LTA DataMall Bus Arrival API (every 20 seconds)",
        "No offline or cached data support mentioned",
        "No support for other transport modes beyond buses"
      ],
      "requirements": [
        "LTA DataMall API key (AccountKey) obtained from LTA DataMall",
        "Node.js version 18 or higher installed",
        "Internet access to connect to LTA DataMall API",
        "Ability to run npm commands for installation and development"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples with curl commands, detailed tool descriptions, configuration requirements, limitations on data update frequency, and deployment guidance.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Singapore Bus Arrival MCP Server\n\nAn MCP (Model Context Protocol) server that provides real-time bus arrival information for Singapore using the LTA DataMall API.\n\n## Features\n\n- Get real-time bus arrival times for any Singapore bus stop\n- Filter by specific bus service number (optional)\n- Shows next 3 buses with:\n  - Estimated arrival time in minutes\n  - Bus capacity status (Seats/Standing Available/Limited Standing)\n  - Bus type (Single/Double Deck/Bendy)\n  - Wheelchair accessibility\n  - Operator information\n\n## Prerequisites\n\n1. **LTA DataMall API Key**: You need to register for a free API key from LTA DataMall:\n   - Go to [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html)\n   - Create an account and request for API access\n   - Copy your API key (AccountKey)\n\n2.",
        "start_pos": 0,
        "end_pos": 794,
        "token_count_estimate": 198,
        "source_type": "readme",
        "agent_id": "199fada6c3aeba16"
      },
      {
        "chunk_id": 1,
        "text": "R_API_KEY_HERE\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":5,\"method\":\"tools/call\",\"params\":{\"name\":\"get-bus-arrival\",\"arguments\":{\"busStopCode\":\"83139\"}}}'\n\n# Get arrival for a specific bus service at a bus stop\ncurl -X POST \"http://127.0.0.1:3000/mcp?ltaApiKey=YOUR_API_KEY_HERE\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":6,\"method\":\"tools/call\",\"params\":{\"name\":\"get-bus-arrival\",\"arguments\":{\"busStopCode\":\"83139\",\"serviceNo\":\"15\"}}}'\n```\n\nReplace `YOUR_API_KEY_HERE` with your actual LTA DataMall API key.\n\n## Configuration\n\nThe server requires the following configuration:\n\n- `ltaApiKey` (required): Your LTA DataMall API key (AccountKey)\n- `debug` (optional, default: false): Enable debug logging\n\n## API Reference\n\n### Tool: `get-bus-arrival`\n\nGet real-time bus arrival information for a specific bus stop.\n\n**Parameters:**\n- `busStopCode` (required): Bus stop reference code (e.g., \"83139\")\n- `serviceNo` (optional): Specific bus service number to filter (e.g., \"15\")\n\n**Example Response:**\n```\nüìç Bus Stop: 83139\n‚è∞ Updated: 10/7/2025, 2:30:00 PM\n\nüöå Service 15 (GAS)\nNext: 3 min üü¢ (SEA) | Single Deck ‚ôø\n2nd: 8 min üü° (SDA) | Double Deck\n3rd: 15 min üü¢ (SEA) | Single Deck\n\nüöå Service 175 (SMRT)\nNext: 5 min üü¢ (SEA) | Double Deck\n2nd: 12 min üü¢ (SEA) | Double Deck\n3rd: No data\n\nüü¢ Seats Available | üü° Standing Available | üî¥ Limited Standing | ‚ôø Wheelchair Accessible\n```\n\n## Finding Bus Stop Codes\n\nTo find bus stop codes:\n1. Use the LTA DataMall Bus Stops API: `https://datamall2.mytransport.sg/ltaodataservice/BusStops`\n2. Check physical bus stop signs - the code is usually displayed\n3. Use third-party apps like SG BusLeh or transit apps\n\n## Deployment\n\n### Deploy to Smithery\n\n1. Push your code to GitHub\n2. Go to [smithery.ai/new](https://smithery.ai/new)\n3. Connect your repository\n4.",
        "start_pos": 1848,
        "end_pos": 3803,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "199fada6c3aeba16"
      },
      {
        "chunk_id": 2,
        "text": "se third-party apps like SG BusLeh or transit apps\n\n## Deployment\n\n### Deploy to Smithery\n\n1. Push your code to GitHub\n2. Go to [smithery.ai/new](https://smithery.ai/new)\n3. Connect your repository\n4. Smithery will handle the deployment\n\n### Build for Production\n\n```bash\nnpm run build\n```\n\n## Data Update Frequency\n\nThe LTA DataMall Bus Arrival API updates every 20 seconds with real-time information.\n\n## Operators\n\n- **SBST**: SBS Transit\n- **SMRT**: SMRT Corporation\n- **TTS**: Tower Transit Singapore\n- **GAS**: Go-Ahead Singapore\n\n## License\n\nISC",
        "start_pos": 3603,
        "end_pos": 4157,
        "token_count_estimate": 138,
        "source_type": "readme",
        "agent_id": "199fada6c3aeba16"
      }
    ]
  },
  {
    "agent_id": "cc4d7540395a0342",
    "name": "ai.smithery/arjunkmrm-test2",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@arjunkmrm/test2/mcp",
    "description": "Greet anyone by name with a friendly message. Explore the origin of 'Hello, World' to add context‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-29T10:33:30.587409Z",
    "indexed_at": "2026-02-18T04:05:26.997192",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Greet anyone by name with a friendly message",
        "Explore the origin of 'Hello, World' to add context"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functionality but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "1c0a0236b656c186",
    "name": "ai.smithery/arjunkmrm-ts-test-2",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@arjunkmrm/ts-test-2/mcp",
    "description": "Greet anyone with a friendly, personalized hello. Explore the origin story of 'Hello, World.' Jump‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-04T02:01:26.06959Z",
    "indexed_at": "2026-02-18T04:05:29.624401",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Greet anyone with a friendly, personalized hello",
        "Provide information about the origin story of 'Hello, World'"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of greeting functionality and a thematic exploration but lacks detailed structure, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b94cbb979eb45644",
    "name": "ai.smithery/arjunkmrm-tutorials",
    "source": "mcp",
    "source_url": "https://github.com/arjunkmrm/tutorials",
    "description": "Analyze stocks and SEC filings to surface key insights, from price and volume to insider activity‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "analyze"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T12:55:16.098975Z",
    "indexed_at": "2026-02-18T04:05:32.093904",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# AI Agent Tutorials & Implementations\n\nA comprehensive collection of production-ready AI agent implementations showcasing different frameworks, protocols, and integration patterns. This repository demonstrates various approaches to building intelligent agents with Model Context Protocol (MCP), multi-agent systems, and real-world integrations.\n\n## Repository Overview\n\nThis repository contains four distinct agent implementations, each demonstrating different architectural patterns and use cases:\n\n| Project | Framework | Key Features | Use Case |\n|---------|-----------|--------------|----------|\n| [agent2agent](#agent2agent) | LangGraph + A2A Protocol | Remote agent communication, Slack integration | Investment research |\n| [mcp-financial](#mcp-financial) | FastMCP + FastAPI | ASGI integration, CLI client | Financial data analysis |\n| [zapier-mcps](#zapier-mcps) | OpenAI Agent SDK | Multi-agent handoffs, Zapier integration | Sales operations automation |\n| [bright-mcp-server-overview](#bright-mcp-server-overview) | Dual: LangGraph + ADK | Memory persistence, extended timeouts | Web scraping & research |\n\n## Project Descriptions\n\n### agent2agent/\n**Investment Research Analyst Agent**\n\nA production-ready investment research agent implementing Google's Agent-to-Agent (A2A) protocol for remote agent communication.\n\n**Key Features:**\n- **Framework**: LangGraph with LangChain\n- **Protocol**: Agent-to-Agent (A2A) for remote communication\n- **Integration**: Slack with Block Kit UI and metadata modals\n- **Architecture**: FastAPI server exposing both A2A endpoints and Slack events\n- **Memory**: Persistent conversation state management\n- **Deployment**: Docker ready with Render.com configuration\n\n**Technical Stack:**\n- LangGraph for agent orchestration\n- FastAPI for A2A protocol implementation\n- Slack Block Kit for interactive UI\n- LangSmith for observability (optional)\n- Docker for containerized deployment\n\n**Use Cases:**\n- Stock summaries and analysis\n- SEC filings research\n- Analyst recommendations\n- Financial data aggregation\n- Investment research workflows\n\n### mcp-financial/\n**Investment Analyst MCP Agent**\n\nA financial data agent powered by FastMCP with ASGI integration, providing both CLI and Slack interfaces.\n\n**Key Features:**\n- **Framework**: FastMCP with FastAPI ASGI integration\n- **Interfaces**: CLI client and Slack bot\n- **Architecture**: MCP server exposed via FastAPI endpoints\n- **Integration**: Direct Slack event handling\n- **Deployment**: Production-ready with health checks\n\n**Technical Stack:**\n- FastMCP for Model Context Protocol implementation\n- FastAPI for ASGI integration\n- Uvicorn for server runtime\n- Slack API for bot functionality\n- MCP Inspector for debugging\n\n**Use Cases:**\n- Financial data analysis\n- Stock price monitoring\n- Earnings analysis\n- Market research\n- Investment insights\n\n### zapier-mcps/\n**Multi-Agent Sales Operations System**\n\nA sophisticated multi-agent system using OpenAI's Agent SDK with Zapier MCP integration for sales automation.\n\n**Key Features:**\n- **Framework**: OpenAI Agent SDK\n- **Architecture**: Multi-agent with intelligent triage\n- **Integration**: Zapier MCP for workflow automation\n- **Agents**: Account Planning Agent, Scheduling Agent, Triage Agent\n- **Handoffs**: Automatic agent delegation based on task type\n\n**Technical Stack:**\n- OpenAI Agent SDK for agent orchestration\n- Zapier MCP for external service integration\n- Pydantic for data validation\n- Async agent execution with Runner\n\n**Agent Roles:**\n- **Triage Agent**: Determines optimal agent for task delegation\n- **Account Planning Agent**: Specializes in account analysis and planning\n- **Scheduling Agent**: Handles meeting scheduling via Google Calendar\n\n**Use Cases:**\n- Sales operations automation\n- Account planning and analysis\n- Meeting scheduling coordination\n- Workflow orchestration\n- Multi-agent task delegation\n\n### bright-mcp-server-overview/\n**Bright Data MCP Research Agent**\n\nA comprehensive research agent powered by Bright Data's web scraping infrastructure, featuring dual AI agent implementations.\n\n**Key Features:**\n- **Dual Framework**: LangGraph (with memory) + Google ADK (with extended timeouts)\n- **Integration**: Bright Data MCP server for web scraping\n- **Slack Interface**: Interactive agent selection via dropdown\n- **Memory**: Persistent conversation memory (LangGraph)\n- **Timeouts**: Extended timeout handling (ADK) for long operations\n- **Specialization**: SEO research, e-commerce intelligence, market analysis\n\n**Technical Stack:**\n- **LangGraph Agent**: OpenAI GPT with MemorySaver checkpointer\n- **ADK Agent**: Google Gemini 2.0 Flash with custom timeout patches\n- **MCP Integration**: Bright Data MCP server for data collection\n- **Slack Integration**: Bot with agent selection and interactive UI\n\n**Agent Comparison:**\n| Feature | LangGraph Agent | ADK Agent |\n|---------|----------------|-----------|\n| Memory | Persistent (checkpointer) | Context-aware (5 messages) |\n| Timeout | Standard (5s) | Extended (60s) |\n| Model | OpenAI GPT | Gemini 2.0 Flash |\n| Best For | Interactive conversations | Long-running operations |\n\n**Use Cases:**\n- SEO keyword research and SERP analysis\n- E-commerce product monitoring and price tracking\n- Competitor analysis and market intelligence\n- Web scraping and data collection\n- Business intelligence and insights\n\n## Getting Started\n\nEach project includes comprehensive setup instructions in its respective README file. General prerequisites include:\n\n### Common Requirements\n- Python 3.9+\n- Valid API keys for respective services\n- Slack workspace access (for Slack integrations)\n- Environment variable configuration\n\n### Quick Start Pattern\n```bash\n# 1. Navigate to desired project\ncd [project-name]/\n\n# 2. Install dependencies\npip install -r requirements.txt\n\n# 3. Configure environment\ncp .env.example .env\n# Edit .env with your API keys\n\n# 4. Run the agent\n# (varies by project - see individual READMEs)\n```\n\n## Architecture Patterns\n\n### Model Context Protocol (MCP)\nThree projects demonstrate different MCP implementation patterns:\n- **FastMCP ASGI**: Direct FastAPI integration\n- **Bright Data MCP**: External MCP server communication\n- **Zapier MCP**: Third-party service integration\n\n### Agent Communication\n- **A2A Protocol**: Remote agent-to-agent communication\n- **Multi-Agent Handoffs**: Intelligent task delegation\n- **State Management**: Persistent conversation memory\n\n### Integration Patterns\n- **Slack Bots**: Event-driven chat interfaces\n- **CLI Clients**: Command-line agent interaction\n- **FastAPI Servers**: RESTful agent endpoints\n- **Container Deployment**: Docker and cloud-ready\n\n## Contributing\n\nEach project welcomes contributions. Please:\n\n1. Fork the repository\n2. Create a feature branch\n3. Follow the project's coding standards\n4. Include tests where applicable\n5. Submit a Pull Request\n\n## License\n\nMIT License - see individual project LICENSE files for details.\n\n## Support & Resources\n\n### Documentation Links\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n- [OpenAI Agent SDK](https://github.com/openai/agent-sdk)\n- [Google ADK](https://developers.google.com/ai/adk)\n- [Slack API](https://api.slack.com/)\n\n### Platform-Specific Support\n- **Bright Data**: [brightdata.com/support](https://brightdata.com/support)\n- **Zapier**: [zapier.com/help](https://zapier.com/help)\n- **Slack**: [api.slack.com/support](https://api.slack.com/support)\n\n---\n\n**Built with ‚ù§Ô∏è demonstrating the future of AI agent development**\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Implement remote agent-to-agent communication using the A2A protocol",
        "Integrate AI agents with Slack for interactive chat interfaces",
        "Deploy MCP servers with FastAPI and ASGI integration",
        "Support multi-agent systems with intelligent task delegation and handoffs",
        "Perform financial data analysis and investment research workflows",
        "Automate sales operations with Zapier MCP integration",
        "Conduct web scraping and market intelligence using Bright Data MCP server",
        "Maintain persistent conversation memory and state management",
        "Provide CLI clients for command-line interaction with agents",
        "Deploy containerized AI agents with Docker and cloud-ready configurations"
      ],
      "limitations": [
        "Standard timeout of 5 seconds for LangGraph agents limits long-running operations",
        "Extended timeout (60 seconds) only available with ADK agents for long operations",
        "Requires valid API keys and Slack workspace access for full functionality",
        "Individual projects have distinct environment and deployment requirements",
        "No explicit mention of support for non-English languages or non-Slack chat platforms"
      ],
      "requirements": [
        "Python 3.9 or higher",
        "Valid API keys for respective AI and integration services (OpenAI, Google Gemini, Bright Data, Slack, Zapier)",
        "Slack workspace access for Slack bot integrations",
        "Environment variables configured per project (.env files)",
        "Docker for containerized deployment (optional but recommended)",
        "Network access for external MCP servers and third-party integrations"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples across multiple projects, clear descriptions of capabilities and architecture patterns, explicit requirements, and known limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# AI Agent Tutorials & Implementations\n\nA comprehensive collection of production-ready AI agent implementations showcasing different frameworks, protocols, and integration patterns. This repository demonstrates various approaches to building intelligent agents with Model Context Protocol (MCP), multi-agent systems, and real-world integrations.\n\n## Repository Overview\n\nThis repository contains four distinct agent implementations, each demonstrating different architectural patterns and use cases:\n\n| Project | Framework | Key Features | Use Case |\n|---------|-----------|--------------|----------|\n| [agent2agent](#agent2agent) | LangGraph + A2A Protocol | Remote agent communication, Slack integration | Investment research |\n| [mcp-financial](#mcp-financial) | FastMCP + FastAPI | ASGI integration, CLI client | Financial data analysis |\n| [zapier-mcps](#zapier-mcps) | OpenAI Agent SDK | Multi-agent handoffs, Zapier integration | Sales operations automation |\n| [bright-mcp-server-overview](#bright-mcp-server-overview) | Dual: LangGraph + ADK | Memory persistence, extended timeouts | Web scraping & research |\n\n## Project Descriptions\n\n### agent2agent/\n**Investment Research Analyst Agent**\n\nA production-ready investment research agent implementing Google's Agent-to-Agent (A2A) protocol for remote agent communication.",
        "start_pos": 0,
        "end_pos": 1329,
        "token_count_estimate": 332,
        "source_type": "readme",
        "agent_id": "b94cbb979eb45644"
      },
      {
        "chunk_id": 1,
        "text": "- LangSmith for observability (optional)\n- Docker for containerized deployment\n\n**Use Cases:**\n- Stock summaries and analysis\n- SEC filings research\n- Analyst recommendations\n- Financial data aggregation\n- Investment research workflows\n\n### mcp-financial/\n**Investment Analyst MCP Agent**\n\nA financial data agent powered by FastMCP with ASGI integration, providing both CLI and Slack interfaces.\n\n**Key Features:**\n- **Framework**: FastMCP with FastAPI ASGI integration\n- **Interfaces**: CLI client and Slack bot\n- **Architecture**: MCP server exposed via FastAPI endpoints\n- **Integration**: Direct Slack event handling\n- **Deployment**: Production-ready with health checks\n\n**Technical Stack:**\n- FastMCP for Model Context Protocol implementation\n- FastAPI for ASGI integration\n- Uvicorn for server runtime\n- Slack API for bot functionality\n- MCP Inspector for debugging\n\n**Use Cases:**\n- Financial data analysis\n- Stock price monitoring\n- Earnings analysis\n- Market research\n- Investment insights\n\n### zapier-mcps/\n**Multi-Agent Sales Operations System**\n\nA sophisticated multi-agent system using OpenAI's Agent SDK with Zapier MCP integration for sales automation.",
        "start_pos": 1848,
        "end_pos": 3017,
        "token_count_estimate": 292,
        "source_type": "readme",
        "agent_id": "b94cbb979eb45644"
      },
      {
        "chunk_id": 2,
        "text": "ng via Google Calendar\n\n**Use Cases:**\n- Sales operations automation\n- Account planning and analysis\n- Meeting scheduling coordination\n- Workflow orchestration\n- Multi-agent task delegation\n\n### bright-mcp-server-overview/\n**Bright Data MCP Research Agent**\n\nA comprehensive research agent powered by Bright Data's web scraping infrastructure, featuring dual AI agent implementations.\n\n**Key Features:**\n- **Dual Framework**: LangGraph (with memory) + Google ADK (with extended timeouts)\n- **Integration**: Bright Data MCP server for web scraping\n- **Slack Interface**: Interactive agent selection via dropdown\n- **Memory**: Persistent conversation memory (LangGraph)\n- **Timeouts**: Extended timeout handling (ADK) for long operations\n- **Specialization**: SEO research, e-commerce intelligence, market analysis\n\n**Technical Stack:**\n- **LangGraph Agent**: OpenAI GPT with MemorySaver checkpointer\n- **ADK Agent**: Google Gemini 2.0 Flash with custom timeout patches\n- **MCP Integration**: Bright Data MCP server for data collection\n- **Slack Integration**: Bot with agent selection and interactive UI\n\n**Agent Comparison:**\n| Feature | LangGraph Agent | ADK Agent |\n|---------|----------------|-----------|\n| Memory | Persistent (checkpointer) | Context-aware (5 messages) |\n| Timeout | Standard (5s) | Extended (60s) |\n| Model | OpenAI GPT | Gemini 2.0 Flash |\n| Best For | Interactive conversations | Long-running operations |\n\n**Use Cases:**\n- SEO keyword research and SERP analysis\n- E-commerce product monitoring and price tracking\n- Competitor analysis and market intelligence\n- Web scraping and data collection\n- Business intelligence and insights\n\n## Getting Started\n\nEach project includes comprehensive setup instructions in its respective README file. General prerequisites include:\n\n### Common Requirements\n- Python 3.9+\n- Valid API keys for respective services\n- Slack workspace access (for Slack integrations)\n- Environment variable configuration\n\n### Quick Start Pattern\n```bash\n# 1.",
        "start_pos": 3696,
        "end_pos": 5695,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "b94cbb979eb45644"
      },
      {
        "chunk_id": 3,
        "text": "Common Requirements\n- Python 3.9+\n- Valid API keys for respective services\n- Slack workspace access (for Slack integrations)\n- Environment variable configuration\n\n### Quick Start Pattern\n```bash\n# 1. Navigate to desired project\ncd [project-name]/\n\n# 2. Install dependencies\npip install -r requirements.txt\n\n# 3. Configure environment\ncp .env.example .env\n# Edit .env with your API keys\n\n# 4. Run the agent\n# (varies by project - see individual READMEs)\n```\n\n## Architecture Patterns\n\n### Model Context Protocol (MCP)\nThree projects demonstrate different MCP implementation patterns:\n- **FastMCP ASGI**: Direct FastAPI integration\n- **Bright Data MCP**: External MCP server communication\n- **Zapier MCP**: Third-party service integration\n\n### Agent Communication\n- **A2A Protocol**: Remote agent-to-agent communication\n- **Multi-Agent Handoffs**: Intelligent task delegation\n- **State Management**: Persistent conversation memory\n\n### Integration Patterns\n- **Slack Bots**: Event-driven chat interfaces\n- **CLI Clients**: Command-line agent interaction\n- **FastAPI Servers**: RESTful agent endpoints\n- **Container Deployment**: Docker and cloud-ready\n\n## Contributing\n\nEach project welcomes contributions. Please:\n\n1. Fork the repository\n2. Create a feature branch\n3. Follow the project's coding standards\n4. Include tests where applicable\n5. Submit a Pull Request\n\n## License\n\nMIT License - see individual project LICENSE files for details.",
        "start_pos": 5495,
        "end_pos": 6936,
        "token_count_estimate": 360,
        "source_type": "readme",
        "agent_id": "b94cbb979eb45644"
      },
      {
        "chunk_id": 4,
        "text": "ttps://brightdata.com/support)\n- **Zapier**: [zapier.com/help](https://zapier.com/help)\n- **Slack**: [api.slack.com/support](https://api.slack.com/support)\n\n---\n\n**Built with ‚ù§Ô∏è demonstrating the future of AI agent development**",
        "start_pos": 7343,
        "end_pos": 7572,
        "token_count_estimate": 57,
        "source_type": "readme",
        "agent_id": "b94cbb979eb45644"
      }
    ]
  },
  {
    "agent_id": "073e253f23430880",
    "name": "ai.smithery/aryankeluskar-poke-video-mcp",
    "source": "mcp",
    "source_url": "https://github.com/aryankeluskar/poke-video-mcp",
    "description": "Search your Flashback video library with natural language to instantly find relevant moments. Get‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T17:42:05.006814Z",
    "indexed_at": "2026-02-18T04:05:34.432151",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Video Query MCP Server\n\nAn MCP (Model Context Protocol) server that provides video search capabilities using natural language queries. This server interfaces with the Flashback video processing API to search through your personal video collection.\n\n## Features\n\n- üîç **Natural Language Search**: Query videos using everyday language (e.g., \"person giving presentation\", \"dog running in park\")\n- ü§ñ **AI-Generated Descriptions**: Returns detailed descriptions of video content including visual analysis and audio transcription\n- üé¨ **Video Clips**: Get direct URLs to relevant 30-second video segments\n- ‚ö° **Fast & Secure**: Presigned URLs with 1-hour expiration for secure access\n- üìä **Relevance Scoring**: Results ranked by semantic similarity to your query\n\n## Setup\n\n### Prerequisites\n- Your Flashback account user ID\n- Access to the Flashback video processing system\n\n### Configuration\nWhen connecting this MCP server to Poke or other MCP clients, you'll need to provide:\n- **user_id**: Your unique Flashback account identifier (e.g., `4087fce3-3d86-4047-b35f-4004b4c19192`)\n\n### Development\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n   ```bash\n   uv run playground\n   ```\n\n### Usage Examples\n\n**Query videos:**\n```\nquery_videos(\"person talking\", max_results=5)\n```\n\n**Get setup help:**\n```\nget_setup_instructions()\n```\n\n## How It Works\n\n1. **Video Processing**: Videos uploaded to Flashback are automatically:\n   - Split into ~30-second segments\n   - Analyzed with AI for visual content\n   - Transcribed for audio content\n   - Stored in a searchable vector database\n\n2. **Search Process**: When you search:\n   - Your query is converted to embeddings\n   - The system finds matching video segments\n   - Returns descriptions and URLs for relevant clips\n\n3. **Results**: Each result includes:\n   - AI-generated description of the video content\n   - Relevance score (0-1, higher = more relevant)\n   - Direct URL to view the video segment\n   - Expiration time for the URL\n\n## API Reference\n\n### Tools\n\n#### `query_videos(query: str, max_results: int = 10) -> str`\nSearch for video clips based on natural language query.\n\n**Parameters:**\n- `query`: Natural language description of what you're looking for\n- `max_results`: Maximum number of results to return (1-15)\n\n**Returns:** Formatted text with video descriptions and URLs\n\n#### `get_setup_instructions() -> str`\nGet detailed setup instructions for the video query system.\n\n**Returns:** Complete setup and usage guide\n\n### Resources\n\n- `api://video-processing`: Information about the underlying video processing API\n\n## Examples\n\n```python\n# Search for specific content\nquery_videos(\"meeting discussion about deadlines\")\nquery_videos(\"someone cooking in kitchen\")\nquery_videos(\"red car driving\")\n\n# Limit results\nquery_videos(\"presentation\", max_results=3)\n```\n\n## Troubleshooting\n\n- **No results found**: Check that videos have been uploaded to your Flashback account\n- **\"No description available\"**: Older videos may need to be re-processed for full descriptions\n- **Expired URLs**: Video URLs expire after 1 hour for security - request fresh results if needed\n\n## Technical Details\n\n- **Backend**: FastAPI service deployed on Modal\n- **Vector Database**: Pinecone for semantic search\n- **AI Models**: Anthropic Claude for visual analysis, OpenAI Whisper for transcription\n- **Storage**: Google Cloud Storage for video files\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Video Query MCP Server üé¨\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)"
    },
    "llm_extracted": {
      "capabilities": [
        "Search videos using natural language queries",
        "Return AI-generated detailed descriptions of video content",
        "Provide direct URLs to relevant 30-second video segments",
        "Rank search results by semantic relevance score",
        "Generate presigned URLs with 1-hour expiration for secure video access",
        "Convert user queries into embeddings for semantic search",
        "Retrieve setup instructions for the video query system"
      ],
      "limitations": [
        "Video URLs expire after 1 hour requiring fresh requests for access",
        "Maximum of 15 results can be returned per query",
        "Older videos may lack full AI-generated descriptions if not re-processed",
        "No results if no videos have been uploaded to the Flashback account"
      ],
      "requirements": [
        "Flashback account user ID",
        "Access to the Flashback video processing system",
        "Deployment environment supporting FastAPI and Modal",
        "GitHub account for deployment to Smithery",
        "Configured Pinecone vector database for semantic search",
        "Google Cloud Storage for video file storage",
        "API access to Anthropic Claude and OpenAI Whisper models"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear descriptions of capabilities and limitations, and explicit environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Video Query MCP Server\n\nAn MCP (Model Context Protocol) server that provides video search capabilities using natural language queries. This server interfaces with the Flashback video processing API to search through your personal video collection.\n\n## Features\n\n- üîç **Natural Language Search**: Query videos using everyday language (e.g., \"person giving presentation\", \"dog running in park\")\n- ü§ñ **AI-Generated Descriptions**: Returns detailed descriptions of video content including visual analysis and audio transcription\n- üé¨ **Video Clips**: Get direct URLs to relevant 30-second video segments\n- ‚ö° **Fast & Secure**: Presigned URLs with 1-hour expiration for secure access\n- üìä **Relevance Scoring**: Results ranked by semantic similarity to your query\n\n## Setup\n\n### Prerequisites\n- Your Flashback account user ID\n- Access to the Flashback video processing system\n\n### Configuration\nWhen connecting this MCP server to Poke or other MCP clients, you'll need to provide:\n- **user_id**: Your unique Flashback account identifier (e.g., `4087fce3-3d86-4047-b35f-4004b4c19192`)\n\n### Development\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n   ```bash\n   uv run playground\n   ```\n\n### Usage Examples\n\n**Query videos:**\n```\nquery_videos(\"person talking\", max_results=5)\n```\n\n**Get setup help:**\n```\nget_setup_instructions()\n```\n\n## How It Works\n\n1. **Video Processing**: Videos uploaded to Flashback are automatically:\n   - Split into ~30-second segments\n   - Analyzed with AI for visual content\n   - Transcribed for audio content\n   - Stored in a searchable vector database\n\n2. **Search Process**: When you search:\n   - Your query is converted to embeddings\n   - The system finds matching video segments\n   - Returns descriptions and URLs for relevant clips\n\n3.",
        "start_pos": 0,
        "end_pos": 1790,
        "token_count_estimate": 447,
        "source_type": "readme",
        "agent_id": "073e253f23430880"
      },
      {
        "chunk_id": 1,
        "text": "ription of the video content\n   - Relevance score (0-1, higher = more relevant)\n   - Direct URL to view the video segment\n   - Expiration time for the URL\n\n## API Reference\n\n### Tools\n\n#### `query_videos(query: str, max_results: int = 10) -> str`\nSearch for video clips based on natural language query.\n\n**Parameters:**\n- `query`: Natural language description of what you're looking for\n- `max_results`: Maximum number of results to return (1-15)\n\n**Returns:** Formatted text with video descriptions and URLs\n\n#### `get_setup_instructions() -> str`\nGet detailed setup instructions for the video query system.\n\n**Returns:** Complete setup and usage guide\n\n### Resources\n\n- `api://video-processing`: Information about the underlying video processing API\n\n## Examples\n\n```python\n# Search for specific content\nquery_videos(\"meeting discussion about deadlines\")\nquery_videos(\"someone cooking in kitchen\")\nquery_videos(\"red car driving\")\n\n# Limit results\nquery_videos(\"presentation\", max_results=3)\n```\n\n## Troubleshooting\n\n- **No results found**: Check that videos have been uploaded to your Flashback account\n- **\"No description available\"**: Older videos may need to be re-processed for full descriptions\n- **Expired URLs**: Video URLs expire after 1 hour for security - request fresh results if needed\n\n## Technical Details\n\n- **Backend**: FastAPI service deployed on Modal\n- **Vector Database**: Pinecone for semantic search\n- **AI Models**: Anthropic Claude for visual analysis, OpenAI Whisper for transcription\n- **Storage**: Google Cloud Storage for video files\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Video Query MCP Server üé¨\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 1848,
        "end_pos": 3855,
        "token_count_estimate": 501,
        "source_type": "readme",
        "agent_id": "073e253f23430880"
      },
      {
        "chunk_id": 2,
        "text": "MCP Server üé¨\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 3655,
        "end_pos": 3855,
        "token_count_estimate": 50,
        "source_type": "readme",
        "agent_id": "073e253f23430880"
      }
    ]
  },
  {
    "agent_id": "e0cf0b4c56101053",
    "name": "ai.smithery/bergeramit-bergeramit-hw3-tech",
    "source": "mcp",
    "source_url": "https://github.com/bergeramit/bergeramit-hw3-tech",
    "description": "Create friendly greetings and add two numbers instantly. Speed up simple tasks and streamline ligh‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T12:49:39.923485Z",
    "indexed_at": "2026-02-18T04:05:36.281015",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": ""
    },
    "llm_extracted": {
      "capabilities": [
        "Create friendly greetings",
        "Add two numbers instantly",
        "Speed up simple tasks",
        "Streamline lightweight operations"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "697269fe0a383099",
    "name": "ai.smithery/bhushangitfull-file-mcp-smith",
    "source": "mcp",
    "source_url": "https://github.com/bhushangitfull/file-mcp-smith",
    "description": "Manage files and folders directly from your workspace. Read and write files, list directories, cre‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T07:58:50.393118Z",
    "indexed_at": "2026-02-18T04:05:38.825484",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# filesystem\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n\n\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server using Smithery CLI",
        "Test the server interactively via a playground",
        "Execute example tools such as greeting commands"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai",
        "Smithery CLI installed",
        "uv tool for running commands"
      ]
    },
    "documentation_quality": 0.35,
    "quality_rationale": "The documentation provides basic setup instructions and a simple usage example but lacks detailed descriptions of capabilities, limitations, or comprehensive usage examples.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# filesystem\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.",
        "start_pos": 0,
        "end_pos": 443,
        "token_count_estimate": 109,
        "source_type": "readme",
        "agent_id": "697269fe0a383099"
      }
    ]
  },
  {
    "agent_id": "33f98952305edb85",
    "name": "ai.smithery/bielacki-igdb-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/bielacki/igdb-mcp-server",
    "description": "Explore and discover video games from the Internet Game Database. Search titles, view detailed inf‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T21:24:56.528261Z",
    "indexed_at": "2026-02-18T04:05:40.372028",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# IGDB MCP Server\n\nAccess the IGDB (Internet Game Database) API through Model Context Protocol (MCP)\n\n[![smithery badge](https://smithery.ai/badge/@bielacki/igdb-mcp-server)](https://smithery.ai/server/@bielacki/igdb-mcp-server)\n[![Python](https://img.shields.io/badge/python-3.12%2B-blue)](https://www.python.org/downloads/)\n[![MCP](https://img.shields.io/badge/MCP-Compatible-green)](https://modelcontextprotocol.io)\n[![FastMCP](https://img.shields.io/badge/FastMCP-Powered-orange)](https://github.com/jlowin/fastmcp)\n[![uv](https://img.shields.io/badge/uv-Package%20Manager-purple)](https://github.com/astral-sh/uv)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![IGDB API](https://img.shields.io/badge/IGDB-API%20v4-red)](https://api-docs.igdb.com)\n\n## Overview\n\nThe IGDB MCP Server provides seamless access to the Internet Game Database (IGDB) through the Model Context Protocol. IGDB is a comprehensive database containing information about video games, including:\n\n- Game metadata (titles, descriptions, ratings)\n- Release dates and platforms\n- Developer and publisher information\n- Genres, themes, and game modes\n- User ratings and hype metrics\n- Cover art and media\n\n### Key Features\n\n- **Full IGDB API Access**: Search games, get detailed information, find trending titles\n- **Smart Caching**: OAuth tokens are cached to minimize authentication overhead\n- **Flexible Queries**: Use simple searches or advanced Apicalypse query language\n- **Pre-built Prompts**: Common queries ready to use\n- **Type-Safe**: Built with Pydantic for robust data validation\n\n## Quick Start\n\n### Get IGDB Credentials\n1. Create a [Twitch account](https://www.twitch.tv) (if you don't have one)\n2. Go to [Twitch Developer Console](https://dev.twitch.tv/console) ‚Üí Register Your Application\n3. Get your **Client ID** and generate a **Client Secret**\n\nüìñ [Full IGDB authentication guide](https://api-docs.igdb.com/#account-creation)\n\n### Option A: install via Smithery\n\nTo install igdb-mcp-server automatically via [Smithery](https://smithery.ai/server/@bielacki/igdb-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @bielacki/igdb-mcp-server\n```\n\n### Option B: install with uvx\n\nInstall [uv](https://github.com/astral-sh/uv).\n\nAdd this to your MCP client's configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"igdb-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"--from\", \"git+https://github.com/bielacki/igdb-mcp-server.git\", \"igdb-mcp-server\"],\n      \"env\": {\n        \"IGDB_CLIENT_ID\": \"your_client_id\",\n        \"IGDB_CLIENT_SECRET\": \"your_secret\"\n      }\n    }\n  }\n}\n```\n\n## Start Exploring\n\nStart exploring with these example prompts:\n\nüîç **Search & Discovery**\n- \"Search for Elden Ring and its expansions\"\n- \"Find all Persona games from the last 5 years\"\n- \"Show me games similar to Hades\"\n\nüìä **Game Information**\n- \"Get details about Baldur's Gate 3\"\n- \"Tell me everything about Cyberpunk 2077 including DLC\"\n- \"What platforms is Hogwarts Legacy available on?\"\n\nüî• **Trending & Popular**\n- \"What are the most anticipated upcoming games?\"\n- \"Show me the highest rated indie games of 2024\"\n- \"Find games with the most hype right now\"\n\nüéØ **Advanced Queries**\n- \"Find games similar to Skyrim with a rating of 85 or higher\"\n- \"List all games by Larian Studios\"\n- \"Show upcoming Silent Hill and Resident Evil games\"\n\n## Core Components\n\n### Tools\n\n| Tool | Description | Parameters | Example Usage |\n|------|-------------|------------|---------------|\n| **search_games** | Search for games by name | ‚Ä¢ `query` (required): Search term<br>‚Ä¢ `fields`: Fields to return (default: basic info)<br>‚Ä¢ `limit`: Results count (1-500, default: 10) | \"Search for Elden Ring games\" |\n| **get_game_details** | Get comprehensive game information | ‚Ä¢ `game_id` (required): IGDB game ID<br>‚Ä¢ `fields`: Fields to return (default: extensive) | \"Get details for game ID 1942\" |\n| **get_most_anticipated_games** | Find upcoming games by hype | ‚Ä¢ `fields`: Fields to return<br>‚Ä¢ `limit`: Results count (1-500, default: 25)<br>‚Ä¢ `min_hypes`: Min hype count (default: 25) | \"Show most anticipated games\" |\n| **custom_query** | Execute Apicalypse queries | ‚Ä¢ `endpoint` (required): API endpoint<br>‚Ä¢ `query` (required): Apicalypse query string | \"Find RPGs rated above 90\" |\n\n### Resources\n\n| Resource | Description | Returns |\n|----------|-------------|---------|\n| **igdb://endpoints** | List of all IGDB API endpoints | Available endpoints with descriptions |\n| **igdb://query-syntax** | Apicalypse query language guide | Syntax reference and examples |\n\n### Pre-built Prompts\n\n| Prompt | Description | Use Case |\n|--------|-------------|----------|\n| **search_game** | Formatted game search results | Quick game discovery with clean output |\n| **game_details** | Comprehensive game information | Full details including ratings, platforms, developers |\n| **most_anticipated** | Trending upcoming games | Discover hyped unreleased games with statistics |\n\n## Troubleshooting\n\n### Authentication Errors\n- **\"IGDB_CLIENT_ID not set\"**: Check your MCP client config has the env variables\n- **\"Invalid credentials\"**: Verify your Client ID and Secret are correct\n- **\"Token expired\"**: The server handles token refresh automatically\n\n### Rate Limiting\nIGDB allows 4 requests per second. The server doesn't implement rate limiting, so:\n- Avoid rapid repeated queries\n- Use field expansion instead of multiple requests\n- Leverage multi-query for batch operations\n\n### Common Query Issues\n- **No results**: Check spelling, try broader search terms\n- **Missing fields**: Some fields may be null; handle gracefully\n- **Query syntax error**: Verify Apicalypse syntax, check semicolons\n\n### Environment Variables\nEnsure your MCP client config includes:\n```json\n\"env\": {\n  \"IGDB_CLIENT_ID\": \"abc123...\",\n  \"IGDB_CLIENT_SECRET\": \"xyz789...\"\n}\n```\n\n## License & Credits\n\n[MIT License](LICENSE) - see LICENSE file for details\n\n\n**Credits**:\n- IGDB API by [IGDB.com](https://www.igdb.com)\n- MCP protocol by Anthropic\n- Built with [FastMCP](https://github.com/jlowin/fastmcp)\n- Published on [Smithery](https://smithery.ai)\n\n---\n\nFor more information about IGDB API capabilities, visit the [official IGDB API documentation](https://api-docs.igdb.com)."
    },
    "llm_extracted": {
      "capabilities": [
        "Search games by name using simple or advanced queries",
        "Retrieve detailed game information including metadata, ratings, platforms, developers, and media",
        "Find trending and most anticipated upcoming games based on hype metrics",
        "Execute custom Apicalypse query language queries against IGDB endpoints",
        "Cache OAuth tokens to minimize authentication overhead",
        "Provide pre-built prompts for common queries like game search, detailed info, and trending games",
        "Support type-safe data validation using Pydantic",
        "List IGDB API endpoints and provide query syntax guidance"
      ],
      "limitations": [
        "Does not implement rate limiting; users must avoid exceeding IGDB's 4 requests per second limit",
        "Some query results may have null fields requiring graceful handling",
        "Requires correct Apicalypse query syntax; syntax errors may occur",
        "Authentication errors if environment variables or credentials are incorrect",
        "No built-in handling for rapid repeated queries or automatic batching beyond multi-query support"
      ],
      "requirements": [
        "Twitch account to register an application",
        "IGDB Client ID and Client Secret from Twitch Developer Console",
        "Python 3.12 or higher environment",
        "MCP client configuration with environment variables IGDB_CLIENT_ID and IGDB_CLIENT_SECRET",
        "Installation via Smithery CLI or uvx package manager"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, environment requirements, troubleshooting guidance, and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# IGDB MCP Server\n\nAccess the IGDB (Internet Game Database) API through Model Context Protocol (MCP)\n\n[![smithery badge](https://smithery.ai/badge/@bielacki/igdb-mcp-server)](https://smithery.ai/server/@bielacki/igdb-mcp-server)\n[![Python](https://img.shields.io/badge/python-3.12%2B-blue)](https://www.python.org/downloads/)\n[![MCP](https://img.shields.io/badge/MCP-Compatible-green)](https://modelcontextprotocol.io)\n[![FastMCP](https://img.shields.io/badge/FastMCP-Powered-orange)](https://github.com/jlowin/fastmcp)\n[![uv](https://img.shields.io/badge/uv-Package%20Manager-purple)](https://github.com/astral-sh/uv)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![IGDB API](https://img.shields.io/badge/IGDB-API%20v4-red)](https://api-docs.igdb.com)\n\n## Overview\n\nThe IGDB MCP Server provides seamless access to the Internet Game Database (IGDB) through the Model Context Protocol. IGDB is a comprehensive database containing information about video games, including:\n\n- Game metadata (titles, descriptions, ratings)\n- Release dates and platforms\n- Developer and publisher information\n- Genres, themes, and game modes\n- User ratings and hype metrics\n- Cover art and media\n\n### Key Features\n\n- **Full IGDB API Access**: Search games, get detailed information, find trending titles\n- **Smart Caching**: OAuth tokens are cached to minimize authentication overhead\n- **Flexible Queries**: Use simple searches or advanced Apicalypse query language\n- **Pre-built Prompts**: Common queries ready to use\n- **Type-Safe**: Built with Pydantic for robust data validation\n\n## Quick Start\n\n### Get IGDB Credentials\n1. Create a [Twitch account](https://www.twitch.tv) (if you don't have one)\n2. Go to [Twitch Developer Console](https://dev.twitch.tv/console) ‚Üí Register Your Application\n3.",
        "start_pos": 0,
        "end_pos": 1841,
        "token_count_estimate": 460,
        "source_type": "readme",
        "agent_id": "33f98952305edb85"
      },
      {
        "chunk_id": 1,
        "text": "ur **Client ID** and generate a **Client Secret**\n\nüìñ [Full IGDB authentication guide](https://api-docs.igdb.com/#account-creation)\n\n### Option A: install via Smithery\n\nTo install igdb-mcp-server automatically via [Smithery](https://smithery.ai/server/@bielacki/igdb-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @bielacki/igdb-mcp-server\n```\n\n### Option B: install with uvx\n\nInstall [uv](https://github.com/astral-sh/uv).",
        "start_pos": 1848,
        "end_pos": 2274,
        "token_count_estimate": 106,
        "source_type": "readme",
        "agent_id": "33f98952305edb85"
      },
      {
        "chunk_id": 2,
        "text": "den Ring games\" |\n| **get_game_details** | Get comprehensive game information | ‚Ä¢ `game_id` (required): IGDB game ID<br>‚Ä¢ `fields`: Fields to return (default: extensive) | \"Get details for game ID 1942\" |\n| **get_most_anticipated_games** | Find upcoming games by hype | ‚Ä¢ `fields`: Fields to return<br>‚Ä¢ `limit`: Results count (1-500, default: 25)<br>‚Ä¢ `min_hypes`: Min hype count (default: 25) | \"Show most anticipated games\" |\n| **custom_query** | Execute Apicalypse queries | ‚Ä¢ `endpoint` (required): API endpoint<br>‚Ä¢ `query` (required): Apicalypse query string | \"Find RPGs rated above 90\" |\n\n### Resources\n\n| Resource | Description | Returns |\n|----------|-------------|---------|\n| **igdb://endpoints** | List of all IGDB API endpoints | Available endpoints with descriptions |\n| **igdb://query-syntax** | Apicalypse query language guide | Syntax reference and examples |\n\n### Pre-built Prompts\n\n| Prompt | Description | Use Case |\n|--------|-------------|----------|\n| **search_game** | Formatted game search results | Quick game discovery with clean output |\n| **game_details** | Comprehensive game information | Full details including ratings, platforms, developers |\n| **most_anticipated** | Trending upcoming games | Discover hyped unreleased games with statistics |\n\n## Troubleshooting\n\n### Authentication Errors\n- **\"IGDB_CLIENT_ID not set\"**: Check your MCP client config has the env variables\n- **\"Invalid credentials\"**: Verify your Client ID and Secret are correct\n- **\"Token expired\"**: The server handles token refresh automatically\n\n### Rate Limiting\nIGDB allows 4 requests per second.",
        "start_pos": 3696,
        "end_pos": 5302,
        "token_count_estimate": 401,
        "source_type": "readme",
        "agent_id": "33f98952305edb85"
      },
      {
        "chunk_id": 3,
        "text": "der search terms\n- **Missing fields**: Some fields may be null; handle gracefully\n- **Query syntax error**: Verify Apicalypse syntax, check semicolons\n\n### Environment Variables\nEnsure your MCP client config includes:\n```json\n\"env\": {\n  \"IGDB_CLIENT_ID\": \"abc123...\",\n  \"IGDB_CLIENT_SECRET\": \"xyz789...\"\n}\n```\n\n## License & Credits\n\n[MIT License](LICENSE) - see LICENSE file for details\n\n\n**Credits**:\n- IGDB API by [IGDB.com](https://www.igdb.com)\n- MCP protocol by Anthropic\n- Built with [FastMCP](https://github.com/jlowin/fastmcp)\n- Published on [Smithery](https://smithery.ai)\n\n---\n\nFor more information about IGDB API capabilities, visit the [official IGDB API documentation](https://api-docs.igdb.com).",
        "start_pos": 5544,
        "end_pos": 6253,
        "token_count_estimate": 177,
        "source_type": "readme",
        "agent_id": "33f98952305edb85"
      }
    ]
  },
  {
    "agent_id": "89f5df0086de2807",
    "name": "ai.smithery/blacklotusdev8-test_m",
    "source": "mcp",
    "source_url": "https://github.com/blacklotusdev8/test_m",
    "description": "Greet anyone by name with a friendly hello. Scrape webpages to extract content for quick reference‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-19T19:12:16.602435Z",
    "indexed_at": "2026-02-18T04:05:42.271165",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# test\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Run with Docker\n\nBuild the image (from the project root):\n\n```bash\ndocker build -t hello-server .\n```\n\nRun the container:\n\n```bash\ndocker run --rm -p 8081:8081 --shm-size=1g hello-server\n```\n\nNotes:\n\n- The server listens on `0.0.0.0:8081` and serves the MCP endpoint at `/mcp`.\n- `--shm-size=1g` improves browser stability for Playwright/Firefox-based scraping.\n\nYou can now connect an MCP client to:\n\n```\nhttp://localhost:8081/mcp\n```\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server built with Smithery",
        "Serve the MCP endpoint at /mcp",
        "Support interactive testing via a playground",
        "Provide example tool functionality such as greeting users",
        "Allow development and customization of server capabilities in Python code",
        "Support deployment to Smithery platform",
        "Run the server inside a Docker container with specified configurations"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key",
        "Docker for containerized deployment (optional)",
        "GitHub account for deployment",
        "Python environment to run the server locally"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, usage examples, development guidance, deployment steps, and prerequisite requirements, but lacks explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# test\n\nAn MCP server built with Smithery.\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Run with Docker\n\nBuild the image (from the project root):\n\n```bash\ndocker build -t hello-server .\n```\n\nRun the container:\n\n```bash\ndocker run --rm -p 8081:8081 --shm-size=1g hello-server\n```\n\nNotes:\n\n- The server listens on `0.0.0.0:8081` and serves the MCP endpoint at `/mcp`.\n- `--shm-size=1g` improves browser stability for Playwright/Firefox-based scraping.\n\nYou can now connect an MCP client to:\n\n```\nhttp://localhost:8081/mcp\n```\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 1351,
        "token_count_estimate": 337,
        "source_type": "readme",
        "agent_id": "89f5df0086de2807"
      }
    ]
  },
  {
    "agent_id": "5e461e0d3bc0988b",
    "name": "ai.smithery/blbl147-xhs-mcp",
    "source": "mcp",
    "source_url": "https://github.com/blbl147/xhs-mcp",
    "description": "ÊêúÁ¥¢Á¨îËÆ∞„ÄÅÊµèËßàÈ¶ñÈ°µÊé®Ëçê„ÄÅÊü•ÁúãÁ¨îËÆ∞ÂÜÖÂÆπ‰∏éËØÑËÆ∫ÔºåÂπ∂ÂèëË°®‰Ω†ÁöÑËØÑËÆ∫„ÄÇÁõ¥Êé•Âú®Â∑•‰ΩúÊµÅ‰∏≠‰∏éÂ∞èÁ∫¢‰π¶ÂÜÖÂÆπ‰∫íÂä®ÔºåÈ´òÊïàË∑üËøõËØùÈ¢ò„ÄÇ",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-15T03:34:24.676763Z",
    "indexed_at": "2026-02-18T04:05:44.232052",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Â∞èÁ∫¢‰π¶MCPÊúçÂä°\n[![smithery badge](https://smithery.ai/badge/@jobsonlook/xhs-mcp)](https://smithery.ai/server/@jobsonlook/xhs-mcp)\n## ÁâπÁÇπ\n- [x] ÈááÁî®jsÈÄÜÂêëÂá∫x-s,x-t,Áõ¥Êé•ËØ∑Ê±ÇhttpÊé•Âè£,Êó†È°ªÁ¨®ÈáçÁöÑplaywright\n- [x] ÊêúÁ¥¢Á¨îËÆ∞\n- [x] Ëé∑ÂèñÁ¨îËÆ∞ÂÜÖÂÆπ\n- [x] Ëé∑ÂèñÁ¨îËÆ∞ÁöÑËØÑËÆ∫\n- [x] ÂèëË°®ËØÑËÆ∫\n\n![ÁâπÊÄß](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/feature.png)\n\n## Âø´ÈÄüÂºÄÂßã\n\n### 1. ÁéØÂ¢É\n * node\n * python 3.12\n * uv (pip install uv)\n\n### 2. ÂÆâË£Ö‰æùËµñ\n```sh\n\ngit clone git@github.com:jobsonlook/xhs-mcp.git\n\ncd xhs-mcp\nuv sync \n\n```\n\n### 3. Ëé∑ÂèñÂ∞èÁ∫¢‰π¶ÁöÑcookie\n[ÊâìÂºÄwebÂ∞èÁ∫¢‰π¶](https://www.xiaohongshu.com/explore)\nÁôªÂΩïÂêéÔºåËé∑ÂèñcookieÔºåÂ∞ÜcookieÈÖçÁΩÆÂà∞Á¨¨4Ê≠•ÁöÑ XHS_COOKIE ÁéØÂ¢ÉÂèòÈáè‰∏≠\n![cookie](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/cookie.png)\n\n### 4. ÈÖçÁΩÆmcp server\n\n```json\n{\n    \"mcpServers\": {\n        \"xhs-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/Users/xxx/xhs-mcp\",\n                \"run\",\n                \"main.py\"\n            ],\n            \"env\": {\n                \"XHS_COOKIE\": \"xxxx\"\n            }\n        }\n    }\n}\n```\n\n## ÂÖçË¥£Â£∞Êòé\nÊú¨È°πÁõÆ‰ªÖÁî®‰∫éÂ≠¶‰π†‰∫§ÊµÅÔºåÁ¶ÅÊ≠¢Áî®‰∫éÂÖ∂‰ªñÁî®ÈÄîÔºå‰ªª‰ΩïÊ∂âÂèäÂïÜ‰∏öÁõàÂà©ÁõÆÁöÑÂùá‰∏çÂæó‰ΩøÁî®ÔºåÂê¶ÂàôÈ£éÈô©Ëá™Ë¥ü„ÄÇ\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search notes on Xiaohongshu",
        "Retrieve note content",
        "Fetch comments on notes",
        "Post comments on notes",
        "Perform HTTP requests using reverse-engineered x-s and x-t tokens without Playwright"
      ],
      "limitations": [
        "Requires valid Xiaohongshu cookie for authentication",
        "Not intended for commercial use",
        "No support for Playwright-based browsing or automation"
      ],
      "requirements": [
        "Node.js environment",
        "Python 3.12",
        "uv package installed via pip",
        "Xiaohongshu login cookie set in XHS_COOKIE environment variable"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, environment setup, usage configuration, feature list, and explicit limitations, making it comprehensive and clear.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Â∞èÁ∫¢‰π¶MCPÊúçÂä°\n[![smithery badge](https://smithery.ai/badge/@jobsonlook/xhs-mcp)](https://smithery.ai/server/@jobsonlook/xhs-mcp)\n## ÁâπÁÇπ\n- [x] ÈááÁî®jsÈÄÜÂêëÂá∫x-s,x-t,Áõ¥Êé•ËØ∑Ê±ÇhttpÊé•Âè£,Êó†È°ªÁ¨®ÈáçÁöÑplaywright\n- [x] ÊêúÁ¥¢Á¨îËÆ∞\n- [x] Ëé∑ÂèñÁ¨îËÆ∞ÂÜÖÂÆπ\n- [x] Ëé∑ÂèñÁ¨îËÆ∞ÁöÑËØÑËÆ∫\n- [x] ÂèëË°®ËØÑËÆ∫\n\n![ÁâπÊÄß](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/feature.png)\n\n## Âø´ÈÄüÂºÄÂßã\n\n### 1. ÁéØÂ¢É\n * node\n * python 3.12\n * uv (pip install uv)\n\n### 2. ÂÆâË£Ö‰æùËµñ\n```sh\n\ngit clone git@github.com:jobsonlook/xhs-mcp.git\n\ncd xhs-mcp\nuv sync \n\n```\n\n### 3. Ëé∑ÂèñÂ∞èÁ∫¢‰π¶ÁöÑcookie\n[ÊâìÂºÄwebÂ∞èÁ∫¢‰π¶](https://www.xiaohongshu.com/explore)\nÁôªÂΩïÂêéÔºåËé∑ÂèñcookieÔºåÂ∞ÜcookieÈÖçÁΩÆÂà∞Á¨¨4Ê≠•ÁöÑ XHS_COOKIE ÁéØÂ¢ÉÂèòÈáè‰∏≠\n![cookie](https://raw.githubusercontent.com/jobsonlook/xhs-mcp/master/docs/cookie.png)\n\n### 4. ÈÖçÁΩÆmcp server\n\n```json\n{\n    \"mcpServers\": {\n        \"xhs-mcp\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"/Users/xxx/xhs-mcp\",\n                \"run\",\n                \"main.py\"\n            ],\n            \"env\": {\n                \"XHS_COOKIE\": \"xxxx\"\n            }\n        }\n    }\n}\n```\n\n## ÂÖçË¥£Â£∞Êòé\nÊú¨È°πÁõÆ‰ªÖÁî®‰∫éÂ≠¶‰π†‰∫§ÊµÅÔºåÁ¶ÅÊ≠¢Áî®‰∫éÂÖ∂‰ªñÁî®ÈÄîÔºå‰ªª‰ΩïÊ∂âÂèäÂïÜ‰∏öÁõàÂà©ÁõÆÁöÑÂùá‰∏çÂæó‰ΩøÁî®ÔºåÂê¶ÂàôÈ£éÈô©Ëá™Ë¥ü„ÄÇ",
        "start_pos": 0,
        "end_pos": 1080,
        "token_count_estimate": 269,
        "source_type": "readme",
        "agent_id": "5e461e0d3bc0988b"
      }
    ]
  },
  {
    "agent_id": "6914a362d60f052d",
    "name": "ai.smithery/blockscout-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/blockscout/mcp-server",
    "description": "Provide AI agents and automation tools with contextual access to blockchain data including balance‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T00:50:33.619952Z",
    "indexed_at": "2026-02-18T04:05:46.257511",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Blockscout MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@blockscout/mcp-server)](https://smithery.ai/server/@blockscout/mcp-server)\n\n<a href=\"https://glama.ai/mcp/servers/@blockscout/mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@blockscout/mcp-server/badge\" alt=\"Blockscout Server MCP server\" />\n</a>\n\nThe Model Context Protocol (MCP) is an open protocol designed to allow AI agents, IDEs, and automation tools to consume, query, and analyze structured data through context-aware APIs.\n\nThis server wraps Blockscout APIs and exposes blockchain data‚Äîbalances, tokens, NFTs, contract metadata‚Äîvia MCP so that AI agents and tools (like Claude, Cursor, or IDEs) can access and analyze it contextually.\n\n**Key Features:**\n\n- Contextual blockchain data access for AI tools\n- Multi-chain support via getting Blockscout instance URLs from Chainscout\n- **Versioned REST API**: Provides a standard, web-friendly interface to all MCP tools. See [API.md](API.md) for full documentation.\n- Custom instructions for MCP host to use the server\n- Intelligent context optimization to conserve LLM tokens while preserving data accessibility\n- Smart response slicing with configurable page sizes to prevent context overflow\n- Opaque cursor pagination using Base64URL-encoded strings instead of complex parameters\n- Automatic truncation of large data fields with clear indicators and access guidance\n- Standardized ToolResponse model with structured JSON responses and follow-up instructions\n- Enhanced observability with MCP progress notifications and periodic updates for long-running operations\n\n## Configuring MCP Clients\n\n### Using Claude Connectors Directory - Recommended\n\nThe easiest way to use the Blockscout MCP server with Claude ( Claude Web, Claude Desktop and Claude Code) is through the official [Anthropic Connectors Directory](https://claude.com/connectors). This provides a native, managed installation experience with automatic updates.\n\n#### Installation\n\n##### Option 1: Direct Link\n\nVisit [claude.com/connectors/blockscout](https://claude.com/connectors/blockscout) and click links in \"Used in\" section to install the Blockscout connector.\n\n##### Option 2: Via Settings\n\n1. Open Claude (Web or Desktop app)\n2. Go to Settings > Connectors > Browse connectors\n3. Search for \"Blockscout\"\n4. Click \"Connect\" to install\n\n> **Note:** Connectors require a paid Claude plan (Pro, Team, Max, or Enterprise).\n\n### Claude Code Setup\n\nTo quickly install the Blockscout MCP server for use with Claude Code, run the following command in your terminal:\n\n```sh\nclaude mcp add --transport http blockscout https://mcp.blockscout.com/mcp\n```\n\nAfter running this command, Blockscout will be available as an MCP server in Claude Code, allowing you to access and analyze blockchain data directly from your coding environment.\n\n### Cursor Setup\n\nUse [this deeplink](https://cursor.com/en/install-mcp?name=blockscout&config=eyJ1cmwiOiJodHRwczovL21jcC5ibG9ja3Njb3V0LmNvbS9tY3AiLCJ0aW1lb3V0IjoxODAwMDB9) to install the Blockscout MCP server in Cursor.\n\n### Gemini CLI Setup\n\n1. Add the following configuration to your `~/.gemini/settings.json` file:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"blockscout\": {\n          \"httpUrl\": \"https://mcp.blockscout.com/mcp\",\n          \"timeout\": 180000\n        }\n      }\n    }\n    ```\n\n2. For detailed Gemini CLI MCP server configuration instructions, see the [official documentation](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md).\n\n## Try Blockscout X-Ray GPT\n\nExperience the power of the Blockscout MCP server through our showcase GPT: **[Blockscout X-Ray](https://chatgpt.com/g/g-68a7f315edf481918641bd0ed1e60f8b-blockscout-x-ray)**\n\nThis GPT demonstrates the full capabilities of the MCP server, providing intelligent blockchain analysis and insights. It's a great way to explore what's possible when AI agents have contextual access to blockchain data.\n\n### Local Development Setup (For Developers)\n\nIf you want to run the server locally for development purposes:\n\n```json\n{\n  \"mcpServers\": {\n    \"blockscout\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"ghcr.io/blockscout/mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n## Technical details\n\nRefer to [SPEC.md](SPEC.md) for the technical details.\n\n## Repository Structure\n\nRefer to [AGENTS.md](AGENTS.md) for the repository structure.\n\n## Testing\n\nRefer to [TESTING.md](TESTING.md) for comprehensive instructions on running both **unit and integration tests**.\n\n## Tool Descriptions\n\n1. `__unlock_blockchain_analysis__()` - Provides custom instructions for the MCP host to use the server. This is a mandatory first step before using other tools.\n2. `get_chains_list()` - Returns a list of all known chains.\n3. `get_address_by_ens_name(name)` - Converts an ENS domain name to its corresponding Ethereum address.\n4. `lookup_token_by_symbol(chain_id, symbol)` - Searches for token addresses by symbol or name, returning multiple potential matches.\n5. `get_contract_abi(chain_id, address)` - Retrieves the ABI (Application Binary Interface) for a smart contract.\n6. `inspect_contract_code(chain_id, address, file_name=None)` - Allows getting the source files of verified contracts.\n7. `get_address_info(chain_id, address)` - Gets comprehensive information about an address including balance, ENS association, contract status, token details, and public tags.\n8. `get_tokens_by_address(chain_id, address, cursor=None)` - Returns detailed ERC20 token holdings for an address with enriched metadata and market data.\n9. `get_block_number(chain_id, [datetime])` - Retrieves the block number and timestamp for a specific date/time or the latest block.\n10. `get_transactions_by_address(chain_id, address, age_from, age_to, methods, cursor=None)` - Gets transactions for an address within a specific time range with optional method filtering.\n11. `get_token_transfers_by_address(chain_id, address, age_from, age_to, token, cursor=None)` - Returns ERC-20 token transfers for an address within a specific time range.\n12. `nft_tokens_by_address(chain_id, address, cursor=None)` - Retrieves NFT tokens owned by an address, grouped by collection.\n13. `get_block_info(chain_id, number_or_hash, include_transactions=False)` - Returns block information including timestamp, gas used, burnt fees, and transaction count. Can optionally include a list of transaction hashes.\n14. `get_transaction_info(chain_id, hash, include_raw_input=False)` - Gets comprehensive transaction information with decoded input parameters and detailed token transfers.\n15. `read_contract(chain_id, address, abi, function_name, args='[]', block='latest')` - Executes a read-only smart contract function and returns its result. The `abi` argument is a JSON object describing the specific function's signature.\n16. `direct_api_call(chain_id, endpoint_path, query_params=None, cursor=None)` - Calls a curated raw Blockscout API endpoint for specialized or chain-specific data.\n\n## Example Prompts for AI Agents\n\n```plaintext\nIs any approval set for OP token on Optimism chain by `zeaver.eth`?\n```\n\n```plaintext\nCalculate the total gas fees paid on Ethereum by address `0xcafe...cafe` in May 2025.\n```\n\n```plaintext\nWhich 10 most recent logs were emitted by `0xFe89cc7aBB2C4183683ab71653C4cdc9B02D44b7`\nbefore `Nov 08 2024 04:21:35 AM (-06:00¬†UTC)`?\n```\n\n```plaintext\nTell me more about the transaction `0xf8a55721f7e2dcf85690aaf81519f7bc820bc58a878fa5f81b12aef5ccda0efb`\non Redstone rollup.\n```\n\n```plaintext\nIs there any blacklisting functionality of USDT token on Arbitrum One?\n```\n\n```plaintext\nWhat is the latest block on Gnosis Chain and who is the block minter?\nWere any funds moved from this minter recently?\n```\n\n```plaintext\nWhen the most recent reward distribution of Kinto token was made to the wallet\n`0x7D467D99028199D99B1c91850C4dea0c82aDDF52` in Kinto chain?\n```\n\n```plaintext\nWhich methods of `0x1c479675ad559DC151F6Ec7ed3FbF8ceE79582B6` on the Ethereum \nmainnet could emit `SequencerBatchDelivered`?\n```\n\n```plaintext\nWhat is the most recent executed cross-chain message sent from the Arbitrum Sepolia\nrollup to the base layer?\n```\n\n## Development & Deployment\n\n### Local Installation\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/blockscout/mcp-server.git\ncd mcp-server\nuv pip install -e . # or `pip install -e .`\n```\n\nTo customize the leading part of the `User-Agent` header used for RPC requests,\nset the `BLOCKSCOUT_MCP_USER_AGENT` environment variable (defaults to\n\"Blockscout MCP\"). The server version is appended automatically.\n\n### Running the Server\n\nThe server runs in `stdio` mode by default:\n\n```bash\npython -m blockscout_mcp_server\n```\n\n**HTTP Mode (MCP only):**\n\nTo run the server in HTTP Streamable mode (stateless, SSE responses by default):\n\n```bash\npython -m blockscout_mcp_server --http\n```\n\nYou can also specify the host and port for the HTTP server:\n\n```bash\npython -m blockscout_mcp_server --http --http-host 0.0.0.0 --http-port 8080\n```\n\n**Development Mode (Plain JSON Responses):**\n\nFor development and testing with simple HTTP clients (curl, Insomnia), you can enable plain JSON responses instead of SSE streams:\n\n```bash\nexport BLOCKSCOUT_DEV_JSON_RESPONSE=true\npython -m blockscout_mcp_server --http\n```\n\n**Note:** This disables Server-Sent Events (SSE) and progress notifications. Only use this for local testing and debugging.\n\n**Tunneling with Ngrok (Development Mode):**\n\nThe Python MCP SDK enforces DNS rebinding protection, which blocks requests from ngrok tunnels by default. To enable\ntunneling for development and testing:\n\n1. Start an ngrok tunnel to your local server:\n\n   ```bash\n   ngrok http 8000\n   ```\n\n1. Configure the allowed host and origin using your ngrok URL:\n\n   ```bash\n   export BLOCKSCOUT_MCP_ALLOWED_HOSTS=\"your-tunnel-id.ngrok-free.app\"\n   export BLOCKSCOUT_MCP_ALLOWED_ORIGINS=\"https://your-tunnel-id.ngrok-free.app\"\n   python -m blockscout_mcp_server --http\n   ```\n\n**Note:** These settings are primarily for development use. When these variables are not set, DNS rebinding protection\nis automatically determined by the server's bind host: enabled for localhost, disabled for non-localhost (e.g.,\n`0.0.0.0`). If your Host header includes a non-standard port, use the `:*` wildcard suffix (e.g.,\n`\"example.com:*\"`) or specify the exact host:port value.\n\nFor more details on ngrok tunneling with MCP servers, see the [OpenAI Apps SDK Examples\ndocumentation](https://github.com/openai/openai-apps-sdk-examples/blob/main/README.md#testing-in-chatgpt).\n\n**HTTP Mode with REST API:**\n\nTo enable the versioned REST API alongside the MCP endpoint, use the `--rest` flag (which requires `--http`).\n\n```bash\npython -m blockscout_mcp_server --http --rest\n```\n\nWith custom host and port:\n\n```bash\npython -m blockscout_mcp_server --http --rest --http-host 0.0.0.0 --http-port 8080\n```\n\n**CLI Options:**\n\n- `--http`: Enables HTTP Streamable mode.\n- `--http-host TEXT`: Host to bind the HTTP server to (default: `127.0.0.1`).\n- `--http-port INTEGER`: Port for the HTTP server (default: `8000`).\n- `--rest`: Enables the REST API (requires `--http`).\n\n### Building Docker Image Locally\n\nBuild the Docker image with the official tag:\n\n```bash\ndocker build -t ghcr.io/blockscout/mcp-server:latest .\n```\n\n### Pulling from GitHub Container Registry\n\nPull the pre-built image:\n\n```bash\ndocker pull ghcr.io/blockscout/mcp-server:latest\n```\n\n### Running with Docker\n\n**HTTP Mode (MCP only):**\n\nTo run the Docker container in HTTP mode with port mapping:\n\n```bash\ndocker run --rm -p 8000:8000 ghcr.io/blockscout/mcp-server:latest python -m blockscout_mcp_server --http --http-host 0.0.0.0\n```\n\nWith custom port:\n\n```bash\ndocker run --rm -p 8080:8080 ghcr.io/blockscout/mcp-server:latest python -m blockscout_mcp_server --http --http-host 0.0.0.0 --http-port 8080\n```\n\n**HTTP Mode with REST API:**\n\nTo run with the REST API enabled:\n\n```bash\ndocker run --rm -p 8000:8000 ghcr.io/blockscout/mcp-server:latest python -m blockscout_mcp_server --http --rest --http-host 0.0.0.0\n```\n\n**Note:** When running in HTTP mode with Docker, use `--http-host 0.0.0.0` to bind to all interfaces so the server is accessible from outside the container.\n\n**Stdio Mode:** The default stdio mode is designed for use with MCP hosts/clients (like Claude Desktop, Cursor) and doesn't make sense to run directly with Docker without an MCP client managing the communication.\n\n### Testing with Claude Desktop\n\nUse MCP bundle to test the server with Claude Desktop.\n\n1. Build the bundle as per instructions in [mcpb/README.md](mcpb/README.md).\n2. Open Claude Desktop.\n3. Double-click to open the `blockscout-mcp-dev.mcpb` file to automatically install the bundle.\n4. Configure the Blockscout MCP Server URL when prompted (default: `http://127.0.0.1:8000/mcp`)\n\n## Privacy and Anonymous Telemetry\n\nTo help us improve the Blockscout MCP Server, community-run instances of the server collect anonymous usage data by default. This helps us understand which tools are most popular and guides our development efforts.\n\n**What we collect:**\n\n- The name of the tool being called (e.g., `get_block_number`).\n- The parameters provided to the tool.\n- The version of the Blockscout MCP Server being used.\n\n**What we DO NOT collect:**\n\n- We do not collect any personal data, IP addresses (the central server uses the sender's IP for geolocation via Mixpanel and then discards it), secrets, or private keys.\n\n### How to Opt-Out\n\nYou can disable this feature at any time by setting the following environment variable:\n\n```bash\nexport BLOCKSCOUT_DISABLE_COMMUNITY_TELEMETRY=true\n```\n\n## License\n\nThis project is primarily distributed under the terms of the MIT license. See [LICENSE](LICENSE) for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide contextual blockchain data access for AI tools",
        "Support multiple blockchain networks via Chainscout URLs",
        "Expose a versioned REST API for blockchain data querying",
        "Convert ENS domain names to Ethereum addresses",
        "Retrieve smart contract ABIs and source code files",
        "Fetch detailed address information including balances, tokens, and tags",
        "List ERC20 token holdings and NFT tokens owned by an address",
        "Query blockchain blocks and transactions with filtering options",
        "Execute read-only smart contract functions with specified ABI",
        "Call raw Blockscout API endpoints for specialized data"
      ],
      "limitations": [
        "Requires paid Claude plans for connector usage",
        "Ngrok tunneling requires manual configuration due to DNS rebinding protection",
        "Development mode disables SSE and progress notifications",
        "No explicit mention of write or transaction submission capabilities"
      ],
      "requirements": [
        "Paid Claude plan (Pro, Team, Max, or Enterprise) for using Claude connectors",
        "Python environment for local development and running the server",
        "Docker for containerized deployment",
        "Environment variables for customizing User-Agent and allowed hosts/origins in development",
        "Network access to Blockscout MCP server endpoints"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions, usage examples, configuration options, and limitations, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Blockscout MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@blockscout/mcp-server)](https://smithery.ai/server/@blockscout/mcp-server)\n\n<a href=\"https://glama.ai/mcp/servers/@blockscout/mcp-server\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@blockscout/mcp-server/badge\" alt=\"Blockscout Server MCP server\" />\n</a>\n\nThe Model Context Protocol (MCP) is an open protocol designed to allow AI agents, IDEs, and automation tools to consume, query, and analyze structured data through context-aware APIs.\n\nThis server wraps Blockscout APIs and exposes blockchain data‚Äîbalances, tokens, NFTs, contract metadata‚Äîvia MCP so that AI agents and tools (like Claude, Cursor, or IDEs) can access and analyze it contextually.\n\n**Key Features:**\n\n- Contextual blockchain data access for AI tools\n- Multi-chain support via getting Blockscout instance URLs from Chainscout\n- **Versioned REST API**: Provides a standard, web-friendly interface to all MCP tools. See [API.md](API.md) for full documentation.\n- Custom instructions for MCP host to use the server\n- Intelligent context optimization to conserve LLM tokens while preserving data accessibility\n- Smart response slicing with configurable page sizes to prevent context overflow\n- Opaque cursor pagination using Base64URL-encoded strings instead of complex parameters\n- Automatic truncation of large data fields with clear indicators and access guidance\n- Standardized ToolResponse model with structured JSON responses and follow-up instructions\n- Enhanced observability with MCP progress notifications and periodic updates for long-running operations\n\n## Configuring MCP Clients\n\n### Using Claude Connectors Directory - Recommended\n\nThe easiest way to use the Blockscout MCP server with Claude ( Claude Web, Claude Desktop and Claude Code) is through the official [Anthropic Connectors Directory](https://claude.com/connectors). This provides a native, managed installation experience with automatic updates.",
        "start_pos": 0,
        "end_pos": 1982,
        "token_count_estimate": 495,
        "source_type": "readme",
        "agent_id": "6914a362d60f052d"
      },
      {
        "chunk_id": 1,
        "text": "Claude Desktop and Claude Code) is through the official [Anthropic Connectors Directory](https://claude.com/connectors). This provides a native, managed installation experience with automatic updates.\n\n#### Installation\n\n##### Option 1: Direct Link\n\nVisit [claude.com/connectors/blockscout](https://claude.com/connectors/blockscout) and click links in \"Used in\" section to install the Blockscout connector.\n\n##### Option 2: Via Settings\n\n1. Open Claude (Web or Desktop app)\n2. Go to Settings > Connectors > Browse connectors\n3. Search for \"Blockscout\"\n4. Click \"Connect\" to install\n\n> **Note:** Connectors require a paid Claude plan (Pro, Team, Max, or Enterprise).\n\n### Claude Code Setup\n\nTo quickly install the Blockscout MCP server for use with Claude Code, run the following command in your terminal:\n\n```sh\nclaude mcp add --transport http blockscout https://mcp.blockscout.com/mcp\n```\n\nAfter running this command, Blockscout will be available as an MCP server in Claude Code, allowing you to access and analyze blockchain data directly from your coding environment.\n\n### Cursor Setup\n\nUse [this deeplink](https://cursor.com/en/install-mcp?name=blockscout&config=eyJ1cmwiOiJodHRwczovL21jcC5ibG9ja3Njb3V0LmNvbS9tY3AiLCJ0aW1lb3V0IjoxODAwMDB9) to install the Blockscout MCP server in Cursor.\n\n### Gemini CLI Setup\n\n1. Add the following configuration to your `~/.gemini/settings.json` file:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"blockscout\": {\n          \"httpUrl\": \"https://mcp.blockscout.com/mcp\",\n          \"timeout\": 180000\n        }\n      }\n    }\n    ```\n\n2. For detailed Gemini CLI MCP server configuration instructions, see the [official documentation](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md).",
        "start_pos": 1782,
        "end_pos": 3533,
        "token_count_estimate": 437,
        "source_type": "readme",
        "agent_id": "6914a362d60f052d"
      },
      {
        "chunk_id": 2,
        "text": "case GPT: **[Blockscout X-Ray](https://chatgpt.com/g/g-68a7f315edf481918641bd0ed1e60f8b-blockscout-x-ray)**\n\nThis GPT demonstrates the full capabilities of the MCP server, providing intelligent blockchain analysis and insights. It's a great way to explore what's possible when AI agents have contextual access to blockchain data.\n\n### Local Development Setup (For Developers)\n\nIf you want to run the server locally for development purposes:\n\n```json\n{\n  \"mcpServers\": {\n    \"blockscout\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\", \"--rm\", \"-i\",\n        \"ghcr.io/blockscout/mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\n## Technical details\n\nRefer to [SPEC.md](SPEC.md) for the technical details.\n\n## Repository Structure\n\nRefer to [AGENTS.md](AGENTS.md) for the repository structure.\n\n## Testing\n\nRefer to [TESTING.md](TESTING.md) for comprehensive instructions on running both **unit and integration tests**.\n\n## Tool Descriptions\n\n1. `__unlock_blockchain_analysis__()` - Provides custom instructions for the MCP host to use the server. This is a mandatory first step before using other tools.\n2. `get_chains_list()` - Returns a list of all known chains.\n3. `get_address_by_ens_name(name)` - Converts an ENS domain name to its corresponding Ethereum address.\n4. `lookup_token_by_symbol(chain_id, symbol)` - Searches for token addresses by symbol or name, returning multiple potential matches.\n5. `get_contract_abi(chain_id, address)` - Retrieves the ABI (Application Binary Interface) for a smart contract.\n6. `inspect_contract_code(chain_id, address, file_name=None)` - Allows getting the source files of verified contracts.\n7. `get_address_info(chain_id, address)` - Gets comprehensive information about an address including balance, ENS association, contract status, token details, and public tags.\n8. `get_tokens_by_address(chain_id, address, cursor=None)` - Returns detailed ERC20 token holdings for an address with enriched metadata and market data.\n9.",
        "start_pos": 3630,
        "end_pos": 5604,
        "token_count_estimate": 493,
        "source_type": "readme",
        "agent_id": "6914a362d60f052d"
      },
      {
        "chunk_id": 3,
        "text": "act status, token details, and public tags.\n8. `get_tokens_by_address(chain_id, address, cursor=None)` - Returns detailed ERC20 token holdings for an address with enriched metadata and market data.\n9. `get_block_number(chain_id, [datetime])` - Retrieves the block number and timestamp for a specific date/time or the latest block.\n10. `get_transactions_by_address(chain_id, address, age_from, age_to, methods, cursor=None)` - Gets transactions for an address within a specific time range with optional method filtering.\n11. `get_token_transfers_by_address(chain_id, address, age_from, age_to, token, cursor=None)` - Returns ERC-20 token transfers for an address within a specific time range.\n12. `nft_tokens_by_address(chain_id, address, cursor=None)` - Retrieves NFT tokens owned by an address, grouped by collection.\n13. `get_block_info(chain_id, number_or_hash, include_transactions=False)` - Returns block information including timestamp, gas used, burnt fees, and transaction count. Can optionally include a list of transaction hashes.\n14. `get_transaction_info(chain_id, hash, include_raw_input=False)` - Gets comprehensive transaction information with decoded input parameters and detailed token transfers.\n15. `read_contract(chain_id, address, abi, function_name, args='[]', block='latest')` - Executes a read-only smart contract function and returns its result. The `abi` argument is a JSON object describing the specific function's signature.\n16. `direct_api_call(chain_id, endpoint_path, query_params=None, cursor=None)` - Calls a curated raw Blockscout API endpoint for specialized or chain-specific data.\n\n## Example Prompts for AI Agents\n\n```plaintext\nIs any approval set for OP token on Optimism chain by `zeaver.eth`?\n```\n\n```plaintext\nCalculate the total gas fees paid on Ethereum by address `0xcafe...cafe` in May 2025.\n```\n\n```plaintext\nWhich 10 most recent logs were emitted by `0xFe89cc7aBB2C4183683ab71653C4cdc9B02D44b7`\nbefore `Nov 08 2024 04:21:35 AM (-06:00¬†UTC)`?",
        "start_pos": 5404,
        "end_pos": 7393,
        "token_count_estimate": 497,
        "source_type": "readme",
        "agent_id": "6914a362d60f052d"
      },
      {
        "chunk_id": 4,
        "text": "Ethereum by address `0xcafe...cafe` in May 2025.\n```\n\n```plaintext\nWhich 10 most recent logs were emitted by `0xFe89cc7aBB2C4183683ab71653C4cdc9B02D44b7`\nbefore `Nov 08 2024 04:21:35 AM (-06:00¬†UTC)`?\n```\n\n```plaintext\nTell me more about the transaction `0xf8a55721f7e2dcf85690aaf81519f7bc820bc58a878fa5f81b12aef5ccda0efb`\non Redstone rollup.\n```\n\n```plaintext\nIs there any blacklisting functionality of USDT token on Arbitrum One?\n```\n\n```plaintext\nWhat is the latest block on Gnosis Chain and who is the block minter?\nWere any funds moved from this minter recently?\n```\n\n```plaintext\nWhen the most recent reward distribution of Kinto token was made to the wallet\n`0x7D467D99028199D99B1c91850C4dea0c82aDDF52` in Kinto chain?\n```\n\n```plaintext\nWhich methods of `0x1c479675ad559DC151F6Ec7ed3FbF8ceE79582B6` on the Ethereum \nmainnet could emit `SequencerBatchDelivered`?\n```\n\n```plaintext\nWhat is the most recent executed cross-chain message sent from the Arbitrum Sepolia\nrollup to the base layer?\n```\n\n## Development & Deployment\n\n### Local Installation\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/blockscout/mcp-server.git\ncd mcp-server\nuv pip install -e . # or `pip install -e .`\n```\n\nTo customize the leading part of the `User-Agent` header used for RPC requests,\nset the `BLOCKSCOUT_MCP_USER_AGENT` environment variable (defaults to\n\"Blockscout MCP\"). The server version is appended automatically.",
        "start_pos": 7193,
        "end_pos": 8636,
        "token_count_estimate": 360,
        "source_type": "readme",
        "agent_id": "6914a362d60f052d"
      },
      {
        "chunk_id": 5,
        "text": "--http-port 8080\n```\n\n**Development Mode (Plain JSON Responses):**\n\nFor development and testing with simple HTTP clients (curl, Insomnia), you can enable plain JSON responses instead of SSE streams:\n\n```bash\nexport BLOCKSCOUT_DEV_JSON_RESPONSE=true\npython -m blockscout_mcp_server --http\n```\n\n**Note:** This disables Server-Sent Events (SSE) and progress notifications. Only use this for local testing and debugging.\n\n**Tunneling with Ngrok (Development Mode):**\n\nThe Python MCP SDK enforces DNS rebinding protection, which blocks requests from ngrok tunnels by default. To enable\ntunneling for development and testing:\n\n1. Start an ngrok tunnel to your local server:\n\n   ```bash\n   ngrok http 8000\n   ```\n\n1. Configure the allowed host and origin using your ngrok URL:\n\n   ```bash\n   export BLOCKSCOUT_MCP_ALLOWED_HOSTS=\"your-tunnel-id.ngrok-free.app\"\n   export BLOCKSCOUT_MCP_ALLOWED_ORIGINS=\"https://your-tunnel-id.ngrok-free.app\"\n   python -m blockscout_mcp_server --http\n   ```\n\n**Note:** These settings are primarily for development use. When these variables are not set, DNS rebinding protection\nis automatically determined by the server's bind host: enabled for localhost, disabled for non-localhost (e.g.,\n`0.0.0.0`). If your Host header includes a non-standard port, use the `:*` wildcard suffix (e.g.,\n`\"example.com:*\"`) or specify the exact host:port value.\n\nFor more details on ngrok tunneling with MCP servers, see the [OpenAI Apps SDK Examples\ndocumentation](https://github.com/openai/openai-apps-sdk-examples/blob/main/README.md#testing-in-chatgpt).\n\n**HTTP Mode with REST API:**\n\nTo enable the versioned REST API alongside the MCP endpoint, use the `--rest` flag (which requires `--http`).\n\n```bash\npython -m blockscout_mcp_server --http --rest\n```\n\nWith custom host and port:\n\n```bash\npython -m blockscout_mcp_server --http --rest --http-host 0.0.0.0 --http-port 8080\n```\n\n**CLI Options:**\n\n- `--http`: Enables HTTP Streamable mode.\n- `--http-host TEXT`: Host to bind the HTTP server to (default: `127.0.0.1`).",
        "start_pos": 9041,
        "end_pos": 11069,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "6914a362d60f052d"
      },
      {
        "chunk_id": 6,
        "text": "server --http --rest --http-host 0.0.0.0 --http-port 8080\n```\n\n**CLI Options:**\n\n- `--http`: Enables HTTP Streamable mode.\n- `--http-host TEXT`: Host to bind the HTTP server to (default: `127.0.0.1`).\n- `--http-port INTEGER`: Port for the HTTP server (default: `8000`).\n- `--rest`: Enables the REST API (requires `--http`).\n\n### Building Docker Image Locally\n\nBuild the Docker image with the official tag:\n\n```bash\ndocker build -t ghcr.io/blockscout/mcp-server:latest .\n```\n\n### Pulling from GitHub Container Registry\n\nPull the pre-built image:\n\n```bash\ndocker pull ghcr.io/blockscout/mcp-server:latest\n```\n\n### Running with Docker\n\n**HTTP Mode (MCP only):**\n\nTo run the Docker container in HTTP mode with port mapping:\n\n```bash\ndocker run --rm -p 8000:8000 ghcr.io/blockscout/mcp-server:latest python -m blockscout_mcp_server --http --http-host 0.0.0.0\n```\n\nWith custom port:\n\n```bash\ndocker run --rm -p 8080:8080 ghcr.io/blockscout/mcp-server:latest python -m blockscout_mcp_server --http --http-host 0.0.0.0 --http-port 8080\n```\n\n**HTTP Mode with REST API:**\n\nTo run with the REST API enabled:\n\n```bash\ndocker run --rm -p 8000:8000 ghcr.io/blockscout/mcp-server:latest python -m blockscout_mcp_server --http --rest --http-host 0.0.0.0\n```\n\n**Note:** When running in HTTP mode with Docker, use `--http-host 0.0.0.0` to bind to all interfaces so the server is accessible from outside the container.\n\n**Stdio Mode:** The default stdio mode is designed for use with MCP hosts/clients (like Claude Desktop, Cursor) and doesn't make sense to run directly with Docker without an MCP client managing the communication.\n\n### Testing with Claude Desktop\n\nUse MCP bundle to test the server with Claude Desktop.\n\n1. Build the bundle as per instructions in [mcpb/README.md](mcpb/README.md).\n2. Open Claude Desktop.\n3. Double-click to open the `blockscout-mcp-dev.mcpb` file to automatically install the bundle.\n4.",
        "start_pos": 10869,
        "end_pos": 12772,
        "token_count_estimate": 475,
        "source_type": "readme",
        "agent_id": "6914a362d60f052d"
      },
      {
        "chunk_id": 7,
        "text": "1. Build the bundle as per instructions in [mcpb/README.md](mcpb/README.md).\n2. Open Claude Desktop.\n3. Double-click to open the `blockscout-mcp-dev.mcpb` file to automatically install the bundle.\n4. Configure the Blockscout MCP Server URL when prompted (default: `http://127.0.0.1:8000/mcp`)\n\n## Privacy and Anonymous Telemetry\n\nTo help us improve the Blockscout MCP Server, community-run instances of the server collect anonymous usage data by default. This helps us understand which tools are most popular and guides our development efforts.\n\n**What we collect:**\n\n- The name of the tool being called (e.g., `get_block_number`).\n- The parameters provided to the tool.\n- The version of the Blockscout MCP Server being used.\n\n**What we DO NOT collect:**\n\n- We do not collect any personal data, IP addresses (the central server uses the sender's IP for geolocation via Mixpanel and then discards it), secrets, or private keys.\n\n### How to Opt-Out\n\nYou can disable this feature at any time by setting the following environment variable:\n\n```bash\nexport BLOCKSCOUT_DISABLE_COMMUNITY_TELEMETRY=true\n```\n\n## License\n\nThis project is primarily distributed under the terms of the MIT license. See [LICENSE](LICENSE) for details.",
        "start_pos": 12572,
        "end_pos": 13796,
        "token_count_estimate": 305,
        "source_type": "readme",
        "agent_id": "6914a362d60f052d"
      }
    ]
  },
  {
    "agent_id": "0bd587512aec7007",
    "name": "ai.smithery/brandonbosco-sigao-scf-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@brandonbosco/sigao-scf-mcp/mcp",
    "description": "Provides access to Civic Plus - See Click Fix, allowing you to interact with your data via an LLM.‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-06T17:06:30.80395Z",
    "indexed_at": "2026-02-18T04:05:48.527683",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to Civic Plus - See Click Fix data",
        "Allow interaction with Civic Plus - See Click Fix data via a large language model"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence providing a basic overview without examples, detailed capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "a4df60a22598e1e8",
    "name": "ai.smithery/brave",
    "source": "mcp",
    "source_url": "https://github.com/brave/brave-search-mcp-server",
    "description": "Visit https://brave.com/search/api/ for a free API key. Search the web, local businesses, images,‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-12T14:23:41.796187Z",
    "indexed_at": "2026-02-18T04:05:49.961572",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing comprehensive search capabilities including web search, local business search, image search, video search, news search, and AI-powered summarization. This project supports both STDIO and HTTP transports, with STDIO as the default mode.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/brave/brave-search-mcp-server)\n\n## Migration\n\n### 1.x to 2.x\n\n#### Default transport now STDIO\n\nTo follow established MCP conventions, the server now defaults to STDIO. If you would like to continue using HTTP, you will need to set the `BRAVE_MCP_TRANSPORT` environment variable to `http`, or provide the runtime argument `--transport http` when launching the server.\n\n#### Response structure of `brave_image_search`\n\nVersion 1.x of the MCP server would return base64-encoded image data along with image URLs. This dramatically slowed down the response, as well as consumed unnecessarily context in the session. Version 2.x removes the base64-encoded data, and returns a response object that more closely reflects the original Brave Search API response. The updated output schema is defined in [`src/tools/images/schemas/output.ts`](https://github.com/brave/brave-search-mcp-server/blob/main/src/tools/images/schemas/output.ts).\n\n## Tools\n\n### Web Search (`brave_web_search`)\nPerforms comprehensive web searches with rich result types and advanced filtering options.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-20, default: 10)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n- `text_decorations` (boolean, optional): Include highlighting markers (default: true)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `result_filter` (array, optional): Filter result types (default: [\"web\", \"query\"])\n- `goggles` (array, optional): Custom re-ranking definitions\n- `units` (string, optional): Measurement units (\"metric\" or \"imperial\")\n- `extra_snippets` (boolean, optional): Get additional excerpts (Pro plans only)\n- `summary` (boolean, optional): Enable summary key generation for AI summarization\n\n### Local Search (`brave_local_search`)\nSearches for local businesses and places with detailed information including ratings, hours, and AI-generated descriptions.\n\n**Parameters:**\n- Same as `brave_web_search` with automatic location filtering\n- Automatically includes \"web\" and \"locations\" in result_filter\n\n**Note:** Requires Pro plan for full local search capabilities. Falls back to web search otherwise.\n\n### Video Search (`brave_video_search`)\nSearches for videos with comprehensive metadata and thumbnail information.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-50, default: 20)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n\n### Image Search (`brave_image_search`)\nSearches for images with automatic fetching and base64 encoding for direct display.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `count` (number, optional): Results per page (1-200, default: 50)\n- `safesearch` (string, optional): Content filtering (\"off\", \"strict\", default: \"strict\")\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n\n### News Search (`brave_news_search`)\nSearches for current news articles with freshness controls and breaking news indicators.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-50, default: 20)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (default: \"pd\" for last 24 hours)\n- `extra_snippets` (boolean, optional): Get additional excerpts (Pro plans only)\n- `goggles` (array, optional): Custom re-ranking definitions\n\n### Summarizer Search (`brave_summarizer`)\nGenerates AI-powered summaries from web search results using Brave's summarization API.\n\n**Parameters:**\n- `key` (string, required): Summary key from web search results (use `summary: true` in web search)\n- `entity_info` (boolean, optional): Include entity information (default: false)\n- `inline_references` (boolean, optional): Add source URL references (default: false)\n\n**Usage:** First perform a web search with `summary: true`, then use the returned summary key with this tool.\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan:\n   - **Free**: 2,000 queries/month, basic web search\n   - **Pro**: Enhanced features including local search, AI summaries, extra snippets\n3. Generate your API key from the [developer dashboard](https://api-dashboard.search.brave.com/app/keys)\n\n### Environment Variables\n\nThe server supports the following environment variables:\n\n- `BRAVE_API_KEY`: Your Brave Search API key (required)\n- `BRAVE_MCP_TRANSPORT`: Transport mode (\"http\" or \"stdio\", default: \"stdio\")\n- `BRAVE_MCP_PORT`: HTTP server port (default: 8000)\n- `BRAVE_MCP_HOST`: HTTP server host (default: \"0.0.0.0\")\n- `BRAVE_MCP_LOG_LEVEL`: Desired logging level(\"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", or \"emergency\", default: \"info\")\n- `BRAVE_MCP_ENABLED_TOOLS`: When used, specifies a whitelist for supported tools\n- `BRAVE_MCP_DISABLED_TOOLS`: When used, specifies a blacklist for supported tools\n- `BRAVE_MCP_STATELESS`: HTTP stateless mode (default: \"true\").  When running on Amazon Bedrock Agentcore, set to \"true\".\n\n### Command Line Options\n\n```bash\nnode dist/index.js [options]\n\nOptions:\n  --brave-api-key <string>    Brave API key\n  --transport <stdio|http>    Transport type (default: stdio)\n  --port <number>             HTTP server port (default: 8080)\n  --host <string>             HTTP server host (default: 0.0.0.0)\n  --logging-level <string>    Desired logging level (one of _debug_, _info_, _notice_, _warning_, _error_, _critical_, _alert_, or _emergency_)\n  --enabled-tools             Tools whitelist (only the specified tools will be enabled)\n  --disabled-tools            Tools blacklist (included tools will be disabled)\n  --stateless  <boolean>      HTTP Stateless flag\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install Brave Search automatically via [Smithery](https://smithery.ai/server/brave):\n\n```bash\nnpx -y @smithery/cli install brave\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n#### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"BRAVE_API_KEY\", \"docker.io/mcp/brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@brave/brave-search-mcp-server\", \"--transport\", \"http\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation, use the one-click installation buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40brave%2Fbrave-search-mcp-server%22%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40brave%2Fbrave-search-mcp-server%22%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)  \n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following to your User Settings (JSON) or `.vscode/mcp.json`:\n\n#### Docker\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key\",\n    }\n  ],\n  \"servers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"BRAVE_API_KEY\", \"mcp/brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key\",\n    }\n  ],\n  \"servers\": {\n    \"brave-search-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@brave/brave-search-mcp-server\", \"--transport\", \"stdio\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n## Build\n\n### Docker\n\n```bash\ndocker build -t mcp/brave-search:latest .\n```\n\n### Local Build\n\n```bash\nnpm install\nnpm run build\n```\n\n## Development\n\n### Prerequisites\n\n- Node.js 22.x or higher\n- npm\n- Brave Search API key\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/brave/brave-search-mcp-server.git\ncd brave-search-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n### Testing via Claude Desktop\n\nAdd a reference to your local build in `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search-dev\": {\n      \"command\": \"node\",\n      \"args\": [\"C:\\\\GitHub\\\\brave-search-mcp-server\\\\dist\\\\index.js\"], // Verify your path\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Testing via MCP Inspector\n\n1. Build and start the server:\n```bash\nnpm run build\nnode dist/index.js\n```\n\n2. In another terminal, start the MCP Inspector:\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\nSTDIO is the default mode. For HTTP mode testing, add `--transport http` to the arguments in the Inspector UI.\n\n### Testing via Smithery.AI\n\n1. Establish and acquire a smithery.ai account and API key\n2. Run `npm run install`, `npm run smithery:build`, and lastly `npm run smithery:dev` to begin testing\n\n### Available Scripts\n\n- `npm run build`: Build the TypeScript project\n- `npm run watch`: Watch for changes and rebuild\n- `npm run format`: Format code with Prettier\n- `npm run format:check`: Check code formatting\n- `npm run prepare`: Format and build (runs automatically on npm install)\n\n- `npm run inspector`: Launch an instance of MCP Inspector\n- `npm run inspector:stdio`: Launch a instance of MCP Inspector, configured for STDIO\n- `npm run smithery:build`: Build the project for smithery.ai\n- `npm run smithery:dev`: Launch the development environment for smithery.ai\n\n### Docker Compose\n\nFor local development with Docker:\n\n```bash\ndocker-compose up --build\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Perform comprehensive web searches with advanced filtering and rich result types",
        "Search for local businesses and places with detailed information and AI-generated descriptions",
        "Search for videos with metadata and thumbnail information",
        "Search for images with automatic fetching and base64 encoding for display",
        "Search for current news articles with freshness controls and breaking news indicators",
        "Generate AI-powered summaries from web search results using Brave's summarization API",
        "Support both STDIO and HTTP transports for communication",
        "Allow customization of search parameters including language, country, safesearch, and pagination",
        "Enable custom re-ranking of results via goggles parameter"
      ],
      "limitations": [
        "Local search full capabilities require a Pro plan; otherwise, it falls back to web search",
        "Extra snippets and AI summarization features require a Pro plan",
        "Pagination offset is limited to a maximum of 9",
        "Web search query length limited to 400 characters and 50 words",
        "Image search base64-encoded data removed in version 2.x to reduce response size",
        "HTTP stateless mode defaults to true and must be configured for specific environments like Amazon Bedrock Agentcore"
      ],
      "requirements": [
        "Brave Search API key is required for operation",
        "Environment variable BRAVE_API_KEY must be set with the API key",
        "Node.js environment to run the server (implied by usage of node command)",
        "Optional environment variables for transport mode, port, host, logging level, and tool enable/disable lists",
        "Pro plan subscription required for enhanced features like local search, AI summaries, and extra snippets"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with parameters, usage examples, configuration options, migration notes, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Brave Search MCP Server\n\nAn MCP server implementation that integrates the Brave Search API, providing comprehensive search capabilities including web search, local business search, image search, video search, news search, and AI-powered summarization. This project supports both STDIO and HTTP transports, with STDIO as the default mode.\n\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/brave/brave-search-mcp-server)\n\n## Migration\n\n### 1.x to 2.x\n\n#### Default transport now STDIO\n\nTo follow established MCP conventions, the server now defaults to STDIO. If you would like to continue using HTTP, you will need to set the `BRAVE_MCP_TRANSPORT` environment variable to `http`, or provide the runtime argument `--transport http` when launching the server.\n\n#### Response structure of `brave_image_search`\n\nVersion 1.x of the MCP server would return base64-encoded image data along with image URLs. This dramatically slowed down the response, as well as consumed unnecessarily context in the session. Version 2.x removes the base64-encoded data, and returns a response object that more closely reflects the original Brave Search API response. The updated output schema is defined in [`src/tools/images/schemas/output.ts`](https://github.com/brave/brave-search-mcp-server/blob/main/src/tools/images/schemas/output.ts).\n\n## Tools\n\n### Web Search (`brave_web_search`)\nPerforms comprehensive web searches with rich result types and advanced filtering options.",
        "start_pos": 0,
        "end_pos": 1478,
        "token_count_estimate": 369,
        "source_type": "readme",
        "agent_id": "a4df60a22598e1e8"
      },
      {
        "chunk_id": 1,
        "text": "onal): Pagination offset (max 9, default: 0)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n- `text_decorations` (boolean, optional): Include highlighting markers (default: true)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `result_filter` (array, optional): Filter result types (default: [\"web\", \"query\"])\n- `goggles` (array, optional): Custom re-ranking definitions\n- `units` (string, optional): Measurement units (\"metric\" or \"imperial\")\n- `extra_snippets` (boolean, optional): Get additional excerpts (Pro plans only)\n- `summary` (boolean, optional): Enable summary key generation for AI summarization\n\n### Local Search (`brave_local_search`)\nSearches for local businesses and places with detailed information including ratings, hours, and AI-generated descriptions.\n\n**Parameters:**\n- Same as `brave_web_search` with automatic location filtering\n- Automatically includes \"web\" and \"locations\" in result_filter\n\n**Note:** Requires Pro plan for full local search capabilities. Falls back to web search otherwise.\n\n### Video Search (`brave_video_search`)\nSearches for videos with comprehensive metadata and thumbnail information.",
        "start_pos": 1848,
        "end_pos": 3153,
        "token_count_estimate": 326,
        "source_type": "readme",
        "agent_id": "a4df60a22598e1e8"
      },
      {
        "chunk_id": 2,
        "text": "\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (\"pd\", \"pw\", \"pm\", \"py\", or date range)\n\n### Image Search (`brave_image_search`)\nSearches for images with automatic fetching and base64 encoding for direct display.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `count` (number, optional): Results per page (1-200, default: 50)\n- `safesearch` (string, optional): Content filtering (\"off\", \"strict\", default: \"strict\")\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n\n### News Search (`brave_news_search`)\nSearches for current news articles with freshness controls and breaking news indicators.\n\n**Parameters:**\n- `query` (string, required): Search terms (max 400 chars, 50 words)\n- `country` (string, optional): Country code (default: \"US\")\n- `search_lang` (string, optional): Search language (default: \"en\")\n- `ui_lang` (string, optional): UI language (default: \"en-US\")\n- `count` (number, optional): Results per page (1-50, default: 20)\n- `offset` (number, optional): Pagination offset (max 9, default: 0)\n- `spellcheck` (boolean, optional): Enable spell checking (default: true)\n- `safesearch` (string, optional): Content filtering (\"off\", \"moderate\", \"strict\", default: \"moderate\")\n- `freshness` (string, optional): Time filter (default: \"pd\" for last 24 hours)\n- `extra_snippets` (boolean, optional): Get additional excerpts (Pro plans only)\n- `goggles` (array, optional): Custom re-ranking definitions\n\n### Summarizer Search (`brave_summarizer`)\nGenerates AI-powered summaries from web search results using Brave's summarization API.",
        "start_pos": 3696,
        "end_pos": 5477,
        "token_count_estimate": 445,
        "source_type": "readme",
        "agent_id": "a4df60a22598e1e8"
      },
      {
        "chunk_id": 3,
        "text": "search results (use `summary: true` in web search)\n- `entity_info` (boolean, optional): Include entity information (default: false)\n- `inline_references` (boolean, optional): Add source URL references (default: false)\n\n**Usage:** First perform a web search with `summary: true`, then use the returned summary key with this tool.\n\n## Configuration\n\n### Getting an API Key\n\n1. Sign up for a [Brave Search API account](https://brave.com/search/api/)\n2. Choose a plan:\n   - **Free**: 2,000 queries/month, basic web search\n   - **Pro**: Enhanced features including local search, AI summaries, extra snippets\n3. Generate your API key from the [developer dashboard](https://api-dashboard.search.brave.com/app/keys)\n\n### Environment Variables\n\nThe server supports the following environment variables:\n\n- `BRAVE_API_KEY`: Your Brave Search API key (required)\n- `BRAVE_MCP_TRANSPORT`: Transport mode (\"http\" or \"stdio\", default: \"stdio\")\n- `BRAVE_MCP_PORT`: HTTP server port (default: 8000)\n- `BRAVE_MCP_HOST`: HTTP server host (default: \"0.0.0.0\")\n- `BRAVE_MCP_LOG_LEVEL`: Desired logging level(\"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", or \"emergency\", default: \"info\")\n- `BRAVE_MCP_ENABLED_TOOLS`: When used, specifies a whitelist for supported tools\n- `BRAVE_MCP_DISABLED_TOOLS`: When used, specifies a blacklist for supported tools\n- `BRAVE_MCP_STATELESS`: HTTP stateless mode (default: \"true\").  When running on Amazon Bedrock Agentcore, set to \"true\".",
        "start_pos": 5544,
        "end_pos": 7017,
        "token_count_estimate": 368,
        "source_type": "readme",
        "agent_id": "a4df60a22598e1e8"
      },
      {
        "chunk_id": 4,
        "text": "g_, _info_, _notice_, _warning_, _error_, _critical_, _alert_, or _emergency_)\n  --enabled-tools             Tools whitelist (only the specified tools will be enabled)\n  --disabled-tools            Tools blacklist (included tools will be disabled)\n  --stateless  <boolean>      HTTP Stateless flag\n```\n\n## Installation\n\n### Installing via Smithery\n\nTo install Brave Search automatically via [Smithery](https://smithery.ai/server/brave):\n\n```bash\nnpx -y @smithery/cli install brave\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n#### Docker\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"BRAVE_API_KEY\", \"docker.io/mcp/brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@brave/brave-search-mcp-server\", \"--transport\", \"http\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Usage with VS Code\n\nFor quick installation, use the one-click installation buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40brave%2Fbrave-search-mcp-server%22%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key",
        "start_pos": 7392,
        "end_pos": 9440,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "a4df60a22598e1e8"
      },
      {
        "chunk_id": 5,
        "text": "NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40brave%2Fbrave-search-mcp-server%22%2C%22--transport%22%2C%22stdio%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)  \n[![Install with Docker in VS Code](https://img.shields.io/badge/VS_Code-Docker-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D) [![Install with Docker in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Docker-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=brave-search&inputs=%5B%7B%22password%22%3Atrue%2C%22id%22%3A%22brave-api-key%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Brave+Search+API+Key%22%7D%5D&config=%7B%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22BRAVE_API_KEY%22%2C%22mcp%2Fbrave-search%22%5D%2C%22env%22%3A%7B%22BRAVE_API_KEY%22%3A%22%24%7Binput%3Abrave-api-key%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following to your User Settings (JSON) or `.vscode/mcp.json`:\n\n#### Docker\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key",
        "start_pos": 9240,
        "end_pos": 11288,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "a4df60a22598e1e8"
      },
      {
        "chunk_id": 6,
        "text": "(JSON) or `.vscode/mcp.json`:\n\n#### Docker\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key\",\n    }\n  ],\n  \"servers\": {\n    \"brave-search\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"-e\", \"BRAVE_API_KEY\", \"mcp/brave-search\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n#### NPX\n\n```json\n{\n  \"inputs\": [\n    {\n      \"password\": true,\n      \"id\": \"brave-api-key\",\n      \"type\": \"promptString\",\n      \"description\": \"Brave Search API Key\",\n    }\n  ],\n  \"servers\": {\n    \"brave-search-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@brave/brave-search-mcp-server\", \"--transport\", \"stdio\"],\n      \"env\": {\n        \"BRAVE_API_KEY\": \"${input:brave-api-key}\"\n      }\n    }\n  }\n}\n```\n\n## Build\n\n### Docker\n\n```bash\ndocker build -t mcp/brave-search:latest .\n```\n\n### Local Build\n\n```bash\nnpm install\nnpm run build\n```\n\n## Development\n\n### Prerequisites\n\n- Node.js 22.x or higher\n- npm\n- Brave Search API key\n\n### Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/brave/brave-search-mcp-server.git\ncd brave-search-mcp-server\n```\n\n2. Install dependencies:\n```bash\nnpm install\n```\n\n3. Build the project:\n```bash\nnpm run build\n```\n\n### Testing via Claude Desktop\n\nAdd a reference to your local build in `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"brave-search-dev\": {\n      \"command\": \"node\",\n      \"args\": [\"C:\\\\GitHub\\\\brave-search-mcp-server\\\\dist\\\\index.js\"], // Verify your path\n      \"env\": {\n        \"BRAVE_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n### Testing via MCP Inspector\n\n1. Build and start the server:\n```bash\nnpm run build\nnode dist/index.js\n```\n\n2. In another terminal, start the MCP Inspector:\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\nSTDIO is the default mode. For HTTP mode testing, add `--transport http` to the arguments in the Inspector UI.",
        "start_pos": 11088,
        "end_pos": 13110,
        "token_count_estimate": 505,
        "source_type": "readme",
        "agent_id": "a4df60a22598e1e8"
      },
      {
        "chunk_id": 7,
        "text": "rt the MCP Inspector:\n```bash\nnpx @modelcontextprotocol/inspector node dist/index.js\n```\n\nSTDIO is the default mode. For HTTP mode testing, add `--transport http` to the arguments in the Inspector UI.\n\n### Testing via Smithery.AI\n\n1. Establish and acquire a smithery.ai account and API key\n2. Run `npm run install`, `npm run smithery:build`, and lastly `npm run smithery:dev` to begin testing\n\n### Available Scripts\n\n- `npm run build`: Build the TypeScript project\n- `npm run watch`: Watch for changes and rebuild\n- `npm run format`: Format code with Prettier\n- `npm run format:check`: Check code formatting\n- `npm run prepare`: Format and build (runs automatically on npm install)\n\n- `npm run inspector`: Launch an instance of MCP Inspector\n- `npm run inspector:stdio`: Launch a instance of MCP Inspector, configured for STDIO\n- `npm run smithery:build`: Build the project for smithery.ai\n- `npm run smithery:dev`: Launch the development environment for smithery.ai\n\n### Docker Compose\n\nFor local development with Docker:\n\n```bash\ndocker-compose up --build\n```\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",
        "start_pos": 12910,
        "end_pos": 14234,
        "token_count_estimate": 330,
        "source_type": "readme",
        "agent_id": "a4df60a22598e1e8"
      }
    ]
  },
  {
    "agent_id": "9e3fa41c51772467",
    "name": "ai.smithery/browserbasehq-mcp-browserbase",
    "source": "mcp",
    "source_url": "https://github.com/browserbase/mcp-server-browserbase",
    "description": "Provides cloud browser automation capabilities using Stagehand and Browserbase, enabling LLMs to i‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-16T17:17:18.587292Z",
    "indexed_at": "2026-02-18T04:05:52.442694",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Browserbase MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@browserbasehq/mcp-browserbase)](https://smithery.ai/server/@browserbasehq/mcp-browserbase)\n\n![cover](assets/cover.png)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\nThis server provides cloud browser automation capabilities using [Browserbase](https://www.browserbase.com/) and [Stagehand](https://github.com/browserbase/stagehand). It enables LLMs to interact with web pages, take screenshots, extract information, and perform automated actions with atomic precision.\n\n## What's New in Stagehand v3\n\nPowered by [Stagehand v3.0](https://github.com/browserbase/stagehand), this MCP server now includes:\n\n- **20-40% Faster Performance**: Speed improvements across all core operations (`act`, `extract`, `observe`) through automatic caching\n- **Enhanced Extraction**: Targeted extraction and observation across iframes and shadow roots\n- **Improved Schemas**: Streamlined extract schemas for more intuitive data extraction\n- **Advanced Selector Support**: CSS selector support with improved element targeting\n- **Multi-Browser Support**: Compatible with Playwright, Puppeteer, and Patchright\n- **New Primitives**: Built-in `page`, `locator`, `frameLocator`, and `deepLocator` for simplified automation\n- **Experimental Features**: Enable cutting-edge capabilities with the `--experimental` flag\n\nFor more details, visit the [Stagehand v3 documentation](https://docs.stagehand.dev/).\n\n## Features\n\n| Feature            | Description                                                 |\n| ------------------ | ----------------------------------------------------------- |\n| Browser Automation | Control and orchestrate cloud browsers via Browserbase      |\n| Data Extraction    | Extract structured data from any webpage                    |\n| Web Interaction    | Navigate, click, and fill forms with ease                   |\n| Screenshots        | Capture full-page and element screenshots                   |\n| Model Flexibility  | Supports multiple models (OpenAI, Claude, Gemini, and more) |\n| Vision Support     | Use annotated screenshots for complex DOMs                  |\n| Session Management | Create, manage, and close browser sessions                  |\n| High Performance   | 20-40% faster operations with automatic caching (v3)        |\n| Advanced Selectors | Enhanced CSS selector support for precise element targeting |\n\n## How to Setup\n\n### Quickstarts:\n\n#### Add to Cursor\n\nCopy and Paste this link in your Browser:\n\n```text\ncursor://anysphere.cursor-deeplink/mcp/install?name=browserbase&config=eyJjb21tYW5kIjoibnB4IEBicm93c2VyYmFzZWhxL21jcCIsImVudiI6eyJCUk9XU0VSQkFTRV9BUElfS0VZIjoiIiwiQlJPV1NFUkJBU0VfUFJPSkVDVF9JRCI6IiIsIkdFTUlOSV9BUElfS0VZIjoiIn19\n```\n\nWe currently support 2 transports for our MCP server, STDIO and SHTTP. We recommend you use SHTTP with our remote hosted url to take advantage of the server at full capacity.\n\n## SHTTP:\n\nTo use the Browserbase MCP Server through our remote hosted URL, add the following to your configuration.\n\nGo to [smithery.ai](https://smithery.ai/server/@browserbasehq/mcp-browserbase) and enter your API keys and configuration to get a remote hosted URL.\nWhen using our remote hosted server, we provide the LLM costs for Gemini, the [best performing model](https://www.stagehand.dev/evals) in [Stagehand](https://www.stagehand.dev).\n\n![Smithery Image](assets/smithery.jpg)\n\nIf your client supports SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"type\": \"http\",\n      \"url\": \"your-smithery-url.com\"\n    }\n  }\n}\n```\n\nIf your client doesn't support SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"your-smithery-url.com\"]\n    }\n  }\n}\n```\n\n## STDIO:\n\nYou can either use our Server hosted on NPM or run it completely locally by cloning this repo.\n\n> **‚ùóÔ∏è Important:** If you want to use a different model you have to add --modelName to the args and provide that respective key as an arg. More info below.\n\n### To run on NPM (Recommended)\n\nGo into your MCP Config JSON and add the Browserbase Server:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThat's it! Reload your MCP client and Claude will be able to use Browserbase.\n\n### To run 100% local:\n\n#### Option 1: Direct installation\n\n```bash\n# Clone the Repo\ngit clone https://github.com/browserbase/mcp-server-browserbase.git\ncd mcp-server-browserbase\n\n# Install the dependencies and build the project\nnpm install && npm run build\n```\n\n#### Option 2: Docker\n\n```bash\n# Clone the Repo\ngit clone https://github.com/browserbase/mcp-server-browserbase.git\ncd mcp-server-browserbase\n\n# Build the Docker image\ndocker build -t mcp-browserbase .\n```\n\nThen in your MCP Config JSON run the server. To run locally we can use STDIO or self-host SHTTP.\n\n### STDIO:\n\n#### Using Direct Installation\n\nTo your MCP Config JSON file add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-server-browserbase/cli.js\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n#### Using Docker\n\nTo your MCP Config JSON file add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"BROWSERBASE_API_KEY\",\n        \"-e\",\n        \"BROWSERBASE_PROJECT_ID\",\n        \"-e\",\n        \"GEMINI_API_KEY\",\n        \"mcp-browserbase\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThen reload your MCP client and you should be good to go!\n\n## Configuration\n\nThe Browserbase MCP server accepts the following command-line flags:\n\n| Flag                       | Description                                                                 |\n| -------------------------- | --------------------------------------------------------------------------- |\n| `--proxies`                | Enable Browserbase proxies for the session                                  |\n| `--advancedStealth`        | Enable Browserbase Advanced Stealth (Only for Scale Plan Users)             |\n| `--keepAlive`              | Enable Browserbase Keep Alive Session                                       |\n| `--contextId <contextId>`  | Specify a Browserbase Context ID to use                                     |\n| `--persist`                | Whether to persist the Browserbase context (default: true)                  |\n| `--port <port>`            | Port to listen on for HTTP/SHTTP transport                                  |\n| `--host <host>`            | Host to bind server to (default: localhost, use 0.0.0.0 for all interfaces) |\n| `--browserWidth <width>`   | Browser viewport width (default: 1024)                                      |\n| `--browserHeight <height>` | Browser viewport height (default: 768)                                      |\n| `--modelName <model>`      | The model to use for Stagehand (default: gemini-2.0-flash)                  |\n| `--modelApiKey <key>`      | API key for the custom model provider (required when using custom models)   |\n| `--experimental`           | Enable experimental features (default: false)                               |\n\nThese flags can be passed directly to the CLI or configured in your MCP configuration file.\n\n### NOTE:\n\nCurrently, these flags can only be used with the local server (npx @browserbasehq/mcp-server-browserbase or Docker).\n\n### Using Configuration Flags with Docker\n\nWhen using Docker, you can pass configuration flags as additional arguments after the image name. Here's an example with the `--proxies` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"BROWSERBASE_API_KEY\",\n        \"-e\",\n        \"BROWSERBASE_PROJECT_ID\",\n        \"-e\",\n        \"GEMINI_API_KEY\",\n        \"mcp-browserbase\",\n        \"--proxies\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nYou can also run the Docker container directly from the command line:\n\n```bash\ndocker run --rm -i \\\n  -e BROWSERBASE_API_KEY=your_api_key \\\n  -e BROWSERBASE_PROJECT_ID=your_project_id \\\n  -e GEMINI_API_KEY=your_gemini_key \\\n  mcp-browserbase --proxies\n```\n\n## Configuration Examples\n\n### Proxies\n\nHere are our docs on [Proxies](https://docs.browserbase.com/features/proxies).\n\nTo use proxies, set the --proxies flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--proxies\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Stealth\n\nHere are our docs on [Advanced Stealth](https://docs.browserbase.com/features/stealth-mode#advanced-stealth-mode).\n\nTo use advanced stealth, set the --advancedStealth flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--advancedStealth\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Contexts\n\nHere are our docs on [Contexts](https://docs.browserbase.com/features/contexts)\n\nTo use contexts, set the --contextId flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--contextId\",\n        \"<YOUR_CONTEXT_ID>\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Browser Viewport Sizing\n\nThe default viewport sizing for a browser session is 1024 x 768. You can adjust the Browser viewport sizing with browserWidth and browserHeight flags.\n\nHere's how to use it for custom browser sizing. We recommend to stick with 16:9 aspect ratios (ie: 1920 x 1080, 1280 x 720, 1024 x 768)\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--browserHeight 1080\",\n        \"--browserWidth 1920\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Experimental Features\n\nStagehand v3 includes experimental features that can be enabled with the `--experimental` flag. These features provide cutting-edge capabilities that are actively being developed and refined.\n\nTo enable experimental features:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--experimental\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n_Note: Experimental features may change or be removed in future releases. Use them at your own discretion._\n\n### Model Configuration\n\nStagehand defaults to using Google's Gemini 2.0 Flash model, but you can configure it to use other models like GPT-4o, Claude, or other providers.\n\n**Important**: When using any custom model (non-default), you must provide your own API key for that model provider using the `--modelApiKey` flag.\n\nHere's how to configure different models:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--modelName\",\n        \"anthropic/claude-sonnet-4.5\",\n        \"--modelApiKey\",\n        \"your-anthropic-api-key\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\"\n      }\n    }\n  }\n}\n```\n\n_Note: The model must be supported in Stagehand. Check out the docs [here](https://docs.stagehand.dev/examples/custom_llms#supported-llms). When using any custom model, you must provide your own API key for that provider._\n\n### Resources\n\nThe server provides access to screenshot resources:\n\n1. **Screenshots** (`screenshot://<screenshot-name>`)\n   - PNG images of captured screenshots\n\n## Key Features\n\n- **AI-Powered Automation**: Natural language commands for web interactions\n- **Multi-Model Support**: Works with OpenAI, Claude, Gemini, and more\n- **Screenshot Capture**: Full-page and element-specific screenshots\n- **Data Extraction**: Intelligent content extraction from web pages\n- **Proxy Support**: Enterprise-grade proxy capabilities\n- **Stealth Mode**: Advanced anti-detection features\n- **Context Persistence**: Maintain authentication and state across sessions\n\nFor more information about the Model Context Protocol, visit:\n\n- [MCP Documentation](https://modelcontextprotocol.io/docs)\n- [MCP Specification](https://spec.modelcontextprotocol.io/)\n\nFor the official MCP Docs:\n\n- [Browserbase MCP](https://docs.browserbase.com/integrations/mcp/introduction)\n\n## License\n\nLicensed under the Apache 2.0 License.\n\nCopyright 2025 Browserbase, Inc.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Control and orchestrate cloud browsers via Browserbase",
        "Extract structured data from any webpage",
        "Navigate, click, and fill forms on web pages",
        "Capture full-page and element screenshots",
        "Manage browser sessions including creation and closure",
        "Support multiple LLM models including OpenAI, Claude, Gemini, and more",
        "Use annotated screenshots for complex DOM interactions",
        "Enable advanced CSS selector support for precise element targeting",
        "Run with multi-browser support including Playwright, Puppeteer, and Patchright",
        "Enable experimental features for cutting-edge browser automation"
      ],
      "limitations": [
        "Configuration flags are only supported with the local server (npx or Docker), not with remote hosted SHTTP",
        "Experimental features may change or be removed in future releases",
        "Requires API keys for Browserbase and model providers to function",
        "Advanced Stealth mode is only available for Scale Plan users"
      ],
      "requirements": [
        "Browserbase API key",
        "Browserbase Project ID",
        "Model API key (e.g., Gemini API key) for model usage",
        "Node.js environment for local installation or Docker for containerized deployment",
        "MCP client supporting either STDIO or SHTTP transport",
        "Optional: Docker installed if using Docker deployment"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed feature descriptions, configuration options, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Browserbase MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@browserbasehq/mcp-browserbase)](https://smithery.ai/server/@browserbasehq/mcp-browserbase)\n\n![cover](assets/cover.png)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you're building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.\n\nThis server provides cloud browser automation capabilities using [Browserbase](https://www.browserbase.com/) and [Stagehand](https://github.com/browserbase/stagehand). It enables LLMs to interact with web pages, take screenshots, extract information, and perform automated actions with atomic precision.\n\n## What's New in Stagehand v3\n\nPowered by [Stagehand v3.0](https://github.com/browserbase/stagehand), this MCP server now includes:\n\n- **20-40% Faster Performance**: Speed improvements across all core operations (`act`, `extract`, `observe`) through automatic caching\n- **Enhanced Extraction**: Targeted extraction and observation across iframes and shadow roots\n- **Improved Schemas**: Streamlined extract schemas for more intuitive data extraction\n- **Advanced Selector Support**: CSS selector support with improved element targeting\n- **Multi-Browser Support**: Compatible with Playwright, Puppeteer, and Patchright\n- **New Primitives**: Built-in `page`, `locator`, `frameLocator`, and `deepLocator` for simplified automation\n- **Experimental Features**: Enable cutting-edge capabilities with the `--experimental` flag\n\nFor more details, visit the [Stagehand v3 documentation](https://docs.stagehand.dev/).",
        "start_pos": 0,
        "end_pos": 1784,
        "token_count_estimate": 446,
        "source_type": "readme",
        "agent_id": "9e3fa41c51772467"
      },
      {
        "chunk_id": 1,
        "text": "|\n| ------------------ | ----------------------------------------------------------- |\n| Browser Automation | Control and orchestrate cloud browsers via Browserbase      |\n| Data Extraction    | Extract structured data from any webpage                    |\n| Web Interaction    | Navigate, click, and fill forms with ease                   |\n| Screenshots        | Capture full-page and element screenshots                   |\n| Model Flexibility  | Supports multiple models (OpenAI, Claude, Gemini, and more) |\n| Vision Support     | Use annotated screenshots for complex DOMs                  |\n| Session Management | Create, manage, and close browser sessions                  |\n| High Performance   | 20-40% faster operations with automatic caching (v3)        |\n| Advanced Selectors | Enhanced CSS selector support for precise element targeting |\n\n## How to Setup\n\n### Quickstarts:\n\n#### Add to Cursor\n\nCopy and Paste this link in your Browser:\n\n```text\ncursor://anysphere.cursor-deeplink/mcp/install?name=browserbase&config=eyJjb21tYW5kIjoibnB4IEBicm93c2VyYmFzZWhxL21jcCIsImVudiI6eyJCUk9XU0VSQkFTRV9BUElfS0VZIjoiIiwiQlJPV1NFUkJBU0VfUFJPSkVDVF9JRCI6IiIsIkdFTUlOSV9BUElfS0VZIjoiIn19\n```\n\nWe currently support 2 transports for our MCP server, STDIO and SHTTP. We recommend you use SHTTP with our remote hosted url to take advantage of the server at full capacity.\n\n## SHTTP:\n\nTo use the Browserbase MCP Server through our remote hosted URL, add the following to your configuration.\n\nGo to [smithery.ai](https://smithery.ai/server/@browserbasehq/mcp-browserbase) and enter your API keys and configuration to get a remote hosted URL.\nWhen using our remote hosted server, we provide the LLM costs for Gemini, the [best performing model](https://www.stagehand.dev/evals) in [Stagehand](https://www.stagehand.dev).",
        "start_pos": 1848,
        "end_pos": 3694,
        "token_count_estimate": 453,
        "source_type": "readme",
        "agent_id": "9e3fa41c51772467"
      },
      {
        "chunk_id": 2,
        "text": "![Smithery Image](assets/smithery.jpg)\n\nIf your client supports SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"type\": \"http\",\n      \"url\": \"your-smithery-url.com\"\n    }\n  }\n}\n```\n\nIf your client doesn't support SHTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"your-smithery-url.com\"]\n    }\n  }\n}\n```\n\n## STDIO:\n\nYou can either use our Server hosted on NPM or run it completely locally by cloning this repo.\n\n> **‚ùóÔ∏è Important:** If you want to use a different model you have to add --modelName to the args and provide that respective key as an arg. More info below.\n\n### To run on NPM (Recommended)\n\nGo into your MCP Config JSON and add the Browserbase Server:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThat's it! Reload your MCP client and Claude will be able to use Browserbase.\n\n### To run 100% local:\n\n#### Option 1: Direct installation\n\n```bash\n# Clone the Repo\ngit clone https://github.com/browserbase/mcp-server-browserbase.git\ncd mcp-server-browserbase\n\n# Install the dependencies and build the project\nnpm install && npm run build\n```\n\n#### Option 2: Docker\n\n```bash\n# Clone the Repo\ngit clone https://github.com/browserbase/mcp-server-browserbase.git\ncd mcp-server-browserbase\n\n# Build the Docker image\ndocker build -t mcp-browserbase .\n```\n\nThen in your MCP Config JSON run the server. To run locally we can use STDIO or self-host SHTTP.",
        "start_pos": 3696,
        "end_pos": 5353,
        "token_count_estimate": 414,
        "source_type": "readme",
        "agent_id": "9e3fa41c51772467"
      },
      {
        "chunk_id": 3,
        "text": "to/mcp-server-browserbase/cli.js\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n#### Using Docker\n\nTo your MCP Config JSON file add the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"BROWSERBASE_API_KEY\",\n        \"-e\",\n        \"BROWSERBASE_PROJECT_ID\",\n        \"-e\",\n        \"GEMINI_API_KEY\",\n        \"mcp-browserbase\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nThen reload your MCP client and you should be good to go!",
        "start_pos": 5544,
        "end_pos": 6294,
        "token_count_estimate": 187,
        "source_type": "readme",
        "agent_id": "9e3fa41c51772467"
      },
      {
        "chunk_id": 4,
        "text": "| Browser viewport width (default: 1024)                                      |\n| `--browserHeight <height>` | Browser viewport height (default: 768)                                      |\n| `--modelName <model>`      | The model to use for Stagehand (default: gemini-2.0-flash)                  |\n| `--modelApiKey <key>`      | API key for the custom model provider (required when using custom models)   |\n| `--experimental`           | Enable experimental features (default: false)                               |\n\nThese flags can be passed directly to the CLI or configured in your MCP configuration file.\n\n### NOTE:\n\nCurrently, these flags can only be used with the local server (npx @browserbasehq/mcp-server-browserbase or Docker).\n\n### Using Configuration Flags with Docker\n\nWhen using Docker, you can pass configuration flags as additional arguments after the image name. Here's an example with the `--proxies` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"BROWSERBASE_API_KEY\",\n        \"-e\",\n        \"BROWSERBASE_PROJECT_ID\",\n        \"-e\",\n        \"GEMINI_API_KEY\",\n        \"mcp-browserbase\",\n        \"--proxies\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\nYou can also run the Docker container directly from the command line:\n\n```bash\ndocker run --rm -i \\\n  -e BROWSERBASE_API_KEY=your_api_key \\\n  -e BROWSERBASE_PROJECT_ID=your_project_id \\\n  -e GEMINI_API_KEY=your_gemini_key \\\n  mcp-browserbase --proxies\n```\n\n## Configuration Examples\n\n### Proxies\n\nHere are our docs on [Proxies](https://docs.browserbase.com/features/proxies).",
        "start_pos": 7392,
        "end_pos": 9163,
        "token_count_estimate": 442,
        "source_type": "readme",
        "agent_id": "9e3fa41c51772467"
      },
      {
        "chunk_id": 5,
        "text": "pServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--proxies\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Advanced Stealth\n\nHere are our docs on [Advanced Stealth](https://docs.browserbase.com/features/stealth-mode#advanced-stealth-mode).\n\nTo use advanced stealth, set the --advancedStealth flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--advancedStealth\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Contexts\n\nHere are our docs on [Contexts](https://docs.browserbase.com/features/contexts)\n\nTo use contexts, set the --contextId flag in your MCP Config:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--contextId\",\n        \"<YOUR_CONTEXT_ID>\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Browser Viewport Sizing\n\nThe default viewport sizing for a browser session is 1024 x 768. You can adjust the Browser viewport sizing with browserWidth and browserHeight flags.\n\nHere's how to use it for custom browser sizing.",
        "start_pos": 9240,
        "end_pos": 10744,
        "token_count_estimate": 376,
        "source_type": "readme",
        "agent_id": "9e3fa41c51772467"
      },
      {
        "chunk_id": 6,
        "text": "\"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n### Experimental Features\n\nStagehand v3 includes experimental features that can be enabled with the `--experimental` flag. These features provide cutting-edge capabilities that are actively being developed and refined.\n\nTo enable experimental features:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\"@browserbasehq/mcp-server-browserbase\", \"--experimental\"],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\",\n        \"GEMINI_API_KEY\": \"\"\n      }\n    }\n  }\n}\n```\n\n_Note: Experimental features may change or be removed in future releases. Use them at your own discretion._\n\n### Model Configuration\n\nStagehand defaults to using Google's Gemini 2.0 Flash model, but you can configure it to use other models like GPT-4o, Claude, or other providers.\n\n**Important**: When using any custom model (non-default), you must provide your own API key for that model provider using the `--modelApiKey` flag.\n\nHere's how to configure different models:\n\n```json\n{\n  \"mcpServers\": {\n    \"browserbase\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@browserbasehq/mcp-server-browserbase\",\n        \"--modelName\",\n        \"anthropic/claude-sonnet-4.5\",\n        \"--modelApiKey\",\n        \"your-anthropic-api-key\"\n      ],\n      \"env\": {\n        \"BROWSERBASE_API_KEY\": \"\",\n        \"BROWSERBASE_PROJECT_ID\": \"\"\n      }\n    }\n  }\n}\n```\n\n_Note: The model must be supported in Stagehand. Check out the docs [here](https://docs.stagehand.dev/examples/custom_llms#supported-llms). When using any custom model, you must provide your own API key for that provider._\n\n### Resources\n\nThe server provides access to screenshot resources:\n\n1.",
        "start_pos": 11088,
        "end_pos": 12876,
        "token_count_estimate": 446,
        "source_type": "readme",
        "agent_id": "9e3fa41c51772467"
      },
      {
        "chunk_id": 7,
        "text": "images of captured screenshots\n\n## Key Features\n\n- **AI-Powered Automation**: Natural language commands for web interactions\n- **Multi-Model Support**: Works with OpenAI, Claude, Gemini, and more\n- **Screenshot Capture**: Full-page and element-specific screenshots\n- **Data Extraction**: Intelligent content extraction from web pages\n- **Proxy Support**: Enterprise-grade proxy capabilities\n- **Stealth Mode**: Advanced anti-detection features\n- **Context Persistence**: Maintain authentication and state across sessions\n\nFor more information about the Model Context Protocol, visit:\n\n- [MCP Documentation](https://modelcontextprotocol.io/docs)\n- [MCP Specification](https://spec.modelcontextprotocol.io/)\n\nFor the official MCP Docs:\n\n- [Browserbase MCP](https://docs.browserbase.com/integrations/mcp/introduction)\n\n## License\n\nLicensed under the Apache 2.0 License.\n\nCopyright 2025 Browserbase, Inc.",
        "start_pos": 12936,
        "end_pos": 13838,
        "token_count_estimate": 225,
        "source_type": "readme",
        "agent_id": "9e3fa41c51772467"
      }
    ]
  },
  {
    "agent_id": "fd57be7932212568",
    "name": "ai.smithery/callmybot-cookbook-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/callmybot/cookbook-mcp-server",
    "description": "Count occurrences of any character in your text instantly. Specify the character and get precise c‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T09:25:11.477139Z",
    "indexed_at": "2026-02-18T04:05:54.708552",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# TypeScript MCP Server with Custom Container\n\nA simple TypeScript MCP server built using the official [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk), Express, and custom Docker container. This example demonstrates how you can host HTTP servers on Smithery using custom containers, with STDIO support for backwards compatibility.\n\nSee the complete guide: https://smithery.ai/docs/migrations/typescript-custom-container\n\n**[Try it live on Smithery](https://smithery.ai/server/@smithery-ai/cookbook-ts-custom-container)**\n\n## Features:\n\n- **CORS**: CORS headers for browser-based MCP clients\n- **Smithery Session Configuration**: handles user's session configuration passed via Smithery ([learn more](https://smithery.ai/docs/build/session-config))\n- **Request Logging Middleware**: Custom middleware for debugging HTTP requests and responses\n- **Server Transport**: Can run with both STDIO and HTTP transports using `TRANSPORT` env variable\n\n## Prerequisites\n\n- Node.js 22 or higher\n- npm package manager\n\n## Project Structure\n\n- `src/index.ts` - Main Express server with MCP HTTP transport\n- `package.json` - Node.js dependencies and scripts\n- `smithery.yaml` - Smithery deployment and session configuration\n- `Dockerfile` - Dockerfile to host server in Smithery\n\n## Quick Start\n\n1. **Install dependencies:**\n   ```bash\n   npm install\n   ```\n\n2. **Run the development server:**\n\n   **HTTP Mode:**\n   ```bash\n   npm run dev\n   ```\n   This will start the server on port 8081.\n\n3. **Test interactively:**\n   Once your server is running in HTTP mode, you can test it interactively using the Smithery playground:\n   ```bash\n   npx -y @smithery/cli playground --port 8081\n   ```\n\n   <img src=\"../../../../public/smithery_playground.png\" alt=\"Smithery Playground\" width=\"800\">\n\n4. **Deploy your own version:**\n   To deploy your own MCP server:\n   - Connect your repository at [https://smithery.ai/new](https://smithery.ai/new)"
    },
    "llm_extracted": {
      "capabilities": [
        "Host HTTP servers on Smithery using custom Docker containers",
        "Support both STDIO and HTTP transports for MCP communication",
        "Handle CORS headers for browser-based MCP clients",
        "Manage user session configuration via Smithery session config",
        "Log HTTP requests and responses for debugging purposes",
        "Deploy MCP servers through Smithery platform integration"
      ],
      "limitations": [],
      "requirements": [
        "Node.js version 22 or higher",
        "npm package manager",
        "Smithery account for deployment and session configuration"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, usage examples, detailed feature descriptions, project structure, and deployment guidance, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# TypeScript MCP Server with Custom Container\n\nA simple TypeScript MCP server built using the official [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk), Express, and custom Docker container. This example demonstrates how you can host HTTP servers on Smithery using custom containers, with STDIO support for backwards compatibility.\n\nSee the complete guide: https://smithery.ai/docs/migrations/typescript-custom-container\n\n**[Try it live on Smithery](https://smithery.ai/server/@smithery-ai/cookbook-ts-custom-container)**\n\n## Features:\n\n- **CORS**: CORS headers for browser-based MCP clients\n- **Smithery Session Configuration**: handles user's session configuration passed via Smithery ([learn more](https://smithery.ai/docs/build/session-config))\n- **Request Logging Middleware**: Custom middleware for debugging HTTP requests and responses\n- **Server Transport**: Can run with both STDIO and HTTP transports using `TRANSPORT` env variable\n\n## Prerequisites\n\n- Node.js 22 or higher\n- npm package manager\n\n## Project Structure\n\n- `src/index.ts` - Main Express server with MCP HTTP transport\n- `package.json` - Node.js dependencies and scripts\n- `smithery.yaml` - Smithery deployment and session configuration\n- `Dockerfile` - Dockerfile to host server in Smithery\n\n## Quick Start\n\n1. **Install dependencies:**\n   ```bash\n   npm install\n   ```\n\n2. **Run the development server:**\n\n   **HTTP Mode:**\n   ```bash\n   npm run dev\n   ```\n   This will start the server on port 8081.\n\n3. **Test interactively:**\n   Once your server is running in HTTP mode, you can test it interactively using the Smithery playground:\n   ```bash\n   npx -y @smithery/cli playground --port 8081\n   ```\n\n   <img src=\"../../../../public/smithery_playground.png\" alt=\"Smithery Playground\" width=\"800\">\n\n4. **Deploy your own version:**\n   To deploy your own MCP server:\n   - Connect your repository at [https://smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 1949,
        "token_count_estimate": 487,
        "source_type": "readme",
        "agent_id": "fd57be7932212568"
      },
      {
        "chunk_id": 1,
        "text": "ground.png\" alt=\"Smithery Playground\" width=\"800\">\n\n4. **Deploy your own version:**\n   To deploy your own MCP server:\n   - Connect your repository at [https://smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 1749,
        "end_pos": 1949,
        "token_count_estimate": 50,
        "source_type": "readme",
        "agent_id": "fd57be7932212568"
      }
    ]
  },
  {
    "agent_id": "621424696fdb3d77",
    "name": "ai.smithery/callmybot-domoticz",
    "source": "mcp",
    "source_url": "https://github.com/callmybot/domoticz",
    "description": "Greet anyone by name with a friendly hello. Explore the origin of 'Hello, World' for context in de‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-18T10:31:39.650923Z",
    "indexed_at": "2026-02-18T04:05:56.508548",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Greet anyone by name with a friendly hello",
        "Provide context about the origin of 'Hello, World'"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with minimal detail, providing only a basic idea of functionality without examples or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "3119413789bef03b",
    "name": "ai.smithery/callmybot-hello-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/callmybot/hello-mcp-server",
    "description": "Generate quick, friendly greetings by name. Personalize salutations for any context. Explore the o‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-18T07:48:26.611267Z",
    "indexed_at": "2026-02-18T04:06:00.548286",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Generate quick, friendly greetings by name",
        "Personalize salutations for any context"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of greeting generation and personalization but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "a4b11210836a1432",
    "name": "ai.smithery/cc25a-openai-api-agent-project123123123",
    "source": "mcp",
    "source_url": "https://github.com/cc25a/openai-api-agent-project",
    "description": "Look up the latest stock prices by ticker symbol across global markets. Get current price and esse‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-17T03:32:54.14302Z",
    "indexed_at": "2026-02-18T04:06:04.723718",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# OpenAI API Agent School - Project\n\nÎ≥∏ ÏûêÎ£åÎäî [(Ï£º)ÏóêÏù¥ÏïÑÏù¥Ï∫êÏä¨](https://aicastle.com)ÏóêÏÑú ÎßåÎì† [**OpenAI APIÎ°ú Î∞∞Ïö∞Îäî Agent Í∞úÎ∞ú Ï≤´Í±∏Ïùå** ](https://openai-api-agent.aicastle.school/)(OpenAI API Agent School) Í∞ïÏùò ÌîÑÎ°úÏ†ùÌä∏ ÏûêÎ£åÏûÖÎãàÎã§.\n\n\n## [0] Install & Build (uv)\n\n```sh\n# uv Install\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# uv Build\nuv sync --frozen && uv cache prune --ci\n```\n\n\n## [1] ÌîÑÎ°úÏ†ùÌä∏ ÏÑ∏ÌåÖ\n\n### 1.1. ÌôòÍ≤Ω Î≥ÄÏàò (.env)\n\n- **.env ÌååÏùº**Î°ú ÏÑ§Ï†ïÌïòÍ±∞ÎÇò **Î∞∞Ìè¨ ÌôòÍ≤ΩÏóêÏÑú ÏßÄÏ†ï**\n- `OPENAI_API_KEY`: Agent Ïï± ÎòêÎäî ÌååÏù∏ÌäúÎãùÌï† Îç∞Ïù¥ÌÑ∞Î•º ÏóÖÎ°úÎìúÌï† Îïå ÏÇ¨Ïö©Ìï† OpenAI API ÌÇ§\n- `PROMPT_ID` Agent Ïï±ÏóêÏÑú ÏÇ¨Ïö©Ìï† OpenAI ÌîÑÎ°¨ÌîÑÌä∏ ID \n- `TITLE`: AgentÏï±Ïùò ÏÉÅÎã® Ï†úÎ™©  \n- `PASSWORD`: ÎπÑÎ∞ÄÎ≤àÌò∏ ÏÑ§Ï†ï (ÎπÑÏõåÎëò Í≤ΩÏö∞ ÎàÑÍµ¨ÎÇò Ï†ëÍ∑º Í∞ÄÎä•)\n    - Agent Ïï±ÏóêÏÑúÎäî Î°úÍ∑∏Ïù∏Ìï¥Ïïº Ï†ëÍ∑º Í∞ÄÎä•Ìï¥Ïßê\n    - MCP ÏÑúÎ≤ÑÏóêÏÑúÎäî `?password=<your-password>`ÏôÄ Í∞ôÏù¥ ÏøºÎ¶¨Ïä§Ìä∏ÎßÅÏúºÎ°ú Ï†ÑÎã¨Ìï¥Ïïº Ï†ëÍ∑º Í∞ÄÎä•\n\n### 1.2. config.overrides.jsonc\n\n- Agent Ïï±ÏóêÏÑú openai api ÏöîÏ≤≠Ïãú responses create ÏóêÏÑú ÎçÆÏñ¥ Ïì∏ Íµ¨ÏÑ± Í∞í\n- **config.overrides.jsonc ÌååÏùº**Î°ú ÏÑ§Ï†ïÌïòÍ±∞ÎÇò **Î∞∞Ìè¨ ÌôòÍ≤ΩÏóêÏÑú ÏßÄÏ†ï**\n- ÌååÏùº ÏúÑÏπò\n    - ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî (Ïö∞ÏÑ† ÏàúÏúÑ)\n    - /etc/secrets/\n\n\n### 1.3. tools.py\n\n- Agent Ïï±ÏóêÏÑú Function CallingÏúºÎ°ú ÏÇ¨Ïö©Ìï† Ìï®Ïàò.\n- ÎòêÎäî MCP ÏÑúÎ≤ÑÏóêÏÑú toolÎ°ú ÏÇ¨Ïö©Ìï† Ìï®Ïàò.\n- ÌååÏùº ÏúÑÏπò: [tools.py](tools.py)\n\n## [2] Ïï± Ïã§Ìñâ\n\n### Ïã§Ìñâ\n\n```sh\nuv run main.py\n```\n\n- Ìè¨Ìä∏: ÌôòÍ≤ΩÎ≥ÄÏàò `PORT`Í∞íÏù¥ ÏßÄÏ†ïÎêú Í≤ΩÏö∞ Ïù¥ Í∞íÏùÑ ÏÇ¨Ïö©ÌïòÎ©∞, Í∑∏Î†áÏßÄ ÏïäÏùÑ Í≤ΩÏö∞ `8000`ÏùÑ ÏÇ¨Ïö©Ìï®.\n\n- agent Ïï± Ï£ºÏÜå: <https://localhost:8000/agent>\n\n- mcp ÏÑúÎ≤Ñ Ï£ºÏÜå: <https://localhost:8000/mcp>\n\n### KEEPALIVE_URL\n- Ïã§Ìñâ Ï§ëÏù∏ Ïï±Ïù¥ ÏùºÏ†ïÏãúÍ∞Ñ ÎèôÏïà Ï†ëÏÜçÏù¥ ÏóÜÏúºÎ©¥ Ïú†Ìú¥ÏÉÅÌÉúÍ∞Ä Îê† Í≤ΩÏö∞ `KEEPALIVE_URL`Î•º github actionsÏùò ÌôòÍ≤ΩÎ≥ÄÏàò(secrets)Ïóê ÏßÄÏ†ïÌïòÏó¨ Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú Ï†ëÏÜçÌïòÎäî cron ÏûëÏóÖÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÏùå.\n- ForkÌïú Í≤ΩÏö∞ ForkÌïú Î†àÌè¨ÏßÄÌÜ†Î¶¨ Ï†ëÏÜçÌïòÏó¨ ÏÉÅÎã®Ïùò Actions ÌÉ≠ÏóêÏÑú ActionsÎ•º ÌôúÏÑ±Ìôî ÌïòÍ≥† .github/workflows/keepalive-url.yml ÏùÑ ÌôúÏÑ±Ìôî ÌïòÏÑ∏Ïöî.\n- Î†àÌè¨ÏßÄÌÜ†Î¶¨ > settings > Secrets and Variables > Actions > New repository secret Ïóê Ï†ëÏÜçÌïòÏó¨ ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÏûÖÎ†• (SecretÏóêÎäî Î≥∏Ïù∏Ïù¥ Î∞∞Ìè¨Ìïú URLÎ°ú ÏûÖÎ†•)\n- ÏòàÏãú\n    - Name: `KEEPALIVE_URL`\n    - Secret: `https://<your-project-name>.onrender.com`\n\n\n## [3] ÌååÏù∏ ÌäúÎãù Îç∞Ïù¥ÌÑ∞\n\n`.env`ÌååÏùºÏóê `OPENAI_API_KEY`Î•º Îì±Î°ùÌï¥Ïïº Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÏóÖÎ°úÎìú Í∞ÄÎä•\n\n### 3.1. SFT (Supervised Fine-tuning)\n\n- Ìè¥Îçî ÏúÑÏπò: `fine_tuning_data/supervised/`\n- Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÏóÖÎ°úÎìú \n    ```sh\n    uv run fine_tuning_data/supervised/convert_and_upload.py\n    ```\n\n### 3.2. DPO (Direct Preference Optimization)\n\n- Ìè¥Îçî ÏúÑÏπò: `fine_tuning_data/preference/`\n- Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÏóÖÎ°úÎìú \n    ```sh\n    uv run fine_tuning_data/preference/convert_and_upload.py\n    ```\n\n### 3.3. RFT (Reinforcement Fine-tuning)\n\n- Ìè¥Îçî ÏúÑÏπò: `fine_tuning_data/reinforcement/`\n- Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÏóÖÎ°úÎìú \n    ```sh\n    uv run fine_tuning_data/reinforcement/convert_and_upload.py\n    ```"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server accessible via /mcp endpoint",
        "Run an agent application accessible via /agent endpoint",
        "Use OpenAI API for agent interactions with configurable prompts",
        "Support function calling via tools defined in tools.py",
        "Upload and manage fine-tuning data for supervised, preference, and reinforcement fine-tuning",
        "Configure server behavior and OpenAI API request overrides via config.overrides.jsonc",
        "Protect access with password authentication via query string",
        "Keep the server alive using periodic external pings configured via KEEPALIVE_URL"
      ],
      "limitations": [
        "Requires OpenAI API key for operation and fine-tuning data upload",
        "Password protection is optional but if set requires query string authentication",
        "Port defaults to 8000 if not specified via environment variable",
        "No explicit mention of rate limits or concurrency handling"
      ],
      "requirements": [
        "OpenAI API key set in environment variable OPENAI_API_KEY",
        "Optional PROMPT_ID environment variable for OpenAI prompt configuration",
        "Optional PASSWORD environment variable for access control",
        "uv tool installed via provided curl script for building and running",
        "Python environment capable of running main.py and fine-tuning upload scripts",
        "config.overrides.jsonc file for OpenAI API request overrides (optional)",
        "GitHub Actions configured with KEEPALIVE_URL secret for uptime maintenance (optional)"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, environment setup, usage examples for running the server and uploading fine-tuning data, configuration details, and notes on access control and uptime maintenance, covering most aspects comprehensively.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# OpenAI API Agent School - Project\n\nÎ≥∏ ÏûêÎ£åÎäî [(Ï£º)ÏóêÏù¥ÏïÑÏù¥Ï∫êÏä¨](https://aicastle.com)ÏóêÏÑú ÎßåÎì† [**OpenAI APIÎ°ú Î∞∞Ïö∞Îäî Agent Í∞úÎ∞ú Ï≤´Í±∏Ïùå** ](https://openai-api-agent.aicastle.school/)(OpenAI API Agent School) Í∞ïÏùò ÌîÑÎ°úÏ†ùÌä∏ ÏûêÎ£åÏûÖÎãàÎã§.\n\n\n## [0] Install & Build (uv)\n\n```sh\n# uv Install\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# uv Build\nuv sync --frozen && uv cache prune --ci\n```\n\n\n## [1] ÌîÑÎ°úÏ†ùÌä∏ ÏÑ∏ÌåÖ\n\n### 1.1. ÌôòÍ≤Ω Î≥ÄÏàò (.env)\n\n- **.env ÌååÏùº**Î°ú ÏÑ§Ï†ïÌïòÍ±∞ÎÇò **Î∞∞Ìè¨ ÌôòÍ≤ΩÏóêÏÑú ÏßÄÏ†ï**\n- `OPENAI_API_KEY`: Agent Ïï± ÎòêÎäî ÌååÏù∏ÌäúÎãùÌï† Îç∞Ïù¥ÌÑ∞Î•º ÏóÖÎ°úÎìúÌï† Îïå ÏÇ¨Ïö©Ìï† OpenAI API ÌÇ§\n- `PROMPT_ID` Agent Ïï±ÏóêÏÑú ÏÇ¨Ïö©Ìï† OpenAI ÌîÑÎ°¨ÌîÑÌä∏ ID \n- `TITLE`: AgentÏï±Ïùò ÏÉÅÎã® Ï†úÎ™©  \n- `PASSWORD`: ÎπÑÎ∞ÄÎ≤àÌò∏ ÏÑ§Ï†ï (ÎπÑÏõåÎëò Í≤ΩÏö∞ ÎàÑÍµ¨ÎÇò Ï†ëÍ∑º Í∞ÄÎä•)\n    - Agent Ïï±ÏóêÏÑúÎäî Î°úÍ∑∏Ïù∏Ìï¥Ïïº Ï†ëÍ∑º Í∞ÄÎä•Ìï¥Ïßê\n    - MCP ÏÑúÎ≤ÑÏóêÏÑúÎäî `?password=<your-password>`ÏôÄ Í∞ôÏù¥ ÏøºÎ¶¨Ïä§Ìä∏ÎßÅÏúºÎ°ú Ï†ÑÎã¨Ìï¥Ïïº Ï†ëÍ∑º Í∞ÄÎä•\n\n### 1.2. config.overrides.jsonc\n\n- Agent Ïï±ÏóêÏÑú openai api ÏöîÏ≤≠Ïãú responses create ÏóêÏÑú ÎçÆÏñ¥ Ïì∏ Íµ¨ÏÑ± Í∞í\n- **config.overrides.jsonc ÌååÏùº**Î°ú ÏÑ§Ï†ïÌïòÍ±∞ÎÇò **Î∞∞Ìè¨ ÌôòÍ≤ΩÏóêÏÑú ÏßÄÏ†ï**\n- ÌååÏùº ÏúÑÏπò\n    - ÌîÑÎ°úÏ†ùÌä∏ Ìè¥Îçî (Ïö∞ÏÑ† ÏàúÏúÑ)\n    - /etc/secrets/\n\n\n### 1.3. tools.py\n\n- Agent Ïï±ÏóêÏÑú Function CallingÏúºÎ°ú ÏÇ¨Ïö©Ìï† Ìï®Ïàò.\n- ÎòêÎäî MCP ÏÑúÎ≤ÑÏóêÏÑú toolÎ°ú ÏÇ¨Ïö©Ìï† Ìï®Ïàò.\n- ÌååÏùº ÏúÑÏπò: [tools.py](tools.py)\n\n## [2] Ïï± Ïã§Ìñâ\n\n### Ïã§Ìñâ\n\n```sh\nuv run main.py\n```\n\n- Ìè¨Ìä∏: ÌôòÍ≤ΩÎ≥ÄÏàò `PORT`Í∞íÏù¥ ÏßÄÏ†ïÎêú Í≤ΩÏö∞ Ïù¥ Í∞íÏùÑ ÏÇ¨Ïö©ÌïòÎ©∞, Í∑∏Î†áÏßÄ ÏïäÏùÑ Í≤ΩÏö∞ `8000`ÏùÑ ÏÇ¨Ïö©Ìï®.\n\n- agent Ïï± Ï£ºÏÜå: <https://localhost:8000/agent>\n\n- mcp ÏÑúÎ≤Ñ Ï£ºÏÜå: <https://localhost:8000/mcp>\n\n### KEEPALIVE_URL\n- Ïã§Ìñâ Ï§ëÏù∏ Ïï±Ïù¥ ÏùºÏ†ïÏãúÍ∞Ñ ÎèôÏïà Ï†ëÏÜçÏù¥ ÏóÜÏúºÎ©¥ Ïú†Ìú¥ÏÉÅÌÉúÍ∞Ä Îê† Í≤ΩÏö∞ `KEEPALIVE_URL`Î•º github actionsÏùò ÌôòÍ≤ΩÎ≥ÄÏàò(secrets)Ïóê ÏßÄÏ†ïÌïòÏó¨ Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú Ï†ëÏÜçÌïòÎäî cron ÏûëÏóÖÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÏùå.\n- ForkÌïú Í≤ΩÏö∞ ForkÌïú Î†àÌè¨ÏßÄÌÜ†Î¶¨ Ï†ëÏÜçÌïòÏó¨ ÏÉÅÎã®Ïùò Actions ÌÉ≠ÏóêÏÑú ActionsÎ•º ÌôúÏÑ±Ìôî ÌïòÍ≥† .github/workflows/keepalive-url.yml ÏùÑ ÌôúÏÑ±Ìôî ÌïòÏÑ∏Ïöî.\n- Î†àÌè¨ÏßÄÌÜ†Î¶¨ > settings > Secrets and Variables > Actions > New repository secret Ïóê Ï†ëÏÜçÌïòÏó¨ ÏïÑÎûòÏôÄ Í∞ôÏù¥ ÏûÖÎ†• (SecretÏóêÎäî Î≥∏Ïù∏Ïù¥ Î∞∞Ìè¨Ìïú URLÎ°ú ÏûÖÎ†•)\n- ÏòàÏãú\n    - Name: `KEEPALIVE_URL`\n    - Secret: `https://<your-project-name>.onrender.com`\n\n\n## [3] ÌååÏù∏ ÌäúÎãù Îç∞Ïù¥ÌÑ∞\n\n`.env`ÌååÏùºÏóê `OPENAI_API_KEY`Î•º Îì±Î°ùÌï¥Ïïº Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÏóÖÎ°úÎìú Í∞ÄÎä•\n\n### 3.1. SFT (Supervised Fine-tuning)\n\n- Ìè¥Îçî ÏúÑÏπò: `fine_tuning_data/supervised/`\n- Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÏóÖÎ°úÎìú \n    ```sh\n    uv run fine_tuning_data/supervised/convert_and_upload.py\n    ```\n\n### 3.2.",
        "start_pos": 0,
        "end_pos": 1927,
        "token_count_estimate": 481,
        "source_type": "readme",
        "agent_id": "a4b11210836a1432"
      },
      {
        "chunk_id": 1,
        "text": "Ìï¥Ïïº Ï†ïÏÉÅÏ†ÅÏúºÎ°ú ÏóÖÎ°úÎìú Í∞ÄÎä•\n\n### 3.1. SFT (Supervised Fine-tuning)\n\n- Ìè¥Îçî ÏúÑÏπò: `fine_tuning_data/supervised/`\n- Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÏóÖÎ°úÎìú \n    ```sh\n    uv run fine_tuning_data/supervised/convert_and_upload.py\n    ```\n\n### 3.2. DPO (Direct Preference Optimization)\n\n- Ìè¥Îçî ÏúÑÏπò: `fine_tuning_data/preference/`\n- Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÏóÖÎ°úÎìú \n    ```sh\n    uv run fine_tuning_data/preference/convert_and_upload.py\n    ```\n\n### 3.3. RFT (Reinforcement Fine-tuning)\n\n- Ìè¥Îçî ÏúÑÏπò: `fine_tuning_data/reinforcement/`\n- Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Î∞è ÏóÖÎ°úÎìú \n    ```sh\n    uv run fine_tuning_data/reinforcement/convert_and_upload.py\n    ```",
        "start_pos": 1727,
        "end_pos": 2284,
        "token_count_estimate": 139,
        "source_type": "readme",
        "agent_id": "a4b11210836a1432"
      }
    ]
  },
  {
    "agent_id": "897c5cbb7aab6845",
    "name": "ai.smithery/cindyloo-dropbox-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/cindyloo/dropbox-mcp-server",
    "description": "Search, browse, and read your Dropbox files. Find documents by name or content, list folders, and‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T02:02:08.583268Z",
    "indexed_at": "2026-02-18T04:06:08.442692",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Dropbox MCP Server\n\nA Model Context Protocol (MCP) server that provides read access to Dropbox files with advanced search and content extraction capabilities.\n\n## Features\n\n- **File Listing**: Browse files and folders in your Dropbox\n- **File Reading**: Read content from various file types (PDF, DOCX, TXT, code files)\n- **Search**: Search for files by name across your Dropbox\n- **Content Search**: Search for specific text within files\n- **File Info**: Get detailed metadata about files\n- **Smart Text Extraction**: Automatically extracts text from PDFs and DOCX files\n\n## Supported File Types\n\n- **PDF** - Text extraction with PyPDF2\n- **DOCX/DOC** - Document text extraction\n- **Text files** - TXT, MD, PY, JS, HTML, CSS, JSON, CSV\n\n## Installation\n\n### Prerequisites\n\n- Python 3.10 or higher\n- A Dropbox account and access token\n\n### Get a Dropbox Access Token\n\n1. Go to the [Dropbox App Console](https://www.dropbox.com/developers/apps)\n2. Click \"Create app\"\n3. Choose \"Scoped access\" and \"Full Dropbox\" access\n4. Name your app\n5. Go to the \"Permissions\" tab and enable:\n   - `files.metadata.read`\n   - `files.content.read`\n6. Go to the \"Settings\" tab and generate an access token\n\n### Install Dependencies\n\n```bash\npip install -e .\n```\n\nOr install manually:\n\n```bash\npip install mcp dropbox pydantic PyPDF2 python-docx\n```\n\n## Configuration\n\nSet your Dropbox access token as an environment variable:\n\n```bash\nexport DROPBOX_ACCESS_TOKEN=\"your_access_token_here\"\n```\n\n### For Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"dropbox\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/your/dropbox_server.py\"],\n      \"env\": {\n        \"DROPBOX_ACCESS_TOKEN\": \"your_access_token_here\"\n      }\n    }\n  }\n}\n```\n\n### For Smithery\n\nThe server will automatically use the `DROPBOX_ACCESS_TOKEN` environment variable when deployed.\n\n## Available Tools\n\n### `list_files`\nList files and folders in a Dropbox directory.\n\n**Parameters:**\n- `folder_path` (optional): Path to folder (empty for root)\n- `max_files` (optional): Maximum items to return (default: 20)\n\n### `search_files`\nSearch for files by name.\n\n**Parameters:**\n- `query`: Search query\n- `file_types` (optional): File types to search (\"all\", \"pdf\", \"docx\", \"txt\", or comma-separated)\n- `max_results` (optional): Maximum results (default: 10)\n\n### `read_file`\nRead the full content of a file.\n\n**Parameters:**\n- `file_path`: Full path to the file\n- `max_length` (optional): Maximum characters to return (default: 5000, 0 for unlimited)\n\n### `get_file_info`\nGet detailed metadata about a file.\n\n**Parameters:**\n- `file_path`: Full path to the file\n\n### `search_file_content`\nSearch for text within specific files.\n\n**Parameters:**\n- `file_paths`: List of file paths to search\n- `query`: Text to search for\n- `context_chars` (optional): Characters of context around matches (default: 100)\n\n## Usage Examples\n\n### List files in root directory\n```python\nlist_files()\n```\n\n### Search for PDF files\n```python\nsearch_files(query=\"invoice\", file_types=\"pdf\", max_results=5)\n```\n\n### Read a specific file\n```python\nread_file(file_path=\"/documents/report.pdf\")\n```\n\n### Search within files\n```python\nsearch_file_content(\n    file_paths=[\"/documents/file1.txt\", \"/documents/file2.pdf\"],\n    query=\"important keyword\"\n)\n```\n\n## Development\n\n### Running Locally\n\n```bash\npython dropbox_server.py\n```\n\n### Testing\n\nMake sure your `DROPBOX_ACCESS_TOKEN` is set, then run the server and test with an MCP client.\n\n## Security Notes\n\n- Keep your Dropbox access token secure and never commit it to version control\n- Use environment variables or secure secret management\n- The server only provides read access to Dropbox files\n- Consider using app-scoped tokens with minimal permissions\n\n## License\n\nMIT License - feel free to use and modify as needed.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Support\n\nFor issues and questions, please open an issue on GitHub.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Browse files and folders in Dropbox",
        "Read content from various file types including PDF, DOCX, TXT, and code files",
        "Search for files by name across Dropbox",
        "Search for specific text within files",
        "Retrieve detailed metadata about files",
        "Automatically extract text from PDFs and DOCX files"
      ],
      "limitations": [
        "Provides read-only access to Dropbox files",
        "Requires Dropbox access token with specific read permissions",
        "Limited to supported file types for content extraction (PDF, DOCX, TXT, and certain code files)"
      ],
      "requirements": [
        "Python 3.10 or higher",
        "Dropbox account",
        "Dropbox access token with 'files.metadata.read' and 'files.content.read' permissions",
        "Environment variable DROPBOX_ACCESS_TOKEN set with the access token",
        "Dependencies: mcp, dropbox, pydantic, PyPDF2, python-docx"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear descriptions of tools and capabilities, explicit limitations, and environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Dropbox MCP Server\n\nA Model Context Protocol (MCP) server that provides read access to Dropbox files with advanced search and content extraction capabilities.\n\n## Features\n\n- **File Listing**: Browse files and folders in your Dropbox\n- **File Reading**: Read content from various file types (PDF, DOCX, TXT, code files)\n- **Search**: Search for files by name across your Dropbox\n- **Content Search**: Search for specific text within files\n- **File Info**: Get detailed metadata about files\n- **Smart Text Extraction**: Automatically extracts text from PDFs and DOCX files\n\n## Supported File Types\n\n- **PDF** - Text extraction with PyPDF2\n- **DOCX/DOC** - Document text extraction\n- **Text files** - TXT, MD, PY, JS, HTML, CSS, JSON, CSV\n\n## Installation\n\n### Prerequisites\n\n- Python 3.10 or higher\n- A Dropbox account and access token\n\n### Get a Dropbox Access Token\n\n1. Go to the [Dropbox App Console](https://www.dropbox.com/developers/apps)\n2. Click \"Create app\"\n3. Choose \"Scoped access\" and \"Full Dropbox\" access\n4. Name your app\n5. Go to the \"Permissions\" tab and enable:\n   - `files.metadata.read`\n   - `files.content.read`\n6. Go to the \"Settings\" tab and generate an access token\n\n### Install Dependencies\n\n```bash\npip install -e .\n```\n\nOr install manually:\n\n```bash\npip install mcp dropbox pydantic PyPDF2 python-docx\n```\n\n## Configuration\n\nSet your Dropbox access token as an environment variable:\n\n```bash\nexport DROPBOX_ACCESS_TOKEN=\"your_access_token_here\"\n```\n\n### For Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"dropbox\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/your/dropbox_server.py\"],\n      \"env\": {\n        \"DROPBOX_ACCESS_TOKEN\": \"your_access_token_here\"\n      }\n    }\n  }\n}\n```\n\n### For Smithery\n\nThe server will automatically use the `DROPBOX_ACCESS_TOKEN` environment variable when deployed.\n\n## Available Tools\n\n### `list_files`\nList files and folders in a Dropbox directory.",
        "start_pos": 0,
        "end_pos": 1963,
        "token_count_estimate": 490,
        "source_type": "readme",
        "agent_id": "897c5cbb7aab6845"
      },
      {
        "chunk_id": 1,
        "text": "### For Smithery\n\nThe server will automatically use the `DROPBOX_ACCESS_TOKEN` environment variable when deployed.\n\n## Available Tools\n\n### `list_files`\nList files and folders in a Dropbox directory.\n\n**Parameters:**\n- `folder_path` (optional): Path to folder (empty for root)\n- `max_files` (optional): Maximum items to return (default: 20)\n\n### `search_files`\nSearch for files by name.\n\n**Parameters:**\n- `query`: Search query\n- `file_types` (optional): File types to search (\"all\", \"pdf\", \"docx\", \"txt\", or comma-separated)\n- `max_results` (optional): Maximum results (default: 10)\n\n### `read_file`\nRead the full content of a file.\n\n**Parameters:**\n- `file_path`: Full path to the file\n- `max_length` (optional): Maximum characters to return (default: 5000, 0 for unlimited)\n\n### `get_file_info`\nGet detailed metadata about a file.\n\n**Parameters:**\n- `file_path`: Full path to the file\n\n### `search_file_content`\nSearch for text within specific files.\n\n**Parameters:**\n- `file_paths`: List of file paths to search\n- `query`: Text to search for\n- `context_chars` (optional): Characters of context around matches (default: 100)\n\n## Usage Examples\n\n### List files in root directory\n```python\nlist_files()\n```\n\n### Search for PDF files\n```python\nsearch_files(query=\"invoice\", file_types=\"pdf\", max_results=5)\n```\n\n### Read a specific file\n```python\nread_file(file_path=\"/documents/report.pdf\")\n```\n\n### Search within files\n```python\nsearch_file_content(\n    file_paths=[\"/documents/file1.txt\", \"/documents/file2.pdf\"],\n    query=\"important keyword\"\n)\n```\n\n## Development\n\n### Running Locally\n\n```bash\npython dropbox_server.py\n```\n\n### Testing\n\nMake sure your `DROPBOX_ACCESS_TOKEN` is set, then run the server and test with an MCP client.",
        "start_pos": 1763,
        "end_pos": 3500,
        "token_count_estimate": 434,
        "source_type": "readme",
        "agent_id": "897c5cbb7aab6845"
      },
      {
        "chunk_id": 2,
        "text": "onment variables or secure secret management\n- The server only provides read access to Dropbox files\n- Consider using app-scoped tokens with minimal permissions\n\n## License\n\nMIT License - feel free to use and modify as needed.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Support\n\nFor issues and questions, please open an issue on GitHub.",
        "start_pos": 3611,
        "end_pos": 3997,
        "token_count_estimate": 96,
        "source_type": "readme",
        "agent_id": "897c5cbb7aab6845"
      }
    ]
  },
  {
    "agent_id": "e7c12866ecb4b061",
    "name": "ai.smithery/clpi-clp-mcp",
    "source": "mcp",
    "source_url": "https://github.com/clpi/clp-mcp",
    "description": "Manage simple context workflows with quick init and add actions. Access the 'Hello, World' origin‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-08T00:23:04.174735Z",
    "indexed_at": "2026-02-18T04:06:10.253038",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage simple context workflows",
        "Initialize context workflows quickly",
        "Add actions to context workflows",
        "Access the 'Hello, World' origin"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "08ab683b7fa159e9",
    "name": "ai.smithery/cpretzinger-ai-assistant-simple",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@cpretzinger/ai-assistant-simple/mcp",
    "description": "UPDATED 9/1/2025! NEW TOOLS! Use the Redis Stream tools with n8n MCP Client Node for use anywhere!‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-15T00:26:36.144736Z",
    "indexed_at": "2026-02-18T04:06:12.449814",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Use Redis Stream tools",
        "Integrate with n8n MCP Client Node"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with minimal information about new tools and integration, lacking detailed capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "4d1e61d52478be56",
    "name": "ai.smithery/cristianoaredes-mcp-dadosbr",
    "source": "mcp",
    "source_url": "https://github.com/cristianoaredes/mcp-dadosbr",
    "description": "# MCP DadosBR Servidor MCP focado em dados p√∫blicos do Brasil. Oferece duas ferramentas simples e‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T12:13:44.073155Z",
    "indexed_at": "2026-02-18T04:06:14.443642",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# MCP DadosBR üáßüá∑\n\n[![smithery badge](https://smithery.ai/badge/@cristianoaredes/mcp-dadosbr)](https://smithery.ai/server/@cristianoaredes/mcp-dadosbr)\n[![npm version](https://badge.fury.io/js/@aredes.me%2Fmcp-dadosbr.svg)](https://www.npmjs.com/package/@aredes.me/mcp-dadosbr)\n[![npm downloads](https://img.shields.io/npm/dm/@aredes.me/mcp-dadosbr.svg)](https://www.npmjs.com/package/@aredes.me/mcp-dadosbr)\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Cloudflare Workers](https://img.shields.io/badge/Cloudflare-Workers-F38020?logo=cloudflare&logoColor=white)](https://mcp-dadosbr.aredes.me/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n> **ü§ñ Servidor Model Context Protocol (MCP) para consulta de dados empresariais brasileiros** ‚Äî traga informa√ß√µes de CNPJ (empresas) e CEP (endere√ßos) diretamente para Claude Desktop, Cursor, Windsurf, Continue.dev e outros assistentes de IA.\n> \n> üöÄ Deploy multiplataforma: Pacote NPM, Cloudflare Workers, Smithery.\n\n_[Portugu√™s](#portugu√™s) | [English](#english)_\n\n---\n\n## Portugu√™s\n\nüáßüá∑ **Servidor MCP para consulta de dados empresariais brasileiros (CNPJ) e valida√ß√£o de endere√ßos (CEP).** Integre essas consultas em minutos em Claude Desktop, Cursor, Windsurf, Continue.dev e qualquer cliente compat√≠vel com MCP.\n\n## ‚ö° Instala√ß√£o R√°pida\n\n```bash\nnpm install -g @aredes.me/mcp-dadosbr\n```\n\nOu execute diretamente com NPX:\n\n```bash\nnpx @aredes.me/mcp-dadosbr\n```\n\n### Via Smithery (1 clique)\n\n```bash\nnpx -y @smithery/cli install @cristianoaredes/mcp-dadosbr --client claude\n```\n\n### Deploy em VPS\n\n[![Deploy to Hostinger VPS](https://img.shields.io/badge/Deploy%20to-Hostinger%20VPS-673DE6?style=for-the-badge&logo=hostinger&logoColor=white)](https://www.hostinger.com.br/cart?product=vps%3Avps_kvm_2&period=12&referral_type=cart_link&REFERRALCODE=FQLCRISTIRC3&referral_id=019a73b2-a3cd-72b8-8141-76eb55275046)\n\n## üîå Configura√ß√£o por IDE / Cliente MCP\n\n### ü§ñ Claude Desktop\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n> ‚ö†Ô∏è **Obrigat√≥rio**: Configure `TAVILY_API_KEY` para usar `cnpj_search` e `cnpj_intelligence`. Obtenha sua chave em [tavily.com](https://tavily.com)\n\n### üéØ Cursor IDE\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### üèÑ Windsurf IDE\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n\n### üîÑ Continue.dev\n```json\n{\n  \"mcpServers\": [\n    {\n      \"name\": \"dadosbr\",\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  ]\n}\n```\n**Localiza√ß√£o**: `~/.continue/config.json`\n\n### üßë‚Äçüíª Claude Code CLI\n\n**Instala√ß√£o via comando `claude mcp add`:**\n```bash\n# Op√ß√£o 1: Servidor local stdio (recomendado para desenvolvimento)\nclaude mcp add --transport stdio dadosbr \\\n  --env TAVILY_API_KEY=tvly-your-api-key-here \\\n  -- npx -y @aredes.me/mcp-dadosbr\n\n# Op√ß√£o 2: Servidor HTTP remoto (Cloudflare Workers)\nclaude mcp add --transport http dadosbr \\\n  https://mcp-dadosbr.aredes.me/mcp\n```\n\n**Verifica√ß√£o:**\n```bash\n# Listar servidores MCP configurados\nclaude mcp list\n\n# Remover se necess√°rio\nclaude mcp remove dadosbr\n```\n\n### ü§ñ Google Gemini CLI\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: `~/.config/gemini/mcp_config.json`\n\n### üì¶ Codex CLI\n```bash\n# Configurar no .codexrc\ncodex mcp add dadosbr npx @aredes.me/mcp-dadosbr\n\n# Ou via environment\nexport CODEX_MCP_SERVERS='{\"dadosbr\":{\"command\":\"npx\",\"args\":[\"@aredes.me/mcp-dadosbr\"],\"env\":{\"TAVILY_API_KEY\":\"tvly-xxx\"}}}'\n```\n\n### üêù Zed Editor\n```json\n{\n  \"context_servers\": {\n    \"dadosbr\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"@aredes.me/mcp-dadosbr\"]\n      },\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: `~/.config/zed/settings.json`\n\n### ü¶ñ Cline (VS Code Extension)\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: VS Code Settings > Extensions > Cline > MCP Servers\n\n### ‚ö° Roo Cline\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: `~/.roo-cline/mcp-settings.json`\n\n### ü§ñ ChatGPT MCP\nPara usar com ChatGPT, configure o servidor Cloudflare Workers como endpoint remoto:\n\n1. **Deploy no Cloudflare Workers**: `npm run deploy`\n2. **Configure no ChatGPT**:\n   - URL do servidor: `https://mcp-dadosbr.your-subdomain.workers.dev`\n   - O ChatGPT detectar√° automaticamente os endpoints OAuth e MCP\n3. **Configure API Key** (opcional, via environment variables no Workers):\n   ```bash\n   TAVILY_API_KEY=\"tvly-your-api-key-here\"\n   ```\n\n**APIs REST dispon√≠veis**:\n- `GET /cnpj/{cnpj}` - Consulta dados de empresa\n- `GET /cep/{cep}` - Consulta dados de endere√ßo\n- `POST /search` - Busca web inteligente\n- `POST /intelligence` - Busca inteligente completa\n- `POST /thinking` - Racioc√≠nio estruturado\n\n**‚úÖ Teste r√°pido**\n```\nPode consultar o CNPJ 11.222.333/0001-81?\n```\n\n## üõ†Ô∏è Ferramentas Dispon√≠veis\n\n- üè¢ **`cnpj_lookup`** ‚Äî raz√£o social, situa√ß√£o cadastral, endere√ßo, CNAE (fonte: OpenCNPJ)\n- üìÆ **`cep_lookup`** ‚Äî logradouro, bairro, cidade, UF, DDD (fonte: OpenCEP)\n- üîç **`cnpj_search`** ‚Äî buscas web com dorks (site:, intext:, filetype:) via Tavily\n- ü§î **`sequentialthinking`** ‚Äî racioc√≠nio estruturado passo a passo\n- üéØ **`cnpj_intelligence`** ‚Äî orquestra m√∫ltiplas consultas e gera relat√≥rio consolidado com filtros de assertividade\n\n> **‚ú® Novidade v0.3.2**: Buscas web agora usam **Tavily** exclusivamente, com filtros autom√°ticos para garantir **100% de precis√£o** nos resultados (valida CNPJ em todos os snippets retornados). Configure `TAVILY_API_KEY` obrigatoriamente.\n\n## üß™ Testes em Linha de Comando\n\n### Servidor HTTP + SSE local\n```bash\nnpm run build\nTAVILY_API_KEY=\"tvly-xxx\" MCP_TRANSPORT=http MCP_HTTP_PORT=3000 node build/lib/adapters/cli.js\n```\n\nEm outro terminal:\n```bash\nTAVILY_API_KEY=\"tvly-xxx\" node scripts/mcp-client.js list-tools\nTAVILY_API_KEY=\"tvly-xxx\" node scripts/mcp-client.js cnpj 28526270000150\nTAVILY_API_KEY=\"tvly-xxx\" MAX_QUERIES=3 MAX_RESULTS=3 node scripts/mcp-client.js intelligence 28526270000150\n```\n\n### Health check r√°pido\n```bash\ncurl -i https://mcp-dadosbr.aredes.me/health\n```\n\n## üåê Deploy Web (Opcional)\n\n**Cloudflare Workers**: https://mcp-dadosbr.aredes.me\n- üîó REST API: `/cnpj/{cnpj}` ¬∑ `/cep/{cep}` ¬∑ `/search` ¬∑ `/intelligence` ¬∑ `/thinking`\n- ü§ñ OpenAPI: `/openapi.json`\n- üìä Health: `/health`\n- üîê OAuth 2.0 + API Key Authentication: Protegido contra abuso\n- ‚ö° Rate Limiting: 30 req/min por IP (configur√°vel)\n\n**Smithery**: `smithery.yaml` para deploy single-click.\n\n### üöÄ Para ChatGPT MCP\n```bash\n# 1. Deploy no Cloudflare\nnpm run build\nnpm run deploy\n\n# 2. Configure no ChatGPT:\n# - Server URL: https://your-subdomain.workers.dev\n# - O ChatGPT detectar√° automaticamente OAuth + MCP endpoints\n```\n\n### üîí Seguran√ßa (Cloudflare Workers)\n\n**API Key Authentication:**\n- **Protegidos**: Endpoints REST (`/cnpj/*`, `/cep/*`, `/search`, `/intelligence`, `/thinking`)\n- **N√£o protegidos**: Protocolo MCP (`/mcp`, `/sse`) - para compatibilidade com AI assistants\n\n```bash\n# Configure API key\nwrangler secret put MCP_API_KEY\n\n# Use via headers (apenas para endpoints REST):\ncurl -H \"X-API-Key: your-key\" https://mcp-dadosbr.aredes.me/cnpj/11222333000181\n# ou\ncurl -H \"Authorization: Bearer your-key\" https://mcp-dadosbr.aredes.me/cnpj/11222333000181\n\n# Endpoints MCP n√£o precisam de autentica√ß√£o:\ncurl -X POST https://mcp-dadosbr.aredes.me/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\"}'\n```\n\n**Rate Limiting:**\n- Padr√£o: 30 requisi√ß√µes por minuto por IP\n- KV-based para escalabilidade\n- Desativ√°vel com `MCP_DISABLE_RATE_LIMIT=true`\n\n## üîß Configura√ß√£o Avan√ßada\n\n### Vari√°veis de Ambiente\n\n**Obrigat√≥rias:**\n- `TAVILY_API_KEY` - Chave da API Tavily para buscas web ([obtenha aqui](https://tavily.com))\n\n**Opcionais:**\n- `MCP_TRANSPORT` - Modo de transporte: `stdio` (padr√£o) ou `http`\n- `MCP_HTTP_PORT` - Porta do servidor HTTP (padr√£o: `3000`)\n- `MCP_API_KEY` - Chave de API para autentica√ß√£o dos endpoints REST\n- `MCP_DISABLE_RATE_LIMIT` - Desabilitar rate limiting (padr√£o: `false`)\n- `MAX_QUERIES` - N√∫mero m√°ximo de queries de busca (padr√£o: `10`)\n- `MAX_RESULTS` - Resultados m√°ximos por query (padr√£o: `5`)\n- `CNPJ_API_BASE_URL` - Endpoint customizado da API de CNPJ (padr√£o: OpenCNPJ)\n- `CEP_API_BASE_URL` - Endpoint customizado da API de CEP (padr√£o: OpenCEP)\n\n### Arquivo de Configura√ß√£o\n\nCrie `.mcprc.json` no diret√≥rio do seu projeto:\n\n```json\n{\n  \"tavilyApiKey\": \"tvly-sua-chave-api\",\n  \"transport\": \"http\",\n  \"httpPort\": 3000,\n  \"cnpjBaseUrl\": \"https://open.cnpja.com/office/\",\n  \"cepBaseUrl\": \"https://opencep.com/v1/\"\n}\n```\n\n## üìö Documenta√ß√£o\n\n- **[Guia de Navega√ß√£o](docs/NAVIGATION.md)** - üß≠ Encontre rapidamente o que procura\n- **[Guia de Configura√ß√£o](docs/CONFIGURATION.md)** - Refer√™ncia completa de configura√ß√£o\n- **[Exemplos de Uso](docs/EXAMPLE_USAGE.md)** - Exemplos pr√°ticos de uso\n- **[Integra√ß√£o com Clientes MCP](docs/MCP_CLIENT_INTEGRATION.md)** - Guias de configura√ß√£o de IDEs\n- **[Deploy no Cloudflare](docs/CLOUDFLARE_DEPLOYMENT.md)** - Deploy em produ√ß√£o\n- **[Provedores de Busca](docs/PROVIDERS.md)** - Compara√ß√£o de provedores\n- **[Guia para Agentes](docs/development/AGENTS.md)** - Guia para agentes de IA\n- **[Documenta√ß√£o Completa PT-BR](docs/pt-br/README.md)** - Documenta√ß√£o completa em portugu√™s\n\n## üíº Casos de Uso\n\n- **Due diligence e compliance** - Verificar registro empresarial e situa√ß√£o legal\n- **E-commerce e log√≠stica** - Valida√ß√£o e verifica√ß√£o de endere√ßos\n- **Pesquisa jur√≠dica** - Processos judiciais, portais gov.br via dorks\n- **Atendimento ao cliente e CRM** - Verifica√ß√£o de cadastro e enriquecimento de dados\n- **An√°lise financeira** - Checagem de antecedentes e investiga√ß√£o de empresas\n- **Vendas e marketing** - Enriquecimento e valida√ß√£o de leads\n\n## üéØ Exemplos de Prompts\n\n**Consulta B√°sica de CNPJ:**\n```\nPode consultar o CNPJ 11.222.333/0001-81 e me dizer sobre essa empresa?\n```\n\n**Valida√ß√£o de Endere√ßo:**\n```\nO CEP 01310-100 √© v√°lido? Qual √© o endere√ßo?\n```\n\n**Investiga√ß√£o de Intelligence:**\n```\nUse cnpj_intelligence para fazer uma investiga√ß√£o completa sobre o CNPJ 11.222.333/0001-81.\nPreciso de informa√ß√µes sobre processos judiciais, not√≠cias e registros governamentais.\n```\n\n**An√°lise Estruturada:**\n```\nUse sequential thinking para planejar e executar uma investiga√ß√£o de due diligence\npara o CNPJ 11.222.333/0001-81. Inclua dados da empresa, pesquisa jur√≠dica\ne an√°lise de presen√ßa online.\n```\n\n## üß¨ Arquitetura\n\n### Componentes Principais\n\n- **Adapters** (`lib/adapters/`) - Implementa√ß√µes espec√≠ficas de plataforma (CLI, Cloudflare, Smithery)\n- **Core** (`lib/core/`) - L√≥gica de neg√≥cio (ferramentas, busca, intelligence, valida√ß√£o)\n- **Infrastructure** (`lib/infrastructure/`) - Preocupa√ß√µes transversais (cache, circuit breaker, rate limiting, logging)\n- **Workers** (`lib/workers/`) - Implementa√ß√£o do Cloudflare Workers\n- **Types** (`lib/types/`) - Defini√ß√µes de tipos TypeScript\n\n### Padr√µes de Design\n\n- **Adapter Pattern** - Suporte a deploy multiplataforma\n- **Circuit Breaker** - Prote√ß√£o contra falhas de API e resili√™ncia\n- **Result Pattern** - Tratamento funcional de erros sem exce√ß√µes\n- **Repository Pattern** - Camada abstrata de acesso a dados\n- **Strategy Pattern** - Provedores de busca plug√°veis\n\n### Stack Tecnol√≥gica\n\n- **Linguagem**: TypeScript (modo estrito)\n- **Runtime**: Node.js 18+\n- **Servidor HTTP**: Express 5.x\n- **MCP SDK**: @modelcontextprotocol/sdk, @genkit-ai/mcp\n- **Busca**: API Tavily\n- **Deploy**: Cloudflare Workers, NPM, Smithery\n- **Testes**: Vitest (88 testes unit√°rios, 100% de aprova√ß√£o)\n\n## ü§ù Contribui√ß√£o & Lan√ßamentos\n\nRecebemos contribui√ß√µes de desenvolvedores do mundo todo!\n\n- **[Guia de Contribui√ß√£o](CONTRIBUTING.md)** - Como contribuir (Portugu√™s & Ingl√™s)\n- **[Guia de Lan√ßamentos](RELEASING.md)** - Processo de release e versionamento\n- **Tokens CI/CD**: Veja `docs/GITHUB_SECRETS_SETUP.md`\n\n### Setup de Desenvolvimento\n\n```bash\n# Clonar reposit√≥rio\ngit clone https://github.com/cristianoaredes/mcp-dadosbr.git\ncd mcp-dadosbr\n\n# Instalar depend√™ncias\nnpm install\n\n# Build\nnpm run build\n\n# Executar testes\nnpm test\n\n# Executar em modo desenvolvimento\nnpm run dev\n```\n\n## ‚ú® Funcionalidades\n\n- ‚úÖ **5 ferramentas MCP** - Consulta CNPJ, consulta CEP, busca web, intelligence, sequential thinking\n- ‚úÖ **Multiplataforma** - NPM, Cloudflare Workers, Smithery\n- ‚úÖ **Pronto para produ√ß√£o** - Circuit breaker, rate limiting, caching, monitoramento\n- ‚úÖ **Type-safe** - TypeScript completo com modo estrito\n- ‚úÖ **Bem testado** - 88 testes unit√°rios, testes de integra√ß√£o abrangentes\n- ‚úÖ **Bem documentado** - Documenta√ß√£o completa em Portugu√™s e Ingl√™s\n- ‚úÖ **Compat√≠vel com LGPD** - Mascaramento de PII em logs\n- ‚úÖ **Escal√°vel** - Cloudflare Workers com deploy global na edge\n- ‚úÖ **Seguro** - Autentica√ß√£o por API key, rate limiting, prote√ß√£o CORS\n- ‚úÖ **Developer-friendly** - Configura√ß√£o simples, √≥tima DX\n\n## üìä M√©tricas de Qualidade\n\n- **Cobertura de Testes**: ~60%\n- **Testes Unit√°rios**: 88 testes, 100% de aprova√ß√£o\n- **TypeScript**: Modo estrito habilitado\n- **Qualidade de C√≥digo**: ESLint, Prettier\n- **Suporte a Plataformas**: Node.js 18+, Cloudflare Workers\n- **Documenta√ß√£o**: 15+ guias em 2 idiomas\n\n## üìÑ Licen√ßa & Cr√©ditos\n\n- MIT License ‚Äî [LICENSE](LICENSE)\n- Dados fornecidos por [OpenCNPJ](https://opencnpj.org/) e [OpenCEP](https://opencep.com/)\n\n## üë®‚Äçüíª Mantenedor\n\n| [Cristiano Aredes](https://github.com/cristianoaredes)                         |\n| ------------------------------------------------------------------------------ |\n| [LinkedIn](https://www.linkedin.com/in/cristianoaredes/) ¬∑ cristiano@aredes.me |\n\n## üåê Links\n\n- **Pacote NPM**: https://www.npmjs.com/package/@aredes.me/mcp-dadosbr\n- **Smithery**: https://smithery.ai/server/@cristianoaredes/mcp-dadosbr\n- **API Live**: https://mcp-dadosbr.aredes.me\n- **GitHub**: https://github.com/cristianoaredes/mcp-dadosbr\n- **Issues**: https://github.com/cristianoaredes/mcp-dadosbr/issues\n- **Discuss√µes**: https://github.com/cristianoaredes/mcp-dadosbr/discussions\n\n---\n\n## English\n\nü§ñ **Model Context Protocol server for Brazilian company (CNPJ) and postal code (CEP) data.** Integrate verified business data into Claude Desktop, Cursor, Windsurf, Continue.dev and any MCP-compatible AI assistant in minutes.\n\n## ‚ö° Quick Install\n\n```bash\nnpm install -g @aredes.me/mcp-dadosbr\n```\n\nOr run directly with NPX:\n\n```bash\nnpx @aredes.me/mcp-dadosbr\n```\n\n### Via Smithery (1-click install)\n\n```bash\nnpx -y @smithery/cli install @cristianoaredes/mcp-dadosbr --client claude\n```\n\n### Deploy to VPS\n\n[![Deploy to Hostinger VPS](https://img.shields.io/badge/Deploy%20to-Hostinger%20VPS-673DE6?style=for-the-badge&logo=hostinger&logoColor=white)](https://www.hostinger.com.br/cart?product=vps%3Avps_kvm_2&period=12&referral_type=cart_link&REFERRALCODE=FQLCRISTIRC3&referral_id=019a73b2-a3cd-72b8-8141-76eb55275046)\n\n## üîå IDE / MCP Client Configuration\n\n### ü§ñ Claude Desktop\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)\n\n> ‚ö†Ô∏è **Required**: Set `TAVILY_API_KEY` to use `cnpj_search` and `cnpj_intelligence`. Get your key at [tavily.com](https://tavily.com)\n\n### üéØ Cursor IDE\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.cursor/config.json`\n\n### üèÑ Windsurf IDE\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.windsurf/config.json`\n\n### üîÑ Continue.dev\n```json\n{\n  \"mcpServers\": [\n    {\n      \"name\": \"dadosbr\",\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  ]\n}\n```\n**Location**: `~/.continue/config.json`\n\n### üßë‚Äçüíª Claude Code CLI\n\n**Installation via `claude mcp add` command:**\n```bash\n# Option 1: Local stdio server (recommended for development)\nclaude mcp add --transport stdio dadosbr \\\n  --env TAVILY_API_KEY=tvly-your-api-key-here \\\n  -- npx -y @aredes.me/mcp-dadosbr\n\n# Option 2: Remote HTTP server (Cloudflare Workers)\nclaude mcp add --transport http dadosbr \\\n  https://mcp-dadosbr.aredes.me/mcp\n```\n\n**Verification:**\n```bash\n# List configured MCP servers\nclaude mcp list\n\n# Remove if needed\nclaude mcp remove dadosbr\n```\n\n### ü§ñ Google Gemini CLI\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.config/gemini/mcp_config.json`\n\n### üì¶ Codex CLI\n```bash\n# Configure in .codexrc\ncodex mcp add dadosbr npx @aredes.me/mcp-dadosbr\n\n# Or via environment\nexport CODEX_MCP_SERVERS='{\"dadosbr\":{\"command\":\"npx\",\"args\":[\"@aredes.me/mcp-dadosbr\"],\"env\":{\"TAVILY_API_KEY\":\"tvly-xxx\"}}}'\n```\n\n### üêù Zed Editor\n```json\n{\n  \"context_servers\": {\n    \"dadosbr\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"@aredes.me/mcp-dadosbr\"]\n      },\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.config/zed/settings.json`\n\n### ü¶ñ Cline (VS Code Extension)\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: VS Code Settings > Extensions > Cline > MCP Servers\n\n### ‚ö° Roo Cline\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.roo-cline/mcp-settings.json`\n\n### ü§ñ ChatGPT MCP\nTo use with ChatGPT, configure the Cloudflare Workers server as a remote endpoint:\n\n1. **Deploy to Cloudflare Workers**: `npm run deploy`\n2. **Configure in ChatGPT**:\n   - Server URL: `https://mcp-dadosbr.your-subdomain.workers.dev`\n   - ChatGPT will automatically detect OAuth and MCP endpoints\n3. **Configure API Key** (optional, via Workers environment variables):\n   ```bash\n   wrangler secret put TAVILY_API_KEY\n   ```\n\n**Available REST APIs**:\n- `GET /cnpj/{cnpj}` - Query company data\n- `GET /cep/{cep}` - Query address data\n- `POST /search` - Intelligent web search\n- `POST /intelligence` - Complete intelligence search\n- `POST /thinking` - Structured reasoning\n\n**‚úÖ Quick test**\n```\nCan you look up CNPJ 11.222.333/0001-81?\n```\n\n## üõ†Ô∏è Available Tools\n\n- üè¢ **`cnpj_lookup`** ‚Äî Company name, tax status, address, CNAE code (source: OpenCNPJ)\n- üìÆ **`cep_lookup`** ‚Äî Street, neighborhood, city, state, area code (source: OpenCEP)\n- üîç **`cnpj_search`** ‚Äî Web searches with dorks (site:, intext:, filetype:) via Tavily\n- ü§î **`sequentialthinking`** ‚Äî Structured step-by-step reasoning\n- üéØ **`cnpj_intelligence`** ‚Äî Orchestrates multiple queries and generates consolidated report with accuracy filters\n\n> **‚ú® New in v0.3.2**: Web searches now use **Tavily** exclusively, with automatic filters ensuring **100% accuracy** (validates CNPJ in all returned snippets). `TAVILY_API_KEY` is required.\n\n## üß™ Command Line Testing\n\n### Local HTTP + SSE server\n```bash\nnpm run build\nTAVILY_API_KEY=\"tvly-xxx\" MCP_TRANSPORT=http MCP_HTTP_PORT=3000 node build/lib/adapters/cli.js\n```\n\nIn another terminal:\n```bash\nTAVILY_API_KEY=\"tvly-xxx\" node scripts/mcp-client.js list-tools\nTAVILY_API_KEY=\"tvly-xxx\" node scripts/mcp-client.js cnpj 28526270000150\nTAVILY_API_KEY=\"tvly-xxx\" MAX_QUERIES=3 MAX_RESULTS=3 node scripts/mcp-client.js intelligence 28526270000150\n```\n\n### Quick health check\n```bash\ncurl -i https://mcp-dadosbr.aredes.me/health\n```\n\n## üåê Web Deployment (Optional)\n\n**Cloudflare Workers**: https://mcp-dadosbr.aredes.me\n- üîó REST API: `/cnpj/{cnpj}` ¬∑ `/cep/{cep}` ¬∑ `/search` ¬∑ `/intelligence` ¬∑ `/thinking`\n- ü§ñ OpenAPI: `/openapi.json`\n- üìä Health: `/health`\n- üîê OAuth 2.0 + API Key Authentication: Protected against abuse\n- ‚ö° Rate Limiting: 30 req/min per IP (configurable)\n\n**Smithery**: `smithery.yaml` for single-click deployment.\n\n### üöÄ For ChatGPT MCP\n```bash\n# 1. Deploy to Cloudflare\nnpm run build\nnpm run deploy\n\n# 2. Configure in ChatGPT:\n# - Server URL: https://your-subdomain.workers.dev\n# - ChatGPT will automatically detect OAuth + MCP endpoints\n```\n\n### üîí Security (Cloudflare Workers)\n\n**API Key Authentication:**\n- **Protected**: REST endpoints (`/cnpj/*`, `/cep/*`, `/search`, `/intelligence`, `/thinking`)\n- **Unprotected**: MCP protocol (`/mcp`, `/sse`) - for AI assistant compatibility\n\n```bash\n# Configure API key\nwrangler secret put MCP_API_KEY\n\n# Use via headers (REST endpoints only):\ncurl -H \"X-API-Key: your-key\" https://mcp-dadosbr.aredes.me/cnpj/11222333000181\n# or\ncurl -H \"Authorization: Bearer your-key\" https://mcp-dadosbr.aredes.me/cnpj/11222333000181\n\n# MCP endpoints don't require authentication:\ncurl -X POST https://mcp-dadosbr.aredes.me/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\"}'\n```\n\n**Rate Limiting:**\n- Default: 30 requests per minute per IP\n- KV-based for scalability\n- Disable with `MCP_DISABLE_RATE_LIMIT=true`\n\n## üîß Advanced Configuration\n\n### Environment Variables\n\n**Required:**\n- `TAVILY_API_KEY` - Tavily API key for web searches ([get it here](https://tavily.com))\n\n**Optional:**\n- `MCP_TRANSPORT` - Transport mode: `stdio` (default) or `http`\n- `MCP_HTTP_PORT` - HTTP server port (default: `3000`)\n- `MCP_API_KEY` - API key for REST endpoint authentication\n- `MCP_DISABLE_RATE_LIMIT` - Disable rate limiting (default: `false`)\n- `MAX_QUERIES` - Maximum number of search queries (default: `10`)\n- `MAX_RESULTS` - Maximum results per query (default: `5`)\n- `CNPJ_API_BASE_URL` - Custom CNPJ API endpoint (default: OpenCNPJ)\n- `CEP_API_BASE_URL` - Custom CEP API endpoint (default: OpenCEP)\n\n### Configuration File\n\nCreate `.mcprc.json` in your project directory:\n\n```json\n{\n  \"tavilyApiKey\": \"tvly-your-api-key\",\n  \"transport\": \"http\",\n  \"httpPort\": 3000,\n  \"cnpjBaseUrl\": \"https://open.cnpja.com/office/\",\n  \"cepBaseUrl\": \"https://opencep.com/v1/\"\n}\n```\n\n## üìö Documentation\n\n- **[Navigation Guide](docs/NAVIGATION.md)** - üß≠ Quickly find what you're looking for\n- **[Configuration Guide](docs/CONFIGURATION.md)** - Complete configuration reference\n- **[Usage Examples](docs/EXAMPLE_USAGE.md)** - Real-world usage examples\n- **[MCP Client Integration](docs/MCP_CLIENT_INTEGRATION.md)** - IDE setup guides\n- **[Cloudflare Deployment](docs/CLOUDFLARE_DEPLOYMENT.md)** - Deploy to production\n- **[Search Providers](docs/PROVIDERS.md)** - Search provider comparison\n- **[Agent Development Guide](docs/development/AGENTS.md)** - Guide for AI agents\n- **[Complete PT-BR Documentation](docs/pt-br/README.md)** - Documenta√ß√£o completa em portugu√™s\n\n## üíº Use Cases\n\n- **Due diligence and compliance** - Verify company registration and legal status\n- **E-commerce and logistics** - Address validation and verification\n- **Legal research** - Court records, government portals via dorks\n- **Customer service and CRM** - Registration verification and data enrichment\n- **Financial analysis** - Company background checks and investigation\n- **Sales and marketing** - Lead enrichment and validation\n\n## üéØ Example Prompts\n\n**Basic CNPJ Lookup:**\n```\nCan you look up CNPJ 11.222.333/0001-81 and tell me about this company?\n```\n\n**Address Validation:**\n```\nIs CEP 01310-100 a valid postal code? What's the address?\n```\n\n**Intelligence Investigation:**\n```\nUse cnpj_intelligence to do a complete investigation on CNPJ 11.222.333/0001-81. \nI need information about legal cases, news, and government records.\n```\n\n**Structured Analysis:**\n```\nUse sequential thinking to plan and execute a due diligence investigation \nfor CNPJ 11.222.333/0001-81. Include company data, legal research, \nand online presence analysis.\n```\n\n## üß¨ Architecture\n\n### Core Components\n\n- **Adapters** (`lib/adapters/`) - Platform-specific implementations (CLI, Cloudflare, Smithery)\n- **Core** (`lib/core/`) - Business logic (tools, search, intelligence, validation)\n- **Infrastructure** (`lib/infrastructure/`) - Cross-cutting concerns (cache, circuit breaker, rate limiting, logging)\n- **Workers** (`lib/workers/`) - Cloudflare Workers implementation\n- **Types** (`lib/types/`) - TypeScript type definitions\n\n### Design Patterns\n\n- **Adapter Pattern** - Multi-platform deployment support\n- **Circuit Breaker** - API failure protection and resilience\n- **Result Pattern** - Functional error handling without exceptions\n- **Repository Pattern** - Abstract data access layer\n- **Strategy Pattern** - Pluggable search providers\n\n### Technology Stack\n\n- **Language**: TypeScript (strict mode)\n- **Runtime**: Node.js 18+\n- **HTTP Server**: Express 5.x\n- **MCP SDK**: @modelcontextprotocol/sdk, @genkit-ai/mcp\n- **Search**: Tavily API\n- **Deployment**: Cloudflare Workers, NPM, Smithery\n- **Testing**: Vitest (88 unit tests, 100% pass rate)\n\n## ü§ù Contributing & Releases\n\nWe welcome contributions from developers worldwide!\n\n- **[Contributing Guide](CONTRIBUTING.md)** - How to contribute (English & Portuguese)\n- **[Release Guide](RELEASING.md)** - Release process and versioning\n- **CI/CD Tokens**: See `docs/GITHUB_SECRETS_SETUP.md`\n\n### Development Setup\n\n```bash\n# Clone repository\ngit clone https://github.com/cristianoaredes/mcp-dadosbr.git\ncd mcp-dadosbr\n\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n\n# Run in development mode\nnpm run dev\n```\n\n## ‚ú® Features\n\n- ‚úÖ **5 MCP tools** - CNPJ lookup, CEP lookup, web search, intelligence, sequential thinking\n- ‚úÖ **Multi-platform** - NPM, Cloudflare Workers, Smithery\n- ‚úÖ **Production-ready** - Circuit breaker, rate limiting, caching, monitoring\n- ‚úÖ **Robust SSE** - Heartbeat (30s), timeout management (50s), graceful shutdown\n- ‚úÖ **Type-safe** - Full TypeScript with strict mode\n- ‚úÖ **Well-tested** - 88 unit tests, comprehensive integration tests\n- ‚úÖ **Well-documented** - Complete docs in Portuguese and English\n- ‚úÖ **LGPD compliant** - PII masking in logs\n- ‚úÖ **Scalable** - Cloudflare Workers with global edge deployment\n- ‚úÖ **Secure** - API key authentication, rate limiting, CORS protection\n- ‚úÖ **Developer-friendly** - Simple setup, great DX\n\n## üìä Quality Metrics\n\n- **Test Coverage**: ~60%\n- **Unit Tests**: 88 tests, 100% pass rate\n- **TypeScript**: Strict mode enabled\n- **Code Quality**: ESLint, Prettier\n- **Platform Support**: Node.js 18+, Cloudflare Workers\n- **Documentation**: 15+ guides in 2 languages\n\n## üìù License & Credits\n\n- **License**: MIT License ‚Äî see [LICENSE](LICENSE)\n- **Data Sources**: \n  - Company data provided by [OpenCNPJ](https://opencnpj.org/)\n  - Postal code data provided by [OpenCEP](https://opencep.com/)\n  - Web search powered by [Tavily](https://tavily.com/)\n\n## üë®‚Äçüíª Maintainer\n\n| [Cristiano Aredes](https://github.com/cristianoaredes)                         |\n| ------------------------------------------------------------------------------ |\n| [LinkedIn](https://www.linkedin.com/in/cristianoaredes/) ¬∑ cristiano@aredes.me |\n\n## üåê Links\n\n- **NPM Package**: https://www.npmjs.com/package/@aredes.me/mcp-dadosbr\n- **Smithery**: https://smithery.ai/server/@cristianoaredes/mcp-dadosbr\n- **Live API**: https://mcp-dadosbr.aredes.me\n- **GitHub**: https://github.com/cristianoaredes/mcp-dadosbr\n- **Issues**: https://github.com/cristianoaredes/mcp-dadosbr/issues\n- **Discussions**: https://github.com/cristianoaredes/mcp-dadosbr/discussions\n\n---\n\n**Made with ‚ù§Ô∏è for the Brazilian developer community üáßüá∑**"
    },
    "llm_extracted": {
      "capabilities": [
        "Query Brazilian company data by CNPJ including social reason, registration status, address, and CNAE",
        "Validate Brazilian addresses by CEP providing street, neighborhood, city, state, and area code",
        "Perform web searches with advanced dork queries for CNPJ data using Tavily",
        "Execute structured step-by-step reasoning for complex investigations",
        "Orchestrate multiple queries and generate consolidated intelligence reports with assertiveness filters",
        "Deploy as an MCP server compatible with multiple AI assistants and IDEs",
        "Provide REST API endpoints for CNPJ, CEP, search, intelligence, and structured thinking",
        "Support multiple deployment platforms including NPM package, Cloudflare Workers, VPS, and Smithery",
        "Enforce API key authentication and rate limiting for REST endpoints"
      ],
      "limitations": [
        "Requires a valid TAVILY_API_KEY for web search and intelligence tools",
        "Rate limiting of 30 requests per minute per IP by default on REST endpoints",
        "REST API endpoints require API key authentication, while MCP protocol endpoints do not",
        "Search results are limited by configurable MAX_QUERIES and MAX_RESULTS environment variables",
        "Does not protect MCP protocol endpoints with authentication to maintain compatibility with AI assistants"
      ],
      "requirements": [
        "TAVILY_API_KEY from tavily.com for using cnpj_search and cnpj_intelligence tools",
        "Node.js environment to run the NPM package or NPX command",
        "Optional MCP_API_KEY for REST API authentication",
        "Compatible MCP client or IDE configured with the server command and environment variables",
        "Cloudflare Workers account and wrangler CLI for deployment on Cloudflare platform"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, tool descriptions, configuration options, deployment guides, security and rate limiting details, and integration instructions for multiple clients.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# MCP DadosBR üáßüá∑\n\n[![smithery badge](https://smithery.ai/badge/@cristianoaredes/mcp-dadosbr)](https://smithery.ai/server/@cristianoaredes/mcp-dadosbr)\n[![npm version](https://badge.fury.io/js/@aredes.me%2Fmcp-dadosbr.svg)](https://www.npmjs.com/package/@aredes.me/mcp-dadosbr)\n[![npm downloads](https://img.shields.io/npm/dm/@aredes.me/mcp-dadosbr.svg)](https://www.npmjs.com/package/@aredes.me/mcp-dadosbr)\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Cloudflare Workers](https://img.shields.io/badge/Cloudflare-Workers-F38020?logo=cloudflare&logoColor=white)](https://mcp-dadosbr.aredes.me/)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n> **ü§ñ Servidor Model Context Protocol (MCP) para consulta de dados empresariais brasileiros** ‚Äî traga informa√ß√µes de CNPJ (empresas) e CEP (endere√ßos) diretamente para Claude Desktop, Cursor, Windsurf, Continue.dev e outros assistentes de IA.\n> \n> üöÄ Deploy multiplataforma: Pacote NPM, Cloudflare Workers, Smithery.\n\n_[Portugu√™s](#portugu√™s) | [English](#english)_\n\n---\n\n## Portugu√™s\n\nüáßüá∑ **Servidor MCP para consulta de dados empresariais brasileiros (CNPJ) e valida√ß√£o de endere√ßos (CEP).** Integre essas consultas em minutos em Claude Desktop, Cursor, Windsurf, Continue.dev e qualquer cliente compat√≠vel com MCP.",
        "start_pos": 0,
        "end_pos": 1405,
        "token_count_estimate": 351,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 1,
        "text": "ostinger.com.br/cart?product=vps%3Avps_kvm_2&period=12&referral_type=cart_link&REFERRALCODE=FQLCRISTIRC3&referral_id=019a73b2-a3cd-72b8-8141-76eb55275046)\n\n## üîå Configura√ß√£o por IDE / Cliente MCP\n\n### ü§ñ Claude Desktop\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n> ‚ö†Ô∏è **Obrigat√≥rio**: Configure `TAVILY_API_KEY` para usar `cnpj_search` e `cnpj_intelligence`.",
        "start_pos": 1848,
        "end_pos": 2446,
        "token_count_estimate": 149,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 2,
        "text": "mcp list\n\n# Remover se necess√°rio\nclaude mcp remove dadosbr\n```\n\n### ü§ñ Google Gemini CLI\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: `~/.config/gemini/mcp_config.json`\n\n### üì¶ Codex CLI\n```bash\n# Configurar no .codexrc\ncodex mcp add dadosbr npx @aredes.me/mcp-dadosbr\n\n# Ou via environment\nexport CODEX_MCP_SERVERS='{\"dadosbr\":{\"command\":\"npx\",\"args\":[\"@aredes.me/mcp-dadosbr\"],\"env\":{\"TAVILY_API_KEY\":\"tvly-xxx\"}}}'\n```\n\n### üêù Zed Editor\n```json\n{\n  \"context_servers\": {\n    \"dadosbr\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"@aredes.me/mcp-dadosbr\"]\n      },\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: `~/.config/zed/settings.json`\n\n### ü¶ñ Cline (VS Code Extension)\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: VS Code Settings > Extensions > Cline > MCP Servers\n\n### ‚ö° Roo Cline\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Localiza√ß√£o**: `~/.roo-cline/mcp-settings.json`\n\n### ü§ñ ChatGPT MCP\nPara usar com ChatGPT, configure o servidor Cloudflare Workers como endpoint remoto:\n\n1. **Deploy no Cloudflare Workers**: `npm run deploy`\n2. **Configure no ChatGPT**:\n   - URL do servidor: `https://mcp-dadosbr.your-subdomain.workers.dev`\n   - O ChatGPT detectar√° automaticamente os endpoints OAuth e MCP\n3.",
        "start_pos": 3696,
        "end_pos": 5488,
        "token_count_estimate": 447,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 3,
        "text": "les no Workers):\n   ```bash\n   TAVILY_API_KEY=\"tvly-your-api-key-here\"\n   ```\n\n**APIs REST dispon√≠veis**:\n- `GET /cnpj/{cnpj}` - Consulta dados de empresa\n- `GET /cep/{cep}` - Consulta dados de endere√ßo\n- `POST /search` - Busca web inteligente\n- `POST /intelligence` - Busca inteligente completa\n- `POST /thinking` - Racioc√≠nio estruturado\n\n**‚úÖ Teste r√°pido**\n```\nPode consultar o CNPJ 11.222.333/0001-81?\n```\n\n## üõ†Ô∏è Ferramentas Dispon√≠veis\n\n- üè¢ **`cnpj_lookup`** ‚Äî raz√£o social, situa√ß√£o cadastral, endere√ßo, CNAE (fonte: OpenCNPJ)\n- üìÆ **`cep_lookup`** ‚Äî logradouro, bairro, cidade, UF, DDD (fonte: OpenCEP)\n- üîç **`cnpj_search`** ‚Äî buscas web com dorks (site:, intext:, filetype:) via Tavily\n- ü§î **`sequentialthinking`** ‚Äî racioc√≠nio estruturado passo a passo\n- üéØ **`cnpj_intelligence`** ‚Äî orquestra m√∫ltiplas consultas e gera relat√≥rio consolidado com filtros de assertividade\n\n> **‚ú® Novidade v0.3.2**: Buscas web agora usam **Tavily** exclusivamente, com filtros autom√°ticos para garantir **100% de precis√£o** nos resultados (valida CNPJ em todos os snippets retornados). Configure `TAVILY_API_KEY` obrigatoriamente.",
        "start_pos": 5544,
        "end_pos": 6663,
        "token_count_estimate": 279,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 4,
        "text": "`/openapi.json`\n- üìä Health: `/health`\n- üîê OAuth 2.0 + API Key Authentication: Protegido contra abuso\n- ‚ö° Rate Limiting: 30 req/min por IP (configur√°vel)\n\n**Smithery**: `smithery.yaml` para deploy single-click.\n\n### üöÄ Para ChatGPT MCP\n```bash\n# 1. Deploy no Cloudflare\nnpm run build\nnpm run deploy\n\n# 2.",
        "start_pos": 7392,
        "end_pos": 7695,
        "token_count_estimate": 75,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 5,
        "text": "`MAX_RESULTS` - Resultados m√°ximos por query (padr√£o: `5`)\n- `CNPJ_API_BASE_URL` - Endpoint customizado da API de CNPJ (padr√£o: OpenCNPJ)\n- `CEP_API_BASE_URL` - Endpoint customizado da API de CEP (padr√£o: OpenCEP)\n\n### Arquivo de Configura√ß√£o\n\nCrie `.mcprc.json` no diret√≥rio do seu projeto:\n\n```json\n{\n  \"tavilyApiKey\": \"tvly-sua-chave-api\",\n  \"transport\": \"http\",\n  \"httpPort\": 3000,\n  \"cnpjBaseUrl\": \"https://open.cnpja.com/office/\",\n  \"cepBaseUrl\": \"https://opencep.com/v1/\"\n}\n```\n\n## üìö Documenta√ß√£o\n\n- **[Guia de Navega√ß√£o](docs/NAVIGATION.md)** - üß≠ Encontre rapidamente o que procura\n- **[Guia de Configura√ß√£o](docs/CONFIGURATION.md)** - Refer√™ncia completa de configura√ß√£o\n- **[Exemplos de Uso](docs/EXAMPLE_USAGE.md)** - Exemplos pr√°ticos de uso\n- **[Integra√ß√£o com Clientes MCP](docs/MCP_CLIENT_INTEGRATION.md)** - Guias de configura√ß√£o de IDEs\n- **[Deploy no Cloudflare](docs/CLOUDFLARE_DEPLOYMENT.md)** - Deploy em produ√ß√£o\n- **[Provedores de Busca](docs/PROVIDERS.md)** - Compara√ß√£o de provedores\n- **[Guia para Agentes](docs/development/AGENTS.md)** - Guia para agentes de IA\n- **[Documenta√ß√£o Completa PT-BR](docs/pt-br/README.md)** - Documenta√ß√£o completa em portugu√™s\n\n## üíº Casos de Uso\n\n- **Due diligence e compliance** - Verificar registro empresarial e situa√ß√£o legal\n- **E-commerce e log√≠stica** - Valida√ß√£o e verifica√ß√£o de endere√ßos\n- **Pesquisa jur√≠dica** - Processos judiciais, portais gov.br via dorks\n- **Atendimento ao cliente e CRM** - Verifica√ß√£o de cadastro e enriquecimento de dados\n- **An√°lise financeira** - Checagem de antecedentes e investiga√ß√£o de empresas\n- **Vendas e marketing** - Enriquecimento e valida√ß√£o de leads\n\n## üéØ Exemplos de Prompts\n\n**Consulta B√°sica de CNPJ:**\n```\nPode consultar o CNPJ 11.222.333/0001-81 e me dizer sobre essa empresa?\n```\n\n**Valida√ß√£o de Endere√ßo:**\n```\nO CEP 01310-100 √© v√°lido? Qual √© o endere√ßo?\n```\n\n**Investiga√ß√£o de Intelligence:**\n```\nUse cnpj_intelligence para fazer uma investiga√ß√£o completa sobre o CNPJ 11.222.333/0001-81.",
        "start_pos": 9240,
        "end_pos": 11243,
        "token_count_estimate": 500,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 6,
        "text": "o de Endere√ßo:**\n```\nO CEP 01310-100 √© v√°lido? Qual √© o endere√ßo?\n```\n\n**Investiga√ß√£o de Intelligence:**\n```\nUse cnpj_intelligence para fazer uma investiga√ß√£o completa sobre o CNPJ 11.222.333/0001-81.\nPreciso de informa√ß√µes sobre processos judiciais, not√≠cias e registros governamentais.\n```\n\n**An√°lise Estruturada:**\n```\nUse sequential thinking para planejar e executar uma investiga√ß√£o de due diligence\npara o CNPJ 11.222.333/0001-81. Inclua dados da empresa, pesquisa jur√≠dica\ne an√°lise de presen√ßa online.\n```\n\n## üß¨ Arquitetura\n\n### Componentes Principais\n\n- **Adapters** (`lib/adapters/`) - Implementa√ß√µes espec√≠ficas de plataforma (CLI, Cloudflare, Smithery)\n- **Core** (`lib/core/`) - L√≥gica de neg√≥cio (ferramentas, busca, intelligence, valida√ß√£o)\n- **Infrastructure** (`lib/infrastructure/`) - Preocupa√ß√µes transversais (cache, circuit breaker, rate limiting, logging)\n- **Workers** (`lib/workers/`) - Implementa√ß√£o do Cloudflare Workers\n- **Types** (`lib/types/`) - Defini√ß√µes de tipos TypeScript\n\n### Padr√µes de Design\n\n- **Adapter Pattern** - Suporte a deploy multiplataforma\n- **Circuit Breaker** - Prote√ß√£o contra falhas de API e resili√™ncia\n- **Result Pattern** - Tratamento funcional de erros sem exce√ß√µes\n- **Repository Pattern** - Camada abstrata de acesso a dados\n- **Strategy Pattern** - Provedores de busca plug√°veis\n\n### Stack Tecnol√≥gica\n\n- **Linguagem**: TypeScript (modo estrito)\n- **Runtime**: Node.js 18+\n- **Servidor HTTP**: Express 5.x\n- **MCP SDK**: @modelcontextprotocol/sdk, @genkit-ai/mcp\n- **Busca**: API Tavily\n- **Deploy**: Cloudflare Workers, NPM, Smithery\n- **Testes**: Vitest (88 testes unit√°rios, 100% de aprova√ß√£o)\n\n## ü§ù Contribui√ß√£o & Lan√ßamentos\n\nRecebemos contribui√ß√µes de desenvolvedores do mundo todo!",
        "start_pos": 11043,
        "end_pos": 12790,
        "token_count_estimate": 436,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 7,
        "text": "an√ßamentos](RELEASING.md)** - Processo de release e versionamento\n- **Tokens CI/CD**: Veja `docs/GITHUB_SECRETS_SETUP.md`\n\n### Setup de Desenvolvimento\n\n```bash\n# Clonar reposit√≥rio\ngit clone https://github.com/cristianoaredes/mcp-dadosbr.git\ncd mcp-dadosbr\n\n# Instalar depend√™ncias\nnpm install\n\n# Build\nnpm run build\n\n# Executar testes\nnpm test\n\n# Executar em modo desenvolvimento\nnpm run dev\n```\n\n## ‚ú® Funcionalidades\n\n- ‚úÖ **5 ferramentas MCP** - Consulta CNPJ, consulta CEP, busca web, intelligence, sequential thinking\n- ‚úÖ **Multiplataforma** - NPM, Cloudflare Workers, Smithery\n- ‚úÖ **Pronto para produ√ß√£o** - Circuit breaker, rate limiting, caching, monitoramento\n- ‚úÖ **Type-safe** - TypeScript completo com modo estrito\n- ‚úÖ **Bem testado** - 88 testes unit√°rios, testes de integra√ß√£o abrangentes\n- ‚úÖ **Bem documentado** - Documenta√ß√£o completa em Portugu√™s e Ingl√™s\n- ‚úÖ **Compat√≠vel com LGPD** - Mascaramento de PII em logs\n- ‚úÖ **Escal√°vel** - Cloudflare Workers com deploy global na edge\n- ‚úÖ **Seguro** - Autentica√ß√£o por API key, rate limiting, prote√ß√£o CORS\n- ‚úÖ **Developer-friendly** - Configura√ß√£o simples, √≥tima DX\n\n## üìä M√©tricas de Qualidade\n\n- **Cobertura de Testes**: ~60%\n- **Testes Unit√°rios**: 88 testes, 100% de aprova√ß√£o\n- **TypeScript**: Modo estrito habilitado\n- **Qualidade de C√≥digo**: ESLint, Prettier\n- **Suporte a Plataformas**: Node.js 18+, Cloudflare Workers\n- **Documenta√ß√£o**: 15+ guias em 2 idiomas\n\n## üìÑ Licen√ßa & Cr√©ditos\n\n- MIT License ‚Äî [LICENSE](LICENSE)\n- Dados fornecidos por [OpenCNPJ](https://opencnpj.org/) e [OpenCEP](https://opencep.com/)\n\n## üë®‚Äçüíª Mantenedor\n\n| [Cristiano Aredes](https://github.com/cristianoaredes)                         |\n| ------------------------------------------------------------------------------ |\n| [LinkedIn](https://www.linkedin.com/in/cristianoaredes/) ¬∑ cristiano@aredes.me |\n\n## üåê Links\n\n- **Pacote NPM**: https://www.npmjs.com/package/@aredes.me/mcp-dadosbr\n- **Smithery**: https://smithery.ai/server/@cristianoaredes/mcp-dadosbr\n- **API Live**: https://mcp-dadosbr.ared",
        "start_pos": 12891,
        "end_pos": 14939,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 8,
        "text": "e |\n\n## üåê Links\n\n- **Pacote NPM**: https://www.npmjs.com/package/@aredes.me/mcp-dadosbr\n- **Smithery**: https://smithery.ai/server/@cristianoaredes/mcp-dadosbr\n- **API Live**: https://mcp-dadosbr.aredes.me\n- **GitHub**: https://github.com/cristianoaredes/mcp-dadosbr\n- **Issues**: https://github.com/cristianoaredes/mcp-dadosbr/issues\n- **Discuss√µes**: https://github.com/cristianoaredes/mcp-dadosbr/discussions\n\n---\n\n## English\n\nü§ñ **Model Context Protocol server for Brazilian company (CNPJ) and postal code (CEP) data.** Integrate verified business data into Claude Desktop, Cursor, Windsurf, Continue.dev and any MCP-compatible AI assistant in minutes.\n\n## ‚ö° Quick Install\n\n```bash\nnpm install -g @aredes.me/mcp-dadosbr\n```\n\nOr run directly with NPX:\n\n```bash\nnpx @aredes.me/mcp-dadosbr\n```\n\n### Via Smithery (1-click install)\n\n```bash\nnpx -y @smithery/cli install @cristianoaredes/mcp-dadosbr --client claude\n```\n\n### Deploy to VPS\n\n[![Deploy to Hostinger VPS](https://img.shields.io/badge/Deploy%20to-Hostinger%20VPS-673DE6?style=for-the-badge&logo=hostinger&logoColor=white)](https://www.hostinger.com.br/cart?product=vps%3Avps_kvm_2&period=12&referral_type=cart_link&REFERRALCODE=FQLCRISTIRC3&referral_id=019a73b2-a3cd-72b8-8141-76eb55275046)\n\n## üîå IDE / MCP Client Configuration\n\n### ü§ñ Claude Desktop\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)\n\n> ‚ö†Ô∏è **Required**: Set `TAVILY_API_KEY` to use `cnpj_search` and `cnpj_intelligence`.",
        "start_pos": 14739,
        "end_pos": 16423,
        "token_count_estimate": 421,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 9,
        "text": "mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.cursor/config.json`\n\n### üèÑ Windsurf IDE\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.windsurf/config.json`\n\n### üîÑ Continue.dev\n```json\n{\n  \"mcpServers\": [\n    {\n      \"name\": \"dadosbr\",\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  ]\n}\n```\n**Location**: `~/.continue/config.json`\n\n### üßë‚Äçüíª Claude Code CLI\n\n**Installation via `claude mcp add` command:**\n```bash\n# Option 1: Local stdio server (recommended for development)\nclaude mcp add --transport stdio dadosbr \\\n  --env TAVILY_API_KEY=tvly-your-api-key-here \\\n  -- npx -y @aredes.me/mcp-dadosbr\n\n# Option 2: Remote HTTP server (Cloudflare Workers)\nclaude mcp add --transport http dadosbr \\\n  https://mcp-dadosbr.aredes.me/mcp\n```\n\n**Verification:**\n```bash\n# List configured MCP servers\nclaude mcp list\n\n# Remove if needed\nclaude mcp remove dadosbr\n```\n\n### ü§ñ Google Gemini CLI\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.config/gemini/mcp_config.json`\n\n### üì¶ Codex CLI\n```bash\n# Configure in .codexrc\ncodex mcp add dadosbr npx @aredes.me/mcp-dadosbr\n\n# Or via environment\nexport CODEX_MCP_SERVERS='{\"dadosbr\":{\"command\":\"npx\",\"args\":[\"@aredes.me/mcp-dadosbr\"],\"env\":{\"TAVILY_API_KEY\":\"tvly-xxx\"}}}'\n```\n\n### üêù Zed Editor\n```json\n{\n  \"context_servers\": {\n    \"dadosbr\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"@aredes.me/mcp-dadosbr\"]\n      },\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.config/zed/settings.json`\n\n### ü¶ñ Cline (VS Code",
        "start_pos": 16587,
        "end_pos": 18635,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 10,
        "text": "\"args\": [\"@aredes.me/mcp-dadosbr\"]\n      },\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.config/zed/settings.json`\n\n### ü¶ñ Cline (VS Code Extension)\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: VS Code Settings > Extensions > Cline > MCP Servers\n\n### ‚ö° Roo Cline\n```json\n{\n  \"mcpServers\": {\n    \"dadosbr\": {\n      \"command\": \"npx\",\n      \"args\": [\"@aredes.me/mcp-dadosbr\"],\n      \"env\": {\n        \"TAVILY_API_KEY\": \"tvly-your-api-key-here\"\n      }\n    }\n  }\n}\n```\n**Location**: `~/.roo-cline/mcp-settings.json`\n\n### ü§ñ ChatGPT MCP\nTo use with ChatGPT, configure the Cloudflare Workers server as a remote endpoint:\n\n1. **Deploy to Cloudflare Workers**: `npm run deploy`\n2. **Configure in ChatGPT**:\n   - Server URL: `https://mcp-dadosbr.your-subdomain.workers.dev`\n   - ChatGPT will automatically detect OAuth and MCP endpoints\n3. **Configure API Key** (optional, via Workers environment variables):\n   ```bash\n   wrangler secret put TAVILY_API_KEY\n   ```\n\n**Available REST APIs**:\n- `GET /cnpj/{cnpj}` - Query company data\n- `GET /cep/{cep}` - Query address data\n- `POST /search` - Intelligent web search\n- `POST /intelligence` - Complete intelligence search\n- `POST /thinking` - Structured reasoning\n\n**‚úÖ Quick test**\n```\nCan you look up CNPJ 11.222.333/0001-81?",
        "start_pos": 18435,
        "end_pos": 19930,
        "token_count_estimate": 373,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 11,
        "text": "g\n- üéØ **`cnpj_intelligence`** ‚Äî Orchestrates multiple queries and generates consolidated report with accuracy filters\n\n> **‚ú® New in v0.3.2**: Web searches now use **Tavily** exclusively, with automatic filters ensuring **100% accuracy** (validates CNPJ in all returned snippets). `TAVILY_API_KEY` is required.\n\n## üß™ Command Line Testing\n\n### Local HTTP + SSE server\n```bash\nnpm run build\nTAVILY_API_KEY=\"tvly-xxx\" MCP_TRANSPORT=http MCP_HTTP_PORT=3000 node build/lib/adapters/cli.js\n```\n\nIn another terminal:\n```bash\nTAVILY_API_KEY=\"tvly-xxx\" node scripts/mcp-client.js list-tools\nTAVILY_API_KEY=\"tvly-xxx\" node scripts/mcp-client.js cnpj 28526270000150\nTAVILY_API_KEY=\"tvly-xxx\" MAX_QUERIES=3 MAX_RESULTS=3 node scripts/mcp-client.js intelligence 28526270000150\n```\n\n### Quick health check\n```bash\ncurl -i https://mcp-dadosbr.aredes.me/health\n```\n\n## üåê Web Deployment (Optional)\n\n**Cloudflare Workers**: https://mcp-dadosbr.aredes.me\n- üîó REST API: `/cnpj/{cnpj}` ¬∑ `/cep/{cep}` ¬∑ `/search` ¬∑ `/intelligence` ¬∑ `/thinking`\n- ü§ñ OpenAPI: `/openapi.json`\n- üìä Health: `/health`\n- üîê OAuth 2.0 + API Key Authentication: Protected against abuse\n- ‚ö° Rate Limiting: 30 req/min per IP (configurable)\n\n**Smithery**: `smithery.yaml` for single-click deployment.\n\n### üöÄ For ChatGPT MCP\n```bash\n# 1. Deploy to Cloudflare\nnpm run build\nnpm run deploy\n\n# 2.",
        "start_pos": 20283,
        "end_pos": 21624,
        "token_count_estimate": 335,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 12,
        "text": "r-key\" https://mcp-dadosbr.aredes.me/cnpj/11222333000181\n# or\ncurl -H \"Authorization: Bearer your-key\" https://mcp-dadosbr.aredes.me/cnpj/11222333000181\n\n# MCP endpoints don't require authentication:\ncurl -X POST https://mcp-dadosbr.aredes.me/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\"}'\n```\n\n**Rate Limiting:**\n- Default: 30 requests per minute per IP\n- KV-based for scalability\n- Disable with `MCP_DISABLE_RATE_LIMIT=true`\n\n## üîß Advanced Configuration\n\n### Environment Variables\n\n**Required:**\n- `TAVILY_API_KEY` - Tavily API key for web searches ([get it here](https://tavily.com))\n\n**Optional:**\n- `MCP_TRANSPORT` - Transport mode: `stdio` (default) or `http`\n- `MCP_HTTP_PORT` - HTTP server port (default: `3000`)\n- `MCP_API_KEY` - API key for REST endpoint authentication\n- `MCP_DISABLE_RATE_LIMIT` - Disable rate limiting (default: `false`)\n- `MAX_QUERIES` - Maximum number of search queries (default: `10`)\n- `MAX_RESULTS` - Maximum results per query (default: `5`)\n- `CNPJ_API_BASE_URL` - Custom CNPJ API endpoint (default: OpenCNPJ)\n- `CEP_API_BASE_URL` - Custom CEP API endpoint (default: OpenCEP)\n\n### Configuration File\n\nCreate `.mcprc.json` in your project directory:\n\n```json\n{\n  \"tavilyApiKey\": \"tvly-your-api-key\",\n  \"transport\": \"http\",\n  \"httpPort\": 3000,\n  \"cnpjBaseUrl\": \"https://open.cnpja.com/office/\",\n  \"cepBaseUrl\": \"https://opencep.com/v1/\"\n}\n```\n\n## üìö Documentation\n\n- **[Navigation Guide](docs/NAVIGATION.md)** - üß≠ Quickly find what you're looking for\n- **[Configuration Guide](docs/CONFIGURATION.md)** - Complete configuration reference\n- **[Usage Examples](docs/EXAMPLE_USAGE.md)** - Real-world usage examples\n- **[MCP Client Integration](docs/MCP_CLIENT_INTEGRATION.md)** - IDE setup guides\n- **[Cloudflare Deployment](docs/CLOUDFLARE_DEPLOYMENT.md)** - Deploy to production\n- **[Search Providers](docs/PROVIDERS.md)** - Search provider comparison\n- **[Agent Development Guide](docs/development/AGENTS.md)** - Guide for AI agents\n- **[Complete PT-BR Docume",
        "start_pos": 22131,
        "end_pos": 24179,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 13,
        "text": "ploy to production\n- **[Search Providers](docs/PROVIDERS.md)** - Search provider comparison\n- **[Agent Development Guide](docs/development/AGENTS.md)** - Guide for AI agents\n- **[Complete PT-BR Documentation](docs/pt-br/README.md)** - Documenta√ß√£o completa em portugu√™s\n\n## üíº Use Cases\n\n- **Due diligence and compliance** - Verify company registration and legal status\n- **E-commerce and logistics** - Address validation and verification\n- **Legal research** - Court records, government portals via dorks\n- **Customer service and CRM** - Registration verification and data enrichment\n- **Financial analysis** - Company background checks and investigation\n- **Sales and marketing** - Lead enrichment and validation\n\n## üéØ Example Prompts\n\n**Basic CNPJ Lookup:**\n```\nCan you look up CNPJ 11.222.333/0001-81 and tell me about this company?\n```\n\n**Address Validation:**\n```\nIs CEP 01310-100 a valid postal code? What's the address?\n```\n\n**Intelligence Investigation:**\n```\nUse cnpj_intelligence to do a complete investigation on CNPJ 11.222.333/0001-81. \nI need information about legal cases, news, and government records.\n```\n\n**Structured Analysis:**\n```\nUse sequential thinking to plan and execute a due diligence investigation \nfor CNPJ 11.222.333/0001-81. Include company data, legal research, \nand online presence analysis.",
        "start_pos": 23979,
        "end_pos": 25303,
        "token_count_estimate": 331,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 14,
        "text": "atform deployment support\n- **Circuit Breaker** - API failure protection and resilience\n- **Result Pattern** - Functional error handling without exceptions\n- **Repository Pattern** - Abstract data access layer\n- **Strategy Pattern** - Pluggable search providers\n\n### Technology Stack\n\n- **Language**: TypeScript (strict mode)\n- **Runtime**: Node.js 18+\n- **HTTP Server**: Express 5.x\n- **MCP SDK**: @modelcontextprotocol/sdk, @genkit-ai/mcp\n- **Search**: Tavily API\n- **Deployment**: Cloudflare Workers, NPM, Smithery\n- **Testing**: Vitest (88 unit tests, 100% pass rate)\n\n## ü§ù Contributing & Releases\n\nWe welcome contributions from developers worldwide!",
        "start_pos": 25827,
        "end_pos": 26481,
        "token_count_estimate": 163,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      },
      {
        "chunk_id": 15,
        "text": "imple setup, great DX\n\n## üìä Quality Metrics\n\n- **Test Coverage**: ~60%\n- **Unit Tests**: 88 tests, 100% pass rate\n- **TypeScript**: Strict mode enabled\n- **Code Quality**: ESLint, Prettier\n- **Platform Support**: Node.js 18+, Cloudflare Workers\n- **Documentation**: 15+ guides in 2 languages\n\n## üìù License & Credits\n\n- **License**: MIT License ‚Äî see [LICENSE](LICENSE)\n- **Data Sources**: \n  - Company data provided by [OpenCNPJ](https://opencnpj.org/)\n  - Postal code data provided by [OpenCEP](https://opencep.com/)\n  - Web search powered by [Tavily](https://tavily.com/)\n\n## üë®‚Äçüíª Maintainer\n\n| [Cristiano Aredes](https://github.com/cristianoaredes)                         |\n| ------------------------------------------------------------------------------ |\n| [LinkedIn](https://www.linkedin.com/in/cristianoaredes/) ¬∑ cristiano@aredes.me |\n\n## üåê Links\n\n- **NPM Package**: https://www.npmjs.com/package/@aredes.me/mcp-dadosbr\n- **Smithery**: https://smithery.ai/server/@cristianoaredes/mcp-dadosbr\n- **Live API**: https://mcp-dadosbr.aredes.me\n- **GitHub**: https://github.com/cristianoaredes/mcp-dadosbr\n- **Issues**: https://github.com/cristianoaredes/mcp-dadosbr/issues\n- **Discussions**: https://github.com/cristianoaredes/mcp-dadosbr/discussions\n\n---\n\n**Made with ‚ù§Ô∏è for the Brazilian developer community üáßüá∑**",
        "start_pos": 27675,
        "end_pos": 28991,
        "token_count_estimate": 329,
        "source_type": "readme",
        "agent_id": "4d1e61d52478be56"
      }
    ]
  },
  {
    "agent_id": "fd4a2a5ac970ddfb",
    "name": "ai.smithery/ctaylor86-mcp-video-download-server",
    "source": "mcp",
    "source_url": "https://github.com/ctaylor86/mcp-video-download-server",
    "description": "Connect your video workflows to cloud storage. Organize and access video assets across projects wi‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-15T11:45:18.173946Z",
    "indexed_at": "2026-02-18T04:06:16.478465",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# MCP Video Cloud Server\n\nA **Remote** Model Context Protocol (MCP) server that downloads videos from social media platforms (YouTube, Facebook, Instagram, TikTok, etc.) and stores them in S3-compatible cloud storage, returning publicly accessible URLs. \n\n**‚ú® Optimized for Smithery.ai deployment** - Deploy in minutes with one-click setup!\n\n## üöÄ Quick Deploy to Smithery.ai\n\n1. **Fork this repository** to your GitHub account\n2. **Set up Cloudflare R2** (or AWS S3) storage\n3. **Deploy on Smithery.ai** - Click \"Deploy\" and connect your GitHub repo\n4. **Configure your S3 credentials** in Smithery's interface\n5. **Start using** - Connect to Claude Desktop and download videos!\n\nüëâ **[See detailed deployment instructions](DEPLOYMENT.md)**\n\n## ‚ú® Features\n\n- **Multi-Platform Support**: Download from 1000+ sites via yt-dlp\n- **Cloud Storage**: Automatic upload to S3/R2 with public URLs\n- **Transcript Extraction**: Get clean text transcripts\n- **Thumbnail Extraction**: Extract video thumbnails\n- **Audio Extraction**: Extract audio in MP3 format\n- **Metadata Retrieval**: Get comprehensive video information\n- **Remote Access**: Runs in the cloud, no local setup needed\n- **Smithery Integration**: One-click deployment and scaling\n\n## üéØ Supported Platforms\n\n- YouTube\n- Facebook\n- Instagram\n- TikTok\n- Twitter/X\n- Vimeo\n- Twitch\n- And 1000+ other sites\n\n## üõ†Ô∏è Available Tools\n\n### `download_video_to_cloud`\nDownload a video and store it in cloud storage.\n- **Input**: Video URL, quality preference\n- **Output**: Public URL, filename, file size, metadata\n\n### `download_audio_to_cloud`\nExtract audio from a video and store it in cloud storage.\n- **Input**: Video URL\n- **Output**: Public URL, filename, file size, metadata\n\n### `extract_transcript_to_cloud`\nExtract subtitles/transcript and store as clean text.\n- **Input**: Video URL, language code\n- **Output**: Public URL, filename, transcript preview\n\n### `extract_thumbnail_to_cloud`\nExtract video thumbnail and store it in cloud storage.\n- **Input**: Video URL\n- **Output**: Public URL, filename\n\n### `get_video_metadata`\nGet comprehensive video information without downloading.\n- **Input**: Video URL\n- **Output**: Title, uploader, duration, views, description, etc.\n\n## üîß Configuration\n\nThe server requires S3-compatible storage configuration:\n\n```yaml\ns3Endpoint: https://your-account.r2.cloudflarestorage.com\ns3Region: auto\ns3AccessKeyId: your-access-key\ns3SecretAccessKey: your-secret-key\ns3BucketName: your-bucket-name\ns3PublicUrlBase: https://your-custom-domain.com  # Optional\n```\n\n## üìÅ File Organization\n\nFiles are organized in your S3 bucket:\n- `videos/` - Downloaded video files\n- `audio/` - Extracted audio files\n- `transcripts/` - Transcript text files\n- `thumbnails/` - Thumbnail images\n\n## üîí Security & Privacy\n\n- All files stored in **your own** S3 bucket\n- No data permanently stored on the server\n- Temporary files automatically cleaned up\n- Credentials securely managed by Smithery\n- Server runs in isolated containers\n\n## üí∞ Cost Considerations\n\n- **Smithery Hosting**: Check current pricing plans\n- **Cloudflare R2**: ~$0.015/GB/month storage + bandwidth\n- **Processing**: Included in Smithery hosting\n\n## üèÉ‚Äç‚ôÇÔ∏è Local Development\n\nIf you want to run locally for development:\n\n### Prerequisites\n- Node.js 20+\n- yt-dlp installed\n- S3 credentials\n\n### Setup\n```bash\ngit clone <your-fork>\ncd mcp-video-cloud-server\nnpm install\nnpm run build:stdio\n```\n\n### Environment Variables\n```bash\nS3_ENDPOINT=your-endpoint\nS3_REGION=auto\nS3_ACCESS_KEY_ID=your-key\nS3_SECRET_ACCESS_KEY=your-secret\nS3_BUCKET_NAME=your-bucket\n```\n\n### Run\n```bash\nnpm run start:stdio\n```\n\n## üß™ Testing with Smithery\n\nUse Smithery's development environment:\n\n```bash\nnpm run dev\n```\n\nThis starts a development server with the Smithery playground for testing.\n\n## ü§ù Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Test with `npm run dev`\n5. Submit a pull request\n\n## üìÑ License\n\nMIT License - see LICENSE file for details\n\n## üÜò Support\n\n- **Deployment Issues**: Check [DEPLOYMENT.md](DEPLOYMENT.md)\n- **Smithery Support**: Visit https://smithery.ai/support\n- **Bug Reports**: Create a GitHub issue\n- **Feature Requests**: Create a GitHub issue\n\n---\n\n**Ready to deploy?** üëâ **[Follow the deployment guide](DEPLOYMENT.md)**\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Download videos from 1000+ social media platforms including YouTube, Facebook, Instagram, TikTok, Twitter/X, Vimeo, and Twitch",
        "Upload downloaded videos automatically to S3-compatible cloud storage and provide publicly accessible URLs",
        "Extract clean text transcripts from videos and store them in cloud storage",
        "Extract video thumbnails and store them in cloud storage",
        "Extract audio from videos in MP3 format and store it in cloud storage",
        "Retrieve comprehensive video metadata without downloading the video",
        "Deploy and run remotely in the cloud with one-click setup optimized for Smithery.ai",
        "Support local development with Node.js and yt-dlp for testing and customization"
      ],
      "limitations": [
        "Requires S3-compatible cloud storage for file uploads; no local storage persistence",
        "Temporary files are cleaned up automatically, so no permanent server-side storage",
        "Cost depends on Smithery hosting plans and cloud storage bandwidth and storage fees",
        "No explicit mention of support for DRM-protected or private videos",
        "Limited to platforms supported by yt-dlp (1000+ sites), may not cover all video sources"
      ],
      "requirements": [
        "S3-compatible cloud storage account (e.g., Cloudflare R2 or AWS S3) with access credentials",
        "Node.js version 20 or higher for local development",
        "yt-dlp installed locally for local development",
        "Smithery.ai account for one-click deployment and credential management",
        "GitHub account to fork and deploy the repository"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for all tools, configuration details, supported platforms, limitations, security considerations, and contribution guidelines.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# MCP Video Cloud Server\n\nA **Remote** Model Context Protocol (MCP) server that downloads videos from social media platforms (YouTube, Facebook, Instagram, TikTok, etc.) and stores them in S3-compatible cloud storage, returning publicly accessible URLs. \n\n**‚ú® Optimized for Smithery.ai deployment** - Deploy in minutes with one-click setup!\n\n## üöÄ Quick Deploy to Smithery.ai\n\n1. **Fork this repository** to your GitHub account\n2. **Set up Cloudflare R2** (or AWS S3) storage\n3. **Deploy on Smithery.ai** - Click \"Deploy\" and connect your GitHub repo\n4. **Configure your S3 credentials** in Smithery's interface\n5. **Start using** - Connect to Claude Desktop and download videos!\n\nüëâ **[See detailed deployment instructions](DEPLOYMENT.md)**\n\n## ‚ú® Features\n\n- **Multi-Platform Support**: Download from 1000+ sites via yt-dlp\n- **Cloud Storage**: Automatic upload to S3/R2 with public URLs\n- **Transcript Extraction**: Get clean text transcripts\n- **Thumbnail Extraction**: Extract video thumbnails\n- **Audio Extraction**: Extract audio in MP3 format\n- **Metadata Retrieval**: Get comprehensive video information\n- **Remote Access**: Runs in the cloud, no local setup needed\n- **Smithery Integration**: One-click deployment and scaling\n\n## üéØ Supported Platforms\n\n- YouTube\n- Facebook\n- Instagram\n- TikTok\n- Twitter/X\n- Vimeo\n- Twitch\n- And 1000+ other sites\n\n## üõ†Ô∏è Available Tools\n\n### `download_video_to_cloud`\nDownload a video and store it in cloud storage.\n- **Input**: Video URL, quality preference\n- **Output**: Public URL, filename, file size, metadata\n\n### `download_audio_to_cloud`\nExtract audio from a video and store it in cloud storage.\n- **Input**: Video URL\n- **Output**: Public URL, filename, file size, metadata\n\n### `extract_transcript_to_cloud`\nExtract subtitles/transcript and store as clean text.\n- **Input**: Video URL, language code\n- **Output**: Public URL, filename, transcript preview\n\n### `extract_thumbnail_to_cloud`\nExtract video thumbnail and store it in cloud storage.",
        "start_pos": 0,
        "end_pos": 1994,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "fd4a2a5ac970ddfb"
      },
      {
        "chunk_id": 1,
        "text": "ore as clean text.\n- **Input**: Video URL, language code\n- **Output**: Public URL, filename, transcript preview\n\n### `extract_thumbnail_to_cloud`\nExtract video thumbnail and store it in cloud storage.\n- **Input**: Video URL\n- **Output**: Public URL, filename\n\n### `get_video_metadata`\nGet comprehensive video information without downloading.\n- **Input**: Video URL\n- **Output**: Title, uploader, duration, views, description, etc.\n\n## üîß Configuration\n\nThe server requires S3-compatible storage configuration:\n\n```yaml\ns3Endpoint: https://your-account.r2.cloudflarestorage.com\ns3Region: auto\ns3AccessKeyId: your-access-key\ns3SecretAccessKey: your-secret-key\ns3BucketName: your-bucket-name\ns3PublicUrlBase: https://your-custom-domain.com  # Optional\n```\n\n## üìÅ File Organization\n\nFiles are organized in your S3 bucket:\n- `videos/` - Downloaded video files\n- `audio/` - Extracted audio files\n- `transcripts/` - Transcript text files\n- `thumbnails/` - Thumbnail images\n\n## üîí Security & Privacy\n\n- All files stored in **your own** S3 bucket\n- No data permanently stored on the server\n- Temporary files automatically cleaned up\n- Credentials securely managed by Smithery\n- Server runs in isolated containers\n\n## üí∞ Cost Considerations\n\n- **Smithery Hosting**: Check current pricing plans\n- **Cloudflare R2**: ~$0.015/GB/month storage + bandwidth\n- **Processing**: Included in Smithery hosting\n\n## üèÉ‚Äç‚ôÇÔ∏è Local Development\n\nIf you want to run locally for development:\n\n### Prerequisites\n- Node.js 20+\n- yt-dlp installed\n- S3 credentials\n\n### Setup\n```bash\ngit clone <your-fork>\ncd mcp-video-cloud-server\nnpm install\nnpm run build:stdio\n```\n\n### Environment Variables\n```bash\nS3_ENDPOINT=your-endpoint\nS3_REGION=auto\nS3_ACCESS_KEY_ID=your-key\nS3_SECRET_ACCESS_KEY=your-secret\nS3_BUCKET_NAME=your-bucket\n```\n\n### Run\n```bash\nnpm run start:stdio\n```\n\n## üß™ Testing with Smithery\n\nUse Smithery's development environment:\n\n```bash\nnpm run dev\n```\n\nThis starts a development server with the Smithery playground for testing.\n\n## ü§ù Contributing\n\n1.",
        "start_pos": 1794,
        "end_pos": 3822,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "fd4a2a5ac970ddfb"
      },
      {
        "chunk_id": 2,
        "text": "dio\n```\n\n## üß™ Testing with Smithery\n\nUse Smithery's development environment:\n\n```bash\nnpm run dev\n```\n\nThis starts a development server with the Smithery playground for testing.\n\n## ü§ù Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Test with `npm run dev`\n5. Submit a pull request\n\n## üìÑ License\n\nMIT License - see LICENSE file for details\n\n## üÜò Support\n\n- **Deployment Issues**: Check [DEPLOYMENT.md](DEPLOYMENT.md)\n- **Smithery Support**: Visit https://smithery.ai/support\n- **Bug Reports**: Create a GitHub issue\n- **Feature Requests**: Create a GitHub issue\n\n---\n\n**Ready to deploy?** üëâ **[Follow the deployment guide](DEPLOYMENT.md)**",
        "start_pos": 3622,
        "end_pos": 4302,
        "token_count_estimate": 169,
        "source_type": "readme",
        "agent_id": "fd4a2a5ac970ddfb"
      }
    ]
  },
  {
    "agent_id": "fef95d831b017449",
    "name": "ai.smithery/cuongpo-coti-mcp",
    "source": "mcp",
    "source_url": "https://github.com/cuongpo/coti-mcp",
    "description": "Connect to the COTI blockchain to manage accounts, transfer native tokens, and deploy and operate‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T07:26:17.819584Z",
    "indexed_at": "2026-02-18T04:06:18.225102",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# <img src=\"https://coti.io/images/favicon.ico\" height=\"32\"> COTI MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@davibauer/coti-mcp)](https://smithery.ai/server/@davibauer/coti-mcp)\n\nA Model Context Protocol (MCP) server that enables AI applications to interact with the COTI blockchain, specializing in private token operations using COTI's Multi-Party Computation (MPC) technology.\n\n## Features\n\n**Complete Account Management**\n- Create, import, and export COTI accounts\n- Switch between networks (testnet/devnet/mainnet)\n- Generate AES keys and manage account credentials\n- Sign messages and verify signatures\n- Multi-account support with seamless switching\n\n**Private ERC20 Token Operations**\n- Deploy new private ERC20 contracts\n- Comprehensive token information and allowance management\n- Secure minting, transfer, and approval operations\n- Balance, decimals, and supply tracking\n\n**Private ERC721 NFT Operations**\n- Deploy new private NFT collections\n- Complete approval and ownership management\n- Safe minting and transfer operations\n- Token URI, balance, and collection supply tracking\n\n**Transaction Management**\n- Call arbitrary smart contract functions\n- Monitor transaction status and retrieve logs\n- Decode event data from transaction logs\n- Full transaction lifecycle management\n\n**Privacy & Encryption**\n- Value encryption and decryption using COTI AES keys\n- Full privacy support through MPC technology\n- Secure transaction processing and message signing\n\n## Available Tools\n\n**Account Management (12 tools)**\n- `change_default_account` - Switch between configured accounts\n- `create_account` - Create new COTI account\n- `decrypt_value` - Decrypt values with COTI AES key\n- `encrypt_value` - Encrypt values with COTI AES key\n- `export_accounts` - Export account configurations\n- `generate_aes_key` - Generate new AES encryption key\n- `get_current_network` - Get currently configured network\n- `import_accounts` - Import account configurations\n- `list_accounts` - List all configured accounts\n- `sign_message` - Sign messages with account private key\n- `switch_network` - Switch between COTI networks\n- `verify_signature` - Verify message signatures\n\n**Private ERC20 Operations (8 tools)**\n- `approve_erc20_spender` - Approve spender for ERC20 tokens\n- `deploy_private_erc20_contract` - Deploy new private ERC20 contract\n- `get_private_erc20_allowance` - Get ERC20 token allowance\n- `get_private_erc20_balance` - Get private token balance\n- `get_private_erc20_decimals` - Get token decimals\n- `get_private_erc20_total_supply` - Get token total supply\n- `mint_private_erc20_token` - Mint private ERC20 tokens\n- `transfer_private_erc20` - Transfer private ERC20 tokens\n\n**Private ERC721 Operations (11 tools)**\n- `approve_private_erc721` - Approve spender for specific NFT\n- `deploy_private_erc721_contract` - Deploy new private NFT contract\n- `get_private_erc721_approved` - Get approved spender for NFT\n- `get_private_erc721_balance` - Get NFT balance for account\n- `get_private_erc721_is_approved_for_all` - Check operator approval status\n- `get_private_erc721_token_owner` - Get NFT token owner\n- `get_private_erc721_token_uri` - Get NFT token URI\n- `get_private_erc721_total_supply` - Get NFT collection total supply\n- `mint_private_erc721_token` - Mint new private NFT\n- `set_private_erc721_approval_for_all` - Set operator approval for all NFTs\n- `transfer_private_erc721` - Transfer private NFTs\n\n**Transaction Management (4 tools)**\n- `call_contract_function` - Call smart contract functions\n- `decode_event_data` - Decode transaction event data\n- `get_transaction_logs` - Get transaction event logs\n- `get_transaction_status` - Get transaction status and details\n\n**Native Token Operations (2 tools)**\n- `get_native_balance` - Get native COTI token balance\n- `transfer_native` - Transfer native COTI tokens\n\n## Requirements\n\n- Node.js v18 or higher\n- COTI AES Key for API authentication\n- COTI Private Key for signing transactions\n- COTI Public Key corresponding to the private key\n\n## Setup\n\n### Installation\n\n```bash\ngit clone https://github.com/davibauer/coti-mcp.git\ncd coti-mcp\nnpm install\nnpm run build\n```\n\n### Distribution\n\n**Smithery**\n```bash\nnpx -y @smithery/cli install @davibauer/coti-mcp --client claude\n```\n\n**Local Configuration**\n\nAdd to your Claude desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"coti-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/build/index.js\"],\n      \"env\": {\n        \"COTI_MCP_AES_KEY\": \"your_aes_key_here\",\n        \"COTI_MCP_PUBLIC_KEY\": \"your_public_key_here\",\n        \"COTI_MCP_PRIVATE_KEY\": \"your_private_key_here\",\n        \"COTI_MCP_CURRENT_PUBLIC_KEY\": \"current_account_public_key\"\n      }\n    }\n  }\n}\n```\n\n**Multi-Account Support**\n\nConfigure multiple accounts using comma-separated values:\n\n```json\n\"env\": {\n  \"COTI_MCP_AES_KEY\": \"key1,key2,key3\",\n  \"COTI_MCP_PUBLIC_KEY\": \"pub1,pub2,pub3\",\n  \"COTI_MCP_PRIVATE_KEY\": \"priv1,priv2,priv3\",\n  \"COTI_MCP_CURRENT_PUBLIC_KEY\": \"pub1\"\n}\n```\n\n## Resources\n\n- [COTI Documentation](https://docs.coti.io)\n- [Smithery Server Page](https://smithery.ai/server/@davibauer/coti-mcp)\n\n## License\n\nMIT License - see LICENSE file for details."
    },
    "llm_extracted": {
      "capabilities": [
        "Create, import, export, and manage multiple COTI accounts",
        "Switch between COTI networks including testnet, devnet, and mainnet",
        "Generate AES keys and manage account credentials securely",
        "Sign messages and verify signatures using account private keys",
        "Deploy and manage private ERC20 tokens including minting, transferring, and approval",
        "Deploy and manage private ERC721 NFTs including minting, transferring, and approval",
        "Call arbitrary smart contract functions and manage full transaction lifecycle",
        "Encrypt and decrypt values using COTI AES keys for privacy",
        "Monitor transaction status, retrieve logs, and decode event data",
        "Manage native COTI token balances and transfers"
      ],
      "limitations": [],
      "requirements": [
        "Node.js version 18 or higher",
        "COTI AES Key for API authentication",
        "COTI Private Key for signing transactions",
        "COTI Public Key corresponding to the private key"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions, usage examples for multi-account configuration, explicit requirements, and links to additional resources.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# <img src=\"https://coti.io/images/favicon.ico\" height=\"32\"> COTI MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@davibauer/coti-mcp)](https://smithery.ai/server/@davibauer/coti-mcp)\n\nA Model Context Protocol (MCP) server that enables AI applications to interact with the COTI blockchain, specializing in private token operations using COTI's Multi-Party Computation (MPC) technology.",
        "start_pos": 0,
        "end_pos": 394,
        "token_count_estimate": 98,
        "source_type": "readme",
        "agent_id": "fef95d831b017449"
      },
      {
        "chunk_id": 1,
        "text": "ryption key\n- `get_current_network` - Get currently configured network\n- `import_accounts` - Import account configurations\n- `list_accounts` - List all configured accounts\n- `sign_message` - Sign messages with account private key\n- `switch_network` - Switch between COTI networks\n- `verify_signature` - Verify message signatures\n\n**Private ERC20 Operations (8 tools)**\n- `approve_erc20_spender` - Approve spender for ERC20 tokens\n- `deploy_private_erc20_contract` - Deploy new private ERC20 contract\n- `get_private_erc20_allowance` - Get ERC20 token allowance\n- `get_private_erc20_balance` - Get private token balance\n- `get_private_erc20_decimals` - Get token decimals\n- `get_private_erc20_total_supply` - Get token total supply\n- `mint_private_erc20_token` - Mint private ERC20 tokens\n- `transfer_private_erc20` - Transfer private ERC20 tokens\n\n**Private ERC721 Operations (11 tools)**\n- `approve_private_erc721` - Approve spender for specific NFT\n- `deploy_private_erc721_contract` - Deploy new private NFT contract\n- `get_private_erc721_approved` - Get approved spender for NFT\n- `get_private_erc721_balance` - Get NFT balance for account\n- `get_private_erc721_is_approved_for_all` - Check operator approval status\n- `get_private_erc721_token_owner` - Get NFT token owner\n- `get_private_erc721_token_uri` - Get NFT token URI\n- `get_private_erc721_total_supply` - Get NFT collection total supply\n- `mint_private_erc721_token` - Mint new private NFT\n- `set_private_erc721_approval_for_all` - Set operator approval for all NFTs\n- `transfer_private_erc721` - Transfer private NFTs\n\n**Transaction Management (4 tools)**\n- `call_contract_function` - Call smart contract functions\n- `decode_event_data` - Decode transaction event data\n- `get_transaction_logs` - Get transaction event logs\n- `get_transaction_status` - Get transaction status and details\n\n**Native Token Operations (2 tools)**\n- `get_native_balance` - Get native COTI token balance\n- `transfer_native` - Transfer native COTI tokens\n\n## Requirements\n\n- Node.js v18 or higher\n- COTI AES K",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "fef95d831b017449"
      },
      {
        "chunk_id": 2,
        "text": "s\n\n**Native Token Operations (2 tools)**\n- `get_native_balance` - Get native COTI token balance\n- `transfer_native` - Transfer native COTI tokens\n\n## Requirements\n\n- Node.js v18 or higher\n- COTI AES Key for API authentication\n- COTI Private Key for signing transactions\n- COTI Public Key corresponding to the private key\n\n## Setup\n\n### Installation\n\n```bash\ngit clone https://github.com/davibauer/coti-mcp.git\ncd coti-mcp\nnpm install\nnpm run build\n```\n\n### Distribution\n\n**Smithery**\n```bash\nnpx -y @smithery/cli install @davibauer/coti-mcp --client claude\n```\n\n**Local Configuration**\n\nAdd to your Claude desktop configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"coti-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/build/index.js\"],\n      \"env\": {\n        \"COTI_MCP_AES_KEY\": \"your_aes_key_here\",\n        \"COTI_MCP_PUBLIC_KEY\": \"your_public_key_here\",\n        \"COTI_MCP_PRIVATE_KEY\": \"your_private_key_here\",\n        \"COTI_MCP_CURRENT_PUBLIC_KEY\": \"current_account_public_key\"\n      }\n    }\n  }\n}\n```\n\n**Multi-Account Support**\n\nConfigure multiple accounts using comma-separated values:\n\n```json\n\"env\": {\n  \"COTI_MCP_AES_KEY\": \"key1,key2,key3\",\n  \"COTI_MCP_PUBLIC_KEY\": \"pub1,pub2,pub3\",\n  \"COTI_MCP_PRIVATE_KEY\": \"priv1,priv2,priv3\",\n  \"COTI_MCP_CURRENT_PUBLIC_KEY\": \"pub1\"\n}\n```\n\n## Resources\n\n- [COTI Documentation](https://docs.coti.io)\n- [Smithery Server Page](https://smithery.ai/server/@davibauer/coti-mcp)\n\n## License\n\nMIT License - see LICENSE file for details.",
        "start_pos": 3696,
        "end_pos": 5176,
        "token_count_estimate": 370,
        "source_type": "readme",
        "agent_id": "fef95d831b017449"
      }
    ]
  },
  {
    "agent_id": "9a27ff795aa53834",
    "name": "ai.smithery/data-mindset-sts-google-forms-mcp",
    "source": "mcp",
    "source_url": "https://github.com/data-mindset/sts-google-forms-mcp",
    "description": "Create and manage Google Forms to run surveys and collect data. Add text and multiple-choice quest‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage",
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-05T09:13:16.21757Z",
    "indexed_at": "2026-02-18T04:06:20.401191",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create Google Forms to run surveys",
        "Manage Google Forms",
        "Add text questions to Google Forms",
        "Add multiple-choice questions to Google Forms",
        "Collect data through Google Forms"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of capabilities but lacks detailed structure, examples, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "64c8b2fa1cb33789",
    "name": "ai.smithery/demomagic-duckchain-mcp",
    "source": "mcp",
    "source_url": "https://github.com/demomagic/duckchain-mcp",
    "description": "Explore blockchain data across addresses, tokens, blocks, and transactions. Investigate any transa‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T05:32:58.075994Z",
    "indexed_at": "2026-02-18T04:06:25.355613",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# DuckChain MCP Server\n\nA comprehensive Model Context Protocol (MCP) server that provides complete access to blockchain data via the BlockScout API v2 specification. DuckChain enables AI assistants to perform deep blockchain analysis, including transaction tracing, address exploration, token management, smart contract analysis, and comprehensive market research with 56+ specialized tools.\n\n## Features\n\n### üîç Search & Discovery\n- **Blockchain Search**: Search for addresses, tokens, blocks, or transactions\n- **Search Redirects**: Check if queries should redirect to specific pages\n\n### üìä Transaction Analysis\n- **Transaction Details**: Get comprehensive transaction information by hash\n- **Transaction Filtering**: Filter transactions by status, type, or method\n- **Token Transfers**: Retrieve token transfer details for specific transactions\n- **Internal Transactions**: Access internal transaction data for specific transactions\n- **Transaction Logs**: Get detailed logs for transaction analysis\n- **Raw Trace**: Access raw transaction trace data\n- **State Changes**: Monitor state changes caused by transactions\n- **Transaction Summary**: Get concise transaction summaries\n\n### üß± Block Operations\n- **Block Data**: Access block information with optional type filtering\n- **Block Details**: Get specific block details by number or hash\n- **Block Transactions**: Retrieve all transactions in a specific block\n- **Block Withdrawals**: Access withdrawal data for specific blocks\n- **Main Page Blocks**: Get blocks optimized for main page display\n\n### üè† Address Analysis\n- **Address Details**: Get comprehensive address information\n- **Address Transactions**: Access all transactions for specific addresses\n- **Token Balances**: Check token balances for any address\n- **Balance History**: Track coin balance changes over time\n- **NFT Management**: Access NFT data and collections for addresses\n- **Address Counters**: Get transaction and activity counters\n- **Validated Blocks**: See blocks validated by specific addresses\n- **Withdrawal History**: Track withdrawal activities\n\n### üí∞ Token Operations\n- **Token Management**: Complete token lifecycle management\n- **Token Details**: Get comprehensive token information\n- **Token Transfers**: Access all token transfer data\n- **Token Holders**: Analyze token distribution and holders\n- **Token Instances**: Manage token instances and metadata\n- **Token Counters**: Track token activity statistics\n- **Metadata Refresh**: Update token instance metadata\n\n### üìú Smart Contract Analysis\n- **Contract Details**: Get comprehensive smart contract information\n- **Contract Lists**: Browse all smart contracts\n- **Contract Counters**: Track smart contract activity\n- **Contract Analysis**: Analyze contract interactions and usage\n\n### üìà Statistics & Charts\n- **Blockchain Stats**: Get comprehensive blockchain statistics\n- **Transaction Charts**: Access transaction volume and activity data\n- **Market Charts**: Retrieve price and volume visualization data\n\n### üîß Internal Operations\n- **Internal Transactions**: Access internal transaction data\n- **Indexing Status**: Monitor blockchain indexing progress\n\n## üöÄ What's New\n\n### Complete API Coverage\n- **56+ MCP Tools** covering all BlockScout API v2 endpoints\n- **Advanced Transaction Analysis** with logs, traces, and state changes\n- **Comprehensive Address Analysis** including NFT and balance history\n- **Full Token Management** with instances and metadata refresh\n- **Smart Contract Analysis** with detailed contract information\n- **Deep Block Analysis** including withdrawals and transaction details\n\n### Enhanced Capabilities\n- **No Authentication Required** - Works out of the box\n- **Type-Safe Implementation** - Full Pydantic validation\n- **Comprehensive Error Handling** - User-friendly error messages\n- **Async Performance** - Fast, non-blocking operations\n- **Modular Design** - Easy to extend and maintain\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n- Python 3.10 or higher\n\n## Supported Networks\n\nDuckChain supports all BlockScout-compatible networks including:\n\n- **Ethereum Mainnet** (`blockscout.com/eth/mainnet`)\n- **Polygon** (`blockscout.com/matic/mainnet`)\n- **BSC** (`blockscout.com/bsc/mainnet`)\n- **Arbitrum** (`blockscout.com/arbitrum/mainnet`)\n- **Optimism** (`blockscout.com/optimism/mainnet`)\n- **Avalanche** (`blockscout.com/avax/mainnet`)\n- **Fantom** (`blockscout.com/ftm/mainnet`)\n- **Gnosis Chain** (`blockscout.com/gnosis/mainnet`)\n- **POA Core** (`blockscout.com/poa/core`) - Default\n\n## Getting Started\n\n1. **Install dependencies**:\n   ```bash\n   uv sync\n   ```\n\n2. **Run the server**:\n   ```bash\n   uv run dev\n   ```\n\n3. **Test interactively**:\n   ```bash\n   uv run playground\n   ```\n\n### No Authentication Required\n\n- **Free and public access** - Most BlockScout instances are free and public\n- **No API key needed** - DuckChain works without any authentication\n- **Simple setup** - Just configure the server URL and you're ready to go\n\n## Usage Examples\n\n### Search Operations\n```\nSearch for USDT token information\nSearch for address 0x1234...\nFind block 12345678\n```\n\n### Transaction Analysis\n```\nGet transaction details for 0xabc123...\nGet token transfers for transaction 0xdef456...\nGet internal transactions for transaction 0xdef456...\nGet transaction logs for 0xdef456...\nGet raw trace for transaction 0xdef456...\nGet state changes for transaction 0xdef456...\nGet transaction summary for 0xdef456...\nFilter transactions by method \"transfer\"\n```\n\n### Block Analysis\n```\nGet block details for block 12345678\nGet all transactions in block 12345678\nGet withdrawals for block 12345678\nGet blocks with type filtering\n```\n\n### Address Analysis\n```\nGet address details for 0x1234...\nGet all transactions for address 0x1234...\nGet token balances for address 0x1234...\nGet coin balance history for address 0x1234...\nGet NFT collections for address 0x1234...\nGet blocks validated by address 0x1234...\n```\n\n### Token Management\n```\nGet token details for 0xabc123...\nGet token holders for 0xabc123...\nGet token transfers for 0xabc123...\nGet token instances for 0xabc123...\nRefetch token instance metadata\n```\n\n### Smart Contract Analysis\n```\nGet smart contract details for 0xdef456...\nGet smart contracts list\nGet smart contract counters\n```\n\n### Market Research\n```\nGet blockchain statistics\nShow transaction chart data\nDisplay market chart for price analysis\n```\n\n## Available Tools\n\n### Search Tools\n- `search_blockchain`: Search for addresses, tokens, blocks, or transactions\n- `check_search_redirect`: Check if search should redirect to specific page\n\n### Transaction Tools\n- `get_transactions`: Get transactions with filtering options\n- `get_transaction_details`: Get detailed transaction information\n- `get_transaction_token_transfers`: Get token transfers for a transaction\n- `get_transaction_internal_transactions`: Get internal transactions for a transaction\n- `get_transaction_logs`: Get logs for a transaction\n- `get_transaction_raw_trace`: Get raw trace for a transaction\n- `get_transaction_state_changes`: Get state changes for a transaction\n- `get_transaction_summary`: Get summary for a transaction\n\n### Block Tools\n- `get_blocks`: Get blocks with optional type filtering\n- `get_block_details`: Get specific block details by number or hash\n- `get_block_transactions`: Get transactions for a specific block\n- `get_block_withdrawals`: Get withdrawals for a specific block\n\n### Address Tools\n- `get_addresses_list`: Get addresses list\n- `get_address_details`: Get address details by hash\n- `get_address_counters`: Get address counters\n- `get_address_transactions`: Get transactions for a specific address\n- `get_address_token_transfers`: Get token transfers for a specific address\n- `get_address_internal_transactions`: Get internal transactions for a specific address\n- `get_address_logs`: Get logs for a specific address\n- `get_address_blocks_validated`: Get blocks validated by a specific address\n- `get_address_token_balances`: Get token balances for a specific address\n- `get_address_tokens`: Get tokens for a specific address\n- `get_address_coin_balance_history`: Get coin balance history for a specific address\n- `get_address_coin_balance_history_by_day`: Get coin balance history by day for a specific address\n- `get_address_withdrawals`: Get withdrawals for a specific address\n- `get_address_nft`: Get NFT for a specific address\n- `get_address_nft_collections`: Get NFT collections for a specific address\n\n### Token Tools\n- `get_token_transfers`: Get all token transfers\n- `get_tokens_list`: Get tokens list\n- `get_token_details`: Get token details by address hash\n- `get_token_transfers_by_token`: Get transfers for a specific token\n- `get_token_holders`: Get holders for a specific token\n- `get_token_counters`: Get counters for a specific token\n- `get_token_instances`: Get instances for a specific token\n- `get_token_instance_details`: Get specific token instance details\n- `get_token_instance_transfers`: Get transfers for a specific token instance\n- `get_token_instance_holders`: Get holders for a specific token instance\n- `get_token_instance_transfers_count`: Get transfers count for a specific token instance\n- `refetch_token_instance_metadata_tool`: Refetch metadata for a specific token instance\n\n### Smart Contract Tools\n- `get_smart_contracts_list`: Get smart contracts list\n- `get_smart_contracts_counters`: Get smart contracts counters\n- `get_smart_contract_details`: Get smart contract details by address hash\n\n### Internal Tools\n- `get_internal_transactions`: Get internal transactions\n\n### Main Page Tools\n- `get_main_page_transactions`: Get transactions for main page\n- `get_main_page_blocks`: Get blocks for main page\n- `get_indexing_status`: Get blockchain indexing status\n\n### Statistics Tools\n- `get_blockchain_stats`: Get blockchain statistics\n- `get_transactions_chart`: Get transaction chart data\n- `get_market_chart`: Get market chart data\n\n## Resources\n\n- `duckchain://api-docs`: Complete API documentation\n- `duckchain://supported-chains`: List of supported blockchain networks\n\n## Prompts\n\n- `analyze_transaction`: Generate analysis prompts for transactions\n- `explore_address`: Generate exploration prompts for addresses\n- `research_token`: Generate research prompts for tokens\n\n## Development\n\nThe server code is located in `src/duckchain/server.py`. The implementation follows the BlockScout API v2 specification and provides comprehensive error handling and type safety.\n\n### Key Components\n\n- **DuckChainAPI**: Async HTTP client for BlockScout API with 56+ endpoints\n- **ConfigSchema**: Pydantic model for configuration validation\n- **MCP Tools**: 56+ individual tools covering all major blockchain operations\n- **Resources**: Documentation and reference materials\n- **Prompts**: Pre-built prompts for common blockchain analysis tasks\n\n### API Coverage\n\n- **Search & Discovery**: 2 tools\n- **Transaction Analysis**: 8 tools (including logs, traces, state changes)\n- **Block Operations**: 4 tools (including withdrawals and detailed analysis)\n- **Address Analysis**: 15 tools (comprehensive address data and history)\n- **Token Management**: 12 tools (including instances and metadata)\n- **Smart Contract Analysis**: 3 tools\n- **Statistics & Charts**: 3 tools\n- **Internal Operations**: 1 tool\n- **Main Page Data**: 3 tools\n\n**Total: 56+ MCP tools covering the complete BlockScout API v2 specification**\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Clone repository at [duckchain-mcp](https://github.com/demomagic/duckchain-mcp)\n\n2. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n\n## License\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Support\n\nFor support and questions, please open an issue on GitHub or contact the DuckChain team.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search for blockchain addresses, tokens, blocks, or transactions",
        "Retrieve detailed transaction information including logs, traces, and state changes",
        "Filter transactions by status, type, or method",
        "Access comprehensive block data including transactions and withdrawals",
        "Analyze addresses with token balances, NFT collections, transaction history, and validated blocks",
        "Manage tokens including details, transfers, holders, instances, and metadata refresh",
        "Perform smart contract analysis with contract details, lists, and activity counters",
        "Generate blockchain statistics and market charts for price and volume analysis",
        "Monitor blockchain indexing status and internal transactions"
      ],
      "limitations": [
        "Requires Smithery API key for deployment but works without authentication for most BlockScout instances",
        "Supports only BlockScout-compatible blockchain networks",
        "Python 3.10 or higher required",
        "No explicit rate limits mentioned but dependent on underlying BlockScout API availability and policies"
      ],
      "requirements": [
        "Python 3.10 or higher",
        "Smithery API key for deployment on Smithery platform",
        "Access to BlockScout-compatible blockchain network endpoints"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, a full list of tools, supported networks, prerequisites, limitations, and deployment guidance, making it excellent in coverage and clarity.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# DuckChain MCP Server\n\nA comprehensive Model Context Protocol (MCP) server that provides complete access to blockchain data via the BlockScout API v2 specification. DuckChain enables AI assistants to perform deep blockchain analysis, including transaction tracing, address exploration, token management, smart contract analysis, and comprehensive market research with 56+ specialized tools.",
        "start_pos": 0,
        "end_pos": 391,
        "token_count_estimate": 97,
        "source_type": "readme",
        "agent_id": "64c8b2fa1cb33789"
      },
      {
        "chunk_id": 1,
        "text": ": Access NFT data and collections for addresses\n- **Address Counters**: Get transaction and activity counters\n- **Validated Blocks**: See blocks validated by specific addresses\n- **Withdrawal History**: Track withdrawal activities\n\n### üí∞ Token Operations\n- **Token Management**: Complete token lifecycle management\n- **Token Details**: Get comprehensive token information\n- **Token Transfers**: Access all token transfer data\n- **Token Holders**: Analyze token distribution and holders\n- **Token Instances**: Manage token instances and metadata\n- **Token Counters**: Track token activity statistics\n- **Metadata Refresh**: Update token instance metadata\n\n### üìú Smart Contract Analysis\n- **Contract Details**: Get comprehensive smart contract information\n- **Contract Lists**: Browse all smart contracts\n- **Contract Counters**: Track smart contract activity\n- **Contract Analysis**: Analyze contract interactions and usage\n\n### üìà Statistics & Charts\n- **Blockchain Stats**: Get comprehensive blockchain statistics\n- **Transaction Charts**: Access transaction volume and activity data\n- **Market Charts**: Retrieve price and volume visualization data\n\n### üîß Internal Operations\n- **Internal Transactions**: Access internal transaction data\n- **Indexing Status**: Monitor blockchain indexing progress\n\n## üöÄ What's New\n\n### Complete API Coverage\n- **56+ MCP Tools** covering all BlockScout API v2 endpoints\n- **Advanced Transaction Analysis** with logs, traces, and state changes\n- **Comprehensive Address Analysis** including NFT and balance history\n- **Full Token Management** with instances and metadata refresh\n- **Smart Contract Analysis** with detailed contract information\n- **Deep Block Analysis** including withdrawals and transaction details\n\n### Enhanced Capabilities\n- **No Authentication Required** - Works out of the box\n- **Type-Safe Implementation** - Full Pydantic validation\n- **Comprehensive Error Handling** - User-friendly error messages\n- **Async Performance** - Fast, non-blocking operations\n- **Modular Design** - Easy to exten",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "64c8b2fa1cb33789"
      },
      {
        "chunk_id": 2,
        "text": "plementation** - Full Pydantic validation\n- **Comprehensive Error Handling** - User-friendly error messages\n- **Async Performance** - Fast, non-blocking operations\n- **Modular Design** - Easy to extend and maintain\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n- Python 3.10 or higher\n\n## Supported Networks\n\nDuckChain supports all BlockScout-compatible networks including:\n\n- **Ethereum Mainnet** (`blockscout.com/eth/mainnet`)\n- **Polygon** (`blockscout.com/matic/mainnet`)\n- **BSC** (`blockscout.com/bsc/mainnet`)\n- **Arbitrum** (`blockscout.com/arbitrum/mainnet`)\n- **Optimism** (`blockscout.com/optimism/mainnet`)\n- **Avalanche** (`blockscout.com/avax/mainnet`)\n- **Fantom** (`blockscout.com/ftm/mainnet`)\n- **Gnosis Chain** (`blockscout.com/gnosis/mainnet`)\n- **POA Core** (`blockscout.com/poa/core`) - Default\n\n## Getting Started\n\n1. **Install dependencies**:\n   ```bash\n   uv sync\n   ```\n\n2. **Run the server**:\n   ```bash\n   uv run dev\n   ```\n\n3. **Test interactively**:\n   ```bash\n   uv run playground\n   ```\n\n### No Authentication Required\n\n- **Free and public access** - Most BlockScout instances are free and public\n- **No API key needed** - DuckChain works without any authentication\n- **Simple setup** - Just configure the server URL and you're ready to go\n\n## Usage Examples\n\n### Search Operations\n```\nSearch for USDT token information\nSearch for address 0x1234...\nFind block 12345678\n```\n\n### Transaction Analysis\n```\nGet transaction details for 0xabc123...\nGet token transfers for transaction 0xdef456...\nGet internal transactions for transaction 0xdef456...\nGet transaction logs for 0xdef456...\nGet raw trace for transaction 0xdef456...\nGet state changes for transaction 0xdef456...\nGet transaction summary for 0xdef456...",
        "start_pos": 3696,
        "end_pos": 5516,
        "token_count_estimate": 455,
        "source_type": "readme",
        "agent_id": "64c8b2fa1cb33789"
      },
      {
        "chunk_id": 3,
        "text": "od \"transfer\"\n```\n\n### Block Analysis\n```\nGet block details for block 12345678\nGet all transactions in block 12345678\nGet withdrawals for block 12345678\nGet blocks with type filtering\n```\n\n### Address Analysis\n```\nGet address details for 0x1234...\nGet all transactions for address 0x1234...\nGet token balances for address 0x1234...\nGet coin balance history for address 0x1234...\nGet NFT collections for address 0x1234...\nGet blocks validated by address 0x1234...\n```\n\n### Token Management\n```\nGet token details for 0xabc123...\nGet token holders for 0xabc123...\nGet token transfers for 0xabc123...\nGet token instances for 0xabc123...\nRefetch token instance metadata\n```\n\n### Smart Contract Analysis\n```\nGet smart contract details for 0xdef456...",
        "start_pos": 5544,
        "end_pos": 6288,
        "token_count_estimate": 186,
        "source_type": "readme",
        "agent_id": "64c8b2fa1cb33789"
      },
      {
        "chunk_id": 4,
        "text": "transactions for a specific block\n- `get_block_withdrawals`: Get withdrawals for a specific block\n\n### Address Tools\n- `get_addresses_list`: Get addresses list\n- `get_address_details`: Get address details by hash\n- `get_address_counters`: Get address counters\n- `get_address_transactions`: Get transactions for a specific address\n- `get_address_token_transfers`: Get token transfers for a specific address\n- `get_address_internal_transactions`: Get internal transactions for a specific address\n- `get_address_logs`: Get logs for a specific address\n- `get_address_blocks_validated`: Get blocks validated by a specific address\n- `get_address_token_balances`: Get token balances for a specific address\n- `get_address_tokens`: Get tokens for a specific address\n- `get_address_coin_balance_history`: Get coin balance history for a specific address\n- `get_address_coin_balance_history_by_day`: Get coin balance history by day for a specific address\n- `get_address_withdrawals`: Get withdrawals for a specific address\n- `get_address_nft`: Get NFT for a specific address\n- `get_address_nft_collections`: Get NFT collections for a specific address\n\n### Token Tools\n- `get_token_transfers`: Get all token transfers\n- `get_tokens_list`: Get tokens list\n- `get_token_details`: Get token details by address hash\n- `get_token_transfers_by_token`: Get transfers for a specific token\n- `get_token_holders`: Get holders for a specific token\n- `get_token_counters`: Get counters for a specific token\n- `get_token_instances`: Get instances for a specific token\n- `get_token_instance_details`: Get specific token instance details\n- `get_token_instance_transfers`: Get transfers for a specific token instance\n- `get_token_instance_holders`: Get holders for a specific token instance\n- `get_token_instance_transfers_count`: Get transfers count for a specific token instance\n- `refetch_token_instance_metadata_tool`: Refetch metadata for a specific token instance\n\n### Smart Contract Tools\n- `get_smart_contracts_list`: Get smart contracts list\n- `get_smart_contracts_cou",
        "start_pos": 7392,
        "end_pos": 9440,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "64c8b2fa1cb33789"
      },
      {
        "chunk_id": 5,
        "text": "nce\n- `refetch_token_instance_metadata_tool`: Refetch metadata for a specific token instance\n\n### Smart Contract Tools\n- `get_smart_contracts_list`: Get smart contracts list\n- `get_smart_contracts_counters`: Get smart contracts counters\n- `get_smart_contract_details`: Get smart contract details by address hash\n\n### Internal Tools\n- `get_internal_transactions`: Get internal transactions\n\n### Main Page Tools\n- `get_main_page_transactions`: Get transactions for main page\n- `get_main_page_blocks`: Get blocks for main page\n- `get_indexing_status`: Get blockchain indexing status\n\n### Statistics Tools\n- `get_blockchain_stats`: Get blockchain statistics\n- `get_transactions_chart`: Get transaction chart data\n- `get_market_chart`: Get market chart data\n\n## Resources\n\n- `duckchain://api-docs`: Complete API documentation\n- `duckchain://supported-chains`: List of supported blockchain networks\n\n## Prompts\n\n- `analyze_transaction`: Generate analysis prompts for transactions\n- `explore_address`: Generate exploration prompts for addresses\n- `research_token`: Generate research prompts for tokens\n\n## Development\n\nThe server code is located in `src/duckchain/server.py`. The implementation follows the BlockScout API v2 specification and provides comprehensive error handling and type safety.",
        "start_pos": 9240,
        "end_pos": 10530,
        "token_count_estimate": 322,
        "source_type": "readme",
        "agent_id": "64c8b2fa1cb33789"
      },
      {
        "chunk_id": 6,
        "text": "iled analysis)\n- **Address Analysis**: 15 tools (comprehensive address data and history)\n- **Token Management**: 12 tools (including instances and metadata)\n- **Smart Contract Analysis**: 3 tools\n- **Statistics & Charts**: 3 tools\n- **Internal Operations**: 1 tool\n- **Main Page Data**: 3 tools\n\n**Total: 56+ MCP tools covering the complete BlockScout API v2 specification**\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Clone repository at [duckchain-mcp](https://github.com/demomagic/duckchain-mcp)\n\n2. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n\n## License\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## Support\n\nFor support and questions, please open an issue on GitHub or contact the DuckChain team.",
        "start_pos": 11088,
        "end_pos": 12010,
        "token_count_estimate": 230,
        "source_type": "readme",
        "agent_id": "64c8b2fa1cb33789"
      }
    ]
  },
  {
    "agent_id": "3b471dd2b48e40b0",
    "name": "ai.smithery/devbrother2024-typescript-mcp-server-boilerplate",
    "source": "mcp",
    "source_url": "https://github.com/devbrother2024/typescript-mcp-server-boilerplate",
    "description": "Kickstart development with a customizable TypeScript template featuring sample tools for greeting,‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T16:12:55.711869Z",
    "indexed_at": "2026-02-18T04:06:27.505031",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# TypeScript MCP Server Î≥¥ÏùºÎü¨ÌîåÎ†àÏù¥Ìä∏\n\nTypeScript MCP SDKÎ•º ÌôúÏö©ÌïòÏó¨ Model Context Protocol (MCP) ÏÑúÎ≤ÑÎ•º Îπ†Î•¥Í≤å Í∞úÎ∞úÌï† Ïàò ÏûàÎäî Î≥¥ÏùºÎü¨ÌîåÎ†àÏù¥Ìä∏ ÌîÑÎ°úÏ†ùÌä∏ÏûÖÎãàÎã§.\n\n## üìÅ ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞\n\n```\ntypescript-mcp-server-boilerplate/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts          # MCP ÏÑúÎ≤Ñ Î©îÏù∏ ÏßÑÏûÖÏ†ê\n‚îú‚îÄ‚îÄ build/                # Ïª¥ÌååÏùºÎêú JavaScript ÌååÏùº (ÎπåÎìú ÌõÑ ÏÉùÏÑ±)\n‚îú‚îÄ‚îÄ package.json          # ÌîÑÎ°úÏ†ùÌä∏ ÏùòÏ°¥ÏÑ± Î∞è Ïä§ÌÅ¨Î¶ΩÌä∏\n‚îú‚îÄ‚îÄ tsconfig.json         # TypeScript ÏÑ§Ï†ï\n‚îî‚îÄ‚îÄ README.md            # ÌîÑÎ°úÏ†ùÌä∏ Î¨∏ÏÑú\n```\n\n## üöÄ ÏãúÏûëÌïòÍ∏∞\n\n### 1. ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\n\n```bash\nnpm install\n```\n\n### 2. ÏÑúÎ≤Ñ Ïù¥Î¶Ñ ÏÑ§Ï†ï\n\n`src/index.ts` ÌååÏùºÏóêÏÑú ÏÑúÎ≤Ñ Ïù¥Î¶ÑÏùÑ ÏàòÏ†ïÌïòÏÑ∏Ïöî:\n\n```typescript\nconst server = new McpServer({\n    name: 'typescript-mcp-server', // Ïó¨Í∏∞Î•º ÏõêÌïòÎäî ÏÑúÎ≤Ñ Ïù¥Î¶ÑÏúºÎ°ú Î≥ÄÍ≤Ω\n    version: '1.0.0',\n    // ÌôúÏÑ±Ìôî ÌïòÍ≥†Ïûê ÌïòÎäî Í∏∞Îä• ÏÑ§Ï†ï\n    capabilities: {\n        tools: {},\n        resources: {}\n    }\n})\n```\n\n> üí° **ÌåÅ**: ÌòÑÏû¨ Î≥¥ÏùºÎü¨ÌîåÎ†àÏù¥Ìä∏ÏóêÎäî Ïù¥ÎØ∏ Í≥ÑÏÇ∞Í∏∞ÏôÄ Ïù∏ÏÇ¨ ÎèÑÍµ¨, Í∑∏Î¶¨Í≥† ÏÑúÎ≤Ñ Ï†ïÎ≥¥ Î¶¨ÏÜåÏä§Í∞Ä ÏòàÏãúÎ°ú Íµ¨ÌòÑÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n\n### 3. ÎπåÎìú\n\n```bash\nnpm run build\n```\n\n### 4. Ïã§Ìñâ\n\n```bash\nnode build/index.js\n```\n\nÎπåÎìúÍ∞Ä ÏÑ±Í≥µÌïòÎ©¥ `build/` ÎîîÎ†âÌÜ†Î¶¨Ïóê Ïª¥ÌååÏùºÎêú JavaScript ÌååÏùºÏù¥ ÏÉùÏÑ±ÎêòÍ≥†, ÏÑúÎ≤ÑÍ∞Ä MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïùò Ïó∞Í≤∞ÏùÑ ÎåÄÍ∏∞Ìï©ÎãàÎã§.\n\n## üõ†Ô∏è Í∞úÎ∞ú Í∞ÄÏù¥Îìú\n\n### MCP ÎèÑÍµ¨(Tool) Ï∂îÍ∞ÄÌïòÍ∏∞\n\nMCP ÏÑúÎ≤ÑÏóê ÏÉàÎ°úÏö¥ ÎèÑÍµ¨Î•º Ï∂îÍ∞ÄÌïòÎ†§Î©¥ `server.tool()` Î©îÏÑúÎìúÏóê **Zod Ïä§ÌÇ§ÎßàÎ•º ÏßÅÏ†ë** Ï†ïÏùòÌïòÏó¨ Îì±Î°ùÌï©ÎãàÎã§:\n\n```typescript\nimport { z } from 'zod'\n\n// Í≥ÑÏÇ∞Í∏∞ ÎèÑÍµ¨ Ï∂îÍ∞Ä\nserver.tool(\n    'calculator',\n    {\n        operation: z\n            .enum(['add', 'subtract', 'multiply', 'divide'])\n            .describe('ÏàòÌñâÌï† Ïó∞ÏÇ∞ (add, subtract, multiply, divide)'),\n        a: z.number().describe('Ï≤´ Î≤àÏß∏ Ïà´Ïûê'),\n        b: z.number().describe('Îëê Î≤àÏß∏ Ïà´Ïûê')\n    },\n    async ({ operation, a, b }) => {\n        // Ïó∞ÏÇ∞ ÏàòÌñâ\n        let result: number\n        switch (operation) {\n            case 'add':\n                result = a + b\n                break\n            case 'subtract':\n                result = a - b\n                break\n            case 'multiply':\n                result = a * b\n                break\n            case 'divide':\n                if (b === 0) throw new Error('0ÏúºÎ°ú ÎÇòÎàå Ïàò ÏóÜÏäµÎãàÎã§')\n                result = a / b\n                break\n            default:\n                throw new Error('ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Ïó∞ÏÇ∞ÏûÖÎãàÎã§')\n        }\n\n        const operationSymbols = {\n            add: '+',\n            subtract: '-',\n            multiply: '√ó',\n            divide: '√∑'\n        } as const\n\n        const operationSymbol =\n            operationSymbols[operation as keyof typeof operationSymbols]\n\n        return {\n            content: [\n                {\n                    type: 'text',\n                    text: `${a} ${operationSymbol} ${b} = ${result}`\n                }\n            ]\n        }\n    }\n)\n```\n\n#### Îçî Î≥µÏû°Ìïú ÎèÑÍµ¨ ÏòàÏãú\n\n```typescript\n// ÎÇ†Ïî® Ï†ïÎ≥¥ Ï°∞Ìöå ÎèÑÍµ¨\nserver.tool(\n    'get_weather',\n    {\n        city: z.string().describe('ÎÇ†Ïî®Î•º Ï°∞ÌöåÌï† ÎèÑÏãúÎ™Ö'),\n        unit: z\n            .enum(['celsius', 'fahrenheit'])\n            .optional()\n            .default('celsius')\n            .describe('Ïò®ÎèÑ Îã®ÏúÑ (Í∏∞Î≥∏Í∞í: celsius)')\n    },\n    async ({ city, unit }) => {\n        try {\n            // Ïã§Ï†ú ÎÇ†Ïî® API Ìò∏Ï∂ú Î°úÏßÅ (ÏòàÏãú)\n            const weatherData = await fetchWeatherData(city, unit)\n\n            return {\n                content: [\n                    {\n                        type: 'text',\n                        text: `${city}Ïùò ÌòÑÏû¨ ÎÇ†Ïî®:\nÏò®ÎèÑ: ${weatherData.temperature}¬∞${unit === 'celsius' ? 'C' : 'F'}\nÎÇ†Ïî®: ${weatherData.condition}\nÏäµÎèÑ: ${weatherData.humidity}%\nÌíçÏÜç: ${weatherData.windSpeed}km/h`\n                    }\n                ]\n            }\n        } catch (error) {\n            throw new Error(\n                `ÎÇ†Ïî® Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§: ${(error as Error).message}`\n            )\n        }\n    }\n)\n\n// ÎèÑÏö∞ÎØ∏ Ìï®Ïàò\nasync function fetchWeatherData(city: string, unit: string) {\n    // Ïã§Ï†ú ÎÇ†Ïî® API Ìò∏Ï∂ú Íµ¨ÌòÑ\n    // Ïó¨Í∏∞ÏÑúÎäî ÏòàÏãú Îç∞Ïù¥ÌÑ∞ Î∞òÌôò\n    return {\n        temperature: unit === 'celsius' ? 22 : 72,\n        condition: 'ÎßëÏùå',\n        humidity: 65,\n        windSpeed: 12\n    }\n}\n```\n\n### Î¶¨ÏÜåÏä§ Ï∂îÍ∞ÄÌïòÍ∏∞\n\nMCP ÏÑúÎ≤ÑÏóê Î¶¨ÏÜåÏä§Î•º Ï∂îÍ∞ÄÌïòÏó¨ Ïô∏Î∂Ä Îç∞Ïù¥ÌÑ∞ÎÇò ÌååÏùºÏóê ÎåÄÌïú Ï†ëÍ∑ºÏùÑ Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§:\n\n```typescript\n// Î¶¨ÏÜåÏä§ Îì±Î°ù\nserver.resource(\n    'example-file',\n    'file://example.txt',\n    {\n        name: 'ÏòàÏãú ÌÖçÏä§Ìä∏ ÌååÏùº',\n        description: 'ÏòàÏãú ÌÖçÏä§Ìä∏ ÌååÏùº ÏÑ§Î™Ö',\n        mimeType: 'text/plain'\n    },\n    async () => {\n        return {\n            contents: [\n                {\n                    uri: 'file://example.txt',\n                    mimeType: 'text/plain',\n                    text: 'ÏòàÏãú ÌååÏùº ÎÇ¥Ïö©ÏûÖÎãàÎã§.'\n                }\n            ]\n        }\n    }\n)\n\n// ÎèôÏ†Å Î¶¨ÏÜåÏä§ ÏòàÏãú\nserver.resource(\n    'app-settings',\n    'config://settings',\n    {\n        name: 'Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏÑ§Ï†ï',\n        description: 'Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùò ÌòÑÏû¨ ÏÑ§Ï†ï Ï†ïÎ≥¥',\n        mimeType: 'application/json'\n    },\n    async () => {\n        const settings = {\n            theme: 'dark',\n            language: 'ko-KR',\n            notifications: true,\n            lastUpdated: new Date().toISOString()\n        }\n\n        return {\n            contents: [\n                {\n                    uri: 'config://settings',\n                    mimeType: 'application/json',\n                    text: JSON.stringify(settings, null, 2)\n                }\n            ]\n        }\n    }\n)\n```\n\n## üì¶ Ï£ºÏöî ÏùòÏ°¥ÏÑ±\n\n-   **@modelcontextprotocol/sdk**: MCP ÌîÑÎ°úÌÜ†ÏΩú Íµ¨ÌòÑÏùÑ ÏúÑÌïú Í≥µÏãù SDK\n-   **zod**: TypeScript Ïö∞ÏÑ† Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù ÎùºÏù¥Î∏åÎü¨Î¶¨\n-   **typescript**: TypeScript Ïª¥ÌååÏùºÎü¨\n\n## üîß Ïä§ÌÅ¨Î¶ΩÌä∏\n\n-   `npm run build`: TypeScriptÎ•º JavaScriptÎ°ú Ïª¥ÌååÏùºÌïòÍ≥† Ïã§Ìñâ Í∂åÌïú ÏÑ§Ï†ï\n\n## üìã ÏÇ¨Ïö© ÏòàÏãú\n\n### ÏôÑÏ†ÑÌïú ÏÑúÎ≤Ñ ÏòàÏãú\n\n```typescript\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js'\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js'\nimport { z } from 'zod'\n\n// ÏÑúÎ≤Ñ ÏÉùÏÑ±\nconst server = new McpServer({\n    name: 'my-mcp-server',\n    version: '1.0.0',\n    capabilities: {\n        tools: {},\n        resources: {}\n    }\n})\n\n// Í∞ÑÎã®Ìïú Ïù∏ÏÇ¨ ÎèÑÍµ¨\nserver.tool(\n    'greet',\n    {\n        name: z.string().describe('Ïù∏ÏÇ¨Ìï† ÏÇ¨ÎûåÏùò Ïù¥Î¶Ñ'),\n        language: z\n            .enum(['ko', 'en'])\n            .optional()\n            .default('ko')\n            .describe('Ïù∏ÏÇ¨ Ïñ∏Ïñ¥ (Í∏∞Î≥∏Í∞í: ko)')\n    },\n    async ({ name, language }) => {\n        const greeting =\n            language === 'ko' ? `ÏïàÎÖïÌïòÏÑ∏Ïöî, ${name}Îãò!` : `Hello, ${name}!`\n\n        return {\n            content: [\n                {\n                    type: 'text',\n                    text: greeting\n                }\n            ]\n        }\n    }\n)\n\n// ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Î¶¨ÏÜåÏä§\nserver.resource(\n    'system-info',\n    'system://info',\n    {\n        name: 'ÏãúÏä§ÌÖú Ï†ïÎ≥¥',\n        description: 'ÏÑúÎ≤ÑÏùò ÌòÑÏû¨ ÏÉÅÌÉú Î∞è ÏãúÏä§ÌÖú Ï†ïÎ≥¥',\n        mimeType: 'application/json'\n    },\n    async () => {\n        const systemInfo = {\n            server: 'my-mcp-server',\n            version: '1.0.0',\n            timestamp: new Date().toISOString(),\n            uptime: process.uptime()\n        }\n\n        return {\n            contents: [\n                {\n                    uri: 'system://info',\n                    mimeType: 'application/json',\n                    text: JSON.stringify(systemInfo, null, 2)\n                }\n            ]\n        }\n    }\n)\n\n// ÏÑúÎ≤Ñ ÏãúÏûë\nasync function main() {\n    const transport = new StdioServerTransport()\n    await server.connect(transport)\n    console.error('MCP ÏÑúÎ≤ÑÍ∞Ä ÏãúÏûëÎêòÏóàÏäµÎãàÎã§')\n}\n\nmain().catch(console.error)\n```\n\n## üîß Cursor MCP Ïó∞Í≤∞\n\nÍ∞úÎ∞úÌïú MCP ÏÑúÎ≤ÑÎ•º CursorÏóêÏÑú ÌÖåÏä§Ìä∏Ìï† Ïàò ÏûàÏäµÎãàÎã§:\n\n### ÏÑ§Ï†ï ÌååÏùº ÏàòÏ†ï\n\n`./.cursor/mcp.json` ÌååÏùºÏùÑ Ìé∏ÏßëÌï©ÎãàÎã§:\n\n```json\n{\n    \"mcpServers\": {\n        \"typescript-mcp-server\": {\n            \"command\": \"node\",\n            \"args\": [\"/ABSOLUTE/PATH/TO/YOUR/PROJECT/build/index.js\"]\n        }\n    }\n}\n```\n\n> **Ï£ºÏùò**: Ï†àÎåÄ Í≤ΩÎ°úÎ•º ÏÇ¨Ïö©Ìï¥Ïïº Ìï©ÎãàÎã§. `pwd` Î™ÖÎ†πÏñ¥Î°ú ÌòÑÏû¨ Í≤ΩÎ°úÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.\n\n### ÌÖåÏä§Ìä∏ Î™ÖÎ†πÏñ¥\n\nCursor MCPÏóêÏÑú Îã§ÏùåÍ≥º Í∞ôÏù¥ ÌÖåÏä§Ìä∏Ìï¥Î≥º Ïàò ÏûàÏäµÎãàÎã§:\n\n-   \"5 ÎçîÌïòÍ∏∞ 3ÏùÄ ÏñºÎßàÏïº?\" (Í≥ÑÏÇ∞Í∏∞ ÎèÑÍµ¨ ÌÖåÏä§Ìä∏)\n-   \"ÏïàÎÖïÌïòÏÑ∏Ïöî ÎùºÍ≥† Ïù∏ÏÇ¨Ìï¥Ï§ò\" (Ïù∏ÏÇ¨ ÎèÑÍµ¨ ÌÖåÏä§Ìä∏)\n-   ÏÑúÎ≤Ñ Ï†ïÎ≥¥ Î¶¨ÏÜåÏä§ Ï°∞Ìöå\n\n## üîó Ï∞∏Í≥† ÏûêÎ£å\n\n-   [Model Context Protocol Í≥µÏãù Î¨∏ÏÑú](https://modelcontextprotocol.io/)\n-   [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n-   [Node.js MCP ÏÑúÎ≤Ñ Í∞úÎ∞ú Í∞ÄÏù¥Îìú](https://modelcontextprotocol.io/docs/develop/build-server#node)\n-   [Zod Î¨∏ÏÑú](https://zod.dev/)\n\n## üìÑ ÎùºÏù¥ÏÑ†Ïä§\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Create MCP servers using TypeScript",
        "Define and register custom tools with Zod schemas for input validation",
        "Implement asynchronous tool handlers for various operations",
        "Add static and dynamic resources accessible via URIs",
        "Build and run MCP servers with Node.js",
        "Integrate with MCP clients such as Cursor for testing",
        "Provide example tools like calculator, greeting, and weather information",
        "Expose server information as a resource"
      ],
      "limitations": [
        "No built-in support for external API calls; users must implement them (e.g., weather API)",
        "Requires manual configuration of server name and capabilities",
        "No explicit rate limiting or concurrency controls mentioned",
        "No mention of authentication or security features"
      ],
      "requirements": [
        "Node.js environment to run the server",
        "npm to install dependencies",
        "@modelcontextprotocol/sdk package",
        "zod package for schema validation",
        "TypeScript compiler for building the project",
        "Absolute path configuration for Cursor MCP integration"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples including tool and resource creation, integration guidance, and lists dependencies and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# TypeScript MCP Server Î≥¥ÏùºÎü¨ÌîåÎ†àÏù¥Ìä∏\n\nTypeScript MCP SDKÎ•º ÌôúÏö©ÌïòÏó¨ Model Context Protocol (MCP) ÏÑúÎ≤ÑÎ•º Îπ†Î•¥Í≤å Í∞úÎ∞úÌï† Ïàò ÏûàÎäî Î≥¥ÏùºÎü¨ÌîåÎ†àÏù¥Ìä∏ ÌîÑÎ°úÏ†ùÌä∏ÏûÖÎãàÎã§.\n\n## üìÅ ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞\n\n```\ntypescript-mcp-server-boilerplate/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts          # MCP ÏÑúÎ≤Ñ Î©îÏù∏ ÏßÑÏûÖÏ†ê\n‚îú‚îÄ‚îÄ build/                # Ïª¥ÌååÏùºÎêú JavaScript ÌååÏùº (ÎπåÎìú ÌõÑ ÏÉùÏÑ±)\n‚îú‚îÄ‚îÄ package.json          # ÌîÑÎ°úÏ†ùÌä∏ ÏùòÏ°¥ÏÑ± Î∞è Ïä§ÌÅ¨Î¶ΩÌä∏\n‚îú‚îÄ‚îÄ tsconfig.json         # TypeScript ÏÑ§Ï†ï\n‚îî‚îÄ‚îÄ README.md            # ÌîÑÎ°úÏ†ùÌä∏ Î¨∏ÏÑú\n```\n\n## üöÄ ÏãúÏûëÌïòÍ∏∞\n\n### 1. ÏùòÏ°¥ÏÑ± ÏÑ§Ïπò\n\n```bash\nnpm install\n```\n\n### 2. ÏÑúÎ≤Ñ Ïù¥Î¶Ñ ÏÑ§Ï†ï\n\n`src/index.ts` ÌååÏùºÏóêÏÑú ÏÑúÎ≤Ñ Ïù¥Î¶ÑÏùÑ ÏàòÏ†ïÌïòÏÑ∏Ïöî:\n\n```typescript\nconst server = new McpServer({\n    name: 'typescript-mcp-server', // Ïó¨Í∏∞Î•º ÏõêÌïòÎäî ÏÑúÎ≤Ñ Ïù¥Î¶ÑÏúºÎ°ú Î≥ÄÍ≤Ω\n    version: '1.0.0',\n    // ÌôúÏÑ±Ìôî ÌïòÍ≥†Ïûê ÌïòÎäî Í∏∞Îä• ÏÑ§Ï†ï\n    capabilities: {\n        tools: {},\n        resources: {}\n    }\n})\n```\n\n> üí° **ÌåÅ**: ÌòÑÏû¨ Î≥¥ÏùºÎü¨ÌîåÎ†àÏù¥Ìä∏ÏóêÎäî Ïù¥ÎØ∏ Í≥ÑÏÇ∞Í∏∞ÏôÄ Ïù∏ÏÇ¨ ÎèÑÍµ¨, Í∑∏Î¶¨Í≥† ÏÑúÎ≤Ñ Ï†ïÎ≥¥ Î¶¨ÏÜåÏä§Í∞Ä ÏòàÏãúÎ°ú Íµ¨ÌòÑÎêòÏñ¥ ÏûàÏäµÎãàÎã§.\n\n### 3. ÎπåÎìú\n\n```bash\nnpm run build\n```\n\n### 4. Ïã§Ìñâ\n\n```bash\nnode build/index.js\n```\n\nÎπåÎìúÍ∞Ä ÏÑ±Í≥µÌïòÎ©¥ `build/` ÎîîÎ†âÌÜ†Î¶¨Ïóê Ïª¥ÌååÏùºÎêú JavaScript ÌååÏùºÏù¥ ÏÉùÏÑ±ÎêòÍ≥†, ÏÑúÎ≤ÑÍ∞Ä MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïùò Ïó∞Í≤∞ÏùÑ ÎåÄÍ∏∞Ìï©ÎãàÎã§.",
        "start_pos": 0,
        "end_pos": 962,
        "token_count_estimate": 240,
        "source_type": "readme",
        "agent_id": "3b471dd2b48e40b0"
      },
      {
        "chunk_id": 1,
        "text": "result = a / b\n                break\n            default:\n                throw new Error('ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Ïó∞ÏÇ∞ÏûÖÎãàÎã§')\n        }\n\n        const operationSymbols = {\n            add: '+',\n            subtract: '-',\n            multiply: '√ó',\n            divide: '√∑'\n        } as const\n\n        const operationSymbol =\n            operationSymbols[operation as keyof typeof operationSymbols]\n\n        return {\n            content: [\n                {\n                    type: 'text',\n                    text: `${a} ${operationSymbol} ${b} = ${result}`\n                }\n            ]\n        }\n    }\n)\n```\n\n#### Îçî Î≥µÏû°Ìïú ÎèÑÍµ¨ ÏòàÏãú\n\n```typescript\n// ÎÇ†Ïî® Ï†ïÎ≥¥ Ï°∞Ìöå ÎèÑÍµ¨\nserver.tool(\n    'get_weather',\n    {\n        city: z.string().describe('ÎÇ†Ïî®Î•º Ï°∞ÌöåÌï† ÎèÑÏãúÎ™Ö'),\n        unit: z\n            .enum(['celsius', 'fahrenheit'])\n            .optional()\n            .default('celsius')\n            .describe('Ïò®ÎèÑ Îã®ÏúÑ (Í∏∞Î≥∏Í∞í: celsius)')\n    },\n    async ({ city, unit }) => {\n        try {\n            // Ïã§Ï†ú ÎÇ†Ïî® API Ìò∏Ï∂ú Î°úÏßÅ (ÏòàÏãú)\n            const weatherData = await fetchWeatherData(city, unit)\n\n            return {\n                content: [\n                    {\n                        type: 'text',\n                        text: `${city}Ïùò ÌòÑÏû¨ ÎÇ†Ïî®:\nÏò®ÎèÑ: ${weatherData.temperature}¬∞${unit === 'celsius' ? 'C' : 'F'}\nÎÇ†Ïî®: ${weatherData.condition}\nÏäµÎèÑ: ${weatherData.humidity}%\nÌíçÏÜç: ${weatherData.windSpeed}km/h`\n                    }\n                ]\n            }\n        } catch (error) {\n            throw new Error(\n                `ÎÇ†Ïî® Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§: ${(error as Error).message}`\n            )\n        }\n    }\n)\n\n// ÎèÑÏö∞ÎØ∏ Ìï®Ïàò\nasync function fetchWeatherData(city: string, unit: string) {\n    // Ïã§Ï†ú ÎÇ†Ïî® API Ìò∏Ï∂ú Íµ¨ÌòÑ\n    // Ïó¨Í∏∞ÏÑúÎäî ÏòàÏãú Îç∞Ïù¥ÌÑ∞ Î∞òÌôò\n    return {\n        temperature: unit === 'celsius' ? 22 : 72,\n        condition: 'ÎßëÏùå',\n        humidity: 65,\n        windSpeed: 12\n    }\n}\n```\n\n### Î¶¨ÏÜåÏä§ Ï∂îÍ∞ÄÌïòÍ∏∞\n\nMCP ÏÑúÎ≤ÑÏóê Î¶¨ÏÜåÏä§Î•º Ï∂îÍ∞ÄÌïòÏó¨ Ïô∏Î∂Ä Îç∞Ïù¥ÌÑ∞ÎÇò ÌååÏùºÏóê ÎåÄÌïú Ï†ëÍ∑ºÏùÑ Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§:\n\n```typescript\n// Î¶¨ÏÜåÏä§ Îì±Î°ù\nserver.resource(\n    'example-file',\n    'file://example.txt',\n    {\n        name: 'ÏòàÏãú ÌÖçÏä§Ìä∏ ÌååÏùº',\n        description:",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "3b471dd2b48e40b0"
      },
      {
        "chunk_id": 2,
        "text": "ÏÜåÏä§ Ï∂îÍ∞ÄÌïòÍ∏∞\n\nMCP ÏÑúÎ≤ÑÏóê Î¶¨ÏÜåÏä§Î•º Ï∂îÍ∞ÄÌïòÏó¨ Ïô∏Î∂Ä Îç∞Ïù¥ÌÑ∞ÎÇò ÌååÏùºÏóê ÎåÄÌïú Ï†ëÍ∑ºÏùÑ Ï†úÍ≥µÌï† Ïàò ÏûàÏäµÎãàÎã§:\n\n```typescript\n// Î¶¨ÏÜåÏä§ Îì±Î°ù\nserver.resource(\n    'example-file',\n    'file://example.txt',\n    {\n        name: 'ÏòàÏãú ÌÖçÏä§Ìä∏ ÌååÏùº',\n        description: 'ÏòàÏãú ÌÖçÏä§Ìä∏ ÌååÏùº ÏÑ§Î™Ö',\n        mimeType: 'text/plain'\n    },\n    async () => {\n        return {\n            contents: [\n                {\n                    uri: 'file://example.txt',\n                    mimeType: 'text/plain',\n                    text: 'ÏòàÏãú ÌååÏùº ÎÇ¥Ïö©ÏûÖÎãàÎã§.'\n                }\n            ]\n        }\n    }\n)\n\n// ÎèôÏ†Å Î¶¨ÏÜåÏä§ ÏòàÏãú\nserver.resource(\n    'app-settings',\n    'config://settings',\n    {\n        name: 'Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏÑ§Ï†ï',\n        description: 'Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùò ÌòÑÏû¨ ÏÑ§Ï†ï Ï†ïÎ≥¥',\n        mimeType: 'application/json'\n    },\n    async () => {\n        const settings = {\n            theme: 'dark',\n            language: 'ko-KR',\n            notifications: true,\n            lastUpdated: new Date().toISOString()\n        }\n\n        return {\n            contents: [\n                {\n                    uri: 'config://settings',\n                    mimeType: 'application/json',\n                    text: JSON.stringify(settings, null, 2)\n                }\n            ]\n        }\n    }\n)\n```\n\n## üì¶ Ï£ºÏöî ÏùòÏ°¥ÏÑ±\n\n-   **@modelcontextprotocol/sdk**: MCP ÌîÑÎ°úÌÜ†ÏΩú Íµ¨ÌòÑÏùÑ ÏúÑÌïú Í≥µÏãù SDK\n-   **zod**: TypeScript Ïö∞ÏÑ† Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù ÎùºÏù¥Î∏åÎü¨Î¶¨\n-   **typescript**: TypeScript Ïª¥ÌååÏùºÎü¨\n\n## üîß Ïä§ÌÅ¨Î¶ΩÌä∏\n\n-   `npm run build`: TypeScriptÎ•º JavaScriptÎ°ú Ïª¥ÌååÏùºÌïòÍ≥† Ïã§Ìñâ Í∂åÌïú ÏÑ§Ï†ï\n\n## üìã ÏÇ¨Ïö© ÏòàÏãú\n\n### ÏôÑÏ†ÑÌïú ÏÑúÎ≤Ñ ÏòàÏãú\n\n```typescript\nimport { McpServer } from '@modelcontextprotocol/sdk/server/mcp.js'\nimport { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js'\nimport { z } from 'zod'\n\n// ÏÑúÎ≤Ñ ÏÉùÏÑ±\nconst server = new McpServer({\n    name: 'my-mcp-server',\n    version: '1.0.0',\n    capabilities: {\n        tools: {},\n        resources: {}\n    }\n})\n\n// Í∞ÑÎã®Ìïú Ïù∏ÏÇ¨ ÎèÑÍµ¨\nserver.tool(\n    'greet',\n    {\n        name: z.string().describe('Ïù∏ÏÇ¨Ìï† ÏÇ¨ÎûåÏùò Ïù¥Î¶Ñ'),\n        language: z\n            .enum(['ko', 'en'])\n            .optional()\n            .default('ko')\n            .describe('Ïù∏ÏÇ¨ Ïñ∏Ïñ¥ (Í∏∞Î≥∏Í∞í: ko)')\n    },\n    async ({ name, language }) =>",
        "start_pos": 3696,
        "end_pos": 5744,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "3b471dd2b48e40b0"
      },
      {
        "chunk_id": 3,
        "text": "'Ïù∏ÏÇ¨Ìï† ÏÇ¨ÎûåÏùò Ïù¥Î¶Ñ'),\n        language: z\n            .enum(['ko', 'en'])\n            .optional()\n            .default('ko')\n            .describe('Ïù∏ÏÇ¨ Ïñ∏Ïñ¥ (Í∏∞Î≥∏Í∞í: ko)')\n    },\n    async ({ name, language }) => {\n        const greeting =\n            language === 'ko' ? `ÏïàÎÖïÌïòÏÑ∏Ïöî, ${name}Îãò!` : `Hello, ${name}!`\n\n        return {\n            content: [\n                {\n                    type: 'text',\n                    text: greeting\n                }\n            ]\n        }\n    }\n)\n\n// ÏãúÏä§ÌÖú Ï†ïÎ≥¥ Î¶¨ÏÜåÏä§\nserver.resource(\n    'system-info',\n    'system://info',\n    {\n        name: 'ÏãúÏä§ÌÖú Ï†ïÎ≥¥',\n        description: 'ÏÑúÎ≤ÑÏùò ÌòÑÏû¨ ÏÉÅÌÉú Î∞è ÏãúÏä§ÌÖú Ï†ïÎ≥¥',\n        mimeType: 'application/json'\n    },\n    async () => {\n        const systemInfo = {\n            server: 'my-mcp-server',\n            version: '1.0.0',\n            timestamp: new Date().toISOString(),\n            uptime: process.uptime()\n        }\n\n        return {\n            contents: [\n                {\n                    uri: 'system://info',\n                    mimeType: 'application/json',\n                    text: JSON.stringify(systemInfo, null, 2)\n                }\n            ]\n        }\n    }\n)\n\n// ÏÑúÎ≤Ñ ÏãúÏûë\nasync function main() {\n    const transport = new StdioServerTransport()\n    await server.connect(transport)\n    console.error('MCP ÏÑúÎ≤ÑÍ∞Ä ÏãúÏûëÎêòÏóàÏäµÎãàÎã§')\n}\n\nmain().catch(console.error)\n```\n\n## üîß Cursor MCP Ïó∞Í≤∞\n\nÍ∞úÎ∞úÌïú MCP ÏÑúÎ≤ÑÎ•º CursorÏóêÏÑú ÌÖåÏä§Ìä∏Ìï† Ïàò ÏûàÏäµÎãàÎã§:\n\n### ÏÑ§Ï†ï ÌååÏùº ÏàòÏ†ï\n\n`./.cursor/mcp.json` ÌååÏùºÏùÑ Ìé∏ÏßëÌï©ÎãàÎã§:\n\n```json\n{\n    \"mcpServers\": {\n        \"typescript-mcp-server\": {\n            \"command\": \"node\",\n            \"args\": [\"/ABSOLUTE/PATH/TO/YOUR/PROJECT/build/index.js\"]\n        }\n    }\n}\n```\n\n> **Ï£ºÏùò**: Ï†àÎåÄ Í≤ΩÎ°úÎ•º ÏÇ¨Ïö©Ìï¥Ïïº Ìï©ÎãàÎã§. `pwd` Î™ÖÎ†πÏñ¥Î°ú ÌòÑÏû¨ Í≤ΩÎ°úÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî.",
        "start_pos": 5544,
        "end_pos": 7217,
        "token_count_estimate": 418,
        "source_type": "readme",
        "agent_id": "3b471dd2b48e40b0"
      },
      {
        "chunk_id": 4,
        "text": "Î¨∏ÏÑú](https://modelcontextprotocol.io/)\n-   [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n-   [Node.js MCP ÏÑúÎ≤Ñ Í∞úÎ∞ú Í∞ÄÏù¥Îìú](https://modelcontextprotocol.io/docs/develop/build-server#node)\n-   [Zod Î¨∏ÏÑú](https://zod.dev/)\n\n## üìÑ ÎùºÏù¥ÏÑ†Ïä§\n\nMIT",
        "start_pos": 7392,
        "end_pos": 7651,
        "token_count_estimate": 64,
        "source_type": "readme",
        "agent_id": "3b471dd2b48e40b0"
      }
    ]
  },
  {
    "agent_id": "908aaf759b48f0ef",
    "name": "ai.smithery/docfork-mcp",
    "source": "mcp",
    "source_url": "https://github.com/docfork/docfork-mcp",
    "description": "@latest documentation and code examples to 9000+ libraries for LLMs and AI code editors in a singl‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-12T18:25:16.14187Z",
    "indexed_at": "2026-02-18T04:06:29.255276",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![Docfork cover](https://docfork.com/cover.png)](https://docfork.com)\n\n# Docfork MCP - Up-to-date Docs for AI Agents\n\n<a href=\"https://cursor.com/en/install-mcp?name=docfork&config=eyJ1cmwiOiJodHRwczovL21jcC5kb2Nmb3JrLmNvbS9tY3AifQ%3D%3D\"><img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" height=\"32\" alt=\"Add to Cursor\"/></a>&nbsp;&nbsp;<a href=\"https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22docfork%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22docfork%40latest%22%5D%7D\"><img src=\"https://img.shields.io/badge/Add%20to%20VS%20Code-007ACC?style=for-the-badge&logo=visualstudiocode&logoColor=white\" height=\"32\" alt=\"Add to VS Code\"/></a>&nbsp;&nbsp;<a href=\"https://app.docfork.com/signup\"><img src=\"https://img.shields.io/badge/Get%20Free%20API%20Key-F02A2B?style=for-the-badge&logo=fire&logoColor=white\" height=\"32\" alt=\"Get Free API Key\"/></a>\n\n<a href=\"https://docfork.com\"><img alt=\"Website\" src=\"https://img.shields.io/badge/Website-docfork.com-blue?style=flat-square\" /></a>&nbsp;&nbsp;<a href=\"https://www.npmjs.com/package/docfork\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/docfork?style=flat-square&color=red\" /></a>&nbsp;&nbsp;<a href=\"https://www.npmjs.com/package/docfork\"><img alt=\"npm downloads\" src=\"https://img.shields.io/npm/dm/docfork?style=flat-square\" /></a>&nbsp;&nbsp;<a href=\"./LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/npm/l/docfork?style=flat-square\" /></a>\n\n**Lock your agent's context to your stack.**\n\nDefine a **Docfork Cabinet** ‚Äî `Next.js 16` + `Drizzle ORM` + `Better Auth` ‚Äî and every query returns only docs from your stack. No more bloated results. No more hallucinations.\n\n## ‚ö° Built for Precision\n\nDocumentation context as precise as your dependency lockfile:\n\n- **Cabinets** ‚Äî Lock your agent to a verified stack. Only your libraries. Fully isolated.\n\n- **10,000+ libraries** ‚Äî Pre-chunked docs and code examples. ~200ms edge retrieval.\n\n- **Team-ready** ‚Äî Share Cabinets and API keys across your org. Same context, every engineer.\n\n> **Set a Cabinet:** `Next.js 16` + `Drizzle ORM` + `Better Auth`.\n> Your agent only sees docs for your stack. No stray Express docs. No Prisma confusion.\n\n## üöÄ Quick Start\n\n### 1. Get your Free API Key\n\nSign up at **[docfork.com](https://app.docfork.com/signup)** ‚Äî free: 1,000 requests/month, 5 team seats.\n\n### 2. Install MCP\n\n<details>\n<summary><b>Install in Cursor</b></summary>\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server`\n\nPaste this into `~/.cursor/mcp.json`. For project-scoped config, create `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://cursor.com/docs/context/mcp) for more info.\n\n> Since Cursor 1.0, click the buttons below to install instantly.\n\n#### Cursor Remote Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=docfork&config=eyJ1cmwiOiJodHRwczovL21jcC5kb2Nmb3JrLmNvbS9tY3AifQ%3D%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"docfork\": {\n      \"url\": \"https://mcp.docfork.com/mcp\",\n      \"headers\": {\n        \"DOCFORK_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=docfork&config=eyJjb21tYW5kIjoibnB4IC15IGRvY2ZvcmsifQ%3D%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"docfork\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"docfork\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Claude Code</b></summary>\n\nRun this command. See [Claude Code MCP docs](https://code.claude.com/docs/en/mcp) for more info.\n\n#### Claude Code Local Server Connection\n\n```sh\nclaude mcp add docfork -- npx -y docfork --api-key YOUR_API_KEY\n```\n\n#### Claude Code Remote Server Connection\n\n```sh\nclaude mcp add --header \"DOCFORK_API_KEY: YOUR_API_KEY\" --transport http docfork https://mcp.docfork.com/mcp\n```\n\n</details>\n\n<details>\n<summary><b>Install in OpenCode</b></summary>\n\nAdd this to your OpenCode configuration file. See [OpenCode MCP docs](https://opencode.ai/docs/mcp-servers) for more info.\n\n#### OpenCode Remote Server Connection\n\n```jsonc\n{\n  \"$schema\": \"https://opencode.ai/config.json\",\n  \"mcp\": {\n    \"docfork\": {\n      \"type\": \"remote\",\n      \"url\": \"https://mcp.docfork.com/mcp\",\n      \"headers\": {\n        \"DOCFORK_API_KEY\": \"YOUR_API_KEY\",\n      },\n      \"enabled\": true,\n    },\n  },\n}\n```\n\n#### OpenCode Local Server Connection\n\n```jsonc\n{\n  \"$schema\": \"https://opencode.ai/config.json\",\n  \"mcp\": {\n    \"docfork\": {\n      \"type\": \"local\",\n      \"command\": [\"npx\", \"-y\", \"docfork\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"enabled\": true,\n    },\n  },\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Cline</b></summary>\n\nAdd this to your Cline `cline_mcp_settings.json` file. To access it: Click the MCP Servers icon in the top navigation bar ‚Üí Select the \"Configure\" tab ‚Üí Click \"Configure MCP Servers\" at the bottom. See [Cline MCP docs](https://docs.cline.bot/mcp/configuring-mcp-servers) for more info.\n\n#### Cline Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"docfork\": {\n      \"url\": \"https://mcp.docfork.com/mcp\",\n      \"type\": \"streamableHttp\",\n      \"headers\": {\n        \"DOCFORK_API_KEY\": \"YOUR_API_KEY\"\n      },\n      \"alwaysAllow\": [\"query_docs\", \"fetch_url\"],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n#### Cline Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"docfork\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"docfork\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"alwaysAllow\": [\"query_docs\", \"fetch_url\"],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n</details>\n\n**[Windsurf, Roo Code, and 40+ more ‚Üí](https://docs.docfork.com/clients/overview)**\n\n<details>\n<summary><b>OAuth Authentication</b></summary>\n\nDocfork supports [MCP OAuth specs](https://modelcontextprotocol.io/specification/latest/basic/authorization). Change your endpoint to use OAuth:\n\n```diff\n- \"url\": \"https://mcp.docfork.com/mcp\"\n+ \"url\": \"https://mcp.docfork.com/mcp/oauth\"\n```\n\n_Note: OAuth is for remote HTTP connections only. [View OAuth Guide ‚Üí](https://docs.docfork.com/core/authentication)_\n\n</details>\n\n### 3. Just say `use docfork`\n\nAdd `use docfork` to any prompt:\n\n```txt\nImplement a secure authentication flow using Better Auth and Supabase. use docfork\n```\n\n### 4. Make it automatic\n\nAdd a rule so Docfork stays active ‚Äî skip the prompt suffix.\n\n> [!NOTE]\n> **[Add Rule to Cursor (One-Click)](<https://cursor.com/link/rule?name=docfork-policy&text=You+have+access+to+the+docfork+MCP+server.+To+ensure+the+code+you+write+is+accurate+and+up-to-date%2C+you+must+follow+these+requirements%3A%0A%0A1.+Auto-Invoke%3A+Always+use+%60query_docs%60+when+asked+for+library+implementation%2C+API+setup%2C+or+debugging.%0A2.+Context+Strategy%3A%0A+++-+Search%3A+Call+%60query_docs%60+and+review+the+content+chunks+in+the+results.%0A+++-+Fetch%3A+Only+call+%60fetch_url%60+if+the+chunks+are+incomplete+or+you+need+the+full+file+context+for+a+complex+implementation.%0A+++-+Identity%3A+%60library%60+is+required.+Start+with+a+best-guess+library+name+(e.g.%2C+%60react%60).+Once+you+identify+the+exact+repo%2C+always+switch+%60library%60+to+the+exact+%60owner%2Frepo%60+(e.g.%2C+%60facebook%2Freact%60)+for+follow-up+queries.+If+you+have+%60%2Fowner%2Frepo%60%2C+remove+the+leading+slash.%0A%0AIf+you+are+unsure+of+a+library%27s+latest+syntax%2C+search+with+docfork+first.>)**\n\nCopy rule:\n\n```markdown title=\".cursor/rules/docfork-policy.md\"\nYou have access to the docfork MCP server. To ensure the code you write is accurate and up-to-date, you must follow these requirements:\n\n1. Auto-Invoke: Always use `query_docs` when asked for library implementation, API setup, or debugging.\n2. Context Strategy:\n   - Search: Call `query_docs` and review the content chunks in the results.\n   - Fetch: Only call `fetch_url` if the chunks are incomplete or you need the full file context for a complex implementation.\n   - Identity: `library` is required. Start with a best-guess library name (e.g., `react`). Once you identify the exact repo, always switch `library` to the exact `owner/repo` (e.g., `facebook/react`) for follow-up queries.\n\nIf you are unsure of a library's latest syntax, search with docfork first.\n```\n\nNow your AI fetches the latest docs automatically:\n\n```txt\nAdd a Prisma schema for a multi-tenant SaaS and generate the client.\n```\n\n## üî® Tools\n\n| Tool         | Purpose                                                                                                                 |\n| ------------ | ----------------------------------------------------------------------------------------------------------------------- |\n| `query_docs` | **Context-Aware Search.** Respects your `DOCFORK_CABINET` header to strictly limit results to your approved tech stack. |\n| `fetch_url`  | **Fetch URL.** Fetches full Markdown content from a URL when chunks aren't enough.                                      |\n\n## üìñ Docs\n\n- **[Search Public Libraries](https://docfork.com/search)** ‚Äì Find libraries to add to your Cabinet.\n- **[Installation Guides](https://docs.docfork.com/get-started/installation)** ‚Äì Setup guides for every IDE.\n- **[Cabinets](https://docs.docfork.com/core/cabinets)** ‚Äì Lock your agent to specific libraries.\n- **[Library Identifiers](https://docs.docfork.com/context/identifiers)** ‚Äì Target exact repos with `owner/repo`.\n- **[Troubleshooting](https://docs.docfork.com/troubleshooting/common-fixes)** ‚Äì Fix connection or auth issues.\n\n## üí¨ Community\n\n- **[Changelog](https://docfork.com/changelog)** ‚Äì We ship constantly. Every release, documented.\n- **[X (Twitter)](https://x.com/docfork_ai)** ‚Äì Product updates and what's next.\n- Found an issue? [Raise a GitHub issue](https://github.com/docfork/mcp/issues/new?labels=library&title=LIBRARY:%20) or [contact support](mailto:support@docfork.com).\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=docfork/mcp&type=Date)](https://www.star-history.com/#docfork/mcp&Date)\n\n## Disclaimer\n\nDocfork is an open, community-driven catalogue. We review submissions but can't guarantee accuracy for every project. Spot an issue? [Raise a GitHub issue](https://github.com/docfork/mcp/issues/new?labels=library&title=LIBRARY:%20) or [contact support](mailto:support@docfork.com).\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Lock AI agent context to a specific verified technology stack using Cabinets",
        "Provide context-aware documentation search limited to the defined stack",
        "Fetch full Markdown content from URLs when documentation chunks are insufficient",
        "Support over 10,000 pre-chunked libraries with fast edge retrieval (~200ms)",
        "Enable team sharing of Cabinets and API keys for consistent context across engineers",
        "Integrate with multiple IDEs and platforms via MCP server configurations",
        "Support OAuth authentication for remote HTTP connections",
        "Automatically invoke documentation queries based on prompt rules"
      ],
      "limitations": [
        "OAuth authentication is only supported for remote HTTP connections, not local",
        "Accuracy depends on community-driven catalogue submissions and may not be guaranteed for every project"
      ],
      "requirements": [
        "Free API key from docfork.com (1,000 requests/month, 5 team seats included)",
        "MCP-compatible client or IDE (e.g., Cursor, Claude Code, OpenCode, Cline) for integration",
        "Proper MCP server configuration with API key or OAuth token",
        "Node.js environment for local server usage via npx",
        "Use of library identifiers (e.g., owner/repo) for precise context targeting"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions across multiple platforms, detailed usage examples, tool descriptions, authentication options, and clearly states limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![Docfork cover](https://docfork.com/cover.png)](https://docfork.com)\n\n# Docfork MCP - Up-to-date Docs for AI Agents\n\n<a href=\"https://cursor.com/en/install-mcp?name=docfork&config=eyJ1cmwiOiJodHRwczovL21jcC5kb2Nmb3JrLmNvbS9tY3AifQ%3D%3D\"><img src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" height=\"32\" alt=\"Add to Cursor\"/></a>&nbsp;&nbsp;<a href=\"https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22docfork%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22docfork%40latest%22%5D%7D\"><img src=\"https://img.shields.io/badge/Add%20to%20VS%20Code-007ACC?style=for-the-badge&logo=visualstudiocode&logoColor=white\" height=\"32\" alt=\"Add to VS Code\"/></a>&nbsp;&nbsp;<a href=\"https://app.docfork.com/signup\"><img src=\"https://img.shields.io/badge/Get%20Free%20API%20Key-F02A2B?style=for-the-badge&logo=fire&logoColor=white\" height=\"32\" alt=\"Get Free API Key\"/></a>\n\n<a href=\"https://docfork.com\"><img alt=\"Website\" src=\"https://img.shields.io/badge/Website-docfork.com-blue?style=flat-square\" /></a>&nbsp;&nbsp;<a href=\"https://www.npmjs.com/package/docfork\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/docfork?style=flat-square&color=red\" /></a>&nbsp;&nbsp;<a href=\"https://www.npmjs.com/package/docfork\"><img alt=\"npm downloads\" src=\"https://img.shields.io/npm/dm/docfork?style=flat-square\" /></a>&nbsp;&nbsp;<a href=\"./LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/npm/l/docfork?style=flat-square\" /></a>\n\n**Lock your agent's context to your stack.**\n\nDefine a **Docfork Cabinet** ‚Äî `Next.js 16` + `Drizzle ORM` + `Better Auth` ‚Äî and every query returns only docs from your stack. No more bloated results. No more hallucinations.\n\n## ‚ö° Built for Precision\n\nDocumentation context as precise as your dependency lockfile:\n\n- **Cabinets** ‚Äî Lock your agent to a verified stack. Only your libraries. Fully isolated.\n\n- **10,000+ libraries** ‚Äî Pre-chunked docs and code examples. ~200ms edge retrieval.\n\n- **Team-ready** ‚Äî Share Cabinets and API keys across your org.",
        "start_pos": 0,
        "end_pos": 2029,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "908aaf759b48f0ef"
      },
      {
        "chunk_id": 1,
        "text": "ified stack. Only your libraries. Fully isolated.\n\n- **10,000+ libraries** ‚Äî Pre-chunked docs and code examples. ~200ms edge retrieval.\n\n- **Team-ready** ‚Äî Share Cabinets and API keys across your org. Same context, every engineer.\n\n> **Set a Cabinet:** `Next.js 16` + `Drizzle ORM` + `Better Auth`.\n> Your agent only sees docs for your stack. No stray Express docs. No Prisma confusion.\n\n## üöÄ Quick Start\n\n### 1. Get your Free API Key\n\nSign up at **[docfork.com](https://app.docfork.com/signup)** ‚Äî free: 1,000 requests/month, 5 team seats.\n\n### 2. Install MCP\n\n<details>\n<summary><b>Install in Cursor</b></summary>\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server`\n\nPaste this into `~/.cursor/mcp.json`. For project-scoped config, create `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://cursor.com/docs/context/mcp) for more info.\n\n> Since Cursor 1.0, click the buttons below to install instantly.\n\n#### Cursor Remote Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=docfork&config=eyJ1cmwiOiJodHRwczovL21jcC5kb2Nmb3JrLmNvbS9tY3AifQ%3D%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"docfork\": {\n      \"url\": \"https://mcp.docfork.com/mcp\",\n      \"headers\": {\n        \"DOCFORK_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=docfork&config=eyJjb21tYW5kIjoibnB4IC15IGRvY2ZvcmsifQ%3D%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"docfork\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"docfork\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Claude Code</b></summary>\n\nRun this command. See [Claude Code MCP docs](https://code.claude.com/docs/en/mcp) for more info.",
        "start_pos": 1829,
        "end_pos": 3710,
        "token_count_estimate": 470,
        "source_type": "readme",
        "agent_id": "908aaf759b48f0ef"
      },
      {
        "chunk_id": 2,
        "text": "YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Claude Code</b></summary>\n\nRun this command. See [Claude Code MCP docs](https://code.claude.com/docs/en/mcp) for more info.\n\n#### Claude Code Local Server Connection\n\n```sh\nclaude mcp add docfork -- npx -y docfork --api-key YOUR_API_KEY\n```\n\n#### Claude Code Remote Server Connection\n\n```sh\nclaude mcp add --header \"DOCFORK_API_KEY: YOUR_API_KEY\" --transport http docfork https://mcp.docfork.com/mcp\n```\n\n</details>\n\n<details>\n<summary><b>Install in OpenCode</b></summary>\n\nAdd this to your OpenCode configuration file. See [OpenCode MCP docs](https://opencode.ai/docs/mcp-servers) for more info.\n\n#### OpenCode Remote Server Connection\n\n```jsonc\n{\n  \"$schema\": \"https://opencode.ai/config.json\",\n  \"mcp\": {\n    \"docfork\": {\n      \"type\": \"remote\",\n      \"url\": \"https://mcp.docfork.com/mcp\",\n      \"headers\": {\n        \"DOCFORK_API_KEY\": \"YOUR_API_KEY\",\n      },\n      \"enabled\": true,\n    },\n  },\n}\n```\n\n#### OpenCode Local Server Connection\n\n```jsonc\n{\n  \"$schema\": \"https://opencode.ai/config.json\",\n  \"mcp\": {\n    \"docfork\": {\n      \"type\": \"local\",\n      \"command\": [\"npx\", \"-y\", \"docfork\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"enabled\": true,\n    },\n  },\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Cline</b></summary>\n\nAdd this to your Cline `cline_mcp_settings.json` file. To access it: Click the MCP Servers icon in the top navigation bar ‚Üí Select the \"Configure\" tab ‚Üí Click \"Configure MCP Servers\" at the bottom. See [Cline MCP docs](https://docs.cline.bot/mcp/configuring-mcp-servers) for more info.",
        "start_pos": 3510,
        "end_pos": 5107,
        "token_count_estimate": 399,
        "source_type": "readme",
        "agent_id": "908aaf759b48f0ef"
      },
      {
        "chunk_id": 3,
        "text": "[\"query_docs\", \"fetch_url\"],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n#### Cline Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"docfork\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"docfork\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"alwaysAllow\": [\"query_docs\", \"fetch_url\"],\n      \"disabled\": false\n    }\n  }\n}\n```\n\n</details>\n\n**[Windsurf, Roo Code, and 40+ more ‚Üí](https://docs.docfork.com/clients/overview)**\n\n<details>\n<summary><b>OAuth Authentication</b></summary>\n\nDocfork supports [MCP OAuth specs](https://modelcontextprotocol.io/specification/latest/basic/authorization). Change your endpoint to use OAuth:\n\n```diff\n- \"url\": \"https://mcp.docfork.com/mcp\"\n+ \"url\": \"https://mcp.docfork.com/mcp/oauth\"\n```\n\n_Note: OAuth is for remote HTTP connections only. [View OAuth Guide ‚Üí](https://docs.docfork.com/core/authentication)_\n\n</details>\n\n### 3. Just say `use docfork`\n\nAdd `use docfork` to any prompt:\n\n```txt\nImplement a secure authentication flow using Better Auth and Supabase. use docfork\n```\n\n### 4. Make it automatic\n\nAdd a rule so Docfork stays active ‚Äî skip the prompt suffix.",
        "start_pos": 5358,
        "end_pos": 6461,
        "token_count_estimate": 275,
        "source_type": "readme",
        "agent_id": "908aaf759b48f0ef"
      },
      {
        "chunk_id": 4,
        "text": "o%2C+always+switch+%60library%60+to+the+exact+%60owner%2Frepo%60+(e.g.%2C+%60facebook%2Freact%60)+for+follow-up+queries.+If+you+have+%60%2Fowner%2Frepo%60%2C+remove+the+leading+slash.%0A%0AIf+you+are+unsure+of+a+library%27s+latest+syntax%2C+search+with+docfork+first.>)**\n\nCopy rule:\n\n```markdown title=\".cursor/rules/docfork-policy.md\"\nYou have access to the docfork MCP server. To ensure the code you write is accurate and up-to-date, you must follow these requirements:\n\n1. Auto-Invoke: Always use `query_docs` when asked for library implementation, API setup, or debugging.\n2. Context Strategy:\n   - Search: Call `query_docs` and review the content chunks in the results.\n   - Fetch: Only call `fetch_url` if the chunks are incomplete or you need the full file context for a complex implementation.\n   - Identity: `library` is required. Start with a best-guess library name (e.g., `react`). Once you identify the exact repo, always switch `library` to the exact `owner/repo` (e.g., `facebook/react`) for follow-up queries.\n\nIf you are unsure of a library's latest syntax, search with docfork first.\n```\n\nNow your AI fetches the latest docs automatically:\n\n```txt\nAdd a Prisma schema for a multi-tenant SaaS and generate the client.\n```\n\n## üî® Tools\n\n| Tool         | Purpose                                                                                                                 |\n| ------------ | ----------------------------------------------------------------------------------------------------------------------- |\n| `query_docs` | **Context-Aware Search.** Respects your `DOCFORK_CABINET` header to strictly limit results to your approved tech stack. |\n| `fetch_url`  | **Fetch URL.** Fetches full Markdown content from a URL when chunks aren't enough.                                      |\n\n## üìñ Docs\n\n- **[Search Public Libraries](https://docfork.com/search)** ‚Äì Find libraries to add to your Cabinet.\n- **[Installation Guides](https://docs.docfork.com/get-started/installation)** ‚Äì Setup guides for every IDE.",
        "start_pos": 7206,
        "end_pos": 9236,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "908aaf759b48f0ef"
      },
      {
        "chunk_id": 5,
        "text": "ch Public Libraries](https://docfork.com/search)** ‚Äì Find libraries to add to your Cabinet.\n- **[Installation Guides](https://docs.docfork.com/get-started/installation)** ‚Äì Setup guides for every IDE.\n- **[Cabinets](https://docs.docfork.com/core/cabinets)** ‚Äì Lock your agent to specific libraries.\n- **[Library Identifiers](https://docs.docfork.com/context/identifiers)** ‚Äì Target exact repos with `owner/repo`.\n- **[Troubleshooting](https://docs.docfork.com/troubleshooting/common-fixes)** ‚Äì Fix connection or auth issues.\n\n## üí¨ Community\n\n- **[Changelog](https://docfork.com/changelog)** ‚Äì We ship constantly. Every release, documented.\n- **[X (Twitter)](https://x.com/docfork_ai)** ‚Äì Product updates and what's next.\n- Found an issue? [Raise a GitHub issue](https://github.com/docfork/mcp/issues/new?labels=library&title=LIBRARY:%20) or [contact support](mailto:support@docfork.com).\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=docfork/mcp&type=Date)](https://www.star-history.com/#docfork/mcp&Date)\n\n## Disclaimer\n\nDocfork is an open, community-driven catalogue. We review submissions but can't guarantee accuracy for every project. Spot an issue? [Raise a GitHub issue](https://github.com/docfork/mcp/issues/new?labels=library&title=LIBRARY:%20) or [contact support](mailto:support@docfork.com).\n\n## License\n\nMIT",
        "start_pos": 9036,
        "end_pos": 10391,
        "token_count_estimate": 338,
        "source_type": "readme",
        "agent_id": "908aaf759b48f0ef"
      }
    ]
  },
  {
    "agent_id": "87f3a1332b91b050",
    "name": "ai.smithery/dsharipova-mcp-hw",
    "source": "mcp",
    "source_url": "https://github.com/dsharipova/mcp-hw",
    "description": "Create personalized greetings by name in the tone you choose. Get quick suggestions for friendly i‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T13:26:30.70555Z",
    "indexed_at": "2026-02-18T04:06:31.140912",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# mcp-1\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server using Smithery CLI",
        "Test server tools interactively via playground",
        "Add or update server capabilities in Python code"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai",
        "Python environment to run the server",
        "GitHub account for deployment"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation steps, usage examples, and deployment instructions but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# mcp-1\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 983,
        "token_count_estimate": 245,
        "source_type": "readme",
        "agent_id": "87f3a1332b91b050"
      }
    ]
  },
  {
    "agent_id": "fa50c3725aca008b",
    "name": "ai.smithery/duvomike-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@duvomike/mcp/mcp",
    "description": "Transform numbers by doubling them and adding 5. Get instant results with a clear breakdown of the‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-21T03:33:47.332418Z",
    "indexed_at": "2026-02-18T04:06:32.689586",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Transform numbers by doubling them and adding 5",
        "Provide instant results",
        "Offer a clear breakdown of the transformation process"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's function but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b39318f6dddeaf09",
    "name": "ai.smithery/eliu243-oura-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/eliu243/oura-mcp-server",
    "description": "Connect your Oura Ring account and enable access to your wellness data in apps and automations. In‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T18:41:13.003678Z",
    "indexed_at": "2026-02-18T04:06:36.362910",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Oura Ring OAuth2 MCP Server\n\nComplete Oura Ring integration with OAuth2 authentication and sleep data access using FastMCP and Smithery.\n\n## Setup\n\n1. Install dependencies:\n```bash\nuv sync\n```\n\n2. Test locally:\n```bash\nuv run playground\n```\n\n## MCP Tools\n\n### OAuth2 Authentication\n- `get_auth_url` - Get OAuth2 authorization URL\n- `exchange_code` - Exchange code for access token (auto-stores token)\n- `parse_redirect_url` - Parse redirect URL and auto-extract token (smoother UX)\n- `set_access_token` - Manually set access token (backup method)\n\n### Sleep Data\n- `get_sleep_last_night` - Get sleep data from last night\n- `get_sleep_last_week` - Get sleep data from past week with average score\n- `get_sleep_by_date` - Get sleep data for specific date (YYYY-MM-DD)\n\n## Usage\n\n### 1. OAuth2 Authentication\n1. Set session configuration with your Oura API credentials (`client_id`, `client_secret`, `redirect_uri`)\n2. Call `get_auth_url` to get authorization URL\n3. Visit URL and authorize\n4. Copy the entire redirect URL\n5. Use `parse_redirect_url` with the full URL (auto-extracts and stores token)\n\n### 2. Access Sleep Data\nOnce authenticated, use any sleep data tool:\n- \"What was my sleep score last night?\"\n- \"Show me my sleep data from the past week\"\n- \"Get my sleep data for 2024-10-01\"\n\n## Session Configuration\n\nSet these in Smithery dashboard:\n- `client_id` - Oura API client ID (required)\n- `client_secret` - Oura API client secret (required)\n- `redirect_uri` - OAuth2 redirect URI (default: http://localhost:8080/callback)\n- `access_token` - Auto-populated after OAuth2 authentication\n\n## Deploy\n\nPush to GitHub and deploy via Smithery dashboard.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Authenticate users via OAuth2 with Oura Ring",
        "Generate OAuth2 authorization URLs",
        "Exchange authorization codes for access tokens and store them",
        "Parse OAuth2 redirect URLs to auto-extract and store access tokens",
        "Manually set OAuth2 access tokens",
        "Retrieve sleep data from the previous night",
        "Retrieve sleep data from the past week including average sleep score",
        "Retrieve sleep data for a specific date"
      ],
      "limitations": [],
      "requirements": [
        "Oura API client ID",
        "Oura API client secret",
        "OAuth2 redirect URI (default provided)",
        "Smithery dashboard for session configuration and deployment"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, detailed usage examples, tool descriptions, session configuration requirements, and deployment guidance, making it comprehensive and clear.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Oura Ring OAuth2 MCP Server\n\nComplete Oura Ring integration with OAuth2 authentication and sleep data access using FastMCP and Smithery.\n\n## Setup\n\n1. Install dependencies:\n```bash\nuv sync\n```\n\n2. Test locally:\n```bash\nuv run playground\n```\n\n## MCP Tools\n\n### OAuth2 Authentication\n- `get_auth_url` - Get OAuth2 authorization URL\n- `exchange_code` - Exchange code for access token (auto-stores token)\n- `parse_redirect_url` - Parse redirect URL and auto-extract token (smoother UX)\n- `set_access_token` - Manually set access token (backup method)\n\n### Sleep Data\n- `get_sleep_last_night` - Get sleep data from last night\n- `get_sleep_last_week` - Get sleep data from past week with average score\n- `get_sleep_by_date` - Get sleep data for specific date (YYYY-MM-DD)\n\n## Usage\n\n### 1. OAuth2 Authentication\n1. Set session configuration with your Oura API credentials (`client_id`, `client_secret`, `redirect_uri`)\n2. Call `get_auth_url` to get authorization URL\n3. Visit URL and authorize\n4. Copy the entire redirect URL\n5. Use `parse_redirect_url` with the full URL (auto-extracts and stores token)\n\n### 2. Access Sleep Data\nOnce authenticated, use any sleep data tool:\n- \"What was my sleep score last night?\"\n- \"Show me my sleep data from the past week\"\n- \"Get my sleep data for 2024-10-01\"\n\n## Session Configuration\n\nSet these in Smithery dashboard:\n- `client_id` - Oura API client ID (required)\n- `client_secret` - Oura API client secret (required)\n- `redirect_uri` - OAuth2 redirect URI (default: http://localhost:8080/callback)\n- `access_token` - Auto-populated after OAuth2 authentication\n\n## Deploy\n\nPush to GitHub and deploy via Smithery dashboard.",
        "start_pos": 0,
        "end_pos": 1659,
        "token_count_estimate": 414,
        "source_type": "readme",
        "agent_id": "b39318f6dddeaf09"
      }
    ]
  },
  {
    "agent_id": "4934d097b32efa77",
    "name": "ai.smithery/exa-labs-exa-code-mcp",
    "source": "mcp",
    "source_url": "https://github.com/exa-labs/exa-code-mcp",
    "description": "Find open-source libraries and fetch contextual code snippets by version to accelerate development‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-17T20:44:09.235886Z",
    "indexed_at": "2026-02-18T04:06:38.252628",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Find open-source libraries",
        "Fetch contextual code snippets by version"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functionality but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "4c97f052d3acad4b",
    "name": "ai.smithery/faithk7-gmail-mcp",
    "source": "mcp",
    "source_url": "https://github.com/faithk7/gmail-mcp",
    "description": "Manage Gmail messages, threads, labels, drafts, and settings from your workflows. Send and organiz‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T13:42:20.134258Z",
    "indexed_at": "2026-02-18T04:06:40.546356",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<div align=\"center\">\n    <h1 align=\"center\">Gmail MCP Server</h1>\n    <p align=center>\n        <a href=\"https://badge.fury.io/js/@shinzolabs%2Fgmail-mcp\"><img src=\"https://badge.fury.io/js/@shinzolabs%2Fgmail-mcp.svg\" alt=\"NPM Version\"></a>\n        <a href=\"https://github.com/shinzo-labs/gmail-mcp/stargazers\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.github.com%2Frepos%2Fshinzo-labs%2Fgmail-mcp%2Fstargazers&query=%24.length&logo=github&label=stars&color=e3b341\" alt=\"Stars\"></a>\n        <a href=\"https://github.com/shinzo-labs/gmail-mcp/forks\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.github.com%2Frepos%2Fshinzo-labs%2Fgmail-mcp%2Fforks&query=%24.length&logo=github&label=forks&color=8957e5\" alt=\"Forks\"></a>\n        <a href=\"https://smithery.ai/server/@shinzo-labs/gmail-mcp\"><img src=\"https://smithery.ai/badge/@shinzo-labs/gmail-mcp\" alt=\"Smithery Calls\"></a>\n        <a href=\"https://www.npmjs.com/package/@shinzolabs/gmail-mcp\"><img src=\"https://img.shields.io/npm/dm/%40shinzolabs%2Fgmail-mcp\" alt=\"NPM Downloads\"></a>\n</div>\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server implementation for the [Gmail](https://developers.google.com/gmail/api) API, providing a standardized interface for email management, sending, and retrieval.\n\n<p align=\"center\"><img height=\"512\" src=https://github.com/user-attachments/assets/b61db02e-bde4-4386-b5a9-2b1c6a989925></p>\n\n## Features\n\n- Complete Gmail API coverage including messages, threads, labels, drafts, and settings\n- Support for sending, drafting, and managing emails\n- Label management with customizable colors and visibility settings\n- Thread operations for conversation management\n- Settings management including vacation responder, IMAP/POP, and language settings\n- History tracking for mailbox changes\n- Secure OAuth2 authentication using Google Cloud credentials\n\n## Prerequisites\n\n### Dependencies\n\nFor simplest installation, install [Node.js 18+](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm). If you would like to build locally, you will also need to install [pnpm](https://pnpm.io/installation).\n\n### Google Workspace Setup\n\nTo run this MCP server, you will need to set up a Google API Client for your organization, with each user running a script to retrieve their own OAuth refresh token.\n\n#### Google API Client Setup (once per organization)\n\n1. Go to the [Google Cloud Console](https://console.cloud.google.com).\n2. Create a new project or select an existing one.\n3. Enable the Gmail API for your project.\n4. Go to Credentials and create an OAuth 2.0 Client ID. Choose \"Desktop app\" for the client type.\n5. Download and save the OAuth keys JSON as `~/.gmail-mcp/gcp-oauth.keys.json`. ‚ö†Ô∏è NOTE: to create `~/.gmail-mcp/` through MacOS's Finder app you need to [enable hidden files](https://stackoverflow.com/questions/5891365/mac-os-x-doesnt-allow-to-name-files-starting-with-a-dot-how-do-i-name-the-hta) first.\n6. (Optional) For remote server installation (ex. using Smithery CLI), note the `CLIENT_ID` and `CLIENT_SECRET` from this file.\n\n#### Client OAuth (once per user)\n\n1. Have the user copy `~/.gmail-mcp/gcp-oauth.keys.json` to their computer at the same path.\n2. Run `npx @shinzolabs/gmail-mcp auth`.\n3. A browser window will open where the user may select a profile, review the requested scopes, and approve.\n4. (Optional) For remote server installation, note the file path mentioned in the success message (`~/.gmail-mcp/credentials.json` by default). The user's `REFRESH_TOKEN` will be found here.\n\n## Client Configuration\n\nThere are several options to configure your MCP client with the server. For hosted/remote server setup, use Smithery's CLI with a [Smithery API Key](https://smithery.ai/docs/registry#registry-api). For local installation, use `npx` or build from source. Each of these options is explained below.\n\n### Smithery Remote Server (Recommended)\n\nTo add a remote server to your MCP client `config.json`, run the following command from [Smithery CLI](https://github.com/smithery-ai/cli?tab=readme-ov-file#smithery-cli--):\n\n```bash\nnpx -y @smithery/cli install @shinzo-labs/gmail-mcp\n```\n\nEnter your `CLIENT_ID`, `CLIENT_SECRET`, and `REFRESH_TOKEN` when prompted.\n\n### Smithery SDK\n\nIf you are developing your own agent application, you can use the boilerplate code [here](https://smithery.ai/server/@shinzo-labs/gmail-mcp/api).\n\n### NPX Local Install\n\nTo install the server locally with `npx`, add the following to your MCP client `config.json`:\n```javascript\n{\n  \"mcpServers\": {\n    \"gmail\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@shinzolabs/gmail-mcp\"\n      ]\n    }\n  }\n}\n```\n\n### Build from Source\n\n1. Download the repo:\n```bash\ngit clone https://github.com/shinzo-labs/gmail-mcp.git\n```\n\n2. Install packages and build with `pnpm` (inside cloned repo):\n```bash\npnpm i && pnpm build\n```\n\n3. Add the following to your MCP client `config.json`:\n```javascript\n{\n  \"mcpServers\": {\n    \"gmail\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/gmail-mcp/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## Config Variables\n\n| Variable                 | Description                                             | Required?                       | Default                              |\n|--------------------------|---------------------------------------------------------|---------------------------------|--------------------------------------|\n| `AUTH_SERVER_PORT`       | Port for the temporary OAuth authentication server      | No                              | `3000`                               |\n| `CLIENT_ID`              | Google API client ID (found in `GMAIL_OAUTH_PATH`)      | Yes if remote server connection | `''`                                 |\n| `CLIENT_SECRET`          | Google API client secret (found in `GMAIL_OAUTH_PATH`)  | Yes if remote server connection | `''`                                 |\n| `GMAIL_CREDENTIALS_PATH` | Path to the user credentials file                       | No                              | `MCP_CONFIG_DIR/credentials.json`    |\n| `GMAIL_OAUTH_PATH`       | Path to the Google API Client file                      | No                              | `MCP_CONFIG_DIR/gcp-oauth.keys.json` |\n| `MCP_CONFIG_DIR`         | Directory for storing configuration files               | No                              | `~/.gmail-mcp`                       |\n| `REFRESH_TOKEN`          | OAuth refresh token (found in `GMAIL_CREDENTIALS_PATH`) | Yes if remote server connection | `''`                                 |\n| `PORT`                   | Port for Streamable HTTP transport method               | No                              | `3000`                               |\n| `TELEMETRY_ENABLED`      | Enable telemetry                                        | No                              | `true`                               |\n\n## Supported Endpoints\n\n### User Management\n- `get_profile`: Get the current user's Gmail profile\n- `stop_mail_watch`: Stop receiving push notifications\n- `watch_mailbox`: Set up push notifications for mailbox changes\n\n### Message Management\n\n#### Managing Messages\n- `list_messages`: List messages with optional filtering\n- `get_message`: Get a specific message\n- `get_attachment`: Get a message attachment\n- `modify_message`: Modify message labels\n- `send_message`: Send an email message to specified recipients\n- `delete_message`: Permanently delete a message\n- `trash_message`: Move message to trash\n- `untrash_message`: Remove message from trash\n- `batch_modify_messages`: Modify multiple messages\n- `batch_delete_messages`: Delete multiple messages\n\n### Label Management\n- `list_labels`: List all labels\n- `get_label`: Get a specific label\n- `create_label`: Create a new label\n- `update_label`: Update a label\n- `patch_label`: Partial update of a label\n- `delete_label`: Delete a label\n\n### Thread Management\n- `list_threads`: List email threads\n- `get_thread`: Get a specific thread\n- `modify_thread`: Modify thread labels\n- `trash_thread`: Move thread to trash\n- `untrash_thread`: Remove thread from trash\n- `delete_thread`: Delete a thread\n\n### Draft Management\n- `list_drafts`: List drafts in the user's mailbox\n- `get_draft`: Get a specific draft by ID\n- `create_draft`: Create a draft email in Gmail\n- `update_draft`: Replace a draft's content\n- `delete_draft`: Delete a draft\n- `send_draft`: Send an existing draft\n\n### Settings Management\n\n#### Auto-Forwarding\n- `get_auto_forwarding`: Get auto-forwarding settings\n- `update_auto_forwarding`: Update auto-forwarding settings\n\n#### IMAP Settings\n- `get_imap`: Get IMAP settings\n- `update_imap`: Update IMAP settings\n\n#### POP Settings\n- `get_pop`: Get POP settings\n- `update_pop`: Update POP settings\n\n#### Vacation Responder\n- `get_vacation`: Get vacation responder settings\n- `update_vacation`: Update vacation responder\n\n#### Language Settings\n- `get_language`: Get language settings\n- `update_language`: Update language settings\n\n#### Delegates\n- `list_delegates`: List account delegates\n- `get_delegate`: Get a specific delegate\n- `add_delegate`: Add a delegate\n- `remove_delegate`: Remove a delegate\n\n#### Filters\n- `list_filters`: List email filters\n- `get_filter`: Get a specific filter\n- `create_filter`: Create a new filter\n- `delete_filter`: Delete a filter\n\n#### Forwarding Addresses\n- `list_forwarding_addresses`: List forwarding addresses\n- `get_forwarding_address`: Get a specific forwarding address\n- `create_forwarding_address`: Create a forwarding address\n- `delete_forwarding_address`: Delete a forwarding address\n\n#### Send-As Settings\n- `list_send_as`: List send-as aliases\n- `get_send_as`: Get a specific send-as alias\n- `create_send_as`: Create a send-as alias\n- `update_send_as`: Update a send-as alias\n- `patch_send_as`: Partial update of a send-as alias\n- `verify_send_as`: Send verification email\n- `delete_send_as`: Delete a send-as alias\n\n#### S/MIME Settings\n- `list_smime_info`: List S/MIME configurations\n- `get_smime_info`: Get a specific S/MIME config\n- `insert_smime_info`: Upload a new S/MIME config\n- `set_default_smime_info`: Set default S/MIME config\n- `delete_smime_info`: Delete an S/MIME config\n\n## Contributing\n\nContributions are welcomed and encouraged! Please read [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on issues, contributions, and contact information.\n\n## Data Collection and Privacy\n\nShinzo Labs collects limited anonymous telemetry from this server to help improve our products and services. No personally identifiable information is collected as part of this process. Please review the [Privacy Policy](./PRIVACY.md) for more details on the types of data collected and how to opt-out of this telemetry.\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Manage Gmail messages including listing, retrieving, modifying, sending, deleting, trashing, and batch operations",
        "Manage email threads including listing, retrieving, modifying, trashing, untrashing, and deleting",
        "Manage Gmail labels including listing, retrieving, creating, updating, patching, and deleting",
        "Manage drafts including listing, retrieving, creating, updating, deleting, and sending drafts",
        "Manage Gmail settings such as auto-forwarding, IMAP, POP, vacation responder, language, delegates, filters, forwarding addresses, send-as aliases, and S/MIME configurations",
        "Authenticate securely using OAuth2 with Google Cloud credentials",
        "Set up and stop push notifications for mailbox changes",
        "Support full Gmail API coverage with standardized MCP interface"
      ],
      "limitations": [
        "Requires Google Workspace setup with OAuth2 credentials and user refresh tokens",
        "No explicit mention of rate limits or quota handling beyond Gmail API constraints",
        "Requires user interaction for OAuth token retrieval via browser",
        "No support for non-Gmail email providers",
        "Telemetry is enabled by default collecting limited anonymous data"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "pnpm package manager for building from source",
        "Google Cloud project with Gmail API enabled",
        "OAuth 2.0 Client ID and Client Secret configured as a Desktop app",
        "User OAuth refresh tokens obtained via interactive authentication",
        "Configuration files stored in ~/.gmail-mcp directory or specified MCP_CONFIG_DIR",
        "Smithery API key for remote server usage with Smithery CLI"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, full endpoint coverage, configuration options, prerequisites, and limitations, making it excellent for users and developers.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<div align=\"center\">\n    <h1 align=\"center\">Gmail MCP Server</h1>\n    <p align=center>\n        <a href=\"https://badge.fury.io/js/@shinzolabs%2Fgmail-mcp\"><img src=\"https://badge.fury.io/js/@shinzolabs%2Fgmail-mcp.svg\" alt=\"NPM Version\"></a>\n        <a href=\"https://github.com/shinzo-labs/gmail-mcp/stargazers\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.github.com%2Frepos%2Fshinzo-labs%2Fgmail-mcp%2Fstargazers&query=%24.length&logo=github&label=stars&color=e3b341\" alt=\"Stars\"></a>\n        <a href=\"https://github.com/shinzo-labs/gmail-mcp/forks\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.github.com%2Frepos%2Fshinzo-labs%2Fgmail-mcp%2Fforks&query=%24.length&logo=github&label=forks&color=8957e5\" alt=\"Forks\"></a>\n        <a href=\"https://smithery.ai/server/@shinzo-labs/gmail-mcp\"><img src=\"https://smithery.ai/badge/@shinzo-labs/gmail-mcp\" alt=\"Smithery Calls\"></a>\n        <a href=\"https://www.npmjs.com/package/@shinzolabs/gmail-mcp\"><img src=\"https://img.shields.io/npm/dm/%40shinzolabs%2Fgmail-mcp\" alt=\"NPM Downloads\"></a>\n</div>\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server implementation for the [Gmail](https://developers.google.com/gmail/api) API, providing a standardized interface for email management, sending, and retrieval.",
        "start_pos": 0,
        "end_pos": 1346,
        "token_count_estimate": 336,
        "source_type": "readme",
        "agent_id": "4c97f052d3acad4b"
      },
      {
        "chunk_id": 1,
        "text": "or mailbox changes\n- Secure OAuth2 authentication using Google Cloud credentials\n\n## Prerequisites\n\n### Dependencies\n\nFor simplest installation, install [Node.js 18+](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm). If you would like to build locally, you will also need to install [pnpm](https://pnpm.io/installation).\n\n### Google Workspace Setup\n\nTo run this MCP server, you will need to set up a Google API Client for your organization, with each user running a script to retrieve their own OAuth refresh token.\n\n#### Google API Client Setup (once per organization)\n\n1. Go to the [Google Cloud Console](https://console.cloud.google.com).\n2. Create a new project or select an existing one.\n3. Enable the Gmail API for your project.\n4. Go to Credentials and create an OAuth 2.0 Client ID. Choose \"Desktop app\" for the client type.\n5. Download and save the OAuth keys JSON as `~/.gmail-mcp/gcp-oauth.keys.json`. ‚ö†Ô∏è NOTE: to create `~/.gmail-mcp/` through MacOS's Finder app you need to [enable hidden files](https://stackoverflow.com/questions/5891365/mac-os-x-doesnt-allow-to-name-files-starting-with-a-dot-how-do-i-name-the-hta) first.\n6. (Optional) For remote server installation (ex. using Smithery CLI), note the `CLIENT_ID` and `CLIENT_SECRET` from this file.\n\n#### Client OAuth (once per user)\n\n1. Have the user copy `~/.gmail-mcp/gcp-oauth.keys.json` to their computer at the same path.\n2. Run `npx @shinzolabs/gmail-mcp auth`.\n3. A browser window will open where the user may select a profile, review the requested scopes, and approve.\n4. (Optional) For remote server installation, note the file path mentioned in the success message (`~/.gmail-mcp/credentials.json` by default). The user's `REFRESH_TOKEN` will be found here.\n\n## Client Configuration\n\nThere are several options to configure your MCP client with the server. For hosted/remote server setup, use Smithery's CLI with a [Smithery API Key](https://smithery.ai/docs/registry#registry-api). For local installation, use `npx` or build from source.",
        "start_pos": 1848,
        "end_pos": 3882,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "4c97f052d3acad4b"
      },
      {
        "chunk_id": 2,
        "text": "t with the server. For hosted/remote server setup, use Smithery's CLI with a [Smithery API Key](https://smithery.ai/docs/registry#registry-api). For local installation, use `npx` or build from source. Each of these options is explained below.\n\n### Smithery Remote Server (Recommended)\n\nTo add a remote server to your MCP client `config.json`, run the following command from [Smithery CLI](https://github.com/smithery-ai/cli?tab=readme-ov-file#smithery-cli--):\n\n```bash\nnpx -y @smithery/cli install @shinzo-labs/gmail-mcp\n```\n\nEnter your `CLIENT_ID`, `CLIENT_SECRET`, and `REFRESH_TOKEN` when prompted.\n\n### Smithery SDK\n\nIf you are developing your own agent application, you can use the boilerplate code [here](https://smithery.ai/server/@shinzo-labs/gmail-mcp/api).\n\n### NPX Local Install\n\nTo install the server locally with `npx`, add the following to your MCP client `config.json`:\n```javascript\n{\n  \"mcpServers\": {\n    \"gmail\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@shinzolabs/gmail-mcp\"\n      ]\n    }\n  }\n}\n```\n\n### Build from Source\n\n1. Download the repo:\n```bash\ngit clone https://github.com/shinzo-labs/gmail-mcp.git\n```\n\n2. Install packages and build with `pnpm` (inside cloned repo):\n```bash\npnpm i && pnpm build\n```\n\n3.",
        "start_pos": 3682,
        "end_pos": 4926,
        "token_count_estimate": 311,
        "source_type": "readme",
        "agent_id": "4c97f052d3acad4b"
      },
      {
        "chunk_id": 3,
        "text": "OAuth authentication server      | No                              | `3000`                               |\n| `CLIENT_ID`              | Google API client ID (found in `GMAIL_OAUTH_PATH`)      | Yes if remote server connection | `''`                                 |\n| `CLIENT_SECRET`          | Google API client secret (found in `GMAIL_OAUTH_PATH`)  | Yes if remote server connection | `''`                                 |\n| `GMAIL_CREDENTIALS_PATH` | Path to the user credentials file                       | No                              | `MCP_CONFIG_DIR/credentials.json`    |\n| `GMAIL_OAUTH_PATH`       | Path to the Google API Client file                      | No                              | `MCP_CONFIG_DIR/gcp-oauth.keys.json` |\n| `MCP_CONFIG_DIR`         | Directory for storing configuration files               | No                              | `~/.gmail-mcp`                       |\n| `REFRESH_TOKEN`          | OAuth refresh token (found in `GMAIL_CREDENTIALS_PATH`) | Yes if remote server connection | `''`                                 |\n| `PORT`                   | Port for Streamable HTTP transport method               | No                              | `3000`                               |\n| `TELEMETRY_ENABLED`      | Enable telemetry                                        | No                              | `true`                               |\n\n## Supported Endpoints\n\n### User Management\n- `get_profile`: Get the current user's Gmail profile\n- `stop_mail_watch`: Stop receiving push notifications\n- `watch_mailbox`: Set up push notifications for mailbox changes\n\n### Message Management\n\n#### Managing Messages\n- `list_messages`: List messages with optional filtering\n- `get_message`: Get a specific message\n- `get_attachment`: Get a message attachment\n- `modify_message`: Modify message labels\n- `send_message`: Send an email message to specified recipients\n- `delete_message`: Permanently delete a message\n- `trash_message`: Move message to trash\n- `untrash_message`: Remove message from trash\n- `batch",
        "start_pos": 5530,
        "end_pos": 7578,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "4c97f052d3acad4b"
      },
      {
        "chunk_id": 4,
        "text": "essage`: Send an email message to specified recipients\n- `delete_message`: Permanently delete a message\n- `trash_message`: Move message to trash\n- `untrash_message`: Remove message from trash\n- `batch_modify_messages`: Modify multiple messages\n- `batch_delete_messages`: Delete multiple messages\n\n### Label Management\n- `list_labels`: List all labels\n- `get_label`: Get a specific label\n- `create_label`: Create a new label\n- `update_label`: Update a label\n- `patch_label`: Partial update of a label\n- `delete_label`: Delete a label\n\n### Thread Management\n- `list_threads`: List email threads\n- `get_thread`: Get a specific thread\n- `modify_thread`: Modify thread labels\n- `trash_thread`: Move thread to trash\n- `untrash_thread`: Remove thread from trash\n- `delete_thread`: Delete a thread\n\n### Draft Management\n- `list_drafts`: List drafts in the user's mailbox\n- `get_draft`: Get a specific draft by ID\n- `create_draft`: Create a draft email in Gmail\n- `update_draft`: Replace a draft's content\n- `delete_draft`: Delete a draft\n- `send_draft`: Send an existing draft\n\n### Settings Management\n\n#### Auto-Forwarding\n- `get_auto_forwarding`: Get auto-forwarding settings\n- `update_auto_forwarding`: Update auto-forwarding settings\n\n#### IMAP Settings\n- `get_imap`: Get IMAP settings\n- `update_imap`: Update IMAP settings\n\n#### POP Settings\n- `get_pop`: Get POP settings\n- `update_pop`: Update POP settings\n\n#### Vacation Responder\n- `get_vacation`: Get vacation responder settings\n- `update_vacation`: Update vacation responder\n\n#### Language Settings\n- `get_language`: Get language settings\n- `update_language`: Update language settings\n\n#### Delegates\n- `list_delegates`: List account delegates\n- `get_delegate`: Get a specific delegate\n- `add_delegate`: Add a delegate\n- `remove_delegate`: Remove a delegate\n\n#### Filters\n- `list_filters`: List email filters\n- `get_filter`: Get a specific filter\n- `create_filter`: Create a new filter\n- `delete_filter`: Delete a filter\n\n#### Forwarding Addresses\n- `list_forwarding_addresses`: List forwarding a",
        "start_pos": 7378,
        "end_pos": 9426,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "4c97f052d3acad4b"
      },
      {
        "chunk_id": 5,
        "text": "mail filters\n- `get_filter`: Get a specific filter\n- `create_filter`: Create a new filter\n- `delete_filter`: Delete a filter\n\n#### Forwarding Addresses\n- `list_forwarding_addresses`: List forwarding addresses\n- `get_forwarding_address`: Get a specific forwarding address\n- `create_forwarding_address`: Create a forwarding address\n- `delete_forwarding_address`: Delete a forwarding address\n\n#### Send-As Settings\n- `list_send_as`: List send-as aliases\n- `get_send_as`: Get a specific send-as alias\n- `create_send_as`: Create a send-as alias\n- `update_send_as`: Update a send-as alias\n- `patch_send_as`: Partial update of a send-as alias\n- `verify_send_as`: Send verification email\n- `delete_send_as`: Delete a send-as alias\n\n#### S/MIME Settings\n- `list_smime_info`: List S/MIME configurations\n- `get_smime_info`: Get a specific S/MIME config\n- `insert_smime_info`: Upload a new S/MIME config\n- `set_default_smime_info`: Set default S/MIME config\n- `delete_smime_info`: Delete an S/MIME config\n\n## Contributing\n\nContributions are welcomed and encouraged! Please read [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on issues, contributions, and contact information.\n\n## Data Collection and Privacy\n\nShinzo Labs collects limited anonymous telemetry from this server to help improve our products and services. No personally identifiable information is collected as part of this process. Please review the [Privacy Policy](./PRIVACY.md) for more details on the types of data collected and how to opt-out of this telemetry.\n\n## License\n\nMIT",
        "start_pos": 9226,
        "end_pos": 10766,
        "token_count_estimate": 384,
        "source_type": "readme",
        "agent_id": "4c97f052d3acad4b"
      }
    ]
  },
  {
    "agent_id": "dc63452df2296eda",
    "name": "ai.smithery/feeefapp-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@feeefapp/mcp/mcp",
    "description": "Enable AI assistants to interact seamlessly with Feeef e-commerce stores, products, and orders usi‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-03T01:55:09.587519Z",
    "indexed_at": "2026-02-18T04:06:42.197597",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Enable AI assistants to interact with Feeef e-commerce stores",
        "Enable AI assistants to interact with Feeef products",
        "Enable AI assistants to interact with Feeef orders"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's purpose but lacks detailed information, examples, or specific capabilities.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "8ff21bc38ffbe100",
    "name": "ai.smithery/fengyinxia-jimeng-mcp",
    "source": "mcp",
    "source_url": "https://github.com/fengyinxia/jimeng-mcp",
    "description": "Create images and videos from prompts, with options for image mixing, reference images, and start/‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-12T13:13:51.711543Z",
    "indexed_at": "2026-02-18T04:06:43.577958",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Jimeng MCP ÊúçÂä°Âô®\n\n\n‰ΩøÁî®TypeScriptÂÆûÁé∞ÁöÑModel Context Protocol (MCP) ÊúçÂä°Âô®È°πÁõÆÔºåÈõÜÊàê‰∫ÜÂç≥Ê¢¶AIÂõæÂÉèÁîüÊàêÊúçÂä°ÔºåÈÄöËøáÈÄÜÂêëÂ∑•Á®ãÁõ¥Êé•Ë∞ÉÁî®Âç≥Ê¢¶ÂÆòÊñπAPI„ÄÇ\n\n\n## ÂäüËÉΩ\n\n- Âü∫‰∫éTypeScriptÊûÑÂª∫\n- ‰ΩøÁî®tsup‰Ωú‰∏∫ÊûÑÂª∫Â∑•ÂÖ∑\n- ÂÆûÁé∞‰∫ÜMCPÂçèËÆÆÔºåÊîØÊåÅÊ†áÂáÜÁöÑstdioÈÄö‰ø°\n- Áõ¥Êé•Ë∞ÉÁî®Âç≥Ê¢¶AIÂõæÂÉèÁîüÊàêÊúçÂä°ÔºåÊó†ÈúÄÁ¨¨‰∏âÊñπAPI\n- Êèê‰æõÂ§öÁßçÂç≥Ê¢¶Ê®°ÂûãÁöÑÂõæÂÉèÁîüÊàêÂ∑•ÂÖ∑\n- ÊîØÊåÅÂ§öÁßçÂõæÂÉèÂèÇÊï∞Ë∞ÉÊï¥ÔºåÂ¶ÇÂ∞∫ÂØ∏„ÄÅÁ≤æÁªÜÂ∫¶„ÄÅË¥üÈù¢ÊèêÁ§∫ËØçÁ≠â\n- ÊîØÊåÅÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÔºàÈÄöËøáfilePathÂèÇÊï∞ÔºåÊîØÊåÅÊú¨Âú∞ÂõæÁâáÂíåÁΩëÁªúÂõæÁâáÔºâ\n- ÊîØÊåÅËßÜÈ¢ëÁîüÊàêÔºåÊîØÊåÅÊ∑ªÂä†ÂèÇËÄÉÂõæÁâáÔºàÈ¶ñÂ∞æÂ∏ßÈÄöËøáfilePathÂèÇÊï∞ËÆæÁΩÆÔºâ\n\n## ÂÆâË£Ö\n\n### ÈÄöËøáSmitheryÂÆâË£Ö\n\nË¶ÅÈÄöËøá [Smithery](https://smithery.ai/server/@c-rick/jimeng-mcp) Ëá™Âä®‰∏∫Claude DesktopÂÆâË£Öjimeng-mcpÔºåËØ∑ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\n\n```bash\nnpx -y @smithery/cli install @c-rick/jimeng-mcp --client claude\n```\n\n### ÊâãÂä®ÂÆâË£Ö\n```bash\n# ‰ΩøÁî®yarnÂÆâË£Ö‰æùËµñ\nyarn install\n\n# Êàñ‰ΩøÁî®npmÂÆâË£Ö‰æùËµñ\nnpm install\n```\n\n## ÁéØÂ¢ÉÈÖçÁΩÆ\n\nÂú®MCPÂÆ¢Êà∑Á´ØÈÖçÁΩÆÔºàÂ¶ÇClaude DesktopÔºâ‰∏≠ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÔºö\n\nËøõÂÖ•[SmitheryÊâòÁÆ°È°πÁõÆ](https://smithery.ai/server/@c-rick/jimeng-mcp)ÔºåÁÇπÂáªjson, Â°´ÂÖ•JIMENG_API_TOKENÔºå ÁÇπÂáªconnect, ÁîüÊàê‰∏ãÈù¢mcpServers config json\n\n```json\n{\n  \"mcpServers\": {\n    \"jimeng-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@c-rick/jimeng-mcp\",\n        \"--key\",\n        \"[SmitheryÁîüÊàê]\",\n        \"--profile\",\n        \"[SmitheryÁîüÊàê]\"\n      ]\n    }\n  }\n}\n```\n\n### Ëé∑ÂèñJIMENG_API_TOKEN\n\n1. ËÆøÈóÆ [Âç≥Ê¢¶AIÂÆòÁΩë](https://jimeng.jianying.com) Âπ∂ÁôªÂΩïË¥¶Âè∑\n2. ÊåâF12ÊâìÂºÄÊµèËßàÂô®ÂºÄÂèëËÄÖÂ∑•ÂÖ∑\n3. Âú®Application > Cookies‰∏≠ÊâæÂà∞`sessionid`ÁöÑÂÄº\n4. Â∞ÜÊâæÂà∞ÁöÑsessionidÂÄºÈÖçÁΩÆ‰∏∫JIMENG_API_TOKENÁéØÂ¢ÉÂèòÈáè\n\n## ÂºÄÂèë\n\n```bash\n# ÂºÄÂèëÊ®°ÂºèËøêË°å\nyarn dev\n\n# ‰ΩøÁî®nodemonÂºÄÂèëÂπ∂Ëá™Âä®ÈáçÂêØ\nyarn start:dev\n```\n\n## ÊûÑÂª∫\n\n```bash\n# ÊûÑÂª∫È°πÁõÆ\nyarn build\n```\n\n## ËøêË°å\n\n```bash\n# ÂêØÂä®ÊúçÂä°Âô®\nyarn start\n\n# ÊµãËØïMCPÊúçÂä°Âô®\nyarn test\n```\n\n## Claude Desktop ÈÖçÁΩÆÁ§∫‰æã\n\n‰ª•‰∏ãÊòØÂú®Claude Desktop‰∏≠ÈÖçÁΩÆÊ≠§MCPÊúçÂä°Âô®ÁöÑÂÆåÊï¥Á§∫‰æã:\n\n```json\n{\n  \"mcpServers\": {\n    \"jimeng\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/jimeng-mcp/lib/index.js\"],\n      \"env\": {\n        \"JIMENG_API_TOKEN\": \"your_jimeng_session_id_here\"\n      }\n    }\n  }\n}\n```\n\n## Âç≥Ê¢¶AIÂõæÂÉèÁîüÊàê\n\nÊú¨MCPÊúçÂä°Âô®Áõ¥Êé•Ë∞ÉÁî®Âç≥Ê¢¶AIÂõæÂÉèÁîüÊàêAPIÔºåÊèê‰æõÂõæÂÉèÁîüÊàêÂ∑•ÂÖ∑Ôºö\n\n`generateImage` - Êèê‰∫§ÂõæÂÉèÁîüÊàêËØ∑Ê±ÇÂπ∂ËøîÂõûÂõæÂÉèURLÂàóË°®\n- ÂèÇÊï∞Ôºö\n  - `prompt`ÔºöÁîüÊàêÂõæÂÉèÁöÑÊñáÊú¨ÊèèËø∞ÔºàÂøÖÂ°´Ôºâ\n  - `filePath`ÔºöÊú¨Âú∞ÂõæÁâáË∑ØÂæÑÊàñÂõæÁâáURLÔºàÂèØÈÄâÔºåËã•Â°´ÂÜôÂàô‰∏∫ÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÂäüËÉΩÔºâ\n  - `model`ÔºöÊ®°ÂûãÂêçÁß∞ÔºåÂèØÈÄâÂÄº: jimeng-3.0, jimeng-2.1, jimeng-2.0-pro, jimeng-2.0, jimeng-1.4, jimeng-xl-proÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫jimeng-2.1ÔºåÂõæÁâáÊ∑∑ÂêàÊó∂Ëá™Âä®ÂàáÊç¢‰∏∫jimeng-2.0-proÔºâ\n  - `width`ÔºöÂõæÂÉèÂÆΩÂ∫¶ÔºåÈªòËÆ§ÂÄºÔºö1024ÔºàÂèØÈÄâÔºâ\n  - `height`ÔºöÂõæÂÉèÈ´òÂ∫¶ÔºåÈªòËÆ§ÂÄºÔºö1024ÔºàÂèØÈÄâÔºâ\n  - `sample_strength`ÔºöÁ≤æÁªÜÂ∫¶ÔºåÈªòËÆ§ÂÄºÔºö0.5ÔºåËåÉÂõ¥0-1ÔºàÂèØÈÄâÔºâ\n  - `negative_prompt`ÔºöÂèçÂêëÊèêÁ§∫ËØçÔºåÂëäËØâÊ®°Âûã‰∏çË¶ÅÁîüÊàê‰ªÄ‰πàÂÜÖÂÆπÔºàÂèØÈÄâÔºâ\n\n> **Ê≥®ÊÑèÔºö**\n> - `filePath` ÊîØÊåÅÊú¨Âú∞ÁªùÂØπ/Áõ∏ÂØπË∑ØÂæÑÂíåÂõæÁâáURL„ÄÇ\n> - Ëã•ÊåáÂÆö `filePath`ÔºåÂ∞ÜËá™Âä®ËøõÂÖ•ÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÊ®°ÂºèÔºåÂ∫ïÂ±ÇÊ®°ÂûãËá™Âä®ÂàáÊç¢‰∏∫ `jimeng-2.0-pro`„ÄÇ\n> - ÁΩëÁªúÂõæÁâáÈúÄ‰øùËØÅÂèØÂÖ¨ÂºÄËÆøÈóÆ„ÄÇ\n\n### ÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÂäüËÉΩ\n\nÂ¶ÇÈúÄÂü∫‰∫éÂõæÁâáËøõË°åÊ∑∑ÂêàÁîüÊàêÔºåÂè™ÈúÄ‰º†ÂÖ•`filePath`ÂèÇÊï∞ÔºàÊîØÊåÅÊú¨Âú∞Ë∑ØÂæÑÊàñÂõæÁâáURLÔºâÔºåÂç≥ÂèØÂÆûÁé∞ÂõæÁâáÈ£éÊ†ºËûçÂêà„ÄÅÂèÇËÄÉÂõæÁîüÊàêÁ≠âÈ´òÁ∫ßÁé©Ê≥ï„ÄÇ\n\n#### Á§∫‰æãÔºö\n\n```javascript\n// ÂèÇËÄÉÂõæÁâáÊ∑∑ÂêàÁîüÊàê\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"Ê¢µÈ´òÈ£éÊ†ºÁöÑÁå´\",\n    filePath: \"./test.png\", // Êú¨Âú∞ÂõæÁâáË∑ØÂæÑ\n    sample_strength: 0.6\n  }\n});\n```\n\nÊàñ\n\n```javascript\n// ‰ΩøÁî®ÁΩëÁªúÂõæÁâá‰Ωú‰∏∫ÂèÇËÄÉ\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"Êú™Êù•ÂüéÂ∏Ç\",\n    filePath: \"https://example.com/your-image.png\"\n  }\n});\n```\n\n### ÊîØÊåÅÁöÑÊ®°Âûã\n\nÊúçÂä°Âô®ÊîØÊåÅ‰ª•‰∏ãÂç≥Ê¢¶AIÊ®°ÂûãÔºö\n\n- ÂõæÁâáÊ®°Âûã\n- `jimeng-3.1`ÔºöÂç≥Ê¢¶Á¨¨‰∏â‰ª£Ê®°ÂûãÔºå‰∏∞ÂØåÁöÑÁæéÂ≠¶Â§öÊ†∑ÊÄßÔºåÁîªÈù¢Êõ¥È≤úÊòéÁîüÂä® ÔºàÈªòËÆ§Ôºâ\n- `jimeng-3.0`ÔºöÂç≥Ê¢¶Á¨¨‰∏â‰ª£Ê®°ÂûãÔºåÊïàÊûúÊõ¥Â•ΩÔºåÊîØÊåÅÊõ¥Âº∫ÁöÑÂõæÂÉèÁîüÊàêËÉΩÂäõ\n- `jimeng-2.1`ÔºöÂç≥Ê¢¶2.1ÁâàÊú¨Ê®°ÂûãÔºåÈªòËÆ§Ê®°Âûã\n- `jimeng-2.0-pro`ÔºöÂç≥Ê¢¶2.0 ProÁâàÊú¨\n- `jimeng-2.0`ÔºöÂç≥Ê¢¶2.0Ê†áÂáÜÁâàÊú¨\n- `jimeng-1.4`ÔºöÂç≥Ê¢¶1.4ÁâàÊú¨\n- `jimeng-xl-pro`ÔºöÂç≥Ê¢¶XL ProÁâπÊÆäÁâàÊú¨\n- ËßÜÈ¢ëÊ®°Âûã\n- `jimeng-video-3.0-pro`ÔºöÂç≥Ê¢¶ËßÜÈ¢ë3.0 ProÊ®°ÂûãÔºåÈÄÇÂêàÈ´òË¥®ÈáèËßÜÈ¢ëÁîüÊàê\n- `jimeng-video-3.0`ÔºöÂç≥Ê¢¶ËßÜÈ¢ë3.0Ê†áÂáÜÊ®°ÂûãÔºå‰∏ªÂäõËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºàÈªòËÆ§Ôºâ\n- `jimeng-video-2.0-pro`ÔºöÂç≥Ê¢¶ËßÜÈ¢ë2.0 ProÊ®°ÂûãÔºåÂÖºÂÆπÊÄßÂ•ΩÔºåÈÄÇÂêàÂ§öÂú∫ÊôØ\n- `jimeng-video-2.0`ÔºöÂç≥Ê¢¶ËßÜÈ¢ë2.0Ê†áÂáÜÊ®°ÂûãÔºåÈÄÇÂêàÂü∫Á°ÄËßÜÈ¢ëÁîüÊàê\n\n### ÊäÄÊúØÂÆûÁé∞\n\n- Áõ¥Êé•Ë∞ÉÁî®Âç≥Ê¢¶ÂÆòÊñπAPIÔºåÊó†ÈúÄÁ¨¨‰∏âÊñπÊúçÂä°\n- ÈÄÜÂêëÂ∑•Á®ãAPIË∞ÉÁî®ÊµÅÁ®ãÔºåÂÆûÁé∞ÂÆåÊï¥ÁöÑÂõæÂÉèÁîüÊàêËøáÁ®ã\n- ÊîØÊåÅÁßØÂàÜËá™Âä®È¢ÜÂèñÂíå‰ΩøÁî®\n- Âü∫‰∫éÈù¢ÂêëÂØπË±°ËÆæËÆ°ÔºåÂ∞ÜAPIÂÆûÁé∞Â∞ÅË£Ö‰∏∫Á±ª\n- ËøîÂõûÈ´òË¥®ÈáèÂõæÂÉèURLÂàóË°®\n- ÊîØÊåÅÂõæÁâá‰∏ä‰º†ÔºåËá™Âä®Â§ÑÁêÜÊú¨Âú∞/ÁΩëÁªúÂõæÁâáÔºåËá™Âä®ÂàáÊç¢Ê∑∑ÂêàÊ®°Âûã\n- ÂõæÁâáÊ∑∑ÂêàÊó∂Ëá™Âä®‰∏ä‰º†ÂõæÁâáÂà∞Âç≥Ê¢¶‰∫ëÁ´ØÔºåÊµÅÁ®ãÂÖ®Ëá™Âä®\n\n### ‰ΩøÁî®Á§∫‰æã\n\nÈÄöËøáMCPÂçèËÆÆË∞ÉÁî®ÂõæÂÉèÁîüÊàêÂäüËÉΩÔºö\n\n```javascript\n// ÁîüÊàêÂõæÂÉèÔºàÊñáÊú¨ÁîüÊàêÔºâ\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"‰∏ÄÂè™ÂèØÁà±ÁöÑÁå´Âí™Âú®ËçâÂú∞‰∏ä\",\n    model: \"jimeng-3.0\",\n    width: 1024,\n    height: 1024,\n    sample_strength: 0.7,\n    negative_prompt: \"Ê®°Á≥äÔºåÊâ≠Êõ≤Ôºå‰ΩéË¥®Èáè\"\n  }\n});\n\n// ÁîüÊàêÂõæÂÉèÔºàÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÔºâ\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"Êú™Êù•ÂüéÂ∏Ç\",\n    filePath: \"https://example.com/your-image.png\"\n  }\n});\n```\n\n## ÂìçÂ∫îÊ†ºÂºè\n\nAPIÂ∞ÜËøîÂõûÁîüÊàêÁöÑÂõæÂÉèURLÊï∞ÁªÑÔºåÂèØ‰ª•Áõ¥Êé•Âú®ÂêÑÁ±ªÂÆ¢Êà∑Á´Ø‰∏≠ÊòæÁ§∫Ôºö\n\n```javascript\n[\n  \"https://example.com/generated-image-1.jpg\",\n  \"https://example.com/generated-image-2.jpg\",\n  \"https://example.com/generated-image-3.jpg\",\n  \"https://example.com/generated-image-4.jpg\"\n]\n```\n\n## ËµÑÊ∫ê\n\nÊúçÂä°Âô®ËøòÊèê‰æõ‰∫Ü‰ª•‰∏ã‰ø°ÊÅØËµÑÊ∫êÔºö\n\n- `greeting://{name}` - Êèê‰æõ‰∏™ÊÄßÂåñÈóÆÂÄô\n- `info://server` - Êèê‰æõÊúçÂä°Âô®Âü∫Êú¨‰ø°ÊÅØ\n- `jimeng-ai://info` - Êèê‰æõÂç≥Ê¢¶AIÂõæÂÉèÁîüÊàêÊúçÂä°ÁöÑ‰ΩøÁî®ËØ¥Êòé\n\n## CursorÊàñClaude‰ΩøÁî®ÊèêÁ§∫\n\nÂú®CursorÊàñClaude‰∏≠Ôºå‰Ω†ÂèØ‰ª•ËøôÊ†∑‰ΩøÁî®JimengÂõæÂÉèÁîüÊàêÊúçÂä°Ôºö\n\n1. Á°Æ‰øùÂ∑≤ÁªèÈÖçÁΩÆ‰∫ÜMCPÊúçÂä°Âô®\n2. ÊèêÁ§∫Claude/CursorÁîüÊàêÂõæÂÉèÔºå‰æãÂ¶ÇÔºö\n   ```\n   ËØ∑ÁîüÊàê‰∏ÄÂº†ÂÜôÂÆûÈ£éÊ†ºÁöÑÊó•ËêΩ‰∏ãÁöÑÂ±±ËÑâÂõæÁâá\n   ```\n3. Claude/Cursor‰ºöË∞ÉÁî®Jimeng MCPÊúçÂä°Âô®ÁîüÊàêÂõæÂÉèÂπ∂ÊòæÁ§∫\n\n## Â∏∏ËßÅÈóÆÈ¢ò\n\n1. **ÂõæÂÉèÁîüÊàêÂ§±Ë¥•**\n   - Ê£ÄÊü•JIMENG_API_TOKENÊòØÂê¶Ê≠£Á°ÆÈÖçÁΩÆ\n   - ÁôªÂΩïÂç≥Ê¢¶ÂÆòÁΩëÊ£ÄÊü•Ë¥¶Âè∑ÁßØÂàÜÊòØÂê¶ÂÖÖË∂≥\n   - Â∞ùËØïÊõ¥Êç¢ÊèêÁ§∫ËØçÔºåÈÅøÂÖçÊïèÊÑüÂÜÖÂÆπ\n   - Ëã•‰∏∫ÂõæÁâáÊ∑∑ÂêàÔºåÊ£ÄÊü•filePathË∑ØÂæÑ/URLÊòØÂê¶ÊúâÊïà„ÄÅÂõæÁâáÊòØÂê¶ÂèØËÆøÈóÆ\n   - ÁΩëÁªúÂõæÁâáÂª∫ËÆÆ‰ΩøÁî®httpsÁõ¥ÈìæÔºåÈÅøÂÖçÈò≤ÁõóÈìæ/ÊùÉÈôêÈóÆÈ¢ò\n\n2. **ÊúçÂä°Âô®Êó†Ê≥ïÂêØÂä®**\n   - Á°Æ‰øùÂ∑≤ÂÆâË£ÖÊâÄÊúâ‰æùËµñ\n   - Á°Æ‰øùÁéØÂ¢ÉÂèòÈáèÊ≠£Á°ÆËÆæÁΩÆ\n   - Ê£ÄÊü•Node.jsÁâàÊú¨ÊòØÂê¶‰∏∫14.0ÊàñÊõ¥È´ò\n\n## ËÆ∏ÂèØËØÅ\n\nMIT \n\n## Âç≥Ê¢¶AIËßÜÈ¢ëÁîüÊàê\n\nÊú¨MCPÊúçÂä°Âô®ÈõÜÊàê‰∫ÜÂç≥Ê¢¶AIËßÜÈ¢ëÁîüÊàêAPIÔºåÊèê‰æõËßÜÈ¢ëÁîüÊàêÂ∑•ÂÖ∑Ôºö\n\n`generateVideo` - Êèê‰∫§ËßÜÈ¢ëÁîüÊàêËØ∑Ê±ÇÂπ∂ËøîÂõûËßÜÈ¢ëURL\n- ÂèÇÊï∞Ôºö\n  - `prompt`ÔºöÁîüÊàêËßÜÈ¢ëÁöÑÊñáÊú¨ÊèèËø∞ÔºàÂøÖÂ°´Ôºâ\n  - `filePath`ÔºöÈ¶ñÂ∏ßÂíåÂ∞æÂ∏ßÂõæÁâáË∑ØÂæÑÔºåÊîØÊåÅÊï∞ÁªÑÔºåÊúÄÂ§ö2‰∏™ÂÖÉÁ¥†ÔºåÂàÜÂà´‰∏∫È¶ñÂ∏ßÂíåÂ∞æÂ∏ßÔºàÂèØÈÄâÔºâ\n  - `model`ÔºöÊ®°ÂûãÂêçÁß∞ÔºåÈªòËÆ§jimeng-video-3.0ÔºàÂèØÈÄâÔºâ\n  - `resolution`ÔºöÂàÜËæ®ÁéáÔºåÂèØÈÄâ720pÊàñ1080pÔºåÈªòËÆ§720pÔºàÂèØÈÄâÔºâ\n  - `width`ÔºöËßÜÈ¢ëÂÆΩÂ∫¶ÔºåÈªòËÆ§ÂÄºÔºö1024ÔºàÂèØÈÄâÔºâ\n  - `height`ÔºöËßÜÈ¢ëÈ´òÂ∫¶ÔºåÈªòËÆ§ÂÄºÔºö1024ÔºàÂèØÈÄâÔºâ\n  - `refresh_token`ÔºöÂç≥Ê¢¶API‰ª§ÁâåÔºàÂèØÈÄâÔºåÈÄöÂ∏∏‰ªéÁéØÂ¢ÉÂèòÈáèËØªÂèñÔºâ\n  - `req_key`ÔºöËá™ÂÆö‰πâÂèÇÊï∞ÔºåÂÖºÂÆπÊóßÊé•Âè£ÔºàÂèØÈÄâÔºâ\n\n> **Ê≥®ÊÑèÔºö**\n> - `filePath` ÊîØÊåÅÊú¨Âú∞ÁªùÂØπ/Áõ∏ÂØπË∑ØÂæÑÂíåÂõæÁâáURL„ÄÇ\n> - Ëã•ÊåáÂÆö `filePath`ÔºåÂèØÂÆûÁé∞È¶ñÂ∏ß/Â∞æÂ∏ßÂÆöÂà∂ÁöÑËßÜÈ¢ëÁîüÊàê„ÄÇ\n> - ÁΩëÁªúÂõæÁâáÈúÄ‰øùËØÅÂèØÂÖ¨ÂºÄËÆøÈóÆ„ÄÇ\n\n### ‰ΩøÁî®Á§∫‰æã\n\nÈÄöËøáMCPÂçèËÆÆË∞ÉÁî®ËßÜÈ¢ëÁîüÊàêÂäüËÉΩÔºö\n\n```javascript\n// ÁîüÊàêËßÜÈ¢ëÔºàÊñáÊú¨ÁîüÊàêÔºâ\nclient.callTool({\n  name: \"generateVideo\",\n  arguments: {\n    prompt: \"‰∏ÄÂè™Â∞èÁãóÂú®ËçâÂú∞‰∏äÂ•îË∑ëÔºåÈò≥ÂÖâÊòéÂ™öÔºåÈ´òÊ∏Ö\",\n    model: \"jimeng-video-3.0\",\n    resolution: \"720p\",\n    width: 1024,\n    height: 1024\n  }\n});\n\n// ÁîüÊàêËßÜÈ¢ëÔºàÈ¶ñÂ∏ß/Â∞æÂ∏ßÂÆöÂà∂Ôºâ\nclient.callTool({\n  name: \"generateVideo\",\n  arguments: {\n    prompt: \"ÂüéÂ∏ÇÂ§úÊôØÂª∂Êó∂ÊëÑÂΩ±\",\n    filePath: [\"./first.png\", \"./last.png\"],\n    resolution: \"1080p\"\n  }\n});\n```\n\n## ËßÜÈ¢ëÂìçÂ∫îÊ†ºÂºè\n\nAPIÂ∞ÜËøîÂõûÁîüÊàêÁöÑËßÜÈ¢ëURLÂ≠óÁ¨¶‰∏≤ÔºåÂèØ‰ª•Áõ¥Êé•Âú®ÂêÑÁ±ªÂÆ¢Êà∑Á´Ø‰∏≠Êí≠ÊîæÔºö\n\n```javascript\n\"https://example.com/generated-video.mp4\"\n``` \n\n\n## ÊîØÊåÅapiÊúçÂä°ÂêØÂä®\n\nÂ¶ÇÈúÄ‰ª•APIÊúçÂä°ÊñπÂºèÂêØÂä®ÔºàÈÄÇÂêàHTTPÊé•Âè£Ë∞ÉÁî®ÔºâÔºö\n\n```bash\ncp .env.example .env   # Â§çÂà∂ÁéØÂ¢ÉÂèòÈáèÊ®°Êùø\n# Ê†πÊçÆÈúÄË¶ÅÁºñËæë.envÔºåÂ°´ÂÜôJIMENG_API_TOKENÁ≠âÈÖçÁΩÆ\n\n# ÂêØÂä®APIÊúçÂä°\nyarn start:api\n```\n\nAPIÊúçÂä°ÂêØÂä®ÂêéÂ∞ÜÁõëÂê¨ÈÖçÁΩÆÁ´ØÂè£ÔºåÊîØÊåÅÈÄöËøáHTTPÊé•Âè£Ë∞ÉÁî®Âç≥Ê¢¶AIÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÂäüËÉΩ„ÄÇ \n"
    },
    "llm_extracted": {
      "capabilities": [
        "Implement the Model Context Protocol (MCP) with standard stdio communication",
        "Generate images using multiple Jimeng AI models with customizable parameters",
        "Perform image mixing and reference image generation using local or network images",
        "Generate videos with optional first and last frame customization",
        "Directly call the official Jimeng AI API without third-party services",
        "Support automatic image upload and model switching for image mixing",
        "Provide an HTTP API service mode for image and video generation",
        "Return generated image URLs as arrays and video URLs as strings for client display",
        "Support environment variable configuration for authentication tokens"
      ],
      "limitations": [
        "Requires valid JIMENG_API_TOKEN extracted from Jimeng AI official website session cookies",
        "Network images used as references must be publicly accessible to avoid access issues",
        "Node.js version must be 14.0 or higher to run the server",
        "Image mixing mode automatically switches to jimeng-2.0-pro model, limiting model choice",
        "Video generation supports only up to two reference images for first and last frames",
        "Potential failures if account lacks sufficient credits or if prompts contain sensitive content"
      ],
      "requirements": [
        "Node.js version 14.0 or higher installed",
        "JIMENG_API_TOKEN environment variable set with valid sessionid from Jimeng AI website",
        "Access to Jimeng AI official website account to obtain API token",
        "Yarn or npm for dependency installation and project build",
        "Optional Smithery CLI for automated installation and configuration",
        "Network access for uploading images and calling Jimeng AI APIs"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for image and video generation, environment setup including token acquisition, supported models, API response formats, limitations, and development/build commands.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Jimeng MCP ÊúçÂä°Âô®\n\n\n‰ΩøÁî®TypeScriptÂÆûÁé∞ÁöÑModel Context Protocol (MCP) ÊúçÂä°Âô®È°πÁõÆÔºåÈõÜÊàê‰∫ÜÂç≥Ê¢¶AIÂõæÂÉèÁîüÊàêÊúçÂä°ÔºåÈÄöËøáÈÄÜÂêëÂ∑•Á®ãÁõ¥Êé•Ë∞ÉÁî®Âç≥Ê¢¶ÂÆòÊñπAPI„ÄÇ\n\n\n## ÂäüËÉΩ\n\n- Âü∫‰∫éTypeScriptÊûÑÂª∫\n- ‰ΩøÁî®tsup‰Ωú‰∏∫ÊûÑÂª∫Â∑•ÂÖ∑\n- ÂÆûÁé∞‰∫ÜMCPÂçèËÆÆÔºåÊîØÊåÅÊ†áÂáÜÁöÑstdioÈÄö‰ø°\n- Áõ¥Êé•Ë∞ÉÁî®Âç≥Ê¢¶AIÂõæÂÉèÁîüÊàêÊúçÂä°ÔºåÊó†ÈúÄÁ¨¨‰∏âÊñπAPI\n- Êèê‰æõÂ§öÁßçÂç≥Ê¢¶Ê®°ÂûãÁöÑÂõæÂÉèÁîüÊàêÂ∑•ÂÖ∑\n- ÊîØÊåÅÂ§öÁßçÂõæÂÉèÂèÇÊï∞Ë∞ÉÊï¥ÔºåÂ¶ÇÂ∞∫ÂØ∏„ÄÅÁ≤æÁªÜÂ∫¶„ÄÅË¥üÈù¢ÊèêÁ§∫ËØçÁ≠â\n- ÊîØÊåÅÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÔºàÈÄöËøáfilePathÂèÇÊï∞ÔºåÊîØÊåÅÊú¨Âú∞ÂõæÁâáÂíåÁΩëÁªúÂõæÁâáÔºâ\n- ÊîØÊåÅËßÜÈ¢ëÁîüÊàêÔºåÊîØÊåÅÊ∑ªÂä†ÂèÇËÄÉÂõæÁâáÔºàÈ¶ñÂ∞æÂ∏ßÈÄöËøáfilePathÂèÇÊï∞ËÆæÁΩÆÔºâ\n\n## ÂÆâË£Ö\n\n### ÈÄöËøáSmitheryÂÆâË£Ö\n\nË¶ÅÈÄöËøá [Smithery](https://smithery.ai/server/@c-rick/jimeng-mcp) Ëá™Âä®‰∏∫Claude DesktopÂÆâË£Öjimeng-mcpÔºåËØ∑ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö\n\n```bash\nnpx -y @smithery/cli install @c-rick/jimeng-mcp --client claude\n```\n\n### ÊâãÂä®ÂÆâË£Ö\n```bash\n# ‰ΩøÁî®yarnÂÆâË£Ö‰æùËµñ\nyarn install\n\n# Êàñ‰ΩøÁî®npmÂÆâË£Ö‰æùËµñ\nnpm install\n```\n\n## ÁéØÂ¢ÉÈÖçÁΩÆ\n\nÂú®MCPÂÆ¢Êà∑Á´ØÈÖçÁΩÆÔºàÂ¶ÇClaude DesktopÔºâ‰∏≠ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÔºö\n\nËøõÂÖ•[SmitheryÊâòÁÆ°È°πÁõÆ](https://smithery.ai/server/@c-rick/jimeng-mcp)ÔºåÁÇπÂáªjson, Â°´ÂÖ•JIMENG_API_TOKENÔºå ÁÇπÂáªconnect, ÁîüÊàê‰∏ãÈù¢mcpServers config json\n\n```json\n{\n  \"mcpServers\": {\n    \"jimeng-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@smithery/cli@latest\",\n        \"run\",\n        \"@c-rick/jimeng-mcp\",\n        \"--key\",\n        \"[SmitheryÁîüÊàê]\",\n        \"--profile\",\n        \"[SmitheryÁîüÊàê]\"\n      ]\n    }\n  }\n}\n```\n\n### Ëé∑ÂèñJIMENG_API_TOKEN\n\n1. ËÆøÈóÆ [Âç≥Ê¢¶AIÂÆòÁΩë](https://jimeng.jianying.com) Âπ∂ÁôªÂΩïË¥¶Âè∑\n2. ÊåâF12ÊâìÂºÄÊµèËßàÂô®ÂºÄÂèëËÄÖÂ∑•ÂÖ∑\n3. Âú®Application > Cookies‰∏≠ÊâæÂà∞`sessionid`ÁöÑÂÄº\n4.",
        "start_pos": 0,
        "end_pos": 1197,
        "token_count_estimate": 299,
        "source_type": "readme",
        "agent_id": "8ff21bc38ffbe100"
      },
      {
        "chunk_id": 1,
        "text": "ÂõæÁîüÊàêÂäüËÉΩÔºâ\n  - `model`ÔºöÊ®°ÂûãÂêçÁß∞ÔºåÂèØÈÄâÂÄº: jimeng-3.0, jimeng-2.1, jimeng-2.0-pro, jimeng-2.0, jimeng-1.4, jimeng-xl-proÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫jimeng-2.1ÔºåÂõæÁâáÊ∑∑ÂêàÊó∂Ëá™Âä®ÂàáÊç¢‰∏∫jimeng-2.0-proÔºâ\n  - `width`ÔºöÂõæÂÉèÂÆΩÂ∫¶ÔºåÈªòËÆ§ÂÄºÔºö1024ÔºàÂèØÈÄâÔºâ\n  - `height`ÔºöÂõæÂÉèÈ´òÂ∫¶ÔºåÈªòËÆ§ÂÄºÔºö1024ÔºàÂèØÈÄâÔºâ\n  - `sample_strength`ÔºöÁ≤æÁªÜÂ∫¶ÔºåÈªòËÆ§ÂÄºÔºö0.5ÔºåËåÉÂõ¥0-1ÔºàÂèØÈÄâÔºâ\n  - `negative_prompt`ÔºöÂèçÂêëÊèêÁ§∫ËØçÔºåÂëäËØâÊ®°Âûã‰∏çË¶ÅÁîüÊàê‰ªÄ‰πàÂÜÖÂÆπÔºàÂèØÈÄâÔºâ\n\n> **Ê≥®ÊÑèÔºö**\n> - `filePath` ÊîØÊåÅÊú¨Âú∞ÁªùÂØπ/Áõ∏ÂØπË∑ØÂæÑÂíåÂõæÁâáURL„ÄÇ\n> - Ëã•ÊåáÂÆö `filePath`ÔºåÂ∞ÜËá™Âä®ËøõÂÖ•ÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÊ®°ÂºèÔºåÂ∫ïÂ±ÇÊ®°ÂûãËá™Âä®ÂàáÊç¢‰∏∫ `jimeng-2.0-pro`„ÄÇ\n> - ÁΩëÁªúÂõæÁâáÈúÄ‰øùËØÅÂèØÂÖ¨ÂºÄËÆøÈóÆ„ÄÇ\n\n### ÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÂäüËÉΩ\n\nÂ¶ÇÈúÄÂü∫‰∫éÂõæÁâáËøõË°åÊ∑∑ÂêàÁîüÊàêÔºåÂè™ÈúÄ‰º†ÂÖ•`filePath`ÂèÇÊï∞ÔºàÊîØÊåÅÊú¨Âú∞Ë∑ØÂæÑÊàñÂõæÁâáURLÔºâÔºåÂç≥ÂèØÂÆûÁé∞ÂõæÁâáÈ£éÊ†ºËûçÂêà„ÄÅÂèÇËÄÉÂõæÁîüÊàêÁ≠âÈ´òÁ∫ßÁé©Ê≥ï„ÄÇ\n\n#### Á§∫‰æãÔºö\n\n```javascript\n// ÂèÇËÄÉÂõæÁâáÊ∑∑ÂêàÁîüÊàê\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"Ê¢µÈ´òÈ£éÊ†ºÁöÑÁå´\",\n    filePath: \"./test.png\", // Êú¨Âú∞ÂõæÁâáË∑ØÂæÑ\n    sample_strength: 0.6\n  }\n});\n```\n\nÊàñ\n\n```javascript\n// ‰ΩøÁî®ÁΩëÁªúÂõæÁâá‰Ωú‰∏∫ÂèÇËÄÉ\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"Êú™Êù•ÂüéÂ∏Ç\",\n    filePath: \"https://example.com/your-image.png\"\n  }\n});\n```\n\n### ÊîØÊåÅÁöÑÊ®°Âûã\n\nÊúçÂä°Âô®ÊîØÊåÅ‰ª•‰∏ãÂç≥Ê¢¶AIÊ®°ÂûãÔºö\n\n- ÂõæÁâáÊ®°Âûã\n- `jimeng-3.1`ÔºöÂç≥Ê¢¶Á¨¨‰∏â‰ª£Ê®°ÂûãÔºå‰∏∞ÂØåÁöÑÁæéÂ≠¶Â§öÊ†∑ÊÄßÔºåÁîªÈù¢Êõ¥È≤úÊòéÁîüÂä® ÔºàÈªòËÆ§Ôºâ\n- `jimeng-3.0`ÔºöÂç≥Ê¢¶Á¨¨‰∏â‰ª£Ê®°ÂûãÔºåÊïàÊûúÊõ¥Â•ΩÔºåÊîØÊåÅÊõ¥Âº∫ÁöÑÂõæÂÉèÁîüÊàêËÉΩÂäõ\n- `jimeng-2.1`ÔºöÂç≥Ê¢¶2.1ÁâàÊú¨Ê®°ÂûãÔºåÈªòËÆ§Ê®°Âûã\n- `jimeng-2.0-pro`ÔºöÂç≥Ê¢¶2.0 ProÁâàÊú¨\n- `jimeng-2.0`ÔºöÂç≥Ê¢¶2.0Ê†áÂáÜÁâàÊú¨\n- `jimeng-1.4`ÔºöÂç≥Ê¢¶1.4ÁâàÊú¨\n- `jimeng-xl-pro`ÔºöÂç≥Ê¢¶XL ProÁâπÊÆäÁâàÊú¨\n- ËßÜÈ¢ëÊ®°Âûã\n- `jimeng-video-3.0-pro`ÔºöÂç≥Ê¢¶ËßÜÈ¢ë3.0 ProÊ®°ÂûãÔºåÈÄÇÂêàÈ´òË¥®ÈáèËßÜÈ¢ëÁîüÊàê\n- `jimeng-video-3.0`ÔºöÂç≥Ê¢¶ËßÜÈ¢ë3.0Ê†áÂáÜÊ®°ÂûãÔºå‰∏ªÂäõËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÔºàÈªòËÆ§Ôºâ\n- `jimeng-video-2.0-pro`ÔºöÂç≥Ê¢¶ËßÜÈ¢ë2.0 ProÊ®°ÂûãÔºåÂÖºÂÆπÊÄßÂ•ΩÔºåÈÄÇÂêàÂ§öÂú∫ÊôØ\n- `jimeng-video-2.0`ÔºöÂç≥Ê¢¶ËßÜÈ¢ë2.0Ê†áÂáÜÊ®°ÂûãÔºåÈÄÇÂêàÂü∫Á°ÄËßÜÈ¢ëÁîüÊàê\n\n### ÊäÄÊúØÂÆûÁé∞\n\n- Áõ¥Êé•Ë∞ÉÁî®Âç≥Ê¢¶ÂÆòÊñπAPIÔºåÊó†ÈúÄÁ¨¨‰∏âÊñπÊúçÂä°\n- ÈÄÜÂêëÂ∑•Á®ãAPIË∞ÉÁî®ÊµÅÁ®ãÔºåÂÆûÁé∞ÂÆåÊï¥ÁöÑÂõæÂÉèÁîüÊàêËøáÁ®ã\n- ÊîØÊåÅÁßØÂàÜËá™Âä®È¢ÜÂèñÂíå‰ΩøÁî®\n- Âü∫‰∫éÈù¢ÂêëÂØπË±°ËÆæËÆ°ÔºåÂ∞ÜAPIÂÆûÁé∞Â∞ÅË£Ö‰∏∫Á±ª\n- ËøîÂõûÈ´òË¥®ÈáèÂõæÂÉèURLÂàóË°®\n- ÊîØÊåÅÂõæÁâá‰∏ä‰º†ÔºåËá™Âä®Â§ÑÁêÜÊú¨Âú∞/ÁΩëÁªúÂõæÁâáÔºåËá™Âä®ÂàáÊç¢Ê∑∑ÂêàÊ®°Âûã\n- ÂõæÁâáÊ∑∑ÂêàÊó∂Ëá™Âä®‰∏ä‰º†ÂõæÁâáÂà∞Âç≥Ê¢¶‰∫ëÁ´ØÔºåÊµÅÁ®ãÂÖ®Ëá™Âä®\n\n### ‰ΩøÁî®Á§∫‰æã\n\nÈÄöËøáMCPÂçèËÆÆË∞ÉÁî®ÂõæÂÉèÁîüÊàêÂäüËÉΩÔºö\n\n```javascript\n// ÁîüÊàêÂõæÂÉèÔºàÊñáÊú¨ÁîüÊàêÔºâ\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"‰∏ÄÂè™ÂèØÁà±ÁöÑÁå´Âí™Âú®ËçâÂú∞‰∏ä\",\n    model: \"jimeng-3.0\",\n    width: 1024,\n    height: 1024,\n    sample_strength: 0.7,\n    negative_prompt: \"Ê®°Á≥äÔºåÊâ≠Êõ≤Ôºå‰ΩéË¥®Èáè\"\n  }\n});\n\n// ÁîüÊàêÂõæÂÉèÔºàÂõæÁâáÊ∑∑Âêà/ÂèÇËÄÉÂõæÁîüÊàêÔºâ\nclient.callTool({\n  name: \"generateImage\",\n  arguments: {\n    prompt: \"Êú™Êù•ÂüéÂ∏Ç\",\n    filePath: \"https://example.com/your-image.png\"\n  }\n});\n```\n\n## ÂìçÂ∫îÊ†ºÂºè\n\nAPIÂ∞ÜËøîÂõûÁîüÊàêÁöÑÂõæÂÉèURLÊï∞ÁªÑÔºåÂèØ‰ª•Áõ¥Êé•Âú®ÂêÑÁ±ªÂÆ¢Êà∑Á´Ø‰∏≠ÊòæÁ§∫Ôºö\n\n```javascript\n[\n  \"https://example.com/generated-image-1.jpg\",\n  \"https://ex",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "8ff21bc38ffbe100"
      },
      {
        "chunk_id": 2,
        "text": "prompt: \"Êú™Êù•ÂüéÂ∏Ç\",\n    filePath: \"https://example.com/your-image.png\"\n  }\n});\n```\n\n## ÂìçÂ∫îÊ†ºÂºè\n\nAPIÂ∞ÜËøîÂõûÁîüÊàêÁöÑÂõæÂÉèURLÊï∞ÁªÑÔºåÂèØ‰ª•Áõ¥Êé•Âú®ÂêÑÁ±ªÂÆ¢Êà∑Á´Ø‰∏≠ÊòæÁ§∫Ôºö\n\n```javascript\n[\n  \"https://example.com/generated-image-1.jpg\",\n  \"https://example.com/generated-image-2.jpg\",\n  \"https://example.com/generated-image-3.jpg\",\n  \"https://example.com/generated-image-4.jpg\"\n]\n```\n\n## ËµÑÊ∫ê\n\nÊúçÂä°Âô®ËøòÊèê‰æõ‰∫Ü‰ª•‰∏ã‰ø°ÊÅØËµÑÊ∫êÔºö\n\n- `greeting://{name}` - Êèê‰æõ‰∏™ÊÄßÂåñÈóÆÂÄô\n- `info://server` - Êèê‰æõÊúçÂä°Âô®Âü∫Êú¨‰ø°ÊÅØ\n- `jimeng-ai://info` - Êèê‰æõÂç≥Ê¢¶AIÂõæÂÉèÁîüÊàêÊúçÂä°ÁöÑ‰ΩøÁî®ËØ¥Êòé\n\n## CursorÊàñClaude‰ΩøÁî®ÊèêÁ§∫\n\nÂú®CursorÊàñClaude‰∏≠Ôºå‰Ω†ÂèØ‰ª•ËøôÊ†∑‰ΩøÁî®JimengÂõæÂÉèÁîüÊàêÊúçÂä°Ôºö\n\n1. Á°Æ‰øùÂ∑≤ÁªèÈÖçÁΩÆ‰∫ÜMCPÊúçÂä°Âô®\n2. ÊèêÁ§∫Claude/CursorÁîüÊàêÂõæÂÉèÔºå‰æãÂ¶ÇÔºö\n   ```\n   ËØ∑ÁîüÊàê‰∏ÄÂº†ÂÜôÂÆûÈ£éÊ†ºÁöÑÊó•ËêΩ‰∏ãÁöÑÂ±±ËÑâÂõæÁâá\n   ```\n3. Claude/Cursor‰ºöË∞ÉÁî®Jimeng MCPÊúçÂä°Âô®ÁîüÊàêÂõæÂÉèÂπ∂ÊòæÁ§∫\n\n## Â∏∏ËßÅÈóÆÈ¢ò\n\n1. **ÂõæÂÉèÁîüÊàêÂ§±Ë¥•**\n   - Ê£ÄÊü•JIMENG_API_TOKENÊòØÂê¶Ê≠£Á°ÆÈÖçÁΩÆ\n   - ÁôªÂΩïÂç≥Ê¢¶ÂÆòÁΩëÊ£ÄÊü•Ë¥¶Âè∑ÁßØÂàÜÊòØÂê¶ÂÖÖË∂≥\n   - Â∞ùËØïÊõ¥Êç¢ÊèêÁ§∫ËØçÔºåÈÅøÂÖçÊïèÊÑüÂÜÖÂÆπ\n   - Ëã•‰∏∫ÂõæÁâáÊ∑∑ÂêàÔºåÊ£ÄÊü•filePathË∑ØÂæÑ/URLÊòØÂê¶ÊúâÊïà„ÄÅÂõæÁâáÊòØÂê¶ÂèØËÆøÈóÆ\n   - ÁΩëÁªúÂõæÁâáÂª∫ËÆÆ‰ΩøÁî®httpsÁõ¥ÈìæÔºåÈÅøÂÖçÈò≤ÁõóÈìæ/ÊùÉÈôêÈóÆÈ¢ò\n\n2.",
        "start_pos": 3696,
        "end_pos": 4509,
        "token_count_estimate": 202,
        "source_type": "readme",
        "agent_id": "8ff21bc38ffbe100"
      },
      {
        "chunk_id": 3,
        "text": "Êí≠ÊîæÔºö\n\n```javascript\n\"https://example.com/generated-video.mp4\"\n``` \n\n\n## ÊîØÊåÅapiÊúçÂä°ÂêØÂä®\n\nÂ¶ÇÈúÄ‰ª•APIÊúçÂä°ÊñπÂºèÂêØÂä®ÔºàÈÄÇÂêàHTTPÊé•Âè£Ë∞ÉÁî®ÔºâÔºö\n\n```bash\ncp .env.example .env   # Â§çÂà∂ÁéØÂ¢ÉÂèòÈáèÊ®°Êùø\n# Ê†πÊçÆÈúÄË¶ÅÁºñËæë.envÔºåÂ°´ÂÜôJIMENG_API_TOKENÁ≠âÈÖçÁΩÆ\n\n# ÂêØÂä®APIÊúçÂä°\nyarn start:api\n```\n\nAPIÊúçÂä°ÂêØÂä®ÂêéÂ∞ÜÁõëÂê¨ÈÖçÁΩÆÁ´ØÂè£ÔºåÊîØÊåÅÈÄöËøáHTTPÊé•Âè£Ë∞ÉÁî®Âç≥Ê¢¶AIÂõæÂÉèÂíåËßÜÈ¢ëÁîüÊàêÂäüËÉΩ„ÄÇ",
        "start_pos": 5544,
        "end_pos": 5805,
        "token_count_estimate": 64,
        "source_type": "readme",
        "agent_id": "8ff21bc38ffbe100"
      }
    ]
  },
  {
    "agent_id": "37d40c7da33f6314",
    "name": "ai.smithery/fitaf-ai-fitaf-ai-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@fitaf-ai/fitaf-ai-mcp/mcp",
    "description": "Manage workouts, nutrition, goals, and progress across the FitAF platform. Connect wearables, sync‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-12T17:59:11.653728Z",
    "indexed_at": "2026-02-18T04:06:45.259239",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage workouts across the FitAF platform",
        "Manage nutrition plans across the FitAF platform",
        "Manage goals across the FitAF platform",
        "Manage progress tracking across the FitAF platform",
        "Connect wearable devices",
        "Sync data from connected devices"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of functionalities but lacks detailed structure, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "16bc2046964293e3",
    "name": "ai.smithery/fitaf-ai-fitaf-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@fitaf-ai/fitaf-mcp/mcp",
    "description": "Track workouts, nutrition, body metrics, habits, and SMART goals with insights and trends. Connect‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-12T20:09:50.974663Z",
    "indexed_at": "2026-02-18T04:06:49.041100",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Track workouts",
        "Track nutrition",
        "Track body metrics",
        "Track habits",
        "Track SMART goals",
        "Provide insights and trends"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of tracking capabilities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b0c002bca612180f",
    "name": "ai.smithery/flight505-mcp_dincoder",
    "source": "mcp",
    "source_url": "https://github.com/flight505/MCP_DinCoder",
    "description": "Driven Intent Negotiation ‚Äî Contract-Oriented Deterministic Executable Runtime DinCoder brings the‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T21:41:25.261987Z",
    "indexed_at": "2026-02-18T04:06:52.609678",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<div align=\"center\">\n  <img width=\"320\" alt=\"Image@0 5x\" src=\"https://github.com/user-attachments/assets/defd2ef0-5804-431c-8549-618eb3434aee\" />\n</div>\n\n[![smithery badge](https://smithery.ai/badge/@flight505/mcp_dincoder)](https://smithery.ai/server/@flight505/mcp_dincoder)\n\n**D**riven **I**ntent **N**egotiation ‚Äî **C**ontract-**O**riented **D**eterministic **E**xecutable **R**untime\n\n> *The MCP implementation of GitHub's Spec Kit methodology ‚Äî transforming specifications into executable artifacts*\n\n---\n\n## Table of Contents\n\n- [What is DinCoder?](#-what-is-dincoder)\n- [Installation](#-installation)\n- [Quickstart](#-quickstart)\n- [MCP Prompts (AI Workflow Orchestration)](#-mcp-prompts-ai-workflow-orchestration)\n- [Complete Workflow](#-complete-workflow-guide)\n- [Available Tools](#-available-tools)\n- [Examples](#-examples)\n- [Why Spec-Driven Development?](#-why-spec-driven-development)\n- [Roadmap](#-roadmap)\n- [Contributing](#-contributing)\n\n---\n\n## üéØ What is DinCoder?\n\n**An official Model Context Protocol server implementing GitHub's Spec-Driven Development (SDD) methodology**\n\nDinCoder brings the power of [GitHub Spec Kit](https://github.com/github/spec-kit) to any AI coding agent through the Model Context Protocol. It transforms the traditional \"prompt-then-code-dump\" workflow into a systematic, specification-driven process where **specifications don't serve code‚Äîcode serves specifications**.\n\n### What's New in v0.4.0 (Integration & Discovery Update)\n\n#### üéØ MCP Prompts - AI Workflow Orchestration ‚ú®\n\n- **7 workflow prompts** that guide AI agents through complex tasks\n- **Automatic discovery**: AI agents find and use prompts programmatically\n- **Built-in guidance**: Each prompt includes comprehensive workflow instructions\n- **Works everywhere**: Claude Code, VS Code Copilot, OpenAI Codex, Cursor\n- **Natural language**: Just describe what you want - AI uses appropriate prompts automatically\n\n**Available Prompts:**\n- `start_project` - Initialize new spec-driven project\n- `create_spec` - Create feature specification\n- `generate_plan` - Generate implementation plan\n- `create_tasks` - Break down into actionable tasks\n- `review_progress` - Generate progress report\n- `validate_spec` - Check specification quality\n- `next_tasks` - Show actionable tasks\n\n**Note:** These are NOT slash commands you type. They're workflow templates that your AI agent uses automatically when you describe your goals!\n\n#### üß¨ Constitution Tool - Define Your Project's DNA\n\n- **New command:** `constitution_create`\n- Set project-wide principles, constraints, and preferences\n- Ensures consistency across all AI-generated code\n\n#### ‚ùì Clarification Tracking - Systematic Q&A Management\n\n- **New commands:** `clarify_add`, `clarify_resolve`, `clarify_list`\n- Track ambiguities with unique IDs (CLARIFY-001, CLARIFY-002, etc.)\n- Resolve uncertainties with rationale and audit trail\n\n---\n\n## üì¶ Installation\n\n> **üéØ Quick Decision Guide:**\n> - **Using Claude Code?** ‚Üí Install the [Plugin](#-claude-code-plugin-recommended-for-claude-code) (easier, includes slash commands & agents)\n> - **Using VS Code/Codex/Cursor?** ‚Üí Install [MCP Server Only](#installing-via-smithery) (plugins not supported)\n>\n> ‚ö†Ô∏è **Don't install both!** The plugin automatically installs the MCP server - installing both may cause conflicts.\n\n### Prerequisites\n\n- Node.js >= 20.0.0\n- npm or pnpm\n- An MCP-compatible coding assistant with automatic workspace binding (Cursor, Claude Code, Codex, etc.)\n\n### Installing via Smithery\n\nTo install DinCoder automatically via [Smithery](https://smithery.ai/server/@flight505/mcp_dincoder):\n\n```bash\nnpx -y @smithery/cli install @flight505/mcp_dincoder\n```\n\n### Claude Code / VS Code Users\n\n```bash\nclaude mcp add dincoder -- npx -y mcp-dincoder@latest\n```\n\n### Cursor\n\nConfigure the MCP server inside Cursor's MCP settings; once you select a project, Cursor injects the workspace path automatically.\n\n### Other MCP Clients\n\nInstall globally:\n```bash\nnpm install -g mcp-dincoder@latest\n```\n\n> **Recommended clients:** DinCoder expects the MCP client to bind the active project directory automatically so generated specs, plans, and tasks land in the repo you are working on. Cursor, Claude Code, and Codex do this for every request. Claude Desktop's chat UI does not, so commands default to the server's own install directory; only use Claude Desktop if you plan to pass `workspacePath` manually on each call.\n\n### üìÅ Where Files Are Created\n\n**Important:** DinCoder creates all files in your **current working directory** (where you run your AI agent from).\n\n```bash\nyour-project/\n‚îú‚îÄ‚îÄ specs/                    # Created automatically\n‚îÇ   ‚îú‚îÄ‚îÄ 001-feature-name/    # Feature directory (auto-numbered)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constitution.md  # Project principles (optional, recommended first step)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spec.md          # Requirements & user stories\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plan.md          # Technical implementation plan\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks.md         # Executable task list\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research.md      # Technical decisions & research\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clarifications.json  # Q&A tracking\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ contracts/       # API contracts, data models\n‚îÇ   ‚îî‚îÄ‚îÄ 002-next-feature/\n‚îî‚îÄ‚îÄ .dincoder/               # Backward compatibility (legacy)\n```\n\n**Tip:** Launch your MCP client from the project root so every tool writes into the correct repo.\n\n---\n\n## üöÄ Quickstart\n\n### The Spec Kit Workflow\n\nTransform ideas into production-ready code through three powerful commands:\n\n#### 1Ô∏è‚É£ `/specify` ‚Äî Transform Ideas into Specifications\n\n```bash\n/specify Build a team productivity platform with Kanban boards and real-time collaboration\n```\n\n**What happens:**\n- Automatic feature numbering (001, 002, 003...)\n- Branch creation with semantic names\n- Template-based specification generation\n- Structured requirements with user stories\n- Explicit uncertainty markers `[NEEDS CLARIFICATION]`\n\n**Output:** A comprehensive PRD focusing on WHAT users need and WHY‚Äînever HOW to implement.\n\n#### 2Ô∏è‚É£ `/plan` ‚Äî Map Specifications to Technical Decisions\n\n```bash\n/plan Use Next.js with Prisma and PostgreSQL, WebSockets for real-time updates\n```\n\n**What happens:**\n- Analyzes feature specification\n- Ensures constitutional compliance (architectural principles)\n- Translates requirements to technical architecture\n- Generates data models, API contracts, test scenarios\n- Documents technology rationale\n\n**Output:** Complete implementation plan with every decision traced to requirements.\n\n#### 3Ô∏è‚É£ `/tasks` ‚Äî Generate Executable Task Lists\n\n```bash\n/tasks\n```\n\n**What happens:**\n- Analyzes plan and contracts\n- Converts specifications into granular tasks\n- Marks parallelizable work `[P]`\n- Orders tasks by dependencies\n- Creates test-first implementation sequence\n\n**Output:** Numbered task list ready for systematic implementation.\n\n### Real-World Example: Building a Chat System\n\n<details>\n<summary><strong>See how SDD transforms traditional development (click to expand)</strong></summary>\n\n#### Traditional Approach (12+ hours of documentation)\n```text\n1. Write PRD in document (2-3 hours)\n2. Create design documents (2-3 hours)\n3. Set up project structure (30 minutes)\n4. Write technical specs (3-4 hours)\n5. Create test plans (2 hours)\n```\n\n#### SDD with DinCoder (15 minutes total)\n\n```bash\n# Step 1: Create specification (5 minutes)\n/specify Real-time chat with message history, user presence, and typing indicators\n\n# Automatically creates:\n# - Branch \"003-real-time-chat\"\n# - specs/003-real-time-chat/spec.md with:\n#   ‚Ä¢ User stories and personas\n#   ‚Ä¢ Acceptance criteria\n#   ‚Ä¢ [NEEDS CLARIFICATION] markers for ambiguities\n\n# Step 2: Generate implementation plan (5 minutes)\n/plan WebSocket for real-time, PostgreSQL for history, Redis for presence\n\n# Generates:\n# - plan.md with phased implementation\n# - data-model.md (Message, User, Channel schemas)\n# - contracts/websocket-events.json\n# - contracts/rest-api.yaml\n# - research.md with library comparisons\n\n# Step 3: Create task list (5 minutes)\n/tasks\n\n# Produces executable tasks:\n# 1. [P] Create WebSocket contract tests\n# 2. [P] Create REST API contract tests\n# 3. Set up PostgreSQL schema\n# 4. Implement message persistence\n# 5. Add Redis presence tracking\n# ... (numbered, ordered, parallelizable)\n```\n\n**Result:** Complete, executable specifications ready for any AI agent to implement.\n\n</details>\n\n---\n\n## üéØ MCP Prompts (AI Workflow Orchestration)\n\n**New in v0.4.0:** DinCoder includes 7 MCP prompts that provide guided workflows for AI agents. These are **NOT slash commands** you type‚Äîthey're workflow templates that your AI agent (Claude, Copilot, etc.) automatically discovers and uses to help you.\n\n### How MCP Prompts Work\n\nMCP prompts are **invisible to users** but powerful for AI agents:\n\n1. **AI Discovery**: When DinCoder is connected, your AI agent automatically discovers available prompts via the MCP protocol\n2. **AI Invocation**: The AI agent invokes prompts programmatically when they're relevant to your task\n3. **Workflow Guidance**: Each prompt includes comprehensive instructions for multi-step workflows\n4. **Tool Orchestration**: Prompts guide the AI to call multiple DinCoder tools in the correct sequence\n\n**You don't \"run\" these prompts directly.** Just describe what you want in natural language, and your AI agent will use the appropriate prompt workflow automatically!\n\n### Available Workflow Prompts\n\n| Prompt Name | When AI Uses It | What It Does |\n|-------------|-----------------|--------------|\n| `start_project` | You ask to \"start a new project\" | Initializes .dincoder/, creates spec template |\n| `create_spec` | You describe a feature to build | Generates comprehensive specification |\n| `generate_plan` | You ask for implementation plan | Creates technical architecture from spec |\n| `create_tasks` | You ask to break down work | Generates executable task list from plan |\n| `review_progress` | You ask \"how's it going?\" | Shows statistics, charts, next actions |\n| `validate_spec` | You ask to check spec quality | Runs quality gates before implementation |\n| `next_tasks` | You ask \"what's next?\" | Shows unblocked, actionable tasks |\n\n### Example: How Prompts Guide AI Workflows\n\n**You say:** \"Let's start a new task manager project\"\n\n**AI thinks:** *This matches the `start_project` prompt. Let me follow its workflow...*\n\n**AI does:**\n1. Calls `specify_start` tool with projectName=\"task-manager\"\n2. Explains the .dincoder/ structure created\n3. Asks what you want to build\n4. Calls `specify_describe` with your requirements\n5. Validates spec with `spec_validate`\n6. Suggests next steps\n\n**You don't see:** The prompt invocation‚Äîjust the AI following the workflow naturally!\n\n<details>\n<summary><strong>See detailed prompt workflows (click to expand)</strong></summary>\n\n#### 1. `start_project` - Initialize New Spec-Driven Project\n\n**AI receives this workflow when you want to start a project:**\n\n```\n1. Call `specify_start` with projectName and agent type\n2. Explain .dincoder/ directory structure to user\n3. Explain spec-driven workflow: Specify ‚Üí Plan ‚Üí Execute\n4. Ask what they want to build\n5. Guide through specification creation\n```\n\n**Example conversation:**\n- You: \"I want to start a new e-commerce project\"\n- AI: *Invokes start_project prompt, follows workflow*\n- AI: \"I'll initialize a new spec-driven project. What features should the e-commerce platform have?\"\n\n---\n\n#### 2. `create_spec` - Create Feature Specification\n\n**AI receives this workflow when you describe a feature:**\n\n```\n1. Check if .dincoder/ exists (run specify_start if not)\n2. Gather requirements by asking:\n   - What problem does this solve?\n   - Who are the users?\n   - What are success criteria?\n   - What's out of scope?\n3. Call `specify_describe` with complete specification\n4. Call `spec_validate` to check quality\n5. Address validation issues with `spec_refine`\n6. Confirm spec is complete\n```\n\n**Example conversation:**\n- You: \"Build a real-time chat feature with typing indicators\"\n- AI: *Invokes create_spec prompt, asks clarifying questions*\n- AI: \"Let me create a specification. Should the chat support file attachments?\"\n\n---\n\n#### 3. `generate_plan` - Generate Implementation Plan\n\n**AI receives this workflow when planning is needed:**\n\n```\n1. Verify spec exists (guide user to create if missing)\n2. Run `spec_validate` if not already validated\n3. Call `plan_create` with technical constraints\n4. Call `artifacts_analyze` to verify spec-plan alignment\n5. Present plan structure to user\n6. Ask if ready for task generation\n```\n\n**Example conversation:**\n- You: \"How should we implement this?\"\n- AI: *Invokes generate_plan prompt*\n- AI: \"I'll create a technical plan. What's your preferred tech stack? (Next.js, Python/FastAPI, etc.)\"\n\n---\n\n#### 4. `create_tasks` - Break Down into Actionable Tasks\n\n**AI receives this workflow for task generation:**\n\n```\n1. Verify plan exists (guide to create if missing)\n2. Call `tasks_generate` with granular scope\n3. Call `tasks_visualize` to show dependency graph\n4. Call `tasks_stats` to show effort estimates\n5. Call `tasks_filter` with preset:\"next\" for actionable items\n6. Guide user to start first task\n```\n\n**Example conversation:**\n- You: \"Let's break this down into tasks\"\n- AI: *Invokes create_tasks prompt*\n- AI: \"I've generated 23 tasks. Here's the dependency graph... Ready to start with T001?\"\n\n---\n\n#### 5. `review_progress` - Generate Progress Report\n\n**AI receives this workflow for progress checks:**\n\n```\n1. Call `tasks_stats` with charts and blockers\n2. Call `tasks_filter` preset:\"next\" for actionable items\n3. Call `tasks_search` for in-progress tasks\n4. Summarize in structured format:\n   - Overall completion %\n   - Recent accomplishments\n   - Current work\n   - Next actions (unblocked)\n   - Blockers\n   - Recommendations\n5. Include visual progress charts\n```\n\n**Example conversation:**\n- You: \"How's the project coming along?\"\n- AI: *Invokes review_progress prompt*\n- AI: \"We're 67% complete (15/23 tasks). 3 tasks in progress, 2 blocked. Next up: authentication tests.\"\n\n---\n\n#### 6. `validate_spec` - Check Specification Quality\n\n**AI receives this workflow for validation:**\n\n```\n1. Call `spec_validate` with all checks enabled\n2. Check for:\n   - Completeness (all required sections)\n   - Acceptance criteria (testable when/then)\n   - Clarifications (no unresolved markers)\n   - Implementation leakage (no HOW in WHAT)\n3. Call `clarify_list` for pending questions\n4. If validation fails:\n   - List specific issues\n   - Call `spec_refine` to fix\n   - Re-validate\n5. If passes: suggest `plan_create` next\n```\n\n**Example conversation:**\n- You: \"Is the spec ready for implementation?\"\n- AI: *Invokes validate_spec prompt*\n- AI: \"Validation found 2 issues: missing edge cases section, 1 unresolved clarification. Let me fix these...\"\n\n---\n\n#### 7. `next_tasks` - Show Next Actionable Tasks\n\n**AI receives this workflow when you ask what's next:**\n\n```\n1. Call `tasks_filter` with preset:\"next\" and priority sorting\n2. For each task display:\n   - Task ID and description\n   - Metadata (phase, type, priority, effort)\n   - Why it's actionable\n3. Recommend which task to start based on:\n   - Priority level\n   - Dependencies completed\n   - Effort estimate\n4. Offer task details on request\n```\n\n**Example conversation:**\n- You: \"What should I work on next?\"\n- AI: *Invokes next_tasks prompt*\n- AI: \"Top priority: T007 (Implement user authentication, effort: 5). It's unblocked and high priority. Want to start?\"\n\n</details>\n\n### Platform Compatibility\n\nMCP prompts work across all MCP-compatible clients:\n\n| Client | How It Works |\n|--------|--------------|\n| **Claude Code** | Prompts auto-discovered; AI uses them automatically when relevant |\n| **VS Code Copilot** | Prompts available in agent mode; AI invokes based on context |\n| **OpenAI Codex** | Prompts accessible via MCP protocol; AI uses for complex workflows |\n| **Cursor** | MCP prompts integrated into agent workflows |\n\n### The Key Difference: MCP Prompts vs Slash Commands\n\n**Important distinction:**\n\n- **MCP Prompts** (DinCoder workflows): AI agents use these programmatically. You don't type them.\n- **Slash Commands** (Native): User-typed commands like `/help`, `/clear` in Claude Code\n- **Custom Commands** (`.claude/commands/`): Project-specific slash commands you create\n\n**In practice:** You describe what you want in natural language (\"Let's start a new project\"), and your AI agent automatically uses the appropriate MCP prompt workflow!\n\n---\n\n## üîå Claude Code Plugin (Recommended for Claude Code)\n\n**New in v0.5.0:** For the best Claude Code experience, install the **DinCoder Plugin** which bundles slash commands, specialized agents, and automatically installs the MCP server.\n\n> **‚ö†Ô∏è Important:** The plugin **includes** the MCP server - you don't need to install both! Choose **one** installation method:\n> - **Plugin** (Claude Code only) ‚Üí Slash commands + agents + MCP server (all-in-one)\n> - **MCP Server only** (VS Code, Codex, etc.) ‚Üí Just the tools (manual setup required)\n\n### Prerequisites\n\nBefore installing the plugin:\n- ‚úÖ Claude Code version 2.0.13 or higher\n- ‚úÖ Node.js >= 18\n- ‚úÖ npm installed\n\n### Installation\n\n**Step 1: Add the DinCoder marketplace**\n```bash\n# In Claude Code\n/plugin marketplace add flight505/dincoder-plugin\n```\n\n**Step 2: Install the plugin**\n```bash\n/plugin install dincoder\n```\n\n**Step 3: Restart Claude Code**\n- Press `Cmd+Q` (Mac) or `Alt+F4` (Windows) to quit, then reopen\n- Or use `Cmd/Ctrl+Shift+P` ‚Üí \"Developer: Reload Window\"\n\n### Verify Installation\n\nAfter restart, check that:\n- Slash commands appear: `/spec`, `/plan`, `/tasks`, `/progress`, `/validate`, `/next`\n- Agents appear: `@spec-writer`, `@plan-architect`, `@task-manager`\n- MCP server is active in Settings ‚Üí Extensions ‚Üí MCP Servers\n\n### What's Included\n\n‚ú® **Slash Commands** - Quick access without memorizing tool names\n- `/spec` - Create or refine specification\n- `/plan` - Generate implementation plan\n- `/tasks` - Break down into actionable tasks\n- `/progress` - View progress report\n- `/validate` - Check spec quality\n- `/next` - Show next actionable tasks\n\nü§ñ **Specialized Agents** - Expert assistance for each phase\n- `@spec-writer` - Expert at creating validated specifications\n- `@plan-architect` - Expert at designing technical plans\n- `@task-manager` - Expert at managing tasks and progress\n\nüîß **Automatic MCP Server** - Installs and configures `mcp-dincoder@latest` from npm automatically\n   - Runs `npx -y mcp-dincoder@latest` on installation\n   - Always pulls the latest version for bug fixes and features\n   - No manual MCP server setup needed!\n\nüìù **Built-in Documentation** - CLAUDE.md loads automatically with methodology guide\n\n### Plugin vs MCP Server Only\n\n| Feature | Plugin (Claude Code) | MCP Server Only (VS Code/Codex) |\n|---------|--------|-----------------|\n| **Platform** | Claude Code 2.0.13+ | VS Code, Codex, Cursor, etc. |\n| **Slash Commands** | ‚úÖ `/spec`, `/plan`, etc. | ‚ùå Not supported (plugins only) |\n| **Specialized Agents** | ‚úÖ `@spec-writer`, etc. | ‚ùå Not supported (plugins only) |\n| **MCP Tools** | ‚úÖ 30+ tools (auto-installed) | ‚úÖ 30+ tools (manual install) |\n| **Installation** | ‚úÖ Two commands (`marketplace add` + `install`) | ‚ö†Ô∏è Manual `.mcp.json` config |\n| **MCP Server Updates** | ‚úÖ Auto (uses `@latest`) | ‚ö†Ô∏è Manual version bump |\n\n**Choose Your Installation Method:**\n\n- **Claude Code users:** Use the **plugin** (recommended) - get slash commands, agents, and MCP server in one package\n- **VS Code/Codex users:** Use the **MCP server only** (plugins not supported on these platforms)\n\n> **‚ö†Ô∏è Avoid Dual Installation:** If you're using the plugin, **do NOT** also manually configure the MCP server in your MCP settings. The plugin handles this automatically and doing both may cause conflicts.\n\n**Plugin Repository:** [flight505/dincoder-plugin](https://github.com/flight505/dincoder-plugin)\n\n---\n\n## üîå VS Code + GitHub Copilot Integration\n\n**New in v0.6.0:** Full support for VS Code with GitHub Copilot integration through MCP.\n\n### Quick Setup\n\n1. **Copy template files to your project:**\n   ```bash\n   cp -r templates/vscode/.vscode your-project/\n   cp -r templates/vscode/.github your-project/\n   ```\n\n2. **Open project in VS Code:**\n   ```bash\n   cd your-project\n   code .\n   ```\n\n3. **Reload window:**\n   - Press `Cmd+Shift+P` (Mac) or `Ctrl+Shift+P` (Windows/Linux)\n   - Run: `Developer: Reload Window`\n\n4. **Verify setup:**\n   - Open Copilot Chat\n   - Click tools icon\n   - Verify \"dincoder\" appears in tools list\n\n### Using DinCoder with Copilot\n\nIn Copilot Chat, use MCP prompts or reference tools directly:\n\n```\n/mcp.dincoder.start_project my-app\n/mcp.dincoder.create_spec \"Build a task management API\"\n#dincoder.tasks_stats\n```\n\n### What's Included\n\n‚ú® **Template Files:**\n- `.vscode/mcp.json` - MCP server configuration\n- `.vscode/settings.json` - VS Code MCP settings\n- `.github/copilot-instructions.md` - Context for GitHub Copilot\n\nüìñ **Comprehensive Guide:** [docs/integration/vscode.md](docs/integration/vscode.md)\n\nüéØ **Ready-to-Use Templates:** [templates/vscode/](templates/vscode/)\n\n---\n\n## üîå OpenAI Codex Integration\n\n**New in v0.6.0:** Full support for OpenAI Codex (CLI and IDE extension) through MCP.\n\n### Quick Setup (CLI - Recommended)\n\n```bash\n# Add DinCoder MCP server\ncodex mcp add dincoder -- npx -y mcp-dincoder@latest\n\n# Verify\ncodex mcp list\n```\n\n### Quick Setup (Manual Configuration)\n\n1. **Copy global config:**\n   ```bash\n   cp templates/codex/config.toml ~/.codex/config.toml\n   ```\n\n2. **Copy workspace instructions:**\n   ```bash\n   cp templates/codex/.codex your-project/\n   ```\n\n3. **Restart Codex:**\n   ```bash\n   # CLI: Restart terminal\n   # IDE: Reload window\n   ```\n\n### Using DinCoder with Codex\n\n**CLI Commands:**\n```bash\ncodex \"use /mcp.dincoder.start_project my-app\"\ncodex \"use /mcp.dincoder.create_spec 'Build a REST API'\"\n```\n\n**IDE Extension:**\n```\n@dincoder.specify_start\n@dincoder.tasks_filter preset=\"next\"\n```\n\n### What's Included\n\n‚ú® **Template Files:**\n- `config.toml` - Codex MCP server configuration (for `~/.codex/config.toml`)\n- `.codex/instructions.md` - Workspace-specific instructions\n\nüìñ **Comprehensive Guide:** [docs/integration/codex.md](docs/integration/codex.md)\n\nüéØ **Ready-to-Use Templates:** [templates/codex/](templates/codex/)\n\n---\n\n## üö¶ Complete Workflow Guide\n\nThis is your end-to-end guide for using DinCoder with any AI agent (Claude, Copilot, Gemini, Cursor).\n\n### Step-by-Step: From Idea to Implementation\n\n#### 0Ô∏è‚É£ **Define Project Constitution** (Optional but Recommended, 2-3 minutes)\n\n```typescript\n// In your AI agent's chat:\n\"Use constitution_create to define principles for 'task-manager' with these details:\n\nPrinciples:\n- Prefer functional programming over OOP\n- Write tests before implementation (TDD)\n- Keep bundle size under 500KB\n\nConstraints:\n- Node.js >= 20.0.0 required\n- Maximum 3 external dependencies for core functionality\n\nPreferences:\n- Libraries: React Query over Redux, Zod for validation\n- Patterns: Repository pattern for data access\"\n\n// What happens:\n// ‚úì Creates specs/001-task-manager/ directory\n// ‚úì Generates constitution.md with structured principles\n// ‚úì All future specs/plans will respect these constraints\n```\n\n**Why use this:** Constitution ensures consistency across your entire project. AI agents will reference these principles when generating specs and plans.\n\n#### 1Ô∏è‚É£ **Start a New Project** (1 minute)\n\n```typescript\n\"Use specify_start to initialize a new project called 'task-manager' with claude agent\"\n\n// What happens:\n// ‚úì Creates specs/001-task-manager/ directory\n// ‚úì Generates spec.md template\n// ‚úì Creates contracts/ folder\n// ‚úì Initializes research.md\n```\n\n#### 2Ô∏è‚É£ **Describe What You Want** (2-5 minutes)\n\n```typescript\n\"Use specify_describe with this description:\nBuild a task management system where users can:\n- Create tasks with titles, descriptions, and due dates\n- Organize tasks into projects\n- Mark tasks as complete\n- Filter by status and project\n- Get daily summary emails\"\n\n// What happens:\n// ‚úì Updates spec.md with user stories\n// ‚úì Adds acceptance criteria\n// ‚úì Marks uncertainties with [NEEDS CLARIFICATION]\n// ‚úì Separates WHAT from HOW\n```\n\n**Pro tip**: Be specific about user needs, not implementation. Focus on **what users want** and **why they need it**.\n\n#### 3Ô∏è‚É£ **Track Clarifications** (Optional)\n\n```typescript\n// If you notice ambiguities while writing specs:\n\"Use clarify_add with this question:\n'Should task due dates support time zones or just dates?'\nwith context 'Task scheduling requirements'\"\n\n// Later, when you have an answer:\n\"Use clarify_resolve for CLARIFY-001 with resolution:\n'Use UTC timestamps with user timezone preference.'\"\n\n// Check all clarifications:\n\"Use clarify_list to see all pending clarifications\"\n```\n\n#### 4Ô∏è‚É£ **Generate Technical Plan** (2-5 minutes)\n\n```typescript\n\"Use plan_create with these constraints:\n- Next.js 14 with App Router\n- PostgreSQL with Prisma ORM\n- tRPC for type-safe APIs\n- Tailwind CSS for styling\"\n\n// What happens:\n// ‚úì Creates plan.md with phased approach\n// ‚úì Generates data-model.md (Task, Project, User schemas)\n// ‚úì Updates research.md with architecture decisions\n// ‚úì Enforces constitutional compliance\n```\n\n#### 5Ô∏è‚É£ **Create Implementation Tasks** (2-5 minutes)\n\n```typescript\n\"Use tasks_generate with scope 'MVP - core task management'\"\n\n// What happens:\n// ‚úì Creates tasks.md with numbered, ordered tasks\n// ‚úì Marks parallelizable tasks with [P]\n// ‚úì Orders by dependency (contracts ‚Üí tests ‚Üí implementation)\n```\n\n#### 6Ô∏è‚É£ **Implement Systematically**\n\n```typescript\n// Start with first task\n\"Let's implement T001 - Create Prisma schema for Task model\"\n\n// After completing a task\n\"Use tasks_tick with taskId 'T001'\"\n\n// See what's next\n\"Use artifacts_read with artifactType 'tasks'\"\n```\n\n### üéØ Best Practices\n\n1. **Start Small**: Begin with MVP scope, add features iteratively\n2. **Follow the Order**: Specify ‚Üí Plan ‚Üí Tasks ‚Üí Implement\n3. **Review Specs**: Always read the generated spec.md and refine it\n4. **Track Progress**: Use tasks_tick consistently\n5. **Document Decisions**: Use research_append for architecture choices\n\n---\n\n## üõ† Available Tools\n\n### üéØ Core Spec-Driven Development Tools\n\nDinCoder implements the complete Spec Kit workflow through these MCP tools:\n\n#### Phase 1: SPECIFY ‚Äî Create Living Specifications\n\n| Tool | Purpose | Usage Example |\n|------|---------|---------------|\n| `specify_start` | **Initialize project** | `{\"projectName\": \"taskify\", \"agent\": \"claude\"}` |\n| `specify_describe` | **Generate PRD** | `{\"description\": \"Build a photo organizer with albums\"}` |\n| `constitution_create` | **Define project DNA** | `{\"principles\": [...], \"constraints\": [...]}` |\n| `clarify_add` | **Track ambiguities** | `{\"question\": \"Auth method?\", \"context\": \"Security\"}` |\n| `clarify_resolve` | **Resolve uncertainties** | `{\"clarificationId\": \"CLARIFY-001\", \"resolution\": \"...\"}` |\n| `spec_validate` | **Check spec quality** | `{\"checks\": {\"completeness\": true, \"acceptanceCriteria\": true}}` |\n| `research_append` | **Document findings** | `{\"content\": \"WebSocket benchmarks show...\"}` |\n\n#### Phase 2: PLAN ‚Äî Map Specifications to Architecture\n\n| Tool | Purpose | Usage Example |\n|------|---------|---------------|\n| `plan_create` | **Technical planning** | `{\"constraintsText\": \"Next.js, Prisma, PostgreSQL\"}` |\n| `spec_refine` | **Update specs** | `{\"section\": \"requirements\", \"changes\": \"...\"}` |\n| `artifacts_analyze` | **Verify alignment** | `{\"artifacts\": [\"spec\", \"plan\"]}` |\n\n#### Phase 3: TASKS ‚Äî Generate Executable Work Items\n\n| Tool | Purpose | Usage Example |\n|------|---------|---------------|\n| `tasks_generate` | **Create task list** | `{\"scope\": \"MVP\"}` |\n| `tasks_tick` | **Track completion** | `{\"taskId\": \"T001\"}` |\n| `tasks_tick_range` | **Bulk completion** | `{\"taskIds\": [\"T001-T005\"]}` |\n| `tasks_filter` | **Show actionable items** | `{\"preset\": \"next\", \"limit\": 5}` |\n| `tasks_search` | **Find tasks** | `{\"query\": \"authentication\", \"fuzzy\": true}` |\n| `tasks_visualize` | **Dependency graphs** | `{\"format\": \"mermaid\"}` |\n| `tasks_stats` | **Progress analytics** | `{\"groupBy\": \"phase\", \"includeCharts\": true}` |\n\n#### Phase 4: IMPLEMENT ‚Äî Build With Validation\n\n| Tool | Purpose | Usage Example |\n|------|---------|---------------|\n| `git_create_branch` | **Feature isolation** | `{\"branchName\": \"feature/chat-system\"}` |\n| `quality_format` | **Code formatting** | `{\"fix\": true}` |\n| `quality_lint` | **Static analysis** | `{\"fix\": true}` |\n| `quality_test` | **Execute tests** | `{\"coverage\": true}` |\n| `quality_security_audit` | **Check vulnerabilities** | `{\"fix\": false}` |\n\n---\n\n## üìö Examples\n\n### Connect with TypeScript Client\n\n```typescript\nimport { Client } from '@modelcontextprotocol/sdk/client/index.js';\nimport { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/transport/streamable-http.js';\n\nconst transport = new StreamableHTTPClientTransport({\n  url: new URL('http://localhost:3000/mcp'),\n});\n\nconst client = new Client({\n  name: 'my-client',\n  version: '1.0.0',\n});\n\nawait client.connect(transport);\n\n// Use tools\nconst result = await client.callTool('specify_describe', {\n  description: 'A task management API',\n});\n```\n\nSee [examples/](examples/) for complete examples:\n- `local-client.ts` - Connect to local server\n- `spec-workflow.md` - Complete spec-driven workflow\n\n---\n\n## üí° Why Spec-Driven Development?\n\n**TL;DR:** Specifications are executable contracts that generate consistent, maintainable code. Change the spec ‚Üí regenerate the implementation. No more \"vibe coding.\"\n\nFor decades, code has been king. Specifications were scaffolding‚Äîbuilt, used, then discarded. **Spec-Driven Development inverts this power structure:**\n\n- **Specifications Generate Code**: The PRD isn't a guide‚Äîit's the source that produces implementation\n- **Executable Specifications**: Precise, complete specs that eliminate the gap between intent and implementation\n- **Code as Expression**: Code becomes the specification's expression in a particular language/framework\n- **Living Documentation**: Maintain software by evolving specifications, not manually updating code\n\nThis transformation is possible because AI can understand complex specifications and implement them systematically. But raw AI generation without structure produces chaos. DinCoder provides that structure through GitHub's proven Spec Kit methodology.\n\n### Why This Matters Now\n\nThree converging trends make SDD essential:\n\n1. **AI Threshold**: LLMs can reliably translate natural language specifications to working code\n2. **Complexity Growth**: Modern systems integrate dozens of services‚Äîmanual alignment becomes impossible\n3. **Change Velocity**: Requirements change rapidly‚Äîpivots are expected, not exceptional\n\nTraditional development treats changes as disruptions. SDD transforms them into systematic regenerations. Change a requirement ‚Üí update affected plans ‚Üí regenerate implementation.\n\n**Read the full philosophy:** [Why Spec-Driven Development?](docs/WHY_SDD.md)\n\n---\n\n## üó∫ Roadmap\n\nDinCoder is actively evolving! We're at **v0.4.0** with **28/36 stories complete (78%)**.\n\n**Current Phase:** Phase 3 - Advanced Task Management (In Progress)\n\n**Upcoming Features:**\n- Advanced task management (filtering, search, statistics)\n- Multi-feature project support\n- Enhanced collaboration features\n- External integrations (Jira, Linear, GitHub Issues)\n\n**Vote on features and view the complete roadmap:** [docs/ROADMAP.md](docs/ROADMAP.md)\n\n---\n\n## ü§ù Contributing\n\nWe welcome contributions from the community! Whether you have:\n\n- **Feature ideas or requests** - Share your vision for new Spec Kit tools\n- **Bug reports** - Help us identify and fix issues\n- **Template improvements** - Enhance the Spec Kit templates\n- **Tool enhancements** - Extend the MCP server capabilities\n- **Documentation updates** - Improve guides and examples\n\n**Get Started:**\n1. [Open an issue](https://github.com/flight505/mcp-dincoder/issues) to discuss your idea\n2. Fork the repository and create a feature branch\n3. For development: `git clone`, `npm install`, `npm run build`, `npm test`\n4. Submit a pull request with your improvements\n\nWe appreciate all contributions, big or small!\n\n---\n\n## üìÑ License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\n## üìö References\n\n- [Model Context Protocol Specification](https://modelcontextprotocol.io)\n- [Spec Kit Documentation](https://github.com/github/spec-kit)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n\n## üôè Acknowledgments\n\n- [Model Context Protocol](https://github.com/modelcontextprotocol) by Anthropic\n- [Spec Kit](https://github.com/spec-kit) for spec-driven development methodology\n\n---\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Transform natural language ideas into structured feature specifications with user stories and acceptance criteria",
        "Generate detailed technical implementation plans from specifications including architecture, data models, and API contracts",
        "Create executable, ordered, and parallelizable task lists from implementation plans",
        "Provide AI workflow orchestration through 7 MCP prompts for guided multi-step development processes",
        "Automatically discover and invoke workflow prompts programmatically by AI agents",
        "Track and manage clarifications and ambiguities with unique IDs and resolution audit trails",
        "Define project-wide principles, constraints, and preferences via a constitution tool to ensure consistency",
        "Integrate with multiple MCP-compatible AI coding assistants such as Claude Code, VS Code Copilot, OpenAI Codex, and Cursor",
        "Automatically create and organize specification-related files and directories in the current working directory"
      ],
      "limitations": [
        "Does not support direct user invocation of MCP prompts; prompts are used only programmatically by AI agents",
        "Claude Desktop chat UI lacks automatic workspace binding, requiring manual workspacePath parameter passing",
        "Plugin installation recommended only for Claude Code; installing both plugin and MCP server may cause conflicts",
        "Does not support plugins for VS Code, Codex, or Cursor‚Äîonly MCP server installation is supported",
        "Requires MCP client to bind active project directory automatically for correct file placement"
      ],
      "requirements": [
        "Node.js version 20.0.0 or higher",
        "npm or pnpm package manager",
        "An MCP-compatible AI coding assistant with automatic workspace binding (e.g., Cursor, Claude Code, Codex)",
        "For Claude Code users, installation of the DinCoder plugin is recommended",
        "For other clients, global installation of the MCP server package or installation via Smithery CLI"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples including commands and workflows, descriptions of available tools and prompts, file structure explanations, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<div align=\"center\">\n  <img width=\"320\" alt=\"Image@0 5x\" src=\"https://github.com/user-attachments/assets/defd2ef0-5804-431c-8549-618eb3434aee\" />\n</div>\n\n[![smithery badge](https://smithery.ai/badge/@flight505/mcp_dincoder)](https://smithery.ai/server/@flight505/mcp_dincoder)\n\n**D**riven **I**ntent **N**egotiation ‚Äî **C**ontract-**O**riented **D**eterministic **E**xecutable **R**untime\n\n> *The MCP implementation of GitHub's Spec Kit methodology ‚Äî transforming specifications into executable artifacts*\n\n---\n\n## Table of Contents\n\n- [What is DinCoder?](#-what-is-dincoder)\n- [Installation](#-installation)\n- [Quickstart](#-quickstart)\n- [MCP Prompts (AI Workflow Orchestration)](#-mcp-prompts-ai-workflow-orchestration)\n- [Complete Workflow](#-complete-workflow-guide)\n- [Available Tools](#-available-tools)\n- [Examples](#-examples)\n- [Why Spec-Driven Development?](#-why-spec-driven-development)\n- [Roadmap](#-roadmap)\n- [Contributing](#-contributing)\n\n---\n\n## üéØ What is DinCoder?\n\n**An official Model Context Protocol server implementing GitHub's Spec-Driven Development (SDD) methodology**\n\nDinCoder brings the power of [GitHub Spec Kit](https://github.com/github/spec-kit) to any AI coding agent through the Model Context Protocol. It transforms the traditional \"prompt-then-code-dump\" workflow into a systematic, specification-driven process where **specifications don't serve code‚Äîcode serves specifications**.",
        "start_pos": 0,
        "end_pos": 1419,
        "token_count_estimate": 354,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 1,
        "text": "ge**: Just describe what you want - AI uses appropriate prompts automatically\n\n**Available Prompts:**\n- `start_project` - Initialize new spec-driven project\n- `create_spec` - Create feature specification\n- `generate_plan` - Generate implementation plan\n- `create_tasks` - Break down into actionable tasks\n- `review_progress` - Generate progress report\n- `validate_spec` - Check specification quality\n- `next_tasks` - Show actionable tasks\n\n**Note:** These are NOT slash commands you type. They're workflow templates that your AI agent uses automatically when you describe your goals!\n\n#### üß¨ Constitution Tool - Define Your Project's DNA\n\n- **New command:** `constitution_create`\n- Set project-wide principles, constraints, and preferences\n- Ensures consistency across all AI-generated code\n\n#### ‚ùì Clarification Tracking - Systematic Q&A Management\n\n- **New commands:** `clarify_add`, `clarify_resolve`, `clarify_list`\n- Track ambiguities with unique IDs (CLARIFY-001, CLARIFY-002, etc.)\n- Resolve uncertainties with rationale and audit trail\n\n---\n\n## üì¶ Installation\n\n> **üéØ Quick Decision Guide:**\n> - **Using Claude Code?** ‚Üí Install the [Plugin](#-claude-code-plugin-recommended-for-claude-code) (easier, includes slash commands & agents)\n> - **Using VS Code/Codex/Cursor?** ‚Üí Install [MCP Server Only](#installing-via-smithery) (plugins not supported)\n>\n> ‚ö†Ô∏è **Don't install both!** The plugin automatically installs the MCP server - installing both may cause conflicts.",
        "start_pos": 1848,
        "end_pos": 3322,
        "token_count_estimate": 368,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 2,
        "text": "/ VS Code Users\n\n```bash\nclaude mcp add dincoder -- npx -y mcp-dincoder@latest\n```\n\n### Cursor\n\nConfigure the MCP server inside Cursor's MCP settings; once you select a project, Cursor injects the workspace path automatically.\n\n### Other MCP Clients\n\nInstall globally:\n```bash\nnpm install -g mcp-dincoder@latest\n```\n\n> **Recommended clients:** DinCoder expects the MCP client to bind the active project directory automatically so generated specs, plans, and tasks land in the repo you are working on. Cursor, Claude Code, and Codex do this for every request. Claude Desktop's chat UI does not, so commands default to the server's own install directory; only use Claude Desktop if you plan to pass `workspacePath` manually on each call.\n\n### üìÅ Where Files Are Created\n\n**Important:** DinCoder creates all files in your **current working directory** (where you run your AI agent from).\n\n```bash\nyour-project/\n‚îú‚îÄ‚îÄ specs/                    # Created automatically\n‚îÇ   ‚îú‚îÄ‚îÄ 001-feature-name/    # Feature directory (auto-numbered)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constitution.md  # Project principles (optional, recommended first step)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spec.md          # Requirements & user stories\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plan.md          # Technical implementation plan\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks.md         # Executable task list\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ research.md      # Technical decisions & research\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clarifications.json  # Q&A tracking\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ contracts/       # API contracts, data models\n‚îÇ   ‚îî‚îÄ‚îÄ 002-next-feature/\n‚îî‚îÄ‚îÄ .dincoder/               # Backward compatibility (legacy)\n```\n\n**Tip:** Launch your MCP client from the project root so every tool writes into the correct repo.",
        "start_pos": 3696,
        "end_pos": 5339,
        "token_count_estimate": 410,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 3,
        "text": "uild a team productivity platform with Kanban boards and real-time collaboration\n```\n\n**What happens:**\n- Automatic feature numbering (001, 002, 003...)\n- Branch creation with semantic names\n- Template-based specification generation\n- Structured requirements with user stories\n- Explicit uncertainty markers `[NEEDS CLARIFICATION]`\n\n**Output:** A comprehensive PRD focusing on WHAT users need and WHY‚Äînever HOW to implement.\n\n#### 2Ô∏è‚É£ `/plan` ‚Äî Map Specifications to Technical Decisions\n\n```bash\n/plan Use Next.js with Prisma and PostgreSQL, WebSockets for real-time updates\n```\n\n**What happens:**\n- Analyzes feature specification\n- Ensures constitutional compliance (architectural principles)\n- Translates requirements to technical architecture\n- Generates data models, API contracts, test scenarios\n- Documents technology rationale\n\n**Output:** Complete implementation plan with every decision traced to requirements.\n\n#### 3Ô∏è‚É£ `/tasks` ‚Äî Generate Executable Task Lists\n\n```bash\n/tasks\n```\n\n**What happens:**\n- Analyzes plan and contracts\n- Converts specifications into granular tasks\n- Marks parallelizable work `[P]`\n- Orders tasks by dependencies\n- Creates test-first implementation sequence\n\n**Output:** Numbered task list ready for systematic implementation.\n\n### Real-World Example: Building a Chat System\n\n<details>\n<summary><strong>See how SDD transforms traditional development (click to expand)</strong></summary>\n\n#### Traditional Approach (12+ hours of documentation)\n```text\n1. Write PRD in document (2-3 hours)\n2. Create design documents (2-3 hours)\n3. Set up project structure (30 minutes)\n4. Write technical specs (3-4 hours)\n5.",
        "start_pos": 5544,
        "end_pos": 7190,
        "token_count_estimate": 411,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 4,
        "text": "dicators\n\n# Automatically creates:\n# - Branch \"003-real-time-chat\"\n# - specs/003-real-time-chat/spec.md with:\n#   ‚Ä¢ User stories and personas\n#   ‚Ä¢ Acceptance criteria\n#   ‚Ä¢ [NEEDS CLARIFICATION] markers for ambiguities\n\n# Step 2: Generate implementation plan (5 minutes)\n/plan WebSocket for real-time, PostgreSQL for history, Redis for presence\n\n# Generates:\n# - plan.md with phased implementation\n# - data-model.md (Message, User, Channel schemas)\n# - contracts/websocket-events.json\n# - contracts/rest-api.yaml\n# - research.md with library comparisons\n\n# Step 3: Create task list (5 minutes)\n/tasks\n\n# Produces executable tasks:\n# 1. [P] Create WebSocket contract tests\n# 2. [P] Create REST API contract tests\n# 3. Set up PostgreSQL schema\n# 4. Implement message persistence\n# 5. Add Redis presence tracking\n# ... (numbered, ordered, parallelizable)\n```\n\n**Result:** Complete, executable specifications ready for any AI agent to implement.\n\n</details>\n\n---\n\n## üéØ MCP Prompts (AI Workflow Orchestration)\n\n**New in v0.4.0:** DinCoder includes 7 MCP prompts that provide guided workflows for AI agents. These are **NOT slash commands** you type‚Äîthey're workflow templates that your AI agent (Claude, Copilot, etc.) automatically discovers and uses to help you.\n\n### How MCP Prompts Work\n\nMCP prompts are **invisible to users** but powerful for AI agents:\n\n1. **AI Discovery**: When DinCoder is connected, your AI agent automatically discovers available prompts via the MCP protocol\n2. **AI Invocation**: The AI agent invokes prompts programmatically when they're relevant to your task\n3. **Workflow Guidance**: Each prompt includes comprehensive instructions for multi-step workflows\n4. **Tool Orchestration**: Prompts guide the AI to call multiple DinCoder tools in the correct sequence\n\n**You don't \"run\" these prompts directly.** Just describe what you want in natural language, and your AI agent will use the appropriate prompt workflow automatically!",
        "start_pos": 7392,
        "end_pos": 9347,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 5,
        "text": "er tools in the correct sequence\n\n**You don't \"run\" these prompts directly.** Just describe what you want in natural language, and your AI agent will use the appropriate prompt workflow automatically!\n\n### Available Workflow Prompts\n\n| Prompt Name | When AI Uses It | What It Does |\n|-------------|-----------------|--------------|\n| `start_project` | You ask to \"start a new project\" | Initializes .dincoder/, creates spec template |\n| `create_spec` | You describe a feature to build | Generates comprehensive specification |\n| `generate_plan` | You ask for implementation plan | Creates technical architecture from spec |\n| `create_tasks` | You ask to break down work | Generates executable task list from plan |\n| `review_progress` | You ask \"how's it going?\" | Shows statistics, charts, next actions |\n| `validate_spec` | You ask to check spec quality | Runs quality gates before implementation |\n| `next_tasks` | You ask \"what's next?\" | Shows unblocked, actionable tasks |\n\n### Example: How Prompts Guide AI Workflows\n\n**You say:** \"Let's start a new task manager project\"\n\n**AI thinks:** *This matches the `start_project` prompt. Let me follow its workflow...*\n\n**AI does:**\n1. Calls `specify_start` tool with projectName=\"task-manager\"\n2. Explains the .dincoder/ structure created\n3. Asks what you want to build\n4. Calls `specify_describe` with your requirements\n5. Validates spec with `spec_validate`\n6. Suggests next steps\n\n**You don't see:** The prompt invocation‚Äîjust the AI following the workflow naturally!\n\n<details>\n<summary><strong>See detailed prompt workflows (click to expand)</strong></summary>\n\n#### 1. `start_project` - Initialize New Spec-Driven Project\n\n**AI receives this workflow when you want to start a project:**\n\n```\n1. Call `specify_start` with projectName and agent type\n2. Explain .dincoder/ directory structure to user\n3. Explain spec-driven workflow: Specify ‚Üí Plan ‚Üí Execute\n4. Ask what they want to build\n5.",
        "start_pos": 9147,
        "end_pos": 11092,
        "token_count_estimate": 486,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 6,
        "text": "``\n1. Call `specify_start` with projectName and agent type\n2. Explain .dincoder/ directory structure to user\n3. Explain spec-driven workflow: Specify ‚Üí Plan ‚Üí Execute\n4. Ask what they want to build\n5. Guide through specification creation\n```\n\n**Example conversation:**\n- You: \"I want to start a new e-commerce project\"\n- AI: *Invokes start_project prompt, follows workflow*\n- AI: \"I'll initialize a new spec-driven project. What features should the e-commerce platform have?\"\n\n---\n\n#### 2. `create_spec` - Create Feature Specification\n\n**AI receives this workflow when you describe a feature:**\n\n```\n1. Check if .dincoder/ exists (run specify_start if not)\n2. Gather requirements by asking:\n   - What problem does this solve?\n   - Who are the users?\n   - What are success criteria?\n   - What's out of scope?\n3. Call `specify_describe` with complete specification\n4. Call `spec_validate` to check quality\n5. Address validation issues with `spec_refine`\n6. Confirm spec is complete\n```\n\n**Example conversation:**\n- You: \"Build a real-time chat feature with typing indicators\"\n- AI: *Invokes create_spec prompt, asks clarifying questions*\n- AI: \"Let me create a specification. Should the chat support file attachments?\"\n\n---\n\n#### 3. `generate_plan` - Generate Implementation Plan\n\n**AI receives this workflow when planning is needed:**\n\n```\n1. Verify spec exists (guide user to create if missing)\n2. Run `spec_validate` if not already validated\n3. Call `plan_create` with technical constraints\n4. Call `artifacts_analyze` to verify spec-plan alignment\n5. Present plan structure to user\n6. Ask if ready for task generation\n```\n\n**Example conversation:**\n- You: \"How should we implement this?\"\n- AI: *Invokes generate_plan prompt*\n- AI: \"I'll create a technical plan. What's your preferred tech stack? (Next.js, Python/FastAPI, etc.)\"\n\n---\n\n#### 4. `create_tasks` - Break Down into Actionable Tasks\n\n**AI receives this workflow for task generation:**\n\n```\n1. Verify plan exists (guide to create if missing)\n2.",
        "start_pos": 10892,
        "end_pos": 12897,
        "token_count_estimate": 501,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 7,
        "text": "s, Python/FastAPI, etc.)\"\n\n---\n\n#### 4. `create_tasks` - Break Down into Actionable Tasks\n\n**AI receives this workflow for task generation:**\n\n```\n1. Verify plan exists (guide to create if missing)\n2. Call `tasks_generate` with granular scope\n3. Call `tasks_visualize` to show dependency graph\n4. Call `tasks_stats` to show effort estimates\n5. Call `tasks_filter` with preset:\"next\" for actionable items\n6. Guide user to start first task\n```\n\n**Example conversation:**\n- You: \"Let's break this down into tasks\"\n- AI: *Invokes create_tasks prompt*\n- AI: \"I've generated 23 tasks. Here's the dependency graph... Ready to start with T001?\"\n\n---\n\n#### 5. `review_progress` - Generate Progress Report\n\n**AI receives this workflow for progress checks:**\n\n```\n1. Call `tasks_stats` with charts and blockers\n2. Call `tasks_filter` preset:\"next\" for actionable items\n3. Call `tasks_search` for in-progress tasks\n4. Summarize in structured format:\n   - Overall completion %\n   - Recent accomplishments\n   - Current work\n   - Next actions (unblocked)\n   - Blockers\n   - Recommendations\n5. Include visual progress charts\n```\n\n**Example conversation:**\n- You: \"How's the project coming along?\"\n- AI: *Invokes review_progress prompt*\n- AI: \"We're 67% complete (15/23 tasks). 3 tasks in progress, 2 blocked. Next up: authentication tests.\"\n\n---\n\n#### 6. `validate_spec` - Check Specification Quality\n\n**AI receives this workflow for validation:**\n\n```\n1. Call `spec_validate` with all checks enabled\n2. Check for:\n   - Completeness (all required sections)\n   - Acceptance criteria (testable when/then)\n   - Clarifications (no unresolved markers)\n   - Implementation leakage (no HOW in WHAT)\n3. Call `clarify_list` for pending questions\n4. If validation fails:\n   - List specific issues\n   - Call `spec_refine` to fix\n   - Re-validate\n5.",
        "start_pos": 12697,
        "end_pos": 14518,
        "token_count_estimate": 455,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 8,
        "text": "reate` next\n```\n\n**Example conversation:**\n- You: \"Is the spec ready for implementation?\"\n- AI: *Invokes validate_spec prompt*\n- AI: \"Validation found 2 issues: missing edge cases section, 1 unresolved clarification. Let me fix these...\"\n\n---\n\n#### 7. `next_tasks` - Show Next Actionable Tasks\n\n**AI receives this workflow when you ask what's next:**\n\n```\n1. Call `tasks_filter` with preset:\"next\" and priority sorting\n2. For each task display:\n   - Task ID and description\n   - Metadata (phase, type, priority, effort)\n   - Why it's actionable\n3. Recommend which task to start based on:\n   - Priority level\n   - Dependencies completed\n   - Effort estimate\n4. Offer task details on request\n```\n\n**Example conversation:**\n- You: \"What should I work on next?\"\n- AI: *Invokes next_tasks prompt*\n- AI: \"Top priority: T007 (Implement user authentication, effort: 5). It's unblocked and high priority. Want to start?\"\n\n</details>\n\n### Platform Compatibility\n\nMCP prompts work across all MCP-compatible clients:\n\n| Client | How It Works |\n|--------|--------------|\n| **Claude Code** | Prompts auto-discovered; AI uses them automatically when relevant |\n| **VS Code Copilot** | Prompts available in agent mode; AI invokes based on context |\n| **OpenAI Codex** | Prompts accessible via MCP protocol; AI uses for complex workflows |\n| **Cursor** | MCP prompts integrated into agent workflows |\n\n### The Key Difference: MCP Prompts vs Slash Commands\n\n**Important distinction:**\n\n- **MCP Prompts** (DinCoder workflows): AI agents use these programmatically. You don't type them.\n- **Slash Commands** (Native): User-typed commands like `/help`, `/clear` in Claude Code\n- **Custom Commands** (`.claude/commands/`): Project-specific slash commands you create\n\n**In practice:** You describe what you want in natural language (\"Let's start a new project\"), and your AI agent automatically uses the appropriate MCP prompt workflow!",
        "start_pos": 14545,
        "end_pos": 16458,
        "token_count_estimate": 478,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 9,
        "text": "ific slash commands you create\n\n**In practice:** You describe what you want in natural language (\"Let's start a new project\"), and your AI agent automatically uses the appropriate MCP prompt workflow!\n\n---\n\n## üîå Claude Code Plugin (Recommended for Claude Code)\n\n**New in v0.5.0:** For the best Claude Code experience, install the **DinCoder Plugin** which bundles slash commands, specialized agents, and automatically installs the MCP server.",
        "start_pos": 16258,
        "end_pos": 16700,
        "token_count_estimate": 110,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 10,
        "text": "ized Agents** - Expert assistance for each phase\n- `@spec-writer` - Expert at creating validated specifications\n- `@plan-architect` - Expert at designing technical plans\n- `@task-manager` - Expert at managing tasks and progress\n\nüîß **Automatic MCP Server** - Installs and configures `mcp-dincoder@latest` from npm automatically\n   - Runs `npx -y mcp-dincoder@latest` on installation\n   - Always pulls the latest version for bug fixes and features\n   - No manual MCP server setup needed!\n\nüìù **Built-in Documentation** - CLAUDE.md loads automatically with methodology guide\n\n### Plugin vs MCP Server Only\n\n| Feature | Plugin (Claude Code) | MCP Server Only (VS Code/Codex) |\n|---------|--------|-----------------|\n| **Platform** | Claude Code 2.0.13+ | VS Code, Codex, Cursor, etc. |\n| **Slash Commands** | ‚úÖ `/spec`, `/plan`, etc. | ‚ùå Not supported (plugins only) |\n| **Specialized Agents** | ‚úÖ `@spec-writer`, etc. | ‚ùå Not supported (plugins only) |\n| **MCP Tools** | ‚úÖ 30+ tools (auto-installed) | ‚úÖ 30+ tools (manual install) |\n| **Installation** | ‚úÖ Two commands (`marketplace add` + `install`) | ‚ö†Ô∏è Manual `.mcp.json` config |\n| **MCP Server Updates** | ‚úÖ Auto (uses `@latest`) | ‚ö†Ô∏è Manual version bump |\n\n**Choose Your Installation Method:**\n\n- **Claude Code users:** Use the **plugin** (recommended) - get slash commands, agents, and MCP server in one package\n- **VS Code/Codex users:** Use the **MCP server only** (plugins not supported on these platforms)\n\n> **‚ö†Ô∏è Avoid Dual Installation:** If you're using the plugin, **do NOT** also manually configure the MCP server in your MCP settings. The plugin handles this automatically and doing both may cause conflicts.\n\n**Plugin Repository:** [flight505/dincoder-plugin](https://github.com/flight505/dincoder-plugin)\n\n---\n\n## üîå VS Code + GitHub Copilot Integration\n\n**New in v0.6.0:** Full support for VS Code with GitHub Copilot integration through MCP.\n\n### Quick Setup\n\n1.",
        "start_pos": 18106,
        "end_pos": 20034,
        "token_count_estimate": 482,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 11,
        "text": "s://github.com/flight505/dincoder-plugin)\n\n---\n\n## üîå VS Code + GitHub Copilot Integration\n\n**New in v0.6.0:** Full support for VS Code with GitHub Copilot integration through MCP.\n\n### Quick Setup\n\n1. **Copy template files to your project:**\n   ```bash\n   cp -r templates/vscode/.vscode your-project/\n   cp -r templates/vscode/.github your-project/\n   ```\n\n2. **Open project in VS Code:**\n   ```bash\n   cd your-project\n   code .\n   ```\n\n3. **Reload window:**\n   - Press `Cmd+Shift+P` (Mac) or `Ctrl+Shift+P` (Windows/Linux)\n   - Run: `Developer: Reload Window`\n\n4. **Verify setup:**\n   - Open Copilot Chat\n   - Click tools icon\n   - Verify \"dincoder\" appears in tools list\n\n### Using DinCoder with Copilot\n\nIn Copilot Chat, use MCP prompts or reference tools directly:\n\n```\n/mcp.dincoder.start_project my-app\n/mcp.dincoder.create_spec \"Build a task management API\"\n#dincoder.tasks_stats\n```\n\n### What's Included\n\n‚ú® **Template Files:**\n- `.vscode/mcp.json` - MCP server configuration\n- `.vscode/settings.json` - VS Code MCP settings\n- `.github/copilot-instructions.md` - Context for GitHub Copilot\n\nüìñ **Comprehensive Guide:** [docs/integration/vscode.md](docs/integration/vscode.md)\n\nüéØ **Ready-to-Use Templates:** [templates/vscode/](templates/vscode/)\n\n---\n\n## üîå OpenAI Codex Integration\n\n**New in v0.6.0:** Full support for OpenAI Codex (CLI and IDE extension) through MCP.\n\n### Quick Setup (CLI - Recommended)\n\n```bash\n# Add DinCoder MCP server\ncodex mcp add dincoder -- npx -y mcp-dincoder@latest\n\n# Verify\ncodex mcp list\n```\n\n### Quick Setup (Manual Configuration)\n\n1. **Copy global config:**\n   ```bash\n   cp templates/codex/config.toml ~/.codex/config.toml\n   ```\n\n2. **Copy workspace instructions:**\n   ```bash\n   cp templates/codex/.codex your-project/\n   ```\n\n3.",
        "start_pos": 19834,
        "end_pos": 21605,
        "token_count_estimate": 442,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 12,
        "text": "ndow\n   ```\n\n### Using DinCoder with Codex\n\n**CLI Commands:**\n```bash\ncodex \"use /mcp.dincoder.start_project my-app\"\ncodex \"use /mcp.dincoder.create_spec 'Build a REST API'\"\n```\n\n**IDE Extension:**\n```\n@dincoder.specify_start\n@dincoder.tasks_filter preset=\"next\"\n```\n\n### What's Included\n\n‚ú® **Template Files:**\n- `config.toml` - Codex MCP server configuration (for `~/.codex/config.toml`)\n- `.codex/instructions.md` - Workspace-specific instructions\n\nüìñ **Comprehensive Guide:** [docs/integration/codex.md](docs/integration/codex.md)\n\nüéØ **Ready-to-Use Templates:** [templates/codex/](templates/codex/)\n\n---\n\n## üö¶ Complete Workflow Guide\n\nThis is your end-to-end guide for using DinCoder with any AI agent (Claude, Copilot, Gemini, Cursor).\n\n### Step-by-Step: From Idea to Implementation\n\n#### 0Ô∏è‚É£ **Define Project Constitution** (Optional but Recommended, 2-3 minutes)\n\n```typescript\n// In your AI agent's chat:\n\"Use constitution_create to define principles for 'task-manager' with these details:\n\nPrinciples:\n- Prefer functional programming over OOP\n- Write tests before implementation (TDD)\n- Keep bundle size under 500KB\n\nConstraints:\n- Node.js >= 20.0.0 required\n- Maximum 3 external dependencies for core functionality\n\nPreferences:\n- Libraries: React Query over Redux, Zod for validation\n- Patterns: Repository pattern for data access\"\n\n// What happens:\n// ‚úì Creates specs/001-task-manager/ directory\n// ‚úì Generates constitution.md with structured principles\n// ‚úì All future specs/plans will respect these constraints\n```\n\n**Why use this:** Constitution ensures consistency across your entire project. AI agents will reference these principles when generating specs and plans.",
        "start_pos": 21682,
        "end_pos": 23363,
        "token_count_estimate": 420,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 13,
        "text": "// ‚úì Creates specs/001-task-manager/ directory\n// ‚úì Generates spec.md template\n// ‚úì Creates contracts/ folder\n// ‚úì Initializes research.md\n```\n\n#### 2Ô∏è‚É£ **Describe What You Want** (2-5 minutes)\n\n```typescript\n\"Use specify_describe with this description:\nBuild a task management system where users can:\n- Create tasks with titles, descriptions, and due dates\n- Organize tasks into projects\n- Mark tasks as complete\n- Filter by status and project\n- Get daily summary emails\"\n\n// What happens:\n// ‚úì Updates spec.md with user stories\n// ‚úì Adds acceptance criteria\n// ‚úì Marks uncertainties with [NEEDS CLARIFICATION]\n// ‚úì Separates WHAT from HOW\n```\n\n**Pro tip**: Be specific about user needs, not implementation. Focus on **what users want** and **why they need it**.",
        "start_pos": 23530,
        "end_pos": 24293,
        "token_count_estimate": 190,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 14,
        "text": "s.md with numbered, ordered tasks\n// ‚úì Marks parallelizable tasks with [P]\n// ‚úì Orders by dependency (contracts ‚Üí tests ‚Üí implementation)\n```\n\n#### 6Ô∏è‚É£ **Implement Systematically**\n\n```typescript\n// Start with first task\n\"Let's implement T001 - Create Prisma schema for Task model\"\n\n// After completing a task\n\"Use tasks_tick with taskId 'T001'\"\n\n// See what's next\n\"Use artifacts_read with artifactType 'tasks'\"\n```\n\n### üéØ Best Practices\n\n1. **Start Small**: Begin with MVP scope, add features iteratively\n2. **Follow the Order**: Specify ‚Üí Plan ‚Üí Tasks ‚Üí Implement\n3. **Review Specs**: Always read the generated spec.md and refine it\n4. **Track Progress**: Use tasks_tick consistently\n5.",
        "start_pos": 25378,
        "end_pos": 26067,
        "token_count_estimate": 172,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 15,
        "text": "-|---------------|\n| `plan_create` | **Technical planning** | `{\"constraintsText\": \"Next.js, Prisma, PostgreSQL\"}` |\n| `spec_refine` | **Update specs** | `{\"section\": \"requirements\", \"changes\": \"...\"}` |\n| `artifacts_analyze` | **Verify alignment** | `{\"artifacts\": [\"spec\", \"plan\"]}` |\n\n#### Phase 3: TASKS ‚Äî Generate Executable Work Items\n\n| Tool | Purpose | Usage Example |\n|------|---------|---------------|\n| `tasks_generate` | **Create task list** | `{\"scope\": \"MVP\"}` |\n| `tasks_tick` | **Track completion** | `{\"taskId\": \"T001\"}` |\n| `tasks_tick_range` | **Bulk completion** | `{\"taskIds\": [\"T001-T005\"]}` |\n| `tasks_filter` | **Show actionable items** | `{\"preset\": \"next\", \"limit\": 5}` |\n| `tasks_search` | **Find tasks** | `{\"query\": \"authentication\", \"fuzzy\": true}` |\n| `tasks_visualize` | **Dependency graphs** | `{\"format\": \"mermaid\"}` |\n| `tasks_stats` | **Progress analytics** | `{\"groupBy\": \"phase\", \"includeCharts\": true}` |\n\n#### Phase 4: IMPLEMENT ‚Äî Build With Validation\n\n| Tool | Purpose | Usage Example |\n|------|---------|---------------|\n| `git_create_branch` | **Feature isolation** | `{\"branchName\": \"feature/chat-system\"}` |\n| `quality_format` | **Code formatting** | `{\"fix\": true}` |\n| `quality_lint` | **Static analysis** | `{\"fix\": true}` |\n| `quality_test` | **Execute tests** | `{\"coverage\": true}` |\n| `quality_security_audit` | **Check vulnerabilities** | `{\"fix\": false}` |\n\n---\n\n## üìö Examples\n\n### Connect with TypeScript Client\n\n```typescript\nimport { Client } from '@modelcontextprotocol/sdk/client/index.js';\nimport { StreamableHTTPClientTransport } from '@modelcontextprotocol/sdk/transport/streamable-http.js';\n\nconst transport = new StreamableHTTPClientTransport({\n  url: new URL('http://localhost:3000/mcp'),\n});\n\nconst client = new Client({\n  name: 'my-client',\n  version: '1.0.0',\n});\n\nawait client.connect(transport);\n\n// Use tools\nconst result = await client.callTool('specify_describe', {\n  description: 'A task management API',\n});\n```\n\nSee [examples/](examples/) for complete examples:\n- `local-",
        "start_pos": 27226,
        "end_pos": 29274,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 16,
        "text": "onnect(transport);\n\n// Use tools\nconst result = await client.callTool('specify_describe', {\n  description: 'A task management API',\n});\n```\n\nSee [examples/](examples/) for complete examples:\n- `local-client.ts` - Connect to local server\n- `spec-workflow.md` - Complete spec-driven workflow\n\n---\n\n## üí° Why Spec-Driven Development?\n\n**TL;DR:** Specifications are executable contracts that generate consistent, maintainable code. Change the spec ‚Üí regenerate the implementation. No more \"vibe coding.\"\n\nFor decades, code has been king. Specifications were scaffolding‚Äîbuilt, used, then discarded. **Spec-Driven Development inverts this power structure:**\n\n- **Specifications Generate Code**: The PRD isn't a guide‚Äîit's the source that produces implementation\n- **Executable Specifications**: Precise, complete specs that eliminate the gap between intent and implementation\n- **Code as Expression**: Code becomes the specification's expression in a particular language/framework\n- **Living Documentation**: Maintain software by evolving specifications, not manually updating code\n\nThis transformation is possible because AI can understand complex specifications and implement them systematically. But raw AI generation without structure produces chaos. DinCoder provides that structure through GitHub's proven Spec Kit methodology.\n\n### Why This Matters Now\n\nThree converging trends make SDD essential:\n\n1. **AI Threshold**: LLMs can reliably translate natural language specifications to working code\n2. **Complexity Growth**: Modern systems integrate dozens of services‚Äîmanual alignment becomes impossible\n3. **Change Velocity**: Requirements change rapidly‚Äîpivots are expected, not exceptional\n\nTraditional development treats changes as disruptions. SDD transforms them into systematic regenerations. Change a requirement ‚Üí update affected plans ‚Üí regenerate implementation.",
        "start_pos": 29074,
        "end_pos": 30946,
        "token_count_estimate": 468,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 17,
        "text": "ed, not exceptional\n\nTraditional development treats changes as disruptions. SDD transforms them into systematic regenerations. Change a requirement ‚Üí update affected plans ‚Üí regenerate implementation.\n\n**Read the full philosophy:** [Why Spec-Driven Development?](docs/WHY_SDD.md)\n\n---\n\n## üó∫ Roadmap\n\nDinCoder is actively evolving! We're at **v0.4.0** with **28/36 stories complete (78%)**.\n\n**Current Phase:** Phase 3 - Advanced Task Management (In Progress)\n\n**Upcoming Features:**\n- Advanced task management (filtering, search, statistics)\n- Multi-feature project support\n- Enhanced collaboration features\n- External integrations (Jira, Linear, GitHub Issues)\n\n**Vote on features and view the complete roadmap:** [docs/ROADMAP.md](docs/ROADMAP.md)\n\n---\n\n## ü§ù Contributing\n\nWe welcome contributions from the community! Whether you have:\n\n- **Feature ideas or requests** - Share your vision for new Spec Kit tools\n- **Bug reports** - Help us identify and fix issues\n- **Template improvements** - Enhance the Spec Kit templates\n- **Tool enhancements** - Extend the MCP server capabilities\n- **Documentation updates** - Improve guides and examples\n\n**Get Started:**\n1. [Open an issue](https://github.com/flight505/mcp-dincoder/issues) to discuss your idea\n2. Fork the repository and create a feature branch\n3. For development: `git clone`, `npm install`, `npm run build`, `npm test`\n4. Submit a pull request with your improvements\n\nWe appreciate all contributions, big or small!\n\n---\n\n## üìÑ License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\n## üìö References\n\n- [Model Context Protocol Specification](https://modelcontextprotocol.io)\n- [Spec Kit Documentation](https://github.com/github/spec-kit)\n- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)\n\n## üôè Acknowledgments\n\n- [Model Context Protocol](https://github.com/modelcontextprotocol) by Anthropic\n- [Spec Kit](https://github.com/spec-kit) for spec-driven development methodology\n\n---",
        "start_pos": 30746,
        "end_pos": 32728,
        "token_count_estimate": 495,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      },
      {
        "chunk_id": 18,
        "text": "ipt-sdk)\n\n## üôè Acknowledgments\n\n- [Model Context Protocol](https://github.com/modelcontextprotocol) by Anthropic\n- [Spec Kit](https://github.com/spec-kit) for spec-driven development methodology\n\n---",
        "start_pos": 32528,
        "end_pos": 32728,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "b0c002bca612180f"
      }
    ]
  },
  {
    "agent_id": "6b9b1a19d9f0519a",
    "name": "ai.smithery/hithereiamaliff-mcp-datagovmy",
    "source": "mcp",
    "source_url": "https://github.com/hithereiamaliff/mcp-datagovmy",
    "description": "This MCP server provides seamless access to Malaysia's government open data, including datasets, w‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T05:56:30.06684Z",
    "indexed_at": "2026-02-18T04:06:55.105096",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Malaysia Open Data MCP\n\n**MCP Endpoint:** `https://mcp.techmavie.digital/datagovmy/mcp`\n\n**Analytics Dashboard:** [`https://mcp.techmavie.digital/datagovmy/analytics/dashboard`](https://mcp.techmavie.digital/datagovmy/analytics/dashboard)\n\nMCP (Model Context Protocol) server for Malaysia's Open Data APIs, providing easy access to government datasets and collections.\n\nDo note that this is **NOT** an official MCP server by the Government of Malaysia or anyone from Malaysia's Open Data/Jabatan Digital Negara/Ministry of Digital team.\n\n## Features\n\n- **Enhanced Unified Search** with flexible tokenization and synonym expansion\n  - Intelligent query handling with term normalization\n  - Support for plurals and common prefixes (e.g., \"e\" in \"epayment\")\n  - Smart prioritization for different data types\n- **Parquet File Support** using pure JavaScript\n  - Parse Parquet files directly in the browser or Node.js\n  - Support for BROTLI compression\n  - Intelligent date field handling for empty date objects\n  - Increased row limits (up to 500 rows) for comprehensive data retrieval\n  - Fallback to metadata estimation when parsing fails\n  - Automatic dashboard URL mapping for visualization\n- **Hybrid Data Access Architecture**\n  - Pre-generated static indexes for efficient searching\n  - Dynamic API calls for detailed metadata\n- **Multi-Provider Geocoding**\n  - Support for Google Maps, GrabMaps, and Nominatim (OpenStreetMap)\n  - Intelligent service selection based on location and available API keys\n  - GrabMaps optimization for locations in Malaysia\n  - Automatic fallback between providers\n- **Comprehensive Data Sources**\n  - Malaysia's Data Catalogue with rich metadata\n  - Interactive Dashboards for data visualization\n  - Department of Statistics Malaysia (DOSM) data\n  - Weather forecast and warnings\n  - Public transport and GTFS data\n- **Multi-Provider Malaysian Geocoding**\n  - Optimized for Malaysian addresses and locations\n  - Three-tier geocoding system: GrabMaps, Google Maps, and Nominatim\n  - Prioritizes local knowledge with GrabMaps for better Malaysian coverage\n  - Automatic fallback to Nominatim when no API keys are provided\n\n## Architecture\n\nThis MCP server implements a hybrid approach for efficient data access:\n\n- **Pre-generated Static Indexes** for listing and searching datasets and dashboards\n- **Dynamic API Calls** only when specific dataset or dashboard details are requested\n\nThis approach provides several benefits:\n- Faster search and listing operations\n- Reduced API calls to external services\n- Consistent data access patterns\n- Up-to-date detailed information when needed\n\n## Documentation\n\n- **[TOOLS.md](./TOOLS.md)** - Detailed information about available tools and best practices\n- **[PROMPT.md](./PROMPT.md)** - AI integration guidelines and usage patterns\n\n## AI Integration\n\nWhen integrating this MCP server with AI models:\n\n1. **Use the unified search tool first** - Always start with `search_all` for any data queries\n2. **Follow the correct URL patterns** - Use `https://data.gov.my/...` and `https://open.dosm.gov.my/...`\n3. **Leverage Parquet file tools** - Use `parse_parquet_file` to access data directly or `get_parquet_info` for metadata\n4. **Use the hybrid approach** - Static indexes for listing/searching, API calls for details\n5. **Consider dashboard visualization** - For complex data, use the dashboard links provided by `find_dashboard_for_parquet`\n6. **Leverage the multi-provider Malaysian geocoding** - For Malaysian location queries, the system automatically selects the best provider (GrabMaps, Google Maps, or Nominatim) with fallback to Nominatim when no API keys are configured\n\nRefer to [PROMPT.md](./PROMPT.md) for comprehensive AI integration guidelines.\n\n## Installation\n\n```bash\nnpm install\n```\n\n## Quick Start (Hosted Server)\n\nThe easiest way to use this MCP server is via the hosted endpoint. **No installation required!**\n\n**Server URL:**\n```\nhttps://mcp.techmavie.digital/datagovmy/mcp\n```\n\n#### Using Your Own API Keys\n\nYou can provide your own API keys via URL query parameters:\n\n```\nhttps://mcp.techmavie.digital/datagovmy/mcp?googleMapsApiKey=YOUR_KEY\n```\n\nOr via headers:\n- `X-Google-Maps-Api-Key: YOUR_KEY`\n- `X-GrabMaps-Api-Key: YOUR_KEY`\n- `X-AWS-Access-Key-Id: YOUR_KEY`\n- `X-AWS-Secret-Access-Key: YOUR_KEY`\n- `X-AWS-Region: ap-southeast-5`\n\n**Supported Query Parameters:**\n\n| Parameter | Description |\n|-----------|-------------|\n| `googleMapsApiKey` | Google Maps API key for geocoding |\n| `grabMapsApiKey` | GrabMaps API key for Southeast Asia geocoding |\n| `awsAccessKeyId` | AWS Access Key ID for AWS Location Service |\n| `awsSecretAccessKey` | AWS Secret Access Key |\n| `awsRegion` | AWS Region (default: ap-southeast-5) |\n\n> **‚ö†Ô∏è Important: GrabMaps Requirements**\n> \n> To use GrabMaps geocoding, you need **ALL FOUR** parameters:\n> - `grabMapsApiKey`\n> - `awsAccessKeyId`\n> - `awsSecretAccessKey`\n> - `awsRegion`\n> \n> GrabMaps uses AWS Location Service under the hood, so AWS credentials are required alongside the GrabMaps API key.\n\n### Client Configuration\n\nFor Claude Desktop / Cursor / Windsurf, add to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"malaysia-opendata\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://mcp.techmavie.digital/datagovmy/mcp\"\n    }\n  }\n}\n```\n\nWith your own API key:\n```json\n{\n  \"mcpServers\": {\n    \"malaysia-opendata\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://mcp.techmavie.digital/datagovmy/mcp?googleMapsApiKey=YOUR_KEY\"\n    }\n  }\n}\n```\n\n## Self-Hosted (VPS)\n\nIf you prefer to run your own instance, see [deploy/DEPLOYMENT.md](deploy/DEPLOYMENT.md) for detailed VPS deployment instructions with Docker and Nginx.\n\n## Analytics Dashboard\n\nThe hosted server includes a built-in analytics dashboard:\n\n**Dashboard URL:** [`https://mcp.techmavie.digital/datagovmy/analytics/dashboard`](https://mcp.techmavie.digital/datagovmy/analytics/dashboard)\n\n### Analytics Endpoints\n\n| Endpoint | Description |\n|----------|-------------|\n| `/analytics` | Full analytics summary (JSON) |\n| `/analytics/tools` | Detailed tool usage stats (JSON) |\n| `/analytics/dashboard` | Visual dashboard with charts (HTML) |\n\nThe dashboard tracks:\n- Total requests and tool calls\n- Tool usage distribution\n- Hourly request trends (last 24 hours)\n- Requests by endpoint\n- Top clients by user agent\n- Recent tool calls feed\n\nAuto-refreshes every 30 seconds.\n\n## Available Tools\n\n### Data Catalogue\n\n- `list_datasets`: Lists available datasets in the Data Catalogue\n- `get_dataset`: Gets data from a specific dataset in the Data Catalogue\n- `search_datasets`: Searches for datasets in the Data Catalogue\n\n### Department of Statistics Malaysia (DOSM)\n\n- `list_dosm_datasets`: Lists available datasets from DOSM\n- `get_dosm_dataset`: Gets data from a specific DOSM dataset\n\n### Parquet File Handling\n\n- `parse_parquet_file`: Parse and display data from a Parquet file URL\n  - Supports up to 500 rows for comprehensive data analysis\n  - Automatically handles empty date objects with appropriate formatting\n  - Processes BigInt values for proper JSON serialization\n- `get_parquet_info`: Get metadata and structure information about a Parquet file\n- `find_dashboard_for_parquet`: Find the corresponding dashboard URL for a Parquet file\n\n### Weather\n\n- `get_weather_forecast`: Gets weather forecast for Malaysia\n- `get_weather_warnings`: Gets current weather warnings for Malaysia\n- `get_earthquake_warnings`: Gets earthquake warnings for Malaysia\n\n### Transport\n\n- `list_transport_agencies`: Lists available transport agencies with GTFS data\n- `get_transport_data`: Gets GTFS data for a specific transport agency\n\n### GTFS Parsing\n\n- `parse_gtfs_static`: Parses GTFS Static data (ZIP files with CSV data) for a specific transport provider\n- `parse_gtfs_realtime`: Parses GTFS Realtime data (Protocol Buffer format) for vehicle positions\n- `get_transit_routes`: Extracts route information from GTFS data\n- `get_transit_stops`: Extracts stop information from GTFS data, optionally filtered by route\n\n### Test\n\n- `hello`: A simple test tool to verify that the MCP server is working correctly\n\n## Data-Catalogue Information Retrieval\n\nThe MCP server provides robust handling for data-catalogue information retrieval:\n\n### Date Handling in Parquet Files\n\n- **Empty Date Objects**: The system automatically detects and handles empty date objects in parquet files\n- **Dataset-Specific Handling**: Special handling for known datasets like `employment_sector` with annual data from 2001-2022\n- **Pattern Recognition**: Detects date patterns in existing data to maintain consistent formatting\n- **Increased Row Limits**: Supports up to 500 rows (increased from 100) for more comprehensive data analysis\n\n### BigInt Processing\n\n- **Automatic Serialization**: BigInt values are automatically converted to strings for proper JSON serialization\n- **Type Preservation**: Original types are preserved in the schema information\n\n### Schema Detection\n\n- **Automatic Type Inference**: Detects column types including special handling for date fields\n- **Consistent Representation**: Ensures date fields are consistently represented as strings\n\n## Usage Examples\n\n### Get Weather Forecast\n\n```javascript\nconst result = await tools.get_weather_forecast({\n  location: \"Kuala Lumpur\",\n  days: 3\n});\n```\n\n### Search Datasets\n\n```javascript\nconst result = await tools.search_datasets({\n  query: \"population\",\n  limit: 5\n});\n```\n\n### Parse GTFS Data\n\n```javascript\n// Parse GTFS Static data\nconst staticData = await tools.parse_gtfs_static({\n  provider: \"ktmb\"\n});\n\n// Get real-time vehicle positions\nconst realtimeData = await tools.parse_gtfs_realtime({\n  provider: \"prasarana\",\n  category: \"rapid-rail-kl\"\n});\n\n// Get transit routes\nconst routes = await tools.get_transit_routes({\n  provider: \"mybas-johor\"\n});\n\n// Get stops for a specific route\nconst stops = await tools.get_transit_stops({\n  provider: \"prasarana\",\n  category: \"rapid-rail-kl\",\n  route_id: \"LRT-KJ\"\n});\n```\n\n## API Rate Limits\n\nPlease be aware of rate limits for the underlying APIs. Excessive requests may be throttled.\n\n## Project Structure\n\n- `src/index.ts`: Main MCP server implementation and tool registration\n- `src/http-server.ts`: Streamable HTTP server for VPS deployment\n- `src/datacatalogue.tools.ts`: Data Catalogue API tools\n- `src/dashboards.tools.ts`: Dashboard access and search tools\n- `src/dosm.tools.ts`: Department of Statistics Malaysia tools\n- `src/unified-search.tools.ts`: Enhanced unified search with tokenization and synonym expansion\n- `src/parquet.tools.ts`: Parquet file parsing and metadata tools\n- `src/weather.tools.ts`: Weather forecast and warnings tools\n- `src/transport.tools.ts`: Transport and GTFS data tools\n- `src/gtfs.tools.ts`: GTFS parsing and analysis tools\n- `src/flood.tools.ts`: Flood warning and monitoring tools\n- `Dockerfile`: Docker configuration for VPS deployment\n- `docker-compose.yml`: Docker Compose configuration\n- `deploy/`: Deployment files (nginx config, deployment guide)\n- `package.json`: Project dependencies and scripts\n- `tsconfig.json`: TypeScript configuration\n\n## Local Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run HTTP server in development mode\nnpm run dev:http\n\n# Or build and run production version\nnpm run build\nnpm run start:http\n\n# Test health endpoint\ncurl http://localhost:8080/health\n\n# Test MCP endpoint\ncurl -X POST http://localhost:8080/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\"}'\n```\n\n## Troubleshooting\n\n### Container Issues\n\n```bash\n# Check container status\ndocker compose ps\n\n# View logs\ndocker compose logs -f\n\n# Restart container\ndocker compose restart\n```\n\n### Test MCP Connection\n\n```bash\n# List tools\ncurl -X POST https://mcp.techmavie.digital/datagovmy/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\"}'\n\n# Call hello tool\ncurl -X POST https://mcp.techmavie.digital/datagovmy/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/call\",\"params\":{\"name\":\"my_hello\",\"arguments\":{}}}'\n```\n\n## Configuration\n\n### Environment Variables\n\nThis project supports the following configuration options:\n\n**Geocoding Credentials (Optional. Only for GTFS Transit Features Usage)**:\n\nThe following credentials are **only needed if you plan to use the GTFS transit tools** that require geocoding services. Other features like data catalogue access, weather forecasts, and DOSM data do not require these credentials.\n\n- **googleMapsApiKey**: Optional. If provided, the system will use Google Maps API for geocoding location names to coordinates.\n- **grabMapsApiKey**: Optional. Required for GrabMaps geocoding, which is optimized for locations in Malaysia.\n- **awsAccessKeyId**: Required for GrabMaps integration. AWS access key for GrabMaps API authentication.\n- **awsSecretAccessKey**: Required for GrabMaps integration. AWS secret key for GrabMaps API authentication.\n- **awsRegion**: Required for GrabMaps integration. AWS region for GrabMaps API (e.g. 'ap-southeast-5' for Malaysia region or ap-southeast-1 for Singapore region).\n\nIf neither Google Maps nor GrabMaps API keys are provided, the GTFS transit tools will automatically fall back to using Nominatim (OpenStreetMap) API for geocoding, which is free and doesn't require credentials.\n\nYou can set these configuration options in two ways:\n\n1. **Via URL query parameters** when connecting to the hosted server (see Quick Start section)\n2. **As environment variables** for local development or self-hosted deployment\n\n#### Setting up environment variables\n\nCreate a `.env` file in the root directory:\n\n```env\nGOOGLE_MAPS_API_KEY=your_google_api_key_here\nGRABMAPS_API_KEY=your_grab_api_key_here\nAWS_ACCESS_KEY_ID=your_aws_access_key_for_grabmaps\nAWS_SECRET_ACCESS_KEY=your_aws_secret_key_for_grabmaps\nAWS_REGION=ap-southeast-5\n```\n\nThe variables will be automatically loaded when you run the server.\n\n**Note:** For Malaysian locations, GrabMaps provides the most accurate geocoding results, followed by Google Maps. If you don't provide either API key, the system will automatically use Nominatim API instead, which is free but may have less accurate results for some locations in Malaysia.\n\n**Important:** These geocoding credentials are only required for the following GTFS transit tools:\n- `get_transit_routes` - When converting location names to coordinates\n- `get_transit_stops` - When converting location names to coordinates\n- `parse_gtfs_static` - When geocoding is needed for stop locations\n\n**Note about GTFS Realtime Tools:** The `parse_gtfs_realtime` tool is currently in development and has limited availability. Real-time data access through this MCP is experimental and may not be available for all providers or routes. For up-to-date train and bus schedules, bus locations, and arrivals in real-time, please use official transit apps like Google Maps, MyRapid PULSE, Moovit, or Lugo.\n\nAll other tools like data catalogue access, dashboard search, weather forecasts, and DOSM data do not require any geocoding credentials.\n\n## License\n\nMIT - See [LICENSE](./LICENSE) file for details.\n\n## Acknowledgments\n\n- [Malaysia Open Data Portal](https://data.gov.my/)\n- [Department of Statistics Malaysia](https://open.dosm.gov.my/)\n- [Malaysian Meteorological Department](https://www.met.gov.my/)\n- [Google Maps Platform](https://developers.google.com/maps) for geocoding\n- [GrabMaps](https://grabmaps.grab.com/solutions/service-apis) for geocoding\n- [Nominatim](https://nominatim.org/) for geocoding\n- [Model Context Protocol](https://modelcontextprotocol.io/) for the MCP framework\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Perform enhanced unified search with flexible tokenization and synonym expansion",
        "Parse Parquet files directly in browser or Node.js with BROTLI compression support",
        "Access Malaysia government datasets and collections via pre-generated static indexes and dynamic API calls",
        "Provide multi-provider geocoding optimized for Malaysian locations using GrabMaps, Google Maps, and Nominatim with automatic fallback",
        "Retrieve weather forecasts, weather warnings, and earthquake warnings for Malaysia",
        "List and retrieve public transport data including GTFS static and realtime parsing",
        "Extract transit routes and stops information from GTFS data",
        "Provide analytics dashboard with detailed usage statistics and visualizations",
        "Support AI integration with recommended usage patterns and tools for data querying and visualization"
      ],
      "limitations": [
        "Not an official MCP server by the Government of Malaysia or related official agencies",
        "Requires all four parameters (GrabMaps API key, AWS Access Key ID, AWS Secret Access Key, AWS Region) to use GrabMaps geocoding",
        "Underlying APIs may enforce rate limits and throttle excessive requests",
        "Parquet file parsing supports up to 500 rows, which may limit very large datasets",
        "Fallback to metadata estimation when Parquet parsing fails, which may reduce data accuracy"
      ],
      "requirements": [
        "Optional API keys for Google Maps, GrabMaps, and AWS Location Service for geocoding features",
        "Node.js environment for local installation and development",
        "Docker and Docker Compose for VPS self-hosted deployment",
        "Network access to hosted MCP endpoint or self-hosted server",
        "Proper configuration of API keys via URL query parameters or HTTP headers for geocoding services"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, tool descriptions, architecture overview, API key requirements, limitations, and troubleshooting guidance.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Malaysia Open Data MCP\n\n**MCP Endpoint:** `https://mcp.techmavie.digital/datagovmy/mcp`\n\n**Analytics Dashboard:** [`https://mcp.techmavie.digital/datagovmy/analytics/dashboard`](https://mcp.techmavie.digital/datagovmy/analytics/dashboard)\n\nMCP (Model Context Protocol) server for Malaysia's Open Data APIs, providing easy access to government datasets and collections.\n\nDo note that this is **NOT** an official MCP server by the Government of Malaysia or anyone from Malaysia's Open Data/Jabatan Digital Negara/Ministry of Digital team.",
        "start_pos": 0,
        "end_pos": 538,
        "token_count_estimate": 134,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      },
      {
        "chunk_id": 1,
        "text": "ta\n- **Multi-Provider Malaysian Geocoding**\n  - Optimized for Malaysian addresses and locations\n  - Three-tier geocoding system: GrabMaps, Google Maps, and Nominatim\n  - Prioritizes local knowledge with GrabMaps for better Malaysian coverage\n  - Automatic fallback to Nominatim when no API keys are provided\n\n## Architecture\n\nThis MCP server implements a hybrid approach for efficient data access:\n\n- **Pre-generated Static Indexes** for listing and searching datasets and dashboards\n- **Dynamic API Calls** only when specific dataset or dashboard details are requested\n\nThis approach provides several benefits:\n- Faster search and listing operations\n- Reduced API calls to external services\n- Consistent data access patterns\n- Up-to-date detailed information when needed\n\n## Documentation\n\n- **[TOOLS.md](./TOOLS.md)** - Detailed information about available tools and best practices\n- **[PROMPT.md](./PROMPT.md)** - AI integration guidelines and usage patterns\n\n## AI Integration\n\nWhen integrating this MCP server with AI models:\n\n1. **Use the unified search tool first** - Always start with `search_all` for any data queries\n2. **Follow the correct URL patterns** - Use `https://data.gov.my/...` and `https://open.dosm.gov.my/...`\n3. **Leverage Parquet file tools** - Use `parse_parquet_file` to access data directly or `get_parquet_info` for metadata\n4. **Use the hybrid approach** - Static indexes for listing/searching, API calls for details\n5. **Consider dashboard visualization** - For complex data, use the dashboard links provided by `find_dashboard_for_parquet`\n6. **Leverage the multi-provider Malaysian geocoding** - For Malaysian location queries, the system automatically selects the best provider (GrabMaps, Google Maps, or Nominatim) with fallback to Nominatim when no API keys are configured\n\nRefer to [PROMPT.md](./PROMPT.md) for comprehensive AI integration guidelines.\n\n## Installation\n\n```bash\nnpm install\n```\n\n## Quick Start (Hosted Server)\n\nThe easiest way to use this MCP server is via the hosted endpoint.",
        "start_pos": 1848,
        "end_pos": 3878,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      },
      {
        "chunk_id": 2,
        "text": "(./PROMPT.md) for comprehensive AI integration guidelines.\n\n## Installation\n\n```bash\nnpm install\n```\n\n## Quick Start (Hosted Server)\n\nThe easiest way to use this MCP server is via the hosted endpoint. **No installation required!**\n\n**Server URL:**\n```\nhttps://mcp.techmavie.digital/datagovmy/mcp\n```\n\n#### Using Your Own API Keys\n\nYou can provide your own API keys via URL query parameters:\n\n```\nhttps://mcp.techmavie.digital/datagovmy/mcp?googleMapsApiKey=YOUR_KEY\n```\n\nOr via headers:\n- `X-Google-Maps-Api-Key: YOUR_KEY`\n- `X-GrabMaps-Api-Key: YOUR_KEY`\n- `X-AWS-Access-Key-Id: YOUR_KEY`\n- `X-AWS-Secret-Access-Key: YOUR_KEY`\n- `X-AWS-Region: ap-southeast-5`\n\n**Supported Query Parameters:**\n\n| Parameter | Description |\n|-----------|-------------|\n| `googleMapsApiKey` | Google Maps API key for geocoding |\n| `grabMapsApiKey` | GrabMaps API key for Southeast Asia geocoding |\n| `awsAccessKeyId` | AWS Access Key ID for AWS Location Service |\n| `awsSecretAccessKey` | AWS Secret Access Key |\n| `awsRegion` | AWS Region (default: ap-southeast-5) |\n\n> **‚ö†Ô∏è Important: GrabMaps Requirements**\n> \n> To use GrabMaps geocoding, you need **ALL FOUR** parameters:\n> - `grabMapsApiKey`\n> - `awsAccessKeyId`\n> - `awsSecretAccessKey`\n> - `awsRegion`\n> \n> GrabMaps uses AWS Location Service under the hood, so AWS credentials are required alongside the GrabMaps API key.\n\n### Client Configuration\n\nFor Claude Desktop / Cursor / Windsurf, add to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"malaysia-opendata\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://mcp.techmavie.digital/datagovmy/mcp\"\n    }\n  }\n}\n```\n\nWith your own API key:\n```json\n{\n  \"mcpServers\": {\n    \"malaysia-opendata\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://mcp.techmavie.digital/datagovmy/mcp?googleMapsApiKey=YOUR_KEY\"\n    }\n  }\n}\n```\n\n## Self-Hosted (VPS)\n\nIf you prefer to run your own instance, see [deploy/DEPLOYMENT.md](deploy/DEPLOYMENT.md) for detailed VPS deployment instructions with Docker and Nginx.",
        "start_pos": 3678,
        "end_pos": 5699,
        "token_count_estimate": 505,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      },
      {
        "chunk_id": 3,
        "text": "OUR_KEY\"\n    }\n  }\n}\n```\n\n## Self-Hosted (VPS)\n\nIf you prefer to run your own instance, see [deploy/DEPLOYMENT.md](deploy/DEPLOYMENT.md) for detailed VPS deployment instructions with Docker and Nginx.\n\n## Analytics Dashboard\n\nThe hosted server includes a built-in analytics dashboard:\n\n**Dashboard URL:** [`https://mcp.techmavie.digital/datagovmy/analytics/dashboard`](https://mcp.techmavie.digital/datagovmy/analytics/dashboard)\n\n### Analytics Endpoints\n\n| Endpoint | Description |\n|----------|-------------|\n| `/analytics` | Full analytics summary (JSON) |\n| `/analytics/tools` | Detailed tool usage stats (JSON) |\n| `/analytics/dashboard` | Visual dashboard with charts (HTML) |\n\nThe dashboard tracks:\n- Total requests and tool calls\n- Tool usage distribution\n- Hourly request trends (last 24 hours)\n- Requests by endpoint\n- Top clients by user agent\n- Recent tool calls feed\n\nAuto-refreshes every 30 seconds.",
        "start_pos": 5499,
        "end_pos": 6411,
        "token_count_estimate": 228,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      },
      {
        "chunk_id": 4,
        "text": "or Malaysia\n- `get_weather_warnings`: Gets current weather warnings for Malaysia\n- `get_earthquake_warnings`: Gets earthquake warnings for Malaysia\n\n### Transport\n\n- `list_transport_agencies`: Lists available transport agencies with GTFS data\n- `get_transport_data`: Gets GTFS data for a specific transport agency\n\n### GTFS Parsing\n\n- `parse_gtfs_static`: Parses GTFS Static data (ZIP files with CSV data) for a specific transport provider\n- `parse_gtfs_realtime`: Parses GTFS Realtime data (Protocol Buffer format) for vehicle positions\n- `get_transit_routes`: Extracts route information from GTFS data\n- `get_transit_stops`: Extracts stop information from GTFS data, optionally filtered by route\n\n### Test\n\n- `hello`: A simple test tool to verify that the MCP server is working correctly\n\n## Data-Catalogue Information Retrieval\n\nThe MCP server provides robust handling for data-catalogue information retrieval:\n\n### Date Handling in Parquet Files\n\n- **Empty Date Objects**: The system automatically detects and handles empty date objects in parquet files\n- **Dataset-Specific Handling**: Special handling for known datasets like `employment_sector` with annual data from 2001-2022\n- **Pattern Recognition**: Detects date patterns in existing data to maintain consistent formatting\n- **Increased Row Limits**: Supports up to 500 rows (increased from 100) for more comprehensive data analysis\n\n### BigInt Processing\n\n- **Automatic Serialization**: BigInt values are automatically converted to strings for proper JSON serialization\n- **Type Preservation**: Original types are preserved in the schema information\n\n### Schema Detection\n\n- **Automatic Type Inference**: Detects column types including special handling for date fields\n- **Consistent Representation**: Ensures date fields are consistently represented as strings\n\n## Usage Examples\n\n### Get Weather Forecast\n\n```javascript\nconst result = await tools.get_weather_forecast({\n  location: \"Kuala Lumpur\",\n  days: 3\n});\n```\n\n### Search Datasets\n\n```javascript\nconst result = await tools.searc",
        "start_pos": 7347,
        "end_pos": 9395,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      },
      {
        "chunk_id": 5,
        "text": "Get Weather Forecast\n\n```javascript\nconst result = await tools.get_weather_forecast({\n  location: \"Kuala Lumpur\",\n  days: 3\n});\n```\n\n### Search Datasets\n\n```javascript\nconst result = await tools.search_datasets({\n  query: \"population\",\n  limit: 5\n});\n```\n\n### Parse GTFS Data\n\n```javascript\n// Parse GTFS Static data\nconst staticData = await tools.parse_gtfs_static({\n  provider: \"ktmb\"\n});\n\n// Get real-time vehicle positions\nconst realtimeData = await tools.parse_gtfs_realtime({\n  provider: \"prasarana\",\n  category: \"rapid-rail-kl\"\n});\n\n// Get transit routes\nconst routes = await tools.get_transit_routes({\n  provider: \"mybas-johor\"\n});\n\n// Get stops for a specific route\nconst stops = await tools.get_transit_stops({\n  provider: \"prasarana\",\n  category: \"rapid-rail-kl\",\n  route_id: \"LRT-KJ\"\n});\n```\n\n## API Rate Limits\n\nPlease be aware of rate limits for the underlying APIs. Excessive requests may be throttled.",
        "start_pos": 9195,
        "end_pos": 10112,
        "token_count_estimate": 229,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      },
      {
        "chunk_id": 6,
        "text": "Project dependencies and scripts\n- `tsconfig.json`: TypeScript configuration\n\n## Local Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run HTTP server in development mode\nnpm run dev:http\n\n# Or build and run production version\nnpm run build\nnpm run start:http\n\n# Test health endpoint\ncurl http://localhost:8080/health\n\n# Test MCP endpoint\ncurl -X POST http://localhost:8080/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\"}'\n```\n\n## Troubleshooting\n\n### Container Issues\n\n```bash\n# Check container status\ndocker compose ps\n\n# View logs\ndocker compose logs -f\n\n# Restart container\ndocker compose restart\n```\n\n### Test MCP Connection\n\n```bash\n# List tools\ncurl -X POST https://mcp.techmavie.digital/datagovmy/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/list\"}'\n\n# Call hello tool\ncurl -X POST https://mcp.techmavie.digital/datagovmy/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"tools/call\",\"params\":{\"name\":\"my_hello\",\"arguments\":{}}}'\n```\n\n## Configuration\n\n### Environment Variables\n\nThis project supports the following configuration options:\n\n**Geocoding Credentials (Optional. Only for GTFS Transit Features Usage)**:\n\nThe following credentials are **only needed if you plan to use the GTFS transit tools** that require geocoding services. Other features like data catalogue access, weather forecasts, and DOSM data do not require these credentials.\n\n- **googleMapsApiKey**: Optional. If provided, the system will use Google Maps API for geocoding location names to coordinates.\n- **grabMapsApiKey**: Optional. Required for GrabMaps geocoding, which is optimized for locations in Malaysia.\n- **awsAccessKeyId**: Required for GrabMaps integration. AWS access key for GrabMaps API authentication.\n- **awsSecretAccessKey**: Required for GrabMaps integration. AWS secret key for GrabMaps API authentication.\n- **awsRegion**: Required for GrabMaps integration. AWS region for GrabMaps API (e.g.",
        "start_pos": 11043,
        "end_pos": 13083,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      },
      {
        "chunk_id": 7,
        "text": "tion.\n- **awsSecretAccessKey**: Required for GrabMaps integration. AWS secret key for GrabMaps API authentication.\n- **awsRegion**: Required for GrabMaps integration. AWS region for GrabMaps API (e.g. 'ap-southeast-5' for Malaysia region or ap-southeast-1 for Singapore region).\n\nIf neither Google Maps nor GrabMaps API keys are provided, the GTFS transit tools will automatically fall back to using Nominatim (OpenStreetMap) API for geocoding, which is free and doesn't require credentials.\n\nYou can set these configuration options in two ways:\n\n1. **Via URL query parameters** when connecting to the hosted server (see Quick Start section)\n2. **As environment variables** for local development or self-hosted deployment\n\n#### Setting up environment variables\n\nCreate a `.env` file in the root directory:\n\n```env\nGOOGLE_MAPS_API_KEY=your_google_api_key_here\nGRABMAPS_API_KEY=your_grab_api_key_here\nAWS_ACCESS_KEY_ID=your_aws_access_key_for_grabmaps\nAWS_SECRET_ACCESS_KEY=your_aws_secret_key_for_grabmaps\nAWS_REGION=ap-southeast-5\n```\n\nThe variables will be automatically loaded when you run the server.\n\n**Note:** For Malaysian locations, GrabMaps provides the most accurate geocoding results, followed by Google Maps. If you don't provide either API key, the system will automatically use Nominatim API instead, which is free but may have less accurate results for some locations in Malaysia.\n\n**Important:** These geocoding credentials are only required for the following GTFS transit tools:\n- `get_transit_routes` - When converting location names to coordinates\n- `get_transit_stops` - When converting location names to coordinates\n- `parse_gtfs_static` - When geocoding is needed for stop locations\n\n**Note about GTFS Realtime Tools:** The `parse_gtfs_realtime` tool is currently in development and has limited availability. Real-time data access through this MCP is experimental and may not be available for all providers or routes.",
        "start_pos": 12883,
        "end_pos": 14821,
        "token_count_estimate": 484,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      },
      {
        "chunk_id": 8,
        "text": "** The `parse_gtfs_realtime` tool is currently in development and has limited availability. Real-time data access through this MCP is experimental and may not be available for all providers or routes. For up-to-date train and bus schedules, bus locations, and arrivals in real-time, please use official transit apps like Google Maps, MyRapid PULSE, Moovit, or Lugo.\n\nAll other tools like data catalogue access, dashboard search, weather forecasts, and DOSM data do not require any geocoding credentials.\n\n## License\n\nMIT - See [LICENSE](./LICENSE) file for details.\n\n## Acknowledgments\n\n- [Malaysia Open Data Portal](https://data.gov.my/)\n- [Department of Statistics Malaysia](https://open.dosm.gov.my/)\n- [Malaysian Meteorological Department](https://www.met.gov.my/)\n- [Google Maps Platform](https://developers.google.com/maps) for geocoding\n- [GrabMaps](https://grabmaps.grab.com/solutions/service-apis) for geocoding\n- [Nominatim](https://nominatim.org/) for geocoding\n- [Model Context Protocol](https://modelcontextprotocol.io/) for the MCP framework",
        "start_pos": 14621,
        "end_pos": 15677,
        "token_count_estimate": 263,
        "source_type": "readme",
        "agent_id": "6b9b1a19d9f0519a"
      }
    ]
  },
  {
    "agent_id": "b89eccae5afe1c5a",
    "name": "ai.smithery/hithereiamaliff-mcp-nextcloud",
    "source": "mcp",
    "source_url": "https://github.com/hithereiamaliff/mcp-nextcloud",
    "description": "A comprehensive Model Context Protocol (MCP) server that enables AI assistants to interact with yo‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T18:40:25.891252Z",
    "indexed_at": "2026-02-18T04:06:56.606534",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Nextcloud MCP Server\n\n**MCP Endpoint:** `https://mcp.techmavie.digital/nextcloud/mcp`\n\n**Analytics Dashboard:** [`https://mcp.techmavie.digital/nextcloud/analytics/dashboard`](https://mcp.techmavie.digital/nextcloud/analytics/dashboard)\n\n> **Note:** This project is a complete rewrite in TypeScript of the original Python-based [cbcoutinho/nextcloud-mcp-server](https://github.com/cbcoutinho/nextcloud-mcp-server), now with **self-hosted VPS deployment** and **Smithery deployment support**.\n>\n> ### Key Differences from the Original Repository:\n> *   **Language:** This project is written in TypeScript, while the original is in Python.\n> *   **Smithery Support:** Added full support for Smithery deployment and local testing via Smithery playground.\n> *   **Project Structure:** The project structure has been adapted for a Node.js/TypeScript environment with MCP SDK integration.\n> *   **Dependencies:** This project uses npm for package management, whereas the original uses Python's dependency management tools.\n> *   **Deployment:** Now supports both local development and cloud deployment via Smithery.\n\nThe Nextcloud MCP (Model Context Protocol) server allows Large Language Models (LLMs) like OpenAI's GPT, Google's Gemini, or Anthropic's Claude to interact with your Nextcloud instance. This enables automation of various Nextcloud actions across Notes, Calendar, Contacts, Tables, and WebDAV file operations.\n\n## Features\n\nThe server provides integration with multiple Nextcloud apps, enabling LLMs to interact with your Nextcloud data through a comprehensive set of **30 tools** across 5 main categories.\n\n## Supported Nextcloud Apps\n\n| App | Support Status | Description |\n|-----|----------------|-------------|\n| **Notes** | ‚úÖ Full Support | Create, read, update, delete, search, and append to notes. |\n| **Calendar** | ‚úÖ Full Support | Complete calendar integration - manage calendars and events via CalDAV. |\n| **Tables** | ‚úÖ Full Support | Complete table operations - list tables, get schemas, and perform CRUD operations on rows. |\n| **Files (WebDAV)** | ‚úÖ Full Support | Complete file system access - browse directories, read/write files, create/delete resources. |\n| **Contacts** | ‚úÖ Full Support | Create, read, update, and delete contacts and address books via CardDAV. |\n\n## Available Tools (30 Total)\n\n### üìù Notes Tools (5 tools)\n\n| Tool | Description |\n|------|-------------|\n| `nextcloud_notes_create_note` | Create a new note with title, content, and category |\n| `nextcloud_notes_update_note` | Update an existing note by ID with optional title, content, or category |\n| `nextcloud_notes_append_content` | Append content to an existing note with a clear separator |\n| `nextcloud_notes_search_notes` | Search notes by title or content with result filtering |\n| `nextcloud_notes_delete_note` | Delete a note by ID |\n\n### üìÖ Calendar Tools (6 tools)\n\n| Tool | Description |\n|------|-------------|\n| `nextcloud_calendar_list_calendars` | List all available calendars for the user |\n| `nextcloud_calendar_create_event` | Create a calendar event with summary, description, dates, and location |\n| `nextcloud_calendar_list_events` | List events from a calendar with optional date filtering |\n| `nextcloud_calendar_get_event` | Get detailed information about a specific event |\n| `nextcloud_calendar_update_event` | Update any aspect of an existing event |\n| `nextcloud_calendar_delete_event` | Delete a calendar event |\n\n### üë• Contacts Tools (6 tools)\n\n| Tool | Description |\n|------|-------------|\n| `nextcloud_contacts_list_addressbooks` | List all available addressbooks for the user |\n| `nextcloud_contacts_create_addressbook` | Create a new addressbook with display name and description |\n| `nextcloud_contacts_delete_addressbook` | Delete an addressbook by ID |\n| `nextcloud_contacts_list_contacts` | List all contacts in a specific addressbook |\n| `nextcloud_contacts_create_contact` | Create a new contact with full name, emails, phones, addresses, and organizations |\n| `nextcloud_contacts_delete_contact` | Delete a contact from an addressbook |\n\n### üìä Tables Tools (6 tools)\n\n| Tool | Description |\n|------|-------------|\n| `nextcloud_tables_list_tables` | List all tables available to the user |\n| `nextcloud_tables_get_schema` | Get the schema/structure of a specific table including columns |\n| `nextcloud_tables_read_table` | Read all rows from a table |\n| `nextcloud_tables_insert_row` | Insert a new row into a table with key-value data |\n| `nextcloud_tables_update_row` | Update an existing row in a table |\n| `nextcloud_tables_delete_row` | Delete a row from a table |\n\n### üìÅ WebDAV File System Tools (6 tools)\n\n| Tool | Description |\n|------|-------------|\n| `nextcloud_webdav_search_files` | **üîç NEW!** Unified search across filenames, content, and metadata - no need to specify exact paths |\n| `nextcloud_webdav_list_directory` | List files and directories in any Nextcloud path |\n| `nextcloud_webdav_read_file` | Read file content from Nextcloud |\n| `nextcloud_webdav_write_file` | Create or update files in Nextcloud with content |\n| `nextcloud_webdav_create_directory` | Create new directories in Nextcloud |\n| `nextcloud_webdav_delete_resource` | Delete files or directories from Nextcloud |\n\n## üîç Revolutionary Unified WebDAV Search Feature\n\nThe crown jewel of this MCP server is the powerful **unified search system** for WebDAV files, inspired by modern search interfaces like on an another MCP that I have created: [mcp-datagovmy](https://github.com/hithereiamaliff/mcp-datagovmy). This completely transforms how you interact with your Nextcloud files by eliminating the need to specify exact file paths.\n\n### ‚ú® Key Features\n\n- **üéØ Multi-scope Search**: Search across filenames, file content, and metadata simultaneously\n- **üß† Smart File Type Detection**: Automatically handles text files, code, configuration files, documents, and media\n- **üîß Advanced Filtering**: Filter by file type, size range, modification date, and directory\n- **üìà Intelligent Ranking**: Results ranked by relevance with bonuses for recent files and exact matches\n- **üëÄ Content Preview**: Optional content previews for matched text files\n- **‚ö° Performance Optimized**: Intelligent caching, timeout protection, and parallel processing\n- **üõ°Ô∏è Error Recovery**: Fallback strategies prevent timeouts and provide helpful suggestions\n\n### üöÄ Usage Examples\n\n```typescript\n// Basic search - find all files containing \"FAQ Dean List\"\nawait nextcloud_webdav_search_files({\n  query: \"FAQ Dean List\"\n});\n\n// Advanced search - find PDF reports from 2024\nawait nextcloud_webdav_search_files({\n  query: \"report 2024\",\n  fileTypes: [\"pdf\"],\n  searchIn: [\"filename\", \"content\"],\n  limit: 20,\n  includeContent: true,\n  quickSearch: true\n});\n\n// Directory-specific search with date range\nawait nextcloud_webdav_search_files({\n  query: \"meeting notes\",\n  basePath: \"/Documents\",\n  searchIn: [\"filename\", \"content\"],\n  dateRange: {\n    from: \"2024-01-01\",\n    to: \"2024-12-31\"\n  }\n});\n\n// Search by file characteristics\nawait nextcloud_webdav_search_files({\n  query: \"configuration files\",\n  sizeRange: { min: 1024, max: 102400 }, // 1KB - 100KB\n  fileTypes: [\"json\", \"yaml\", \"xml\", \"conf\"]\n});\n\n// Quick search for large directories (optimized)\nawait nextcloud_webdav_search_files({\n  query: \"budget\",\n  basePath: \"/\", // Root directory\n  quickSearch: true, // Enables optimizations\n  limit: 25,\n  maxDepth: 2 // Limit search depth\n});\n```\n\n### üìã Complete Parameter Reference\n\n| Parameter | Type | Default | Description | Example |\n|-----------|------|---------|-------------|---------|\n| `query` | string | *required* | Search terms - supports multiple words | `\"FAQ Dean List\"` |\n| `searchIn` | array | `[\"filename\", \"content\"]` | Search scope: `filename`, `content`, `metadata` | `[\"filename\", \"content\", \"metadata\"]` |\n| `fileTypes` | array | *all types* | File extensions to include | `[\"pdf\", \"txt\", \"md\", \"docx\"]` |\n| `basePath` | string | `\"/\"` | Directory to search in | `\"/Documents/Reports\"` |\n| `limit` | number | `50` | Maximum results to return | `20` |\n| `includeContent` | boolean | `false` | Include content previews for text files | `true` |\n| `caseSensitive` | boolean | `false` | Case-sensitive matching | `true` |\n| `quickSearch` | boolean | `true` | Use optimized mode for root searches | `false` |\n| `maxDepth` | number | `3` | Maximum directory depth (1-10) | `5` |\n| `sizeRange` | object | *unlimited* | File size filters in bytes | `{min: 1024, max: 1048576}` |\n| `dateRange` | object | *all dates* | Last modified date filters | `{from: \"2024-01-01\", to: \"2024-12-31\"}` |\n\n### üéØ Performance Tips\n\n- **For root directory searches**: Use `quickSearch: true` and `maxDepth: 2-3` for faster results\n- **For specific directories**: Use `basePath: \"/Documents\"` instead of searching root \"/\"\n- **For large result sets**: Add `fileTypes` filter to narrow scope\n- **For timeout issues**: Enable `quickSearch` and use smaller `limit` values\n\n### üß™ Test Tool (1 tool)\n\n| Tool | Description |\n|------|-------------|\n| `hello` | Verify server connectivity and list all available tools |\n\n## üîÑ Before vs After: The Search Revolution\n\n### **Before Unified Search**\n```typescript\n// You had to know exact paths\nawait nextcloud_webdav_read_file({\n  path: \"/Documents/Finance/Reports/Q4_Budget_Analysis_2024.pdf\"\n});\n\n// Multiple calls needed to explore\nawait nextcloud_webdav_list_directory({ path: \"/\" });\nawait nextcloud_webdav_list_directory({ path: \"/Documents\" });\nawait nextcloud_webdav_list_directory({ path: \"/Documents/Finance\" });\n// ... and so on\n```\n\n### **After Unified Search** ‚ú®\n```typescript\n// Natural language search across entire Nextcloud!\nawait nextcloud_webdav_search_files({\n  query: \"Q4 budget analysis 2024\",\n  fileTypes: [\"pdf\"]\n});\n\n// Finds files instantly regardless of location!\n```\n\n## üõ†Ô∏è Advanced Search Strategies\n\n### Content-Aware Search\nThe system intelligently extracts and searches content from:\n\n- **üìù Text Files**: `.txt`, `.md`, `.csv` - Full content indexing\n- **üíª Code Files**: `.js`, `.ts`, `.py`, `.html`, `.css` - Syntax-aware search\n- **‚öôÔ∏è Config Files**: `.json`, `.xml`, `.yaml` - Structure-aware indexing\n- **üìÑ Documents**: `.pdf`, `.docx` - Metadata and properties\n- **üé¨ Media Files**: Images, videos - EXIF data and metadata\n\n### Smart Ranking System\nResults are ranked using advanced algorithms:\n\n1. **Exact filename matches** ‚Üí 100 points\n2. **Word boundaries in filenames** ‚Üí 80 points\n3. **Partial filename matches** ‚Üí 60+ points (position bonus)\n4. **Content frequency matches** ‚Üí 50+ points (term density)\n5. **Recent file bonus** ‚Üí +10 points (last 30 days)\n6. **File type preference** ‚Üí +5 points (text/code files)\n7. **Size convenience** ‚Üí +5 points (files under 100KB)\n\n### Error Handling & Recovery\n- **üïê 20-second timeout protection** - Prevents hanging operations\n- **üîÑ Automatic fallback search** - Falls back to directory listing if indexing fails\n- **üí° Intelligent suggestions** - Provides helpful tips for optimization\n- **üìä Performance metrics** - Shows search duration and result counts\n\n## Installation\n\n### Quick Start with npm (Recommended)\n\nInstall directly from npm and run as an MCP server:\n\n```bash\n# Install globally\nnpm install -g mcp-nextcloud\n\n# Or install locally in your project\nnpm install mcp-nextcloud\n```\n\n### Usage as MCP Server\n\nAfter installation, you can run the MCP server directly:\n\n```bash\n# If installed globally\nmcp-nextcloud\n\n# If installed locally\nnpx mcp-nextcloud\n\n# Or using npm script\nnpm exec mcp-nextcloud\n```\n\n**Environment Setup**: Create a `.env` file with your Nextcloud credentials:\n\n```bash\nNEXTCLOUD_HOST=https://your.nextcloud.instance.com\nNEXTCLOUD_USERNAME=your_nextcloud_username\nNEXTCLOUD_PASSWORD=your_nextcloud_app_password\n```\n\n### Integration with LLM Applications\n\nAdd to your MCP client configuration (e.g., Claude Desktop, Continue, etc.):\n\n```json\n{\n  \"mcpServers\": {\n    \"nextcloud\": {\n      \"command\": \"mcp-nextcloud\",\n      \"env\": {\n        \"NEXTCLOUD_HOST\": \"https://your.nextcloud.instance.com\",\n        \"NEXTCLOUD_USERNAME\": \"your_username\",\n        \"NEXTCLOUD_PASSWORD\": \"your_app_password\"\n      }\n    }\n  }\n}\n```\n\n### Prerequisites\n\n*   Node.js 18+\n*   Access to a Nextcloud instance\n*   npm or yarn package manager\n\n### Local Development Setup\n\n1.  Clone the repository:\n    ```bash\n    git clone https://github.com/hithereiamaliff/mcp-nextcloud.git\n    cd mcp-nextcloud\n    ```\n\n2.  Install dependencies:\n    ```bash\n    npm install\n    ```\n\n3.  Configure your Nextcloud credentials (see Configuration section)\n\n4.  Build the project:\n    ```bash\n    npm run build\n    ```\n\n## Configuration\n\n### Environment Variables\n\nCreate a `.env` file in the root directory based on `.env.sample`:\n\n```dotenv\n# .env\nNEXTCLOUD_HOST=https://your.nextcloud.instance.com\nNEXTCLOUD_USERNAME=your_nextcloud_username\nNEXTCLOUD_PASSWORD=your_nextcloud_app_password_or_login_password\n```\n\n**Important Security Note:** Use a dedicated Nextcloud App Password instead of your regular login password. Generate one in your Nextcloud Security settings.\n\n### Smithery Configuration\n\nWhen deploying via Smithery, you can configure credentials through:\n- Environment variables (as above)\n- Smithery configuration interface (recommended for cloud deployment)\n\n## Deployment & Usage\n\n### Option 1: Hosted Server (Recommended)\n\nThe easiest way to use this MCP server is via the hosted endpoint. **No installation required!**\n\n**Endpoint:** `https://mcp.techmavie.digital/nextcloud/mcp`\n\n#### Authentication via URL Query Parameters\n\nYou can provide your Nextcloud credentials directly in the URL:\n\n```\nhttps://mcp.techmavie.digital/nextcloud/mcp?nextcloudHost=https://cloud.example.com&nextcloudUsername=your_user&nextcloudPassword=your_app_password\n```\n\n| Parameter | Description | Example |\n|-----------|-------------|---------|\n| `nextcloudHost` | Your Nextcloud instance URL | `https://cloud.example.com` |\n| `nextcloudUsername` | Your Nextcloud username | `john` |\n| `nextcloudPassword` | Your Nextcloud app password | `xxxxx-xxxxx-xxxxx-xxxxx` |\n\n#### Client Configuration (Claude Desktop / Cursor / Windsurf)\n\n```json\n{\n  \"mcpServers\": {\n    \"nextcloud\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://mcp.techmavie.digital/nextcloud/mcp?nextcloudHost=https://cloud.example.com&nextcloudUsername=your_user&nextcloudPassword=your_app_password\"\n    }\n  }\n}\n```\n\n#### Test with MCP Inspector\n\n```bash\nnpx @modelcontextprotocol/inspector\n# Select \"Streamable HTTP\"\n# Enter URL: https://mcp.techmavie.digital/nextcloud/mcp?nextcloudHost=...&nextcloudUsername=...&nextcloudPassword=...\n```\n\n### Option 2: Self-Hosted (VPS)\n\nIf you prefer to run your own instance, see [deploy/DEPLOYMENT.md](deploy/DEPLOYMENT.md) for detailed VPS deployment instructions with Docker and Nginx.\n\n```bash\n# Using Docker\ndocker compose up -d --build\n\n# Or run directly\nnpm run build\nnpm run start:http\n```\n\n### Option 3: npm Package (CLI)\n\nInstall and run as a local MCP server:\n\n```bash\nnpm install -g mcp-nextcloud\nmcp-nextcloud\n```\n\n### Option 4: Smithery Deployment\n\nFor cloud deployment via Smithery:\n\n```bash\nnpm run dev    # Local development with Smithery playground\nnpm run deploy # Deploy to Smithery cloud\n```\n\n## Publishing to npm\n\n### For Maintainers\n\nTo publish this package to npm:\n\n1. **Prepare the release:**\n   ```bash\n   npm run build\n   npm version patch|minor|major\n   ```\n\n2. **Publish to npm:**\n   ```bash\n   npm publish\n   ```\n\n3. **Verify the publication:**\n   ```bash\n   npm view mcp-nextcloud\n   ```\n\n### Publishing Checklist\n\n- [ ] All tests pass (Smithery deployment confirmed working)\n- [ ] TypeScript builds without errors (`npm run build`)\n- [ ] Version bumped appropriately (`npm version`)\n- [ ] README updated with changes\n- [ ] `.npmignore` properly excludes development files\n- [ ] CLI executable works (`dist/cli.js`)\n\n### Dual Deployment Strategy\n\nThis project supports both deployment methods simultaneously:\n\n- **Smithery**: For cloud deployment and development testing\n- **npm**: For end-user installation and MCP client integration\n\nThe Smithery configuration (`smithery.yaml`) and npm package configuration coexist without interference.\n\n## Smithery Integration\n\nThis project includes full Smithery support with:\n\n- **`smithery.yaml`**: Specifies TypeScript runtime\n- **Development server**: Local testing with hot reload\n- **One-click deployment**: Deploy to cloud with a single command\n- **Configuration management**: Secure credential handling\n- **Playground integration**: Immediate testing interface\n\n## Troubleshooting\n\n### Common Issues\n\n1. **404 Errors on WebDAV/Calendar/Contacts**: \n   - Ensure your Nextcloud credentials are correct\n   - Verify the Nextcloud apps (Calendar, Contacts) are installed and enabled\n   - Check that your app password has the necessary permissions\n\n2. **Authentication Failures**:\n   - Use an App Password instead of your regular password\n   - Verify the `NEXTCLOUD_HOST` URL is correct (including https://)\n   - Ensure the Nextcloud instance is accessible\n\n3. **Missing Tools**:\n   - Run the `hello` tool to verify all 30 tools are available\n   - Check the server logs for any initialization errors\n\n4. **Search Timeout Issues**:\n   - Use `quickSearch: true` for root directory searches\n   - Specify a `basePath` like \"/Documents\" instead of searching root \"/\"\n   - Add `fileTypes` filters to narrow the search scope\n   - Reduce `maxDepth` parameter for faster results\n\n## Development\n\n### Project Structure\n\n```\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ index.ts          # Main Smithery entry point\n‚îÇ   ‚îú‚îÄ‚îÄ http-server.ts    # Streamable HTTP server for VPS deployment\n‚îÇ   ‚îú‚îÄ‚îÄ app.ts            # Legacy entry point\n‚îÇ   ‚îú‚îÄ‚îÄ client/           # Nextcloud API clients\n‚îÇ   ‚îú‚îÄ‚îÄ models/           # TypeScript interfaces\n‚îÇ   ‚îú‚îÄ‚îÄ tools/            # Tool implementations\n‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Utility functions\n‚îú‚îÄ‚îÄ deploy/\n‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md     # VPS deployment guide\n‚îÇ   ‚îî‚îÄ‚îÄ nginx-mcp.conf    # Nginx reverse proxy config\n‚îú‚îÄ‚îÄ .github/\n‚îÇ   ‚îî‚îÄ‚îÄ workflows/\n‚îÇ       ‚îî‚îÄ‚îÄ deploy-vps.yml # GitHub Actions auto-deploy\n‚îú‚îÄ‚îÄ docker-compose.yml    # Docker deployment config\n‚îú‚îÄ‚îÄ Dockerfile            # Container build config\n‚îú‚îÄ‚îÄ smithery.yaml         # Smithery configuration\n‚îú‚îÄ‚îÄ package.json          # Project dependencies and scripts\n‚îî‚îÄ‚îÄ README.md             # This file\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Test with `npm run dev`\n5. Submit a pull request\n\n## License\n\nThis project is licensed under the GNU Affero General Public License v3.0 (AGPL-3.0) - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Original Python implementation by [cbcoutinho](https://github.com/cbcoutinho/nextcloud-mcp-server)\n- Built with [Model Context Protocol](https://modelcontextprotocol.io/)\n- Deployable via [Smithery](https://smithery.ai/)"
    },
    "llm_extracted": {
      "capabilities": [
        "Enable LLMs to interact with Nextcloud Notes including create, read, update, delete, search, and append operations",
        "Manage Nextcloud Calendar events including listing calendars, creating, reading, updating, and deleting events",
        "Handle Nextcloud Contacts and address books with full CRUD operations",
        "Perform comprehensive table operations in Nextcloud including listing tables, retrieving schemas, and CRUD on rows",
        "Access and manipulate Nextcloud files via WebDAV including browsing directories, reading, writing, creating, deleting files and directories",
        "Execute a unified, multi-scope search across Nextcloud WebDAV files by filename, content, and metadata with advanced filtering and ranking",
        "Support smart file type detection and content-aware search for text, code, config files, documents, and media",
        "Provide performance optimizations such as caching, timeout protection, parallel processing, and fallback error recovery",
        "Allow local development and cloud deployment with Smithery support and integration with MCP SDK"
      ],
      "limitations": [
        "No explicit mention of rate limits or API call quotas",
        "Timeout protection limits search operations to 20 seconds",
        "Search depth and result limits must be managed to avoid performance degradation",
        "Requires knowledge of Nextcloud credentials and environment setup for operation",
        "No mention of support for Nextcloud apps beyond Notes, Calendar, Contacts, Tables, and WebDAV"
      ],
      "requirements": [
        "Node.js environment with npm package management",
        "Nextcloud instance with valid credentials (host URL, username, app password)",
        "Environment variables NEXTCLOUD_HOST, NEXTCLOUD_USERNAME, NEXTCLOUD_PASSWORD configured in a .env file",
        "Optional Smithery deployment environment for cloud hosting and local testing",
        "MCP client configured to connect to the MCP server"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with usage examples, advanced feature explanations, limitations, and environment requirements, making it excellent quality.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Nextcloud MCP Server\n\n**MCP Endpoint:** `https://mcp.techmavie.digital/nextcloud/mcp`\n\n**Analytics Dashboard:** [`https://mcp.techmavie.digital/nextcloud/analytics/dashboard`](https://mcp.techmavie.digital/nextcloud/analytics/dashboard)\n\n> **Note:** This project is a complete rewrite in TypeScript of the original Python-based [cbcoutinho/nextcloud-mcp-server](https://github.com/cbcoutinho/nextcloud-mcp-server), now with **self-hosted VPS deployment** and **Smithery deployment support**.\n>\n> ### Key Differences from the Original Repository:\n> *   **Language:** This project is written in TypeScript, while the original is in Python.\n> *   **Smithery Support:** Added full support for Smithery deployment and local testing via Smithery playground.\n> *   **Project Structure:** The project structure has been adapted for a Node.js/TypeScript environment with MCP SDK integration.\n> *   **Dependencies:** This project uses npm for package management, whereas the original uses Python's dependency management tools.\n> *   **Deployment:** Now supports both local development and cloud deployment via Smithery.\n\nThe Nextcloud MCP (Model Context Protocol) server allows Large Language Models (LLMs) like OpenAI's GPT, Google's Gemini, or Anthropic's Claude to interact with your Nextcloud instance. This enables automation of various Nextcloud actions across Notes, Calendar, Contacts, Tables, and WebDAV file operations.\n\n## Features\n\nThe server provides integration with multiple Nextcloud apps, enabling LLMs to interact with your Nextcloud data through a comprehensive set of **30 tools** across 5 main categories.\n\n## Supported Nextcloud Apps\n\n| App | Support Status | Description |\n|-----|----------------|-------------|\n| **Notes** | ‚úÖ Full Support | Create, read, update, delete, search, and append to notes. |\n| **Calendar** | ‚úÖ Full Support | Complete calendar integration - manage calendars and events via CalDAV.",
        "start_pos": 0,
        "end_pos": 1924,
        "token_count_estimate": 481,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 1,
        "text": "-|\n| **Notes** | ‚úÖ Full Support | Create, read, update, delete, search, and append to notes. |\n| **Calendar** | ‚úÖ Full Support | Complete calendar integration - manage calendars and events via CalDAV. |\n| **Tables** | ‚úÖ Full Support | Complete table operations - list tables, get schemas, and perform CRUD operations on rows. |\n| **Files (WebDAV)** | ‚úÖ Full Support | Complete file system access - browse directories, read/write files, create/delete resources. |\n| **Contacts** | ‚úÖ Full Support | Create, read, update, and delete contacts and address books via CardDAV.",
        "start_pos": 1724,
        "end_pos": 2293,
        "token_count_estimate": 142,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 2,
        "text": "available addressbooks for the user |\n| `nextcloud_contacts_create_addressbook` | Create a new addressbook with display name and description |\n| `nextcloud_contacts_delete_addressbook` | Delete an addressbook by ID |\n| `nextcloud_contacts_list_contacts` | List all contacts in a specific addressbook |\n| `nextcloud_contacts_create_contact` | Create a new contact with full name, emails, phones, addresses, and organizations |\n| `nextcloud_contacts_delete_contact` | Delete a contact from an addressbook |\n\n### üìä Tables Tools (6 tools)\n\n| Tool | Description |\n|------|-------------|\n| `nextcloud_tables_list_tables` | List all tables available to the user |\n| `nextcloud_tables_get_schema` | Get the schema/structure of a specific table including columns |\n| `nextcloud_tables_read_table` | Read all rows from a table |\n| `nextcloud_tables_insert_row` | Insert a new row into a table with key-value data |\n| `nextcloud_tables_update_row` | Update an existing row in a table |\n| `nextcloud_tables_delete_row` | Delete a row from a table |\n\n### üìÅ WebDAV File System Tools (6 tools)\n\n| Tool | Description |\n|------|-------------|\n| `nextcloud_webdav_search_files` | **üîç NEW!** Unified search across filenames, content, and metadata - no need to specify exact paths |\n| `nextcloud_webdav_list_directory` | List files and directories in any Nextcloud path |\n| `nextcloud_webdav_read_file` | Read file content from Nextcloud |\n| `nextcloud_webdav_write_file` | Create or update files in Nextcloud with content |\n| `nextcloud_webdav_create_directory` | Create new directories in Nextcloud |\n| `nextcloud_webdav_delete_resource` | Delete files or directories from Nextcloud |\n\n## üîç Revolutionary Unified WebDAV Search Feature\n\nThe crown jewel of this MCP server is the powerful **unified search system** for WebDAV files, inspired by modern search interfaces like on an another MCP that I have created: [mcp-datagovmy](https://github.com/hithereiamaliff/mcp-datagovmy).",
        "start_pos": 3572,
        "end_pos": 5533,
        "token_count_estimate": 490,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 3,
        "text": "powerful **unified search system** for WebDAV files, inspired by modern search interfaces like on an another MCP that I have created: [mcp-datagovmy](https://github.com/hithereiamaliff/mcp-datagovmy). This completely transforms how you interact with your Nextcloud files by eliminating the need to specify exact file paths.",
        "start_pos": 5333,
        "end_pos": 5656,
        "token_count_estimate": 80,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 4,
        "text": "xml\", \"conf\"]\n});\n\n// Quick search for large directories (optimized)\nawait nextcloud_webdav_search_files({\n  query: \"budget\",\n  basePath: \"/\", // Root directory\n  quickSearch: true, // Enables optimizations\n  limit: 25,\n  maxDepth: 2 // Limit search depth\n});\n```\n\n### üìã Complete Parameter Reference\n\n| Parameter | Type | Default | Description | Example |\n|-----------|------|---------|-------------|---------|\n| `query` | string | *required* | Search terms - supports multiple words | `\"FAQ Dean List\"` |\n| `searchIn` | array | `[\"filename\", \"content\"]` | Search scope: `filename`, `content`, `metadata` | `[\"filename\", \"content\", \"metadata\"]` |\n| `fileTypes` | array | *all types* | File extensions to include | `[\"pdf\", \"txt\", \"md\", \"docx\"]` |\n| `basePath` | string | `\"/\"` | Directory to search in | `\"/Documents/Reports\"` |\n| `limit` | number | `50` | Maximum results to return | `20` |\n| `includeContent` | boolean | `false` | Include content previews for text files | `true` |\n| `caseSensitive` | boolean | `false` | Case-sensitive matching | `true` |\n| `quickSearch` | boolean | `true` | Use optimized mode for root searches | `false` |\n| `maxDepth` | number | `3` | Maximum directory depth (1-10) | `5` |\n| `sizeRange` | object | *unlimited* | File size filters in bytes | `{min: 1024, max: 1048576}` |\n| `dateRange` | object | *all dates* | Last modified date filters | `{from: \"2024-01-01\", to: \"2024-12-31\"}` |\n\n### üéØ Performance Tips\n\n- **For root directory searches**: Use `quickSearch: true` and `maxDepth: 2-3` for faster results\n- **For specific directories**: Use `basePath: \"/Documents\"` instead of searching root \"/\"\n- **For large result sets**: Add `fileTypes` filter to narrow scope\n- **For timeout issues**: Enable `quickSearch` and use smaller `limit` values\n\n### üß™ Test Tool (1 tool)\n\n| Tool | Description |\n|------|-------------|\n| `hello` | Verify server connectivity and list all available tools |\n\n## üîÑ Before vs After: The Search Revolution\n\n### **Before Unified Search**\n```typescript\n// You had to know exact paths\na",
        "start_pos": 7181,
        "end_pos": 9229,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 5,
        "text": "------|\n| `hello` | Verify server connectivity and list all available tools |\n\n## üîÑ Before vs After: The Search Revolution\n\n### **Before Unified Search**\n```typescript\n// You had to know exact paths\nawait nextcloud_webdav_read_file({\n  path: \"/Documents/Finance/Reports/Q4_Budget_Analysis_2024.pdf\"\n});\n\n// Multiple calls needed to explore\nawait nextcloud_webdav_list_directory({ path: \"/\" });\nawait nextcloud_webdav_list_directory({ path: \"/Documents\" });\nawait nextcloud_webdav_list_directory({ path: \"/Documents/Finance\" });\n// ... and so on\n```\n\n### **After Unified Search** ‚ú®\n```typescript\n// Natural language search across entire Nextcloud!\nawait nextcloud_webdav_search_files({\n  query: \"Q4 budget analysis 2024\",\n  fileTypes: [\"pdf\"]\n});\n\n// Finds files instantly regardless of location!\n```\n\n## üõ†Ô∏è Advanced Search Strategies\n\n### Content-Aware Search\nThe system intelligently extracts and searches content from:\n\n- **üìù Text Files**: `.txt`, `.md`, `.csv` - Full content indexing\n- **üíª Code Files**: `.js`, `.ts`, `.py`, `.html`, `.css` - Syntax-aware search\n- **‚öôÔ∏è Config Files**: `.json`, `.xml`, `.yaml` - Structure-aware indexing\n- **üìÑ Documents**: `.pdf`, `.docx` - Metadata and properties\n- **üé¨ Media Files**: Images, videos - EXIF data and metadata\n\n### Smart Ranking System\nResults are ranked using advanced algorithms:\n\n1. **Exact filename matches** ‚Üí 100 points\n2. **Word boundaries in filenames** ‚Üí 80 points\n3. **Partial filename matches** ‚Üí 60+ points (position bonus)\n4. **Content frequency matches** ‚Üí 50+ points (term density)\n5. **Recent file bonus** ‚Üí +10 points (last 30 days)\n6. **File type preference** ‚Üí +5 points (text/code files)\n7.",
        "start_pos": 9029,
        "end_pos": 10693,
        "token_count_estimate": 416,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 6,
        "text": "* - Falls back to directory listing if indexing fails\n- **üí° Intelligent suggestions** - Provides helpful tips for optimization\n- **üìä Performance metrics** - Shows search duration and result counts\n\n## Installation\n\n### Quick Start with npm (Recommended)\n\nInstall directly from npm and run as an MCP server:\n\n```bash\n# Install globally\nnpm install -g mcp-nextcloud\n\n# Or install locally in your project\nnpm install mcp-nextcloud\n```\n\n### Usage as MCP Server\n\nAfter installation, you can run the MCP server directly:\n\n```bash\n# If installed globally\nmcp-nextcloud\n\n# If installed locally\nnpx mcp-nextcloud\n\n# Or using npm script\nnpm exec mcp-nextcloud\n```\n\n**Environment Setup**: Create a `.env` file with your Nextcloud credentials:\n\n```bash\nNEXTCLOUD_HOST=https://your.nextcloud.instance.com\nNEXTCLOUD_USERNAME=your_nextcloud_username\nNEXTCLOUD_PASSWORD=your_nextcloud_app_password\n```\n\n### Integration with LLM Applications\n\nAdd to your MCP client configuration (e.g., Claude Desktop, Continue, etc.):\n\n```json\n{\n  \"mcpServers\": {\n    \"nextcloud\": {\n      \"command\": \"mcp-nextcloud\",\n      \"env\": {\n        \"NEXTCLOUD_HOST\": \"https://your.nextcloud.instance.com\",\n        \"NEXTCLOUD_USERNAME\": \"your_username\",\n        \"NEXTCLOUD_PASSWORD\": \"your_app_password\"\n      }\n    }\n  }\n}\n```\n\n### Prerequisites\n\n*   Node.js 18+\n*   Access to a Nextcloud instance\n*   npm or yarn package manager\n\n### Local Development Setup\n\n1.  Clone the repository:\n    ```bash\n    git clone https://github.com/hithereiamaliff/mcp-nextcloud.git\n    cd mcp-nextcloud\n    ```\n\n2.  Install dependencies:\n    ```bash\n    npm install\n    ```\n\n3.  Configure your Nextcloud credentials (see Configuration section)\n\n4.",
        "start_pos": 10877,
        "end_pos": 12566,
        "token_count_estimate": 422,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 7,
        "text": "env.sample`:\n\n```dotenv\n# .env\nNEXTCLOUD_HOST=https://your.nextcloud.instance.com\nNEXTCLOUD_USERNAME=your_nextcloud_username\nNEXTCLOUD_PASSWORD=your_nextcloud_app_password_or_login_password\n```\n\n**Important Security Note:** Use a dedicated Nextcloud App Password instead of your regular login password. Generate one in your Nextcloud Security settings.\n\n### Smithery Configuration\n\nWhen deploying via Smithery, you can configure credentials through:\n- Environment variables (as above)\n- Smithery configuration interface (recommended for cloud deployment)\n\n## Deployment & Usage\n\n### Option 1: Hosted Server (Recommended)\n\nThe easiest way to use this MCP server is via the hosted endpoint. **No installation required!**\n\n**Endpoint:** `https://mcp.techmavie.digital/nextcloud/mcp`\n\n#### Authentication via URL Query Parameters\n\nYou can provide your Nextcloud credentials directly in the URL:\n\n```\nhttps://mcp.techmavie.digital/nextcloud/mcp?nextcloudHost=https://cloud.example.com&nextcloudUsername=your_user&nextcloudPassword=your_app_password\n```\n\n| Parameter | Description | Example |\n|-----------|-------------|---------|\n| `nextcloudHost` | Your Nextcloud instance URL | `https://cloud.example.com` |\n| `nextcloudUsername` | Your Nextcloud username | `john` |\n| `nextcloudPassword` | Your Nextcloud app password | `xxxxx-xxxxx-xxxxx-xxxxx` |\n\n#### Client Configuration (Claude Desktop / Cursor / Windsurf)\n\n```json\n{\n  \"mcpServers\": {\n    \"nextcloud\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://mcp.techmavie.digital/nextcloud/mcp?nextcloudHost=https://cloud.example.com&nextcloudUsername=your_user&nextcloudPassword=your_app_password\"\n    }\n  }\n}\n```\n\n#### Test with MCP Inspector\n\n```bash\nnpx @modelcontextprotocol/inspector\n# Select \"Streamable HTTP\"\n# Enter URL: https://mcp.techmavie.digital/nextcloud/mcp?nextcloudHost=...&nextcloudUsername=...&nextcloudPassword=...",
        "start_pos": 12725,
        "end_pos": 14620,
        "token_count_estimate": 473,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 8,
        "text": "Inspector\n\n```bash\nnpx @modelcontextprotocol/inspector\n# Select \"Streamable HTTP\"\n# Enter URL: https://mcp.techmavie.digital/nextcloud/mcp?nextcloudHost=...&nextcloudUsername=...&nextcloudPassword=...\n```\n\n### Option 2: Self-Hosted (VPS)\n\nIf you prefer to run your own instance, see [deploy/DEPLOYMENT.md](deploy/DEPLOYMENT.md) for detailed VPS deployment instructions with Docker and Nginx.\n\n```bash\n# Using Docker\ndocker compose up -d --build\n\n# Or run directly\nnpm run build\nnpm run start:http\n```\n\n### Option 3: npm Package (CLI)\n\nInstall and run as a local MCP server:\n\n```bash\nnpm install -g mcp-nextcloud\nmcp-nextcloud\n```\n\n### Option 4: Smithery Deployment\n\nFor cloud deployment via Smithery:\n\n```bash\nnpm run dev    # Local development with Smithery playground\nnpm run deploy # Deploy to Smithery cloud\n```\n\n## Publishing to npm\n\n### For Maintainers\n\nTo publish this package to npm:\n\n1. **Prepare the release:**\n   ```bash\n   npm run build\n   npm version patch|minor|major\n   ```\n\n2. **Publish to npm:**\n   ```bash\n   npm publish\n   ```\n\n3. **Verify the publication:**\n   ```bash\n   npm view mcp-nextcloud\n   ```\n\n### Publishing Checklist\n\n- [ ] All tests pass (Smithery deployment confirmed working)\n- [ ] TypeScript builds without errors (`npm run build`)\n- [ ] Version bumped appropriately (`npm version`)\n- [ ] README updated with changes\n- [ ] `.npmignore` properly excludes development files\n- [ ] CLI executable works (`dist/cli.js`)\n\n### Dual Deployment Strategy\n\nThis project supports both deployment methods simultaneously:\n\n- **Smithery**: For cloud deployment and development testing\n- **npm**: For end-user installation and MCP client integration\n\nThe Smithery configuration (`smithery.yaml`) and npm package configuration coexist without interference.",
        "start_pos": 14420,
        "end_pos": 16194,
        "token_count_estimate": 443,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 9,
        "text": "h:\n\n- **`smithery.yaml`**: Specifies TypeScript runtime\n- **Development server**: Local testing with hot reload\n- **One-click deployment**: Deploy to cloud with a single command\n- **Configuration management**: Secure credential handling\n- **Playground integration**: Immediate testing interface\n\n## Troubleshooting\n\n### Common Issues\n\n1. **404 Errors on WebDAV/Calendar/Contacts**: \n   - Ensure your Nextcloud credentials are correct\n   - Verify the Nextcloud apps (Calendar, Contacts) are installed and enabled\n   - Check that your app password has the necessary permissions\n\n2. **Authentication Failures**:\n   - Use an App Password instead of your regular password\n   - Verify the `NEXTCLOUD_HOST` URL is correct (including https://)\n   - Ensure the Nextcloud instance is accessible\n\n3. **Missing Tools**:\n   - Run the `hello` tool to verify all 30 tools are available\n   - Check the server logs for any initialization errors\n\n4.",
        "start_pos": 16268,
        "end_pos": 17199,
        "token_count_estimate": 232,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      },
      {
        "chunk_id": 10,
        "text": "Docker deployment config\n‚îú‚îÄ‚îÄ Dockerfile            # Container build config\n‚îú‚îÄ‚îÄ smithery.yaml         # Smithery configuration\n‚îú‚îÄ‚îÄ package.json          # Project dependencies and scripts\n‚îî‚îÄ‚îÄ README.md             # This file\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Test with `npm run dev`\n5. Submit a pull request\n\n## License\n\nThis project is licensed under the GNU Affero General Public License v3.0 (AGPL-3.0) - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Original Python implementation by [cbcoutinho](https://github.com/cbcoutinho/nextcloud-mcp-server)\n- Built with [Model Context Protocol](https://modelcontextprotocol.io/)\n- Deployable via [Smithery](https://smithery.ai/)",
        "start_pos": 18116,
        "end_pos": 18877,
        "token_count_estimate": 190,
        "source_type": "readme",
        "agent_id": "b89eccae5afe1c5a"
      }
    ]
  },
  {
    "agent_id": "279885362d5e80dd",
    "name": "ai.smithery/hjsh200219-pharminfo-mcp",
    "source": "mcp",
    "source_url": "https://github.com/hjsh200219/pharminfo-mcp",
    "description": "Look up Korean drug ingredient and product data by HIRA component and product codes via Pilldoc. V‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-17T06:55:47.401979Z",
    "indexed_at": "2026-02-18T04:06:58.243174",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Look up Korean drug ingredient data by HIRA component codes",
        "Look up Korean drug product data by HIRA product codes",
        "Access drug information via Pilldoc"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's lookup capabilities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "f594a806f08bf927",
    "name": "ai.smithery/hollaugo-financial-research-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/hollaugo/tutorials",
    "description": "Analyze stocks with summaries, price targets, and analyst recommendations. Track SEC filings, divi‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "analyze"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T16:56:54.604903Z",
    "indexed_at": "2026-02-18T04:07:01.861084",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# AI Agent Tutorials & Implementations\n\nA comprehensive collection of production-ready AI agent implementations showcasing different frameworks, protocols, and integration patterns. This repository demonstrates various approaches to building intelligent agents with Model Context Protocol (MCP), multi-agent systems, and real-world integrations.\n\n## Repository Overview\n\nThis repository contains multiple agent implementations, each demonstrating different architectural patterns and use cases:\n\n| Project | Framework | Key Features | Use Case |\n|---------|-----------|--------------|----------|\n| [agent2agent](#agent2agent) | LangGraph + A2A Protocol | Remote agent communication, Slack integration | Investment research |\n| [mcp-financial](#mcp-financial) | FastMCP + FastAPI | ASGI integration, CLI client | Financial data analysis |\n| [bright-mcp-server-overview](#bright-mcp-server-overview) | Dual: LangGraph + ADK | Memory persistence, extended timeouts | Web scraping & research |\n| [fpl-deepagent](#fpl-deepagent) | FastMCP + React UI | Streamable HTTP, ChatGPT integration | Fantasy Premier League |\n| [task-manager-app](#task-manager-app) | FastMCP + React UI + Supabase | OAuth (Auth0), per-user DB state, Slack notifications | Task management in ChatGPT |\n| [notion-mcp-agent](#notion-mcp-agent) | LangGraph + MCP | Notion integration, database management | Knowledge management |\n| [claude-advanced-tool-use](#claude-advanced-tool-use) | Claude API + FastMCP | PTC, Tool Search, MCP integration | Token-efficient AI agents |\n| [claude-skills](#claude-skills) | Claude Skills API | Document generation, custom skills | PowerPoint, Excel, Word creation |\n| [openai-chatkit-starter-app](#openai-chatkit-starter-app) | Next.js + ChatKit | Agent Builder integration, web component | ChatKit UI development |\n| [mastra-overview](#mastra-overview) | Mastra framework | Multi-LLM orchestration | Framework exploration |\n| [smithery-example](#smithery-example) | Smithery + FastMCP | MCP playground, development tools | MCP development |\n| [mcp-apps](#mcp-apps) | MCP Apps (OpenAI Apps SDK) | Example MCP Apps (weather + stock analysis) | MCP Apps reference implementations |\n\n## Project Descriptions\n\n### agent2agent/\n**Investment Research Analyst Agent**\n\nA production-ready investment research agent implementing Google's Agent-to-Agent (A2A) protocol for remote agent communication.\n\n**Key Features:**\n- **Framework**: LangGraph with LangChain\n- **Protocol**: Agent-to-Agent (A2A) for remote communication\n- **Integration**: Slack with Block Kit UI and metadata modals\n- **Architecture**: FastAPI server exposing both A2A endpoints and Slack events\n- **Memory**: Persistent conversation state management\n- **Deployment**: Docker ready with Render.com configuration\n\n**Technical Stack:**\n- LangGraph for agent orchestration\n- FastAPI for A2A protocol implementation\n- Slack Block Kit for interactive UI\n- LangSmith for observability (optional)\n- Docker for containerized deployment\n\n**Use Cases:**\n- Stock summaries and analysis\n- SEC filings research\n- Analyst recommendations\n- Financial data aggregation\n- Investment research workflows\n\n### mcp-financial/\n**Investment Analyst MCP Agent**\n\nA financial data agent powered by FastMCP with ASGI integration, providing both CLI and Slack interfaces.\n\n**Key Features:**\n- **Framework**: FastMCP with FastAPI ASGI integration\n- **Interfaces**: CLI client and Slack bot\n- **Architecture**: MCP server exposed via FastAPI endpoints\n- **Integration**: Direct Slack event handling\n- **Deployment**: Production-ready with health checks\n\n**Technical Stack:**\n- FastMCP for Model Context Protocol implementation\n- FastAPI for ASGI integration\n- Uvicorn for server runtime\n- Slack API for bot functionality\n- MCP Inspector for debugging\n\n**Use Cases:**\n- Financial data analysis\n- Stock price monitoring\n- Earnings analysis\n- Market research\n- Investment insights\n\n### bright-mcp-server-overview/\n**Bright Data MCP Research Agent**\n\nA comprehensive research agent powered by Bright Data's web scraping infrastructure, featuring dual AI agent implementations.\n\n**Key Features:**\n- **Dual Framework**: LangGraph (with memory) + Google ADK (with extended timeouts)\n- **Integration**: Bright Data MCP server for web scraping\n- **Slack Interface**: Interactive agent selection via dropdown\n- **Memory**: Persistent conversation memory (LangGraph)\n- **Timeouts**: Extended timeout handling (ADK) for long operations\n- **Specialization**: SEO research, e-commerce intelligence, market analysis\n\n**Technical Stack:**\n- **LangGraph Agent**: OpenAI GPT with MemorySaver checkpointer\n- **ADK Agent**: Google Gemini 2.0 Flash with custom timeout patches\n- **MCP Integration**: Bright Data MCP server for data collection\n- **Slack Integration**: Bot with agent selection and interactive UI\n\n**Agent Comparison:**\n| Feature | LangGraph Agent | ADK Agent |\n|---------|----------------|-----------|\n| Memory | Persistent (checkpointer) | Context-aware (5 messages) |\n| Timeout | Standard (5s) | Extended (60s) |\n| Model | OpenAI GPT | Gemini 2.0 Flash |\n| Best For | Interactive conversations | Long-running operations |\n\n**Use Cases:**\n- SEO keyword research and SERP analysis\n- E-commerce product monitoring and price tracking\n- Competitor analysis and market intelligence\n- Web scraping and data collection\n- Business intelligence and insights\n\n### fpl-deepagent/\n**Fantasy Premier League MCP Assistant**\n\nA comprehensive Fantasy Premier League assistant that integrates with ChatGPT through the Model Context Protocol (MCP), featuring beautiful React UI components and real-time FPL data.\n\n**Key Features:**\n- **Framework**: FastMCP with Streamable HTTP transport\n- **UI Integration**: React 18 + TypeScript components for ChatGPT\n- **Real-time Data**: Live FPL API integration with caching and error handling\n- **Design Compliance**: Follows OpenAI Apps SDK design guidelines exactly\n- **Interactive Tools**: Player search, detailed stats, and side-by-side comparison\n\n**Technical Stack:**\n- FastMCP for MCP server implementation\n- React 18 + TypeScript for UI components\n- OpenAI Apps SDK integration with `window.openai` API\n- esbuild for fast, modern bundling\n- Streamable HTTP for bidirectional communication\n\n**UI Components:**\n- **PlayerListComponent**: Interactive player grid with favorites\n- **PlayerDetailComponent**: Detailed player stats and upcoming fixtures\n- **PlayerComparisonComponent**: Side-by-side comparison with highlighted stats\n\n**Use Cases:**\n- Player search and discovery\n- Detailed player statistics and form analysis\n- Player comparison for team selection\n- FPL team optimization\n- Real-time price and form tracking\n\n### task-manager-app/\n**Task Manager ChatGPT App (Apps SDK + MCP + Supabase + OAuth)**\n\nA production-ready tutorial showing how to build a ChatGPT App with:\n- **FastMCP (Streamable HTTP)** as the MCP server\n- **React widgets** rendered inside ChatGPT\n- **Supabase (Postgres)** as authoritative state for tasks/notifications\n- **OAuth (Auth0)** for multi-user authentication (MCP OAuth)\n- Optional **Slack notifications** (send now + schedule)\n\nStart here:\n- `task-manager-app/README.md`\n\n### notion-mcp-agent/\n**Notion Knowledge Management Agent**\n\nA sophisticated agent that integrates with Notion through MCP, providing intelligent database management and knowledge organization capabilities.\n\n**Key Features:**\n- **Framework**: LangGraph with MCP integration\n- **Integration**: Notion API for database operations\n- **Slack Interface**: Interactive knowledge management\n- **Context Management**: Intelligent data aggregation\n- **Database Operations**: Create, read, update, and organize Notion databases\n\n**Technical Stack:**\n- LangGraph for agent orchestration\n- Notion MCP server for database operations\n- Slack API for user interaction\n- Context aggregation for intelligent responses\n\n**Use Cases:**\n- Knowledge base management\n- Database organization and maintenance\n- Content aggregation and structuring\n- Team collaboration workflows\n- Information retrieval and organization\n\n### claude-advanced-tool-use/\n**Claude Advanced Tool Use Tutorial**\n\nA comprehensive tutorial demonstrating Anthropic's Advanced Tool Use features: Programmatic Tool Calling (PTC) and Tool Search. These features enable AI agents to scale to thousands of tools while dramatically reducing token usage.\n\n**Key Features:**\n- **Programmatic Tool Calling (PTC)**: Claude writes Python code that orchestrates tool calls in a sandbox\n- **Tool Search**: Dynamic tool discovery with `defer_loading` for efficient context usage\n- **MCP Integration**: Tool Search combined with MCP servers via `mcp_toolset`\n- **Real-World Examples**: Financial data tools using yfinance\n- **Token Savings**: Up to 98% reduction in token usage for complex tasks\n\n**Technical Stack:**\n- Anthropic Claude API (Sonnet 4.5)\n- Beta headers: `advanced-tool-use-2025-11-20`\n- FastMCP for MCP server implementation\n- Python + yfinance for financial data\n- ngrok for MCP server tunneling\n\n**Examples:**\n- `01_ptc_token_savings.py` - Programmatic Tool Calling with token comparison\n- `02_tool_search.py` - Tool Search with 10 deferred financial tools\n- `03_mcp_tool_search.py` - MCP + Tool Search via ngrok tunnel\n- `mcp_server.py` - FastMCP server exposing financial tools\n\n**Key Concepts:**\n| Feature | Description | Token Savings |\n|---------|-------------|---------------|\n| Programmatic Tool Calling | Tool results stay in sandbox, only `print()` output enters context | 37% |\n| Tool Search | Only load tool definitions when discovered | 85% |\n| Combined | PTC + Tool Search together | Up to 98% |\n\n**Use Cases:**\n- Building AI agents with many tools (100+)\n- Reducing context window bloat from tool definitions\n- Processing large datasets without context overflow\n- MCP server integration with dynamic tool discovery\n- Token-efficient financial analysis agents\n\n### claude-skills/\n**Claude Skills API Implementation**\n\nA comprehensive implementation of Claude's Skills API for automated document generation and custom skill creation.\n\n**Key Features:**\n- **Framework**: Claude Skills API with streaming support\n- **Document Generation**: PowerPoint, Excel, Word, and PDF creation\n- **Custom Skills**: Upload and manage custom skills (8MB limit)\n- **File Management**: List, download, and delete generated files\n- **Multi-Skill Workflows**: Combine multiple skills in single requests\n\n**Technical Stack:**\n- Claude Skills API with beta features\n- Code execution environment (2025-08-25)\n- Files API (2025-04-14)\n- Streaming responses for real-time progress\n- Python SDK with uv package manager\n\n**Utilities:**\n- `list-skills.py` - List all available skills\n- `create-skill.py` - Upload custom skills from directories\n- `use-skill.py` - Generate documents with single skills\n- `multi-skill-demo.py` - Complex workflows with multiple skills\n- `list-files.py` / `download-file.py` / `delete-file.py` - File management\n\n**Use Cases:**\n- Automated PowerPoint presentation generation\n- Excel spreadsheet creation and data analysis\n- Word document generation\n- PDF report creation\n- Custom skill development and deployment\n- Multi-format document workflows\n\n### openai-chatkit-starter-app/\n**ChatKit Web Component Starter**\n\nA minimal Next.js starter template for building ChatKit applications with OpenAI's Agent Builder workflows.\n\n**Key Features:**\n- **Framework**: Next.js with ChatKit web component\n- **Integration**: OpenAI Agent Builder workflows\n- **Customization**: Configurable themes, prompts, and UI\n- **Session Management**: Ready-to-use session endpoint\n- **Deployment**: Domain allowlist verification support\n\n**Technical Stack:**\n- Next.js for application framework\n- OpenAI ChatKit web component (`<openai-chatkit>`)\n- OpenAI API integration\n- TypeScript for type safety\n- Configurable theming system\n\n**Key Components:**\n- Session creation endpoint (`/api/create-session`)\n- ChatKit panel with event handlers\n- Theme and color scheme controls\n- Starter prompts configuration\n- Error overlay for debugging\n\n**Use Cases:**\n- ChatKit application prototyping\n- Agent Builder workflow integration\n- Custom ChatKit UI development\n- OpenAI workflow testing\n- Production ChatKit deployments\n\n### mastra-overview/\n**Mastra Framework Exploration**\n\nAn exploration of the Mastra framework for multi-LLM orchestration and agent management.\n\n**Key Features:**\n- **Framework**: Mastra for multi-LLM orchestration\n- **Multi-LLM**: Support for multiple language models\n- **Orchestration**: Intelligent model selection and routing\n- **Polyfills**: Crypto polyfills for browser compatibility\n\n**Technical Stack:**\n- Mastra framework\n- Multi-LLM integration\n- Browser compatibility polyfills\n- TypeScript configuration\n\n**Use Cases:**\n- Multi-LLM agent systems\n- Model orchestration and routing\n- Framework exploration and evaluation\n- LLM comparison and benchmarking\n\n### smithery-example/\n**MCP Development Playground**\n\nA comprehensive development environment for MCP (Model Context Protocol) with FastMCP integration and testing tools.\n\n**Key Features:**\n- **Framework**: Smithery + FastMCP\n- **Development Tools**: MCP playground and testing environment\n- **Financial Integration**: Example financial server implementation\n- **Testing**: Comprehensive test suite and examples\n- **Documentation**: Development guides and examples\n\n**Technical Stack:**\n- Smithery for MCP development\n- FastMCP for server implementation\n- Testing frameworks for validation\n- Development tooling and playgrounds\n\n**Use Cases:**\n- MCP server development\n- Protocol testing and validation\n- Financial data integration examples\n- Development environment setup\n- MCP learning and exploration\n\n### mcp-apps/\n**MCP Apps Examples (Weather + Stock Analysis)**\n\nTwo minimal example MCP Apps showing how to build UI + server experiences using the MCP Apps extensions.\n\n**Key Features:**\n- **Weather App**: UI + MCP server example with a simple weather workflow\n- **Stock Analysis App**: UI + MCP server example for market/stock analysis\n- **Apps SDK**: Designed to follow MCP Apps extension patterns\n- **Docs Reference**: See the MCP Apps docs for the full guide\n\n**Use Cases:**\n- Learning MCP Apps fundamentals\n- Building UI-backed MCP Apps\n- Reference implementations for new MCP App projects\n\n## Getting Started\n\nEach project includes comprehensive setup instructions in its respective README file. General prerequisites include:\n\n### Common Requirements\n- Python 3.9+ (some projects require newer; see each project README)\n- Valid API keys for respective services\n- Slack workspace access (for Slack integrations)\n- Environment variable configuration\n\n### Quick Start Pattern\n```bash\n# 1. Navigate to desired project\ncd [project-name]/\n\n# 2. Install dependencies\n# Most Python projects here use uv:\nuv sync\n# Some projects use pip/requirements.txt:\n# pip install -r requirements.txt\n\n# 3. Configure environment\ncp .env.example .env\n# Edit .env with your API keys\n\n# 4. Run the agent\n# (varies by project - see individual READMEs)\n```\n\n## Architecture Patterns\n\n### Model Context Protocol (MCP)\nMultiple projects demonstrate different MCP implementation patterns:\n- **FastMCP ASGI**: Direct FastAPI integration (mcp-financial, smithery-example)\n- **FastMCP Streamable HTTP**: Modern bidirectional communication (fpl-deepagent)\n- **Bright Data MCP**: External MCP server communication\n- **Notion MCP**: Database and knowledge management integration\n\n### Agent Communication\n- **A2A Protocol**: Remote agent-to-agent communication (agent2agent)\n- **State Management**: Persistent conversation memory (bright-mcp-server-overview)\n\n### UI Integration Patterns\n- **React + ChatGPT**: OpenAI Apps SDK integration (fpl-deepagent)\n- **Next.js + ChatKit**: Agent Builder workflow integration (openai-chatkit-starter-app)\n- **Slack Bots**: Event-driven chat interfaces (multiple projects)\n- **CLI Clients**: Command-line agent interaction\n\n### Document Generation\n- **Claude Skills API**: Automated document creation with streaming (claude-skills)\n- **Multi-Format Support**: PowerPoint, Excel, Word, PDF generation\n- **Custom Skills**: Uploadable skill packages for specialized tasks\n\n### Development & Testing\n- **MCP Playground**: Development and testing environment (smithery-example)\n- **Multi-LLM Orchestration**: Framework exploration (mastra-overview)\n- **Agent Builder**: OpenAI workflow development (openai-chatkit-starter-app)\n\n### Integration Patterns\n- **Container Deployment**: Docker and cloud-ready\n- **API Integration**: RESTful agent endpoints\n- **Database Integration**: Knowledge management systems\n- **Real-time Data**: Live API integration with caching\n\n## Contributing\n\nEach project welcomes contributions. Please:\n\n1. Fork the repository\n2. Create a feature branch\n3. Follow the project's coding standards\n4. Include tests where applicable\n5. Submit a Pull Request\n\n## License\n\nMIT License - see individual project LICENSE files for details.\n\n## Support & Resources\n\n### Documentation Links\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n- [OpenAI Agent SDK](https://github.com/openai/agent-sdk)\n- [OpenAI Apps SDK](https://developers.openai.com/apps-sdk/)\n- [OpenAI ChatKit](http://openai.github.io/chatkit-js/)\n- [OpenAI Agent Builder](https://platform.openai.com/agent-builder)\n- [Claude Skills API](https://docs.claude.com/en/api/skills-guide)\n- [Claude Programmatic Tool Calling](https://platform.claude.com/docs/en/agents-and-tools/tool-use/programmatic-tool-calling)\n- [Claude Tool Search](https://platform.claude.com/docs/en/agents-and-tools/tool-use/tool-search-tool)\n- [Anthropic Blog - Advanced Tool Use](https://www.anthropic.com/engineering/advanced-tool-use)\n- [Anthropic Blog - Code Execution](https://www.anthropic.com/engineering/code-execution-with-mcp)\n- [Anthropic Console](https://console.anthropic.com/)\n- [Google ADK](https://developers.google.com/ai/adk)\n- [FastMCP](https://github.com/pydantic/fastmcp)\n- [Mastra Framework](https://mastra.ai/)\n- [Smithery](https://smithery.ai/)\n- [Slack API](https://api.slack.com/)\n\n### Platform-Specific Support\n- **Bright Data**: [brightdata.com/support](https://brightdata.com/support)\n- **Notion**: [developers.notion.com](https://developers.notion.com/)\n- **Fantasy Premier League**: [fpl.readthedocs.io](https://fpl.readthedocs.io/en/latest/)\n- **Slack**: [api.slack.com/support](https://api.slack.com/support)\n\n---\n\n**Built with ‚ù§Ô∏è demonstrating the future of AI agent development**\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Implement remote agent communication using Agent-to-Agent (A2A) protocol",
        "Provide financial data analysis and stock market insights via MCP servers",
        "Perform web scraping and market research with persistent memory and extended timeouts",
        "Integrate real-time Fantasy Premier League data with React UI components",
        "Manage tasks and notifications with multi-user authentication and database state",
        "Operate knowledge management and database operations through Notion API integration",
        "Enable token-efficient AI agents using programmatic tool calling and dynamic tool search",
        "Generate and manage documents (PowerPoint, Excel, Word, PDF) via Claude Skills API",
        "Develop ChatKit web applications with OpenAI Agent Builder workflows and customizable UI"
      ],
      "limitations": [
        "Extended timeout support limited to specific agents (e.g., ADK agent with 60s timeout)",
        "Custom skill uploads limited to 8MB size in Claude Skills API",
        "Token savings and tool orchestration benefits depend on using Anthropic Claude API with beta features",
        "Some integrations require external services like Slack, Supabase, Auth0, or Bright Data MCP server",
        "Real-time data integrations depend on external APIs (e.g., FPL API) and caching mechanisms"
      ],
      "requirements": [
        "API keys and tokens for Slack, Notion, Supabase, Auth0, and external data providers",
        "Docker and Render.com configuration for deployment in some agents",
        "Python environment with FastAPI, FastMCP, LangGraph, and related libraries",
        "Access to Anthropic Claude API with beta headers for advanced tool use features",
        "Next.js and TypeScript environment for ChatKit starter app development",
        "ngrok or similar tunneling service for MCP server exposure in some examples"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation and deployment details, multiple usage examples across diverse agents, detailed descriptions of tools and frameworks, explicit limitations, and clear environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# AI Agent Tutorials & Implementations\n\nA comprehensive collection of production-ready AI agent implementations showcasing different frameworks, protocols, and integration patterns. This repository demonstrates various approaches to building intelligent agents with Model Context Protocol (MCP), multi-agent systems, and real-world integrations.",
        "start_pos": 0,
        "end_pos": 345,
        "token_count_estimate": 86,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 1,
        "text": "erview) | Mastra framework | Multi-LLM orchestration | Framework exploration |\n| [smithery-example](#smithery-example) | Smithery + FastMCP | MCP playground, development tools | MCP development |\n| [mcp-apps](#mcp-apps) | MCP Apps (OpenAI Apps SDK) | Example MCP Apps (weather + stock analysis) | MCP Apps reference implementations |\n\n## Project Descriptions\n\n### agent2agent/\n**Investment Research Analyst Agent**\n\nA production-ready investment research agent implementing Google's Agent-to-Agent (A2A) protocol for remote agent communication.\n\n**Key Features:**\n- **Framework**: LangGraph with LangChain\n- **Protocol**: Agent-to-Agent (A2A) for remote communication\n- **Integration**: Slack with Block Kit UI and metadata modals\n- **Architecture**: FastAPI server exposing both A2A endpoints and Slack events\n- **Memory**: Persistent conversation state management\n- **Deployment**: Docker ready with Render.com configuration\n\n**Technical Stack:**\n- LangGraph for agent orchestration\n- FastAPI for A2A protocol implementation\n- Slack Block Kit for interactive UI\n- LangSmith for observability (optional)\n- Docker for containerized deployment\n\n**Use Cases:**\n- Stock summaries and analysis\n- SEC filings research\n- Analyst recommendations\n- Financial data aggregation\n- Investment research workflows\n\n### mcp-financial/\n**Investment Analyst MCP Agent**\n\nA financial data agent powered by FastMCP with ASGI integration, providing both CLI and Slack interfaces.",
        "start_pos": 1848,
        "end_pos": 3307,
        "token_count_estimate": 364,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 2,
        "text": "icorn for server runtime\n- Slack API for bot functionality\n- MCP Inspector for debugging\n\n**Use Cases:**\n- Financial data analysis\n- Stock price monitoring\n- Earnings analysis\n- Market research\n- Investment insights\n\n### bright-mcp-server-overview/\n**Bright Data MCP Research Agent**\n\nA comprehensive research agent powered by Bright Data's web scraping infrastructure, featuring dual AI agent implementations.\n\n**Key Features:**\n- **Dual Framework**: LangGraph (with memory) + Google ADK (with extended timeouts)\n- **Integration**: Bright Data MCP server for web scraping\n- **Slack Interface**: Interactive agent selection via dropdown\n- **Memory**: Persistent conversation memory (LangGraph)\n- **Timeouts**: Extended timeout handling (ADK) for long operations\n- **Specialization**: SEO research, e-commerce intelligence, market analysis\n\n**Technical Stack:**\n- **LangGraph Agent**: OpenAI GPT with MemorySaver checkpointer\n- **ADK Agent**: Google Gemini 2.0 Flash with custom timeout patches\n- **MCP Integration**: Bright Data MCP server for data collection\n- **Slack Integration**: Bot with agent selection and interactive UI\n\n**Agent Comparison:**\n| Feature | LangGraph Agent | ADK Agent |\n|---------|----------------|-----------|\n| Memory | Persistent (checkpointer) | Context-aware (5 messages) |\n| Timeout | Standard (5s) | Extended (60s) |\n| Model | OpenAI GPT | Gemini 2.0 Flash |\n| Best For | Interactive conversations | Long-running operations |\n\n**Use Cases:**\n- SEO keyword research and SERP analysis\n- E-commerce product monitoring and price tracking\n- Competitor analysis and market intelligence\n- Web scraping and data collection\n- Business intelligence and insights\n\n### fpl-deepagent/\n**Fantasy Premier League MCP Assistant**\n\nA comprehensive Fantasy Premier League assistant that integrates with ChatGPT through the Model Context Protocol (MCP), featuring beautiful React UI components and real-time FPL data.",
        "start_pos": 3696,
        "end_pos": 5624,
        "token_count_estimate": 482,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 3,
        "text": "MCP Assistant**\n\nA comprehensive Fantasy Premier League assistant that integrates with ChatGPT through the Model Context Protocol (MCP), featuring beautiful React UI components and real-time FPL data.\n\n**Key Features:**\n- **Framework**: FastMCP with Streamable HTTP transport\n- **UI Integration**: React 18 + TypeScript components for ChatGPT\n- **Real-time Data**: Live FPL API integration with caching and error handling\n- **Design Compliance**: Follows OpenAI Apps SDK design guidelines exactly\n- **Interactive Tools**: Player search, detailed stats, and side-by-side comparison\n\n**Technical Stack:**\n- FastMCP for MCP server implementation\n- React 18 + TypeScript for UI components\n- OpenAI Apps SDK integration with `window.openai` API\n- esbuild for fast, modern bundling\n- Streamable HTTP for bidirectional communication\n\n**UI Components:**\n- **PlayerListComponent**: Interactive player grid with favorites\n- **PlayerDetailComponent**: Detailed player stats and upcoming fixtures\n- **PlayerComparisonComponent**: Side-by-side comparison with highlighted stats\n\n**Use Cases:**\n- Player search and discovery\n- Detailed player statistics and form analysis\n- Player comparison for team selection\n- FPL team optimization\n- Real-time price and form tracking\n\n### task-manager-app/\n**Task Manager ChatGPT App (Apps SDK + MCP + Supabase + OAuth)**\n\nA production-ready tutorial showing how to build a ChatGPT App with:\n- **FastMCP (Streamable HTTP)** as the MCP server\n- **React widgets** rendered inside ChatGPT\n- **Supabase (Postgres)** as authoritative state for tasks/notifications\n- **OAuth (Auth0)** for multi-user authentication (MCP OAuth)\n- Optional **Slack notifications** (send now + schedule)\n\nStart here:\n- `task-manager-app/README.md`\n\n### notion-mcp-agent/\n**Notion Knowledge Management Agent**\n\nA sophisticated agent that integrates with Notion through MCP, providing intelligent database management and knowledge organization capabilities.",
        "start_pos": 5424,
        "end_pos": 7376,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 4,
        "text": "tion-mcp-agent/\n**Notion Knowledge Management Agent**\n\nA sophisticated agent that integrates with Notion through MCP, providing intelligent database management and knowledge organization capabilities.\n\n**Key Features:**\n- **Framework**: LangGraph with MCP integration\n- **Integration**: Notion API for database operations\n- **Slack Interface**: Interactive knowledge management\n- **Context Management**: Intelligent data aggregation\n- **Database Operations**: Create, read, update, and organize Notion databases\n\n**Technical Stack:**\n- LangGraph for agent orchestration\n- Notion MCP server for database operations\n- Slack API for user interaction\n- Context aggregation for intelligent responses\n\n**Use Cases:**\n- Knowledge base management\n- Database organization and maintenance\n- Content aggregation and structuring\n- Team collaboration workflows\n- Information retrieval and organization\n\n### claude-advanced-tool-use/\n**Claude Advanced Tool Use Tutorial**\n\nA comprehensive tutorial demonstrating Anthropic's Advanced Tool Use features: Programmatic Tool Calling (PTC) and Tool Search. These features enable AI agents to scale to thousands of tools while dramatically reducing token usage.",
        "start_pos": 7176,
        "end_pos": 8366,
        "token_count_estimate": 297,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 5,
        "text": "ples:**\n- `01_ptc_token_savings.py` - Programmatic Tool Calling with token comparison\n- `02_tool_search.py` - Tool Search with 10 deferred financial tools\n- `03_mcp_tool_search.py` - MCP + Tool Search via ngrok tunnel\n- `mcp_server.py` - FastMCP server exposing financial tools\n\n**Key Concepts:**\n| Feature | Description | Token Savings |\n|---------|-------------|---------------|\n| Programmatic Tool Calling | Tool results stay in sandbox, only `print()` output enters context | 37% |\n| Tool Search | Only load tool definitions when discovered | 85% |\n| Combined | PTC + Tool Search together | Up to 98% |\n\n**Use Cases:**\n- Building AI agents with many tools (100+)\n- Reducing context window bloat from tool definitions\n- Processing large datasets without context overflow\n- MCP server integration with dynamic tool discovery\n- Token-efficient financial analysis agents\n\n### claude-skills/\n**Claude Skills API Implementation**\n\nA comprehensive implementation of Claude's Skills API for automated document generation and custom skill creation.",
        "start_pos": 9024,
        "end_pos": 10067,
        "token_count_estimate": 260,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 6,
        "text": "- `list-files.py` / `download-file.py` / `delete-file.py` - File management\n\n**Use Cases:**\n- Automated PowerPoint presentation generation\n- Excel spreadsheet creation and data analysis\n- Word document generation\n- PDF report creation\n- Custom skill development and deployment\n- Multi-format document workflows\n\n### openai-chatkit-starter-app/\n**ChatKit Web Component Starter**\n\nA minimal Next.js starter template for building ChatKit applications with OpenAI's Agent Builder workflows.\n\n**Key Features:**\n- **Framework**: Next.js with ChatKit web component\n- **Integration**: OpenAI Agent Builder workflows\n- **Customization**: Configurable themes, prompts, and UI\n- **Session Management**: Ready-to-use session endpoint\n- **Deployment**: Domain allowlist verification support\n\n**Technical Stack:**\n- Next.js for application framework\n- OpenAI ChatKit web component (`<openai-chatkit>`)\n- OpenAI API integration\n- TypeScript for type safety\n- Configurable theming system\n\n**Key Components:**\n- Session creation endpoint (`/api/create-session`)\n- ChatKit panel with event handlers\n- Theme and color scheme controls\n- Starter prompts configuration\n- Error overlay for debugging\n\n**Use Cases:**\n- ChatKit application prototyping\n- Agent Builder workflow integration\n- Custom ChatKit UI development\n- OpenAI workflow testing\n- Production ChatKit deployments\n\n### mastra-overview/\n**Mastra Framework Exploration**\n\nAn exploration of the Mastra framework for multi-LLM orchestration and agent management.",
        "start_pos": 10872,
        "end_pos": 12371,
        "token_count_estimate": 374,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 7,
        "text": "ypeScript configuration\n\n**Use Cases:**\n- Multi-LLM agent systems\n- Model orchestration and routing\n- Framework exploration and evaluation\n- LLM comparison and benchmarking\n\n### smithery-example/\n**MCP Development Playground**\n\nA comprehensive development environment for MCP (Model Context Protocol) with FastMCP integration and testing tools.\n\n**Key Features:**\n- **Framework**: Smithery + FastMCP\n- **Development Tools**: MCP playground and testing environment\n- **Financial Integration**: Example financial server implementation\n- **Testing**: Comprehensive test suite and examples\n- **Documentation**: Development guides and examples\n\n**Technical Stack:**\n- Smithery for MCP development\n- FastMCP for server implementation\n- Testing frameworks for validation\n- Development tooling and playgrounds\n\n**Use Cases:**\n- MCP server development\n- Protocol testing and validation\n- Financial data integration examples\n- Development environment setup\n- MCP learning and exploration\n\n### mcp-apps/\n**MCP Apps Examples (Weather + Stock Analysis)**\n\nTwo minimal example MCP Apps showing how to build UI + server experiences using the MCP Apps extensions.\n\n**Key Features:**\n- **Weather App**: UI + MCP server example with a simple weather workflow\n- **Stock Analysis App**: UI + MCP server example for market/stock analysis\n- **Apps SDK**: Designed to follow MCP Apps extension patterns\n- **Docs Reference**: See the MCP Apps docs for the full guide\n\n**Use Cases:**\n- Learning MCP Apps fundamentals\n- Building UI-backed MCP Apps\n- Reference implementations for new MCP App projects\n\n## Getting Started\n\nEach project includes comprehensive setup instructions in its respective README file. General prerequisites include:\n\n### Common Requirements\n- Python 3.9+ (some projects require newer; see each project README)\n- Valid API keys for respective services\n- Slack workspace access (for Slack integrations)\n- Environment variable configuration\n\n### Quick Start Pattern\n```bash\n# 1. Navigate to desired project\ncd [project-name]/\n\n# 2.",
        "start_pos": 12720,
        "end_pos": 14745,
        "token_count_estimate": 506,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 8,
        "text": "or respective services\n- Slack workspace access (for Slack integrations)\n- Environment variable configuration\n\n### Quick Start Pattern\n```bash\n# 1. Navigate to desired project\ncd [project-name]/\n\n# 2. Install dependencies\n# Most Python projects here use uv:\nuv sync\n# Some projects use pip/requirements.txt:\n# pip install -r requirements.txt\n\n# 3. Configure environment\ncp .env.example .env\n# Edit .env with your API keys\n\n# 4.",
        "start_pos": 14545,
        "end_pos": 14972,
        "token_count_estimate": 106,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 9,
        "text": "elopment (openai-chatkit-starter-app)\n\n### Integration Patterns\n- **Container Deployment**: Docker and cloud-ready\n- **API Integration**: RESTful agent endpoints\n- **Database Integration**: Knowledge management systems\n- **Real-time Data**: Live API integration with caching\n\n## Contributing\n\nEach project welcomes contributions. Please:\n\n1. Fork the repository\n2. Create a feature branch\n3. Follow the project's coding standards\n4. Include tests where applicable\n5. Submit a Pull Request\n\n## License\n\nMIT License - see individual project LICENSE files for details.",
        "start_pos": 16393,
        "end_pos": 16958,
        "token_count_estimate": 141,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      },
      {
        "chunk_id": 10,
        "text": "/support)\n- **Notion**: [developers.notion.com](https://developers.notion.com/)\n- **Fantasy Premier League**: [fpl.readthedocs.io](https://fpl.readthedocs.io/en/latest/)\n- **Slack**: [api.slack.com/support](https://api.slack.com/support)\n\n---\n\n**Built with ‚ù§Ô∏è demonstrating the future of AI agent development**",
        "start_pos": 18241,
        "end_pos": 18552,
        "token_count_estimate": 77,
        "source_type": "readme",
        "agent_id": "f594a806f08bf927"
      }
    ]
  },
  {
    "agent_id": "765b8f8c3cdc173c",
    "name": "ai.smithery/hustcc-mcp-mermaid",
    "source": "mcp",
    "source_url": "https://github.com/hustcc/mcp-mermaid",
    "description": "Generate dynamic Mermaid diagrams and charts with AI assistance. Customize styles and export diagr‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-13T06:08:16.370383Z",
    "indexed_at": "2026-02-18T04:07:03.186908",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# <img src=\"https://mermaid.js.org/favicon.svg\" height=\"24\"/> MCP Mermaid ![](https://badge.mcpx.dev?type=server 'MCP Server')  [![build](https://github.com/hustcc/mcp-mermaid/actions/workflows/build.yml/badge.svg)](https://github.com/hustcc/mcp-mermaid/actions/workflows/build.yml) [![npm Version](https://img.shields.io/npm/v/mcp-mermaid.svg)](https://www.npmjs.com/package/mcp-mermaid) [![smithery badge](https://smithery.ai/badge/@hustcc/mcp-mermaid)](https://smithery.ai/server/@hustcc/mcp-mermaid) [![npm License](https://img.shields.io/npm/l/mcp-mermaid.svg)](https://www.npmjs.com/package/mcp-mermaid) [![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/hustcc/mcp-mermaid)](https://archestra.ai/mcp-catalog/hustcc__mcp-mermaid)\n\nGenerate <img src=\"https://mermaid.js.org/favicon.svg\" height=\"14\"/> [mermaid](https://mermaid.js.org/) diagram and chart with AI MCP dynamically. Also you can use:\n\n- <img src=\"https://mdn.alipayobjects.com/huamei_qa8qxu/afts/img/A*ZFK8SrovcqgAAAAAAAAAAAAAemJ7AQ/original\" height=\"14\"/> [mcp-server-chart](https://github.com/antvis/mcp-server-chart) to generate chart, graph, map.\n- <img src=\"https://mdn.alipayobjects.com/huamei_qa8qxu/afts/img/A*EdkXSojOxqsAAAAAQHAAAAgAemJ7AQ/original\" height=\"14\"/> [Infographic](https://github.com/antvis/Infographic) to generate infographic, such as `Timeline`, `Comparison`, `List`, `Process` and so on.\n\n\n## ‚ú® Features\n\n- Fully support all features and syntax of `Mermaid`.\n- Support configuration of `backgroundColor` and `theme`, enabling large AI models to output rich style configurations.\n\n- Support exporting to `base64`, `svg`, `mermaid`, `file`, and remote-friendly `svg_url`, `png_url` formats, with validation for `Mermaid` to facilitate the model's multi-round output of correct syntax and graphics. Use `outputType: \"file\"` to automatically save PNG diagrams to disk for AI agents, or the URL modes to share diagrams through public mermaid.ink links.\n\n\n<img width=\"720\" alt=\"mcp-mermaid\" src=\"https://mermaid.js.org/header.png\" />\n\n\n## ü§ñ Usage\n\nTo use with `Desktop APP`, such as Claude, VSCode, Cline, Cherry Studio, and so on, add the  MCP server config below. On Mac system:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-mermaid\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mermaid\"\n      ]\n    }\n  }\n}\n```\n\nOn Window system:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-mermaid\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"mcp-mermaid\"\n      ]\n    }\n  }\n}\n```\n\nAlso, you can use it on aliyun, modelscope, glama.ai, smithery.ai or others with HTTP, SSE Protocol.\n\n\n## üö∞ Run with SSE or Streamable transport\n\n### Option 1: Global Installation\n\nInstall the package globally:\n\n```bash\nnpm install -g mcp-mermaid\n```\n\nRun the server with your preferred transport option:\n\n```bash\n# For SSE transport (default endpoint: /sse)\nmcp-mermaid -t sse\n\n# For Streamable transport with custom endpoint\nmcp-mermaid -t streamable\n```\n\n### Option 2: Local Development\n\nIf you're working with the source code locally:\n\n```bash\n# Clone and setup\ngit clone https://github.com/hustcc/mcp-mermaid.git\ncd mcp-mermaid\nnpm install\nnpm run build\n\n# Run with npm scripts\nnpm run start:sse        # SSE transport on port 3033\nnpm run start:streamable # Streamable transport on port 1122\n```\n\n### Access Points\n\nThen you can access the server at:\n\n- SSE transport: `http://localhost:3033/sse`\n- Streamable transport: `http://localhost:1122/mcp` (local) or `http://localhost:3033/mcp` (global)\n\n## üéÆ CLI Options\n\nYou can also use the following CLI options when running the MCP server. Command options by run cli with `-h`.\n\n```plain\nMCP Mermaid CLI\n\nOptions:\n  --transport, -t  Specify the transport protocol: \"stdio\", \"sse\", or \"streamable\" (default: \"stdio\")\n  --port, -p       Specify the port for SSE or streamable transport (default: 3033)\n  --endpoint, -e   Specify the endpoint for the transport:\n                    - For SSE: default is \"/sse\"\n                    - For streamable: default is \"/mcp\"\n  --help, -h       Show this help message\n```\n\n## üî® Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\n### Start the MCP server\n\n**Using MCP Inspector (for debugging):**\n\n```bash\nnpm run start\n```\n\n**Using different transport protocols:**\n\n```bash\n# SSE transport (Server-Sent Events)\nnpm run start:sse\n\n# Streamable HTTP transport\nnpm run start:streamable\n```\n\n**Direct node commands:**\n\n```bash\n# SSE transport on port 3033\nnode build/index.js --transport sse --port 3033\n\n# Streamable HTTP transport on port 1122\nnode build/index.js --transport streamable --port 1122\n\n# STDIO transport (for MCP client integration)\nnode build/index.js --transport stdio\n```\n\n## üìÑ License\n\nMIT@[hustcc](https://github.com/hustcc).\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Generate Mermaid diagrams and charts dynamically using AI MCP",
        "Support all features and syntax of Mermaid",
        "Configure diagram background color and theme",
        "Export diagrams to base64, svg, mermaid syntax, file, svg_url, and png_url formats",
        "Validate Mermaid syntax to ensure correct multi-round output",
        "Save PNG diagrams automatically to disk for AI agents",
        "Serve diagrams via public mermaid.ink links for sharing",
        "Run MCP server with multiple transport protocols including stdio, SSE, and streamable HTTP"
      ],
      "limitations": [],
      "requirements": [
        "Node.js environment with npm",
        "Installation of the mcp-mermaid package (globally or locally)",
        "Access to command line interface for running the server",
        "Optional: Use on platforms supporting HTTP or SSE protocols such as aliyun, modelscope, glama.ai, smithery.ai"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed CLI options, supported features, export formats, and development guidance, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# <img src=\"https://mermaid.js.org/favicon.svg\" height=\"24\"/> MCP Mermaid ![](https://badge.mcpx.dev?type=server 'MCP Server')  [![build](https://github.com/hustcc/mcp-mermaid/actions/workflows/build.yml/badge.svg)](https://github.com/hustcc/mcp-mermaid/actions/workflows/build.yml) [![npm Version](https://img.shields.io/npm/v/mcp-mermaid.svg)](https://www.npmjs.com/package/mcp-mermaid) [![smithery badge](https://smithery.ai/badge/@hustcc/mcp-mermaid)](https://smithery.ai/server/@hustcc/mcp-mermaid) [![npm License](https://img.shields.io/npm/l/mcp-mermaid.svg)](https://www.npmjs.com/package/mcp-mermaid) [![Trust Score](https://archestra.ai/mcp-catalog/api/badge/quality/hustcc/mcp-mermaid)](https://archestra.ai/mcp-catalog/hustcc__mcp-mermaid)\n\nGenerate <img src=\"https://mermaid.js.org/favicon.svg\" height=\"14\"/> [mermaid](https://mermaid.js.org/) diagram and chart with AI MCP dynamically. Also you can use:\n\n- <img src=\"https://mdn.alipayobjects.com/huamei_qa8qxu/afts/img/A*ZFK8SrovcqgAAAAAAAAAAAAAemJ7AQ/original\" height=\"14\"/> [mcp-server-chart](https://github.com/antvis/mcp-server-chart) to generate chart, graph, map.\n- <img src=\"https://mdn.alipayobjects.com/huamei_qa8qxu/afts/img/A*EdkXSojOxqsAAAAAQHAAAAgAemJ7AQ/original\" height=\"14\"/> [Infographic](https://github.com/antvis/Infographic) to generate infographic, such as `Timeline`, `Comparison`, `List`, `Process` and so on.\n\n\n## ‚ú® Features\n\n- Fully support all features and syntax of `Mermaid`.\n- Support configuration of `backgroundColor` and `theme`, enabling large AI models to output rich style configurations.\n\n- Support exporting to `base64`, `svg`, `mermaid`, `file`, and remote-friendly `svg_url`, `png_url` formats, with validation for `Mermaid` to facilitate the model's multi-round output of correct syntax and graphics. Use `outputType: \"file\"` to automatically save PNG diagrams to disk for AI agents, or the URL modes to share diagrams through public mermaid.ink links.",
        "start_pos": 0,
        "end_pos": 1957,
        "token_count_estimate": 489,
        "source_type": "readme",
        "agent_id": "765b8f8c3cdc173c"
      },
      {
        "chunk_id": 1,
        "text": "lti-round output of correct syntax and graphics. Use `outputType: \"file\"` to automatically save PNG diagrams to disk for AI agents, or the URL modes to share diagrams through public mermaid.ink links.\n\n\n<img width=\"720\" alt=\"mcp-mermaid\" src=\"https://mermaid.js.org/header.png\" />\n\n\n## ü§ñ Usage\n\nTo use with `Desktop APP`, such as Claude, VSCode, Cline, Cherry Studio, and so on, add the  MCP server config below. On Mac system:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-mermaid\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mcp-mermaid\"\n      ]\n    }\n  }\n}\n```\n\nOn Window system:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-mermaid\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"mcp-mermaid\"\n      ]\n    }\n  }\n}\n```\n\nAlso, you can use it on aliyun, modelscope, glama.ai, smithery.ai or others with HTTP, SSE Protocol.\n\n\n## üö∞ Run with SSE or Streamable transport\n\n### Option 1: Global Installation\n\nInstall the package globally:\n\n```bash\nnpm install -g mcp-mermaid\n```\n\nRun the server with your preferred transport option:\n\n```bash\n# For SSE transport (default endpoint: /sse)\nmcp-mermaid -t sse\n\n# For Streamable transport with custom endpoint\nmcp-mermaid -t streamable\n```\n\n### Option 2: Local Development\n\nIf you're working with the source code locally:\n\n```bash\n# Clone and setup\ngit clone https://github.com/hustcc/mcp-mermaid.git\ncd mcp-mermaid\nnpm install\nnpm run build\n\n# Run with npm scripts\nnpm run start:sse        # SSE transport on port 3033\nnpm run start:streamable # Streamable transport on port 1122\n```\n\n### Access Points\n\nThen you can access the server at:\n\n- SSE transport: `http://localhost:3033/sse`\n- Streamable transport: `http://localhost:1122/mcp` (local) or `http://localhost:3033/mcp` (global)\n\n## üéÆ CLI Options\n\nYou can also use the following CLI options when running the MCP server. Command options by run cli with `-h`.",
        "start_pos": 1757,
        "end_pos": 3665,
        "token_count_estimate": 477,
        "source_type": "readme",
        "agent_id": "765b8f8c3cdc173c"
      },
      {
        "chunk_id": 2,
        "text": "p://localhost:1122/mcp` (local) or `http://localhost:3033/mcp` (global)\n\n## üéÆ CLI Options\n\nYou can also use the following CLI options when running the MCP server. Command options by run cli with `-h`.\n\n```plain\nMCP Mermaid CLI\n\nOptions:\n  --transport, -t  Specify the transport protocol: \"stdio\", \"sse\", or \"streamable\" (default: \"stdio\")\n  --port, -p       Specify the port for SSE or streamable transport (default: 3033)\n  --endpoint, -e   Specify the endpoint for the transport:\n                    - For SSE: default is \"/sse\"\n                    - For streamable: default is \"/mcp\"\n  --help, -h       Show this help message\n```\n\n## üî® Development\n\nInstall dependencies:\n\n```bash\nnpm install\n```\n\nBuild the server:\n\n```bash\nnpm run build\n```\n\n### Start the MCP server\n\n**Using MCP Inspector (for debugging):**\n\n```bash\nnpm run start\n```\n\n**Using different transport protocols:**\n\n```bash\n# SSE transport (Server-Sent Events)\nnpm run start:sse\n\n# Streamable HTTP transport\nnpm run start:streamable\n```\n\n**Direct node commands:**\n\n```bash\n# SSE transport on port 3033\nnode build/index.js --transport sse --port 3033\n\n# Streamable HTTP transport on port 1122\nnode build/index.js --transport streamable --port 1122\n\n# STDIO transport (for MCP client integration)\nnode build/index.js --transport stdio\n```\n\n## üìÑ License\n\nMIT@[hustcc](https://github.com/hustcc).",
        "start_pos": 3465,
        "end_pos": 4825,
        "token_count_estimate": 339,
        "source_type": "readme",
        "agent_id": "765b8f8c3cdc173c"
      }
    ]
  },
  {
    "agent_id": "410ec0b56ba40c4e",
    "name": "ai.smithery/huuthangntk-claude-vision-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/huuthangntk/claude-vision-mcp-server",
    "description": "Analyze images from multiple angles to extract detailed insights or quick summaries. Describe visu‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "analyze"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T23:20:55.125959Z",
    "indexed_at": "2026-02-18T04:07:05.018622",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Claude Deep Think MCP Server\n\nA powerful Model Context Protocol (MCP) server that provides **proactive deep analytical thinking** using Anthropic's Claude Sonnet 4.5. This tool is designed to be called **BEFORE writing code** when new information arrives.\n\n## ‚ö° Core Concept: Think Before Code\n\n**Use this tool FIRST when new information arrives, BEFORE writing any code:**\n\n- üêõ Error messages or stack traces\n- üìù User requirements or feature requests\n- üí¨ Code review feedback\n- üöÄ Performance issues\n- üîí Security alerts\n- üìö API documentation to integrate\n- üóÑÔ∏è Database problems\n- üí≠ UX/UI feedback\n- üîÑ Breaking changes in dependencies\n- üèóÔ∏è Architectural decisions\n\n**Workflow**: New Info ‚Üí Think Tool ‚Üí Review Insights ‚Üí Write Better Code\n\n## üåü Features\n\n### Deep Think & Analysis (`claude_think`)\nProvides intelligent insights, suggestions, and strategic guidance **before code implementation**. Perfect for:\n- ‚úÖ Understanding context deeply before acting\n- ‚úÖ Identifying potential pitfalls upfront\n- ‚úÖ Suggesting best practices from the start\n- ‚úÖ Offering alternative approaches\n- ‚úÖ Extracting key information for efficient implementation\n- ‚úÖ Strategic decision-making\n- ‚úÖ Problem-solving and architecture planning\n\n**Result**: Fewer bugs, better code quality, faster development!\n\n## üìã Prerequisites\n\n- Node.js 18+ or Bun\n- Anthropic Claude API key ([Get one here](https://console.anthropic.com/))\n- MCP-compatible client (Cursor IDE, Claude Desktop, etc.)\n\n## üöÄ Quick Start\n\n### 1. Installation\n\n```bash\ncd claude-vision-mcp\nbun install\n# or\nnpm install\n```\n\n### 2. Configuration\n\nThe API key is configured when connecting to the MCP server (see Docker or Cursor setup below).\n\n### 3. Build\n\n```bash\nbun run build\n# or\nnpm run build\n```\n\n## üê≥ Docker Setup (Recommended)\n\n### Quick Start\n\n```bash\ncd claude-vision-mcp\n\n# Create .env file with your API key\necho \"ANTHROPIC_API_KEY=your-key-here\" > .env\necho \"CLAUDE_MODEL=claude-sonnet-4-20250514\" >> .env\n\n# Start container\ndocker-compose up -d\n\n# Check status\ndocker ps | grep claude-vision\n```\n\nThe container will auto-restart when Docker Desktop launches.\n\n### Docker Configuration\n\nThe server runs on `http://localhost:8080/mcp` with the following environment variables:\n- `ANTHROPIC_API_KEY` - Your Claude API key (required)\n- `CLAUDE_MODEL` - Model to use (default: claude-sonnet-4-20250514)\n\n## üîß Usage in Cursor IDE\n\n### Docker Connection (Recommended)\n\nAdd to your `~/.cursor/mcp.json` or `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"Claude Deep Think\": {\n      \"url\": \"http://localhost:8080/mcp?apiKey=YOUR_API_KEY&model=claude-sonnet-4-5-20250929\"\n    }\n  }\n}\n```\n\n### Enable Proactive Thinking\n\nCopy the `.cursorrules` file from this repo to your project root. This makes Cursor AI automatically use the think tool before writing code.\n\n```bash\n# From your project directory\ncp claude-vision-mcp/.cursorrules .cursorrules\n```\n\n### Tool Usage Pattern\n\n**Always use this pattern when new information arrives:**\n\n```\nUse the claude_think tool to analyze: [NEW INFORMATION]\n\nContext: [Current situation, tech stack, constraints]\n```\n\n**Examples:**\n\n**Error Message:**\n```\nUse the claude_think tool to analyze:\n\nError: \"TypeError: Cannot read property 'map' of undefined\"\n\nContext: React component rendering users from useState hook\n```\n\n**New Feature:**\n```\nUse the claude_think tool:\n\nRequirement: Add dark mode toggle to header\n\nContext: Next.js 14, need to check if ThemeContext exists\n```\n\n**Performance Issue:**\n```\nUse the claude_think tool:\n\nIssue: Homepage renders 50+ times, parent causing all children to re-render\n\nContext: useState for theme in Header, passed via props to 20+ children\n```\n\n## üìö Examples\n\n### Example 1: Analyzing Technical Decisions\n\n```\nUse the claude_think tool to analyze:\n\n\"I'm building a real-time chat application. Should I use WebSockets, SSE, or HTTP polling?\"\n\nContext: Need to support 100K concurrent users, prioritize ease of implementation\n```\n\n**Expected Output**: Comprehensive comparison with pros, cons, and recommendations\n\n### Example 2: Architecture Planning\n\n```\nUse the claude_think tool to evaluate:\n\n\"What's the best way to structure a multi-tenant SaaS application?\"\n\nContext: PostgreSQL database, Node.js backend, 50-100 tenants expected\n```\n\n### Example 3: Best Practices\n\n```\nUse the claude_think tool:\n\n\"Review this approach to handling user sessions in a Next.js app\"\n\nContext: Using JWT tokens, storing in localStorage, concerned about security\n```\n\n## üõ†Ô∏è Development\n\n### Project Structure\n\n```\nclaude-vision-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts              # Main MCP server implementation\n‚îú‚îÄ‚îÄ .smithery/\n‚îÇ   ‚îî‚îÄ‚îÄ index.cjs            # Built server (generated)\n‚îú‚îÄ‚îÄ package.json              # Dependencies and scripts\n‚îú‚îÄ‚îÄ tsconfig.json            # TypeScript configuration\n‚îú‚îÄ‚îÄ smithery.yaml            # Smithery deployment config\n‚îú‚îÄ‚îÄ Dockerfile               # Docker container definition\n‚îú‚îÄ‚îÄ docker-compose.yml       # Docker Compose configuration\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n\n### Available Scripts\n\n- `bun run build` / `npm run build` - Compile TypeScript\n- `bun run dev` / `npm run dev` - Development server with hot reload\n\n## üîí Security Best Practices\n\n1. **Never commit API keys** - Always use environment variables\n2. **Use .gitignore** - Ensure `.env` files are ignored\n3. **Rotate keys regularly** - Update API keys periodically\n4. **Review tool calls** - Keep manual approval enabled in Cursor\n5. **Use development environments** - Test with non-production data\n\n## üì¶ Docker Management\n\n```bash\n# Start container\ndocker-compose up -d\n\n# View logs\ndocker logs claude-vision-mcp-server -f\n\n# Restart container\ndocker-compose restart\n\n# Stop container\ndocker-compose down\n\n# Rebuild and restart\ndocker-compose up -d --build\n```\n\n## üêõ Troubleshooting\n\n### Issue: Server not connecting in Cursor\n\n**Solutions:**\n1. Verify Docker container is running: `docker ps | grep claude-vision`\n2. Check container logs: `docker logs claude-vision-mcp-server`\n3. Restart Cursor IDE completely\n4. Verify API key in URL is correct\n\n### Issue: API key errors\n\n**Solutions:**\n1. Ensure key starts with `sk-ant-`\n2. Test key at: https://console.anthropic.com/\n3. Check environment variables in container\n4. Verify URL parameter format\n\n### Issue: Container won't start\n\n**Solutions:**\n```bash\n# Check logs\ndocker logs claude-vision-mcp-server\n\n# Verify .env file\ncat .env\n\n# Rebuild from scratch\ndocker-compose down -v\ndocker-compose up -d --build\n```\n\n## üí° Performance\n\nWith Bun runtime:\n- ‚ö° 4x faster package installs\n- ‚ö° 3-4x faster script execution  \n- üì¶ Smaller Docker images\n- üöÄ Faster cold starts\n\n## üìñ Comprehensive Guides\n\n- **[PROACTIVE_THINKING_WORKFLOW.md](./PROACTIVE_THINKING_WORKFLOW.md)** - Complete workflow guide with before/after examples\n- **[THINK_TOOL_EXAMPLES.md](./THINK_TOOL_EXAMPLES.md)** - 10 real-world usage examples\n- **[.cursorrules](./.cursorrules)** - Cursor IDE rules for automatic think-before-code pattern\n\n## üí° Why This Workflow?\n\n### Without Think Tool:\n```\n1. User reports error\n2. AI writes quick fix\n3. Fix creates new bug\n4. Multiple iterations needed\n‚è±Ô∏è Total: 30 minutes, 3 iterations\n```\n\n### With Claude_Think Tool:\n```\n1. User reports error\n2. AI analyzes with claude_think tool (20s)\n3. AI writes comprehensive fix\n4. Works correctly first time\n‚è±Ô∏è Total: 5 minutes, 1 iteration\n```\n\n**Result**: 6x faster, better quality, fewer bugs! üéâ\n\n## üìÑ License\n\nMIT\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìû Support\n\nFor issues or questions:\n- Open an issue on GitHub\n- Check the [MCP Documentation](https://modelcontextprotocol.io/)\n- Read the workflow guides in this repository\n\n## üôè Acknowledgments\n\n- Built with [Anthropic Claude API](https://www.anthropic.com/)\n- Powered by [Model Context Protocol](https://modelcontextprotocol.io/)\n- Containerized with [Bun](https://bun.sh/)"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide proactive deep analytical thinking before code implementation",
        "Analyze error messages, stack traces, and debugging information",
        "Evaluate user requirements and feature requests for strategic guidance",
        "Review code feedback and suggest best practices",
        "Identify potential pitfalls and alternative approaches",
        "Assist in architectural decisions and problem-solving",
        "Extract key information for efficient implementation",
        "Support performance issue analysis and UX/UI feedback integration",
        "Integrate with MCP-compatible clients like Cursor IDE and Claude Desktop"
      ],
      "limitations": [
        "Requires Anthropic Claude API key for operation",
        "Limited to analysis and strategic guidance, does not write code directly",
        "Dependent on MCP-compatible clients for interaction",
        "Runs locally or in Docker environment, no cloud-hosted service mentioned",
        "No explicit support for languages or frameworks beyond examples given"
      ],
      "requirements": [
        "Node.js version 18 or higher, or Bun runtime",
        "Anthropic Claude API key",
        "MCP-compatible client such as Cursor IDE or Claude Desktop",
        "Docker for containerized deployment (optional but recommended)",
        "Environment variables configured for API key and model selection"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed feature descriptions, prerequisites, troubleshooting tips, and security best practices, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Claude Deep Think MCP Server\n\nA powerful Model Context Protocol (MCP) server that provides **proactive deep analytical thinking** using Anthropic's Claude Sonnet 4.5. This tool is designed to be called **BEFORE writing code** when new information arrives.\n\n## ‚ö° Core Concept: Think Before Code\n\n**Use this tool FIRST when new information arrives, BEFORE writing any code:**\n\n- üêõ Error messages or stack traces\n- üìù User requirements or feature requests\n- üí¨ Code review feedback\n- üöÄ Performance issues\n- üîí Security alerts\n- üìö API documentation to integrate\n- üóÑÔ∏è Database problems\n- üí≠ UX/UI feedback\n- üîÑ Breaking changes in dependencies\n- üèóÔ∏è Architectural decisions\n\n**Workflow**: New Info ‚Üí Think Tool ‚Üí Review Insights ‚Üí Write Better Code\n\n## üåü Features\n\n### Deep Think & Analysis (`claude_think`)\nProvides intelligent insights, suggestions, and strategic guidance **before code implementation**. Perfect for:\n- ‚úÖ Understanding context deeply before acting\n- ‚úÖ Identifying potential pitfalls upfront\n- ‚úÖ Suggesting best practices from the start\n- ‚úÖ Offering alternative approaches\n- ‚úÖ Extracting key information for efficient implementation\n- ‚úÖ Strategic decision-making\n- ‚úÖ Problem-solving and architecture planning\n\n**Result**: Fewer bugs, better code quality, faster development!\n\n## üìã Prerequisites\n\n- Node.js 18+ or Bun\n- Anthropic Claude API key ([Get one here](https://console.anthropic.com/))\n- MCP-compatible client (Cursor IDE, Claude Desktop, etc.)\n\n## üöÄ Quick Start\n\n### 1. Installation\n\n```bash\ncd claude-vision-mcp\nbun install\n# or\nnpm install\n```\n\n### 2. Configuration\n\nThe API key is configured when connecting to the MCP server (see Docker or Cursor setup below).\n\n### 3.",
        "start_pos": 0,
        "end_pos": 1689,
        "token_count_estimate": 422,
        "source_type": "readme",
        "agent_id": "410ec0b56ba40c4e"
      },
      {
        "chunk_id": 1,
        "text": "our API key\necho \"ANTHROPIC_API_KEY=your-key-here\" > .env\necho \"CLAUDE_MODEL=claude-sonnet-4-20250514\" >> .env\n\n# Start container\ndocker-compose up -d\n\n# Check status\ndocker ps | grep claude-vision\n```\n\nThe container will auto-restart when Docker Desktop launches.\n\n### Docker Configuration\n\nThe server runs on `http://localhost:8080/mcp` with the following environment variables:\n- `ANTHROPIC_API_KEY` - Your Claude API key (required)\n- `CLAUDE_MODEL` - Model to use (default: claude-sonnet-4-20250514)\n\n## üîß Usage in Cursor IDE\n\n### Docker Connection (Recommended)\n\nAdd to your `~/.cursor/mcp.json` or `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"Claude Deep Think\": {\n      \"url\": \"http://localhost:8080/mcp?apiKey=YOUR_API_KEY&model=claude-sonnet-4-5-20250929\"\n    }\n  }\n}\n```\n\n### Enable Proactive Thinking\n\nCopy the `.cursorrules` file from this repo to your project root. This makes Cursor AI automatically use the think tool before writing code.\n\n```bash\n# From your project directory\ncp claude-vision-mcp/.cursorrules .cursorrules\n```\n\n### Tool Usage Pattern\n\n**Always use this pattern when new information arrives:**\n\n```\nUse the claude_think tool to analyze: [NEW INFORMATION]\n\nContext: [Current situation, tech stack, constraints]\n```\n\n**Examples:**\n\n**Error Message:**\n```\nUse the claude_think tool to analyze:\n\nError: \"TypeError: Cannot read property 'map' of undefined\"\n\nContext: React component rendering users from useState hook\n```\n\n**New Feature:**\n```\nUse the claude_think tool:\n\nRequirement: Add dark mode toggle to header\n\nContext: Next.js 14, need to check if ThemeContext exists\n```\n\n**Performance Issue:**\n```\nUse the claude_think tool:\n\nIssue: Homepage renders 50+ times, parent causing all children to re-render\n\nContext: useState for theme in Header, passed via props to 20+ children\n```\n\n## üìö Examples\n\n### Example 1: Analyzing Technical Decisions\n\n```\nUse the claude_think tool to analyze:\n\n\"I'm building a real-time chat application.",
        "start_pos": 1848,
        "end_pos": 3821,
        "token_count_estimate": 493,
        "source_type": "readme",
        "agent_id": "410ec0b56ba40c4e"
      },
      {
        "chunk_id": 2,
        "text": "eme in Header, passed via props to 20+ children\n```\n\n## üìö Examples\n\n### Example 1: Analyzing Technical Decisions\n\n```\nUse the claude_think tool to analyze:\n\n\"I'm building a real-time chat application. Should I use WebSockets, SSE, or HTTP polling?\"\n\nContext: Need to support 100K concurrent users, prioritize ease of implementation\n```\n\n**Expected Output**: Comprehensive comparison with pros, cons, and recommendations\n\n### Example 2: Architecture Planning\n\n```\nUse the claude_think tool to evaluate:\n\n\"What's the best way to structure a multi-tenant SaaS application?\"\n\nContext: PostgreSQL database, Node.js backend, 50-100 tenants expected\n```\n\n### Example 3: Best Practices\n\n```\nUse the claude_think tool:\n\n\"Review this approach to handling user sessions in a Next.js app\"\n\nContext: Using JWT tokens, storing in localStorage, concerned about security\n```\n\n## üõ†Ô∏è Development\n\n### Project Structure\n\n```\nclaude-vision-mcp/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts              # Main MCP server implementation\n‚îú‚îÄ‚îÄ .smithery/\n‚îÇ   ‚îî‚îÄ‚îÄ index.cjs            # Built server (generated)\n‚îú‚îÄ‚îÄ package.json              # Dependencies and scripts\n‚îú‚îÄ‚îÄ tsconfig.json            # TypeScript configuration\n‚îú‚îÄ‚îÄ smithery.yaml            # Smithery deployment config\n‚îú‚îÄ‚îÄ Dockerfile               # Docker container definition\n‚îú‚îÄ‚îÄ docker-compose.yml       # Docker Compose configuration\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n\n### Available Scripts\n\n- `bun run build` / `npm run build` - Compile TypeScript\n- `bun run dev` / `npm run dev` - Development server with hot reload\n\n## üîí Security Best Practices\n\n1. **Never commit API keys** - Always use environment variables\n2. **Use .gitignore** - Ensure `.env` files are ignored\n3. **Rotate keys regularly** - Update API keys periodically\n4. **Review tool calls** - Keep manual approval enabled in Cursor\n5.",
        "start_pos": 3621,
        "end_pos": 5452,
        "token_count_estimate": 457,
        "source_type": "readme",
        "agent_id": "410ec0b56ba40c4e"
      },
      {
        "chunk_id": 3,
        "text": "t environments** - Test with non-production data\n\n## üì¶ Docker Management\n\n```bash\n# Start container\ndocker-compose up -d\n\n# View logs\ndocker logs claude-vision-mcp-server -f\n\n# Restart container\ndocker-compose restart\n\n# Stop container\ndocker-compose down\n\n# Rebuild and restart\ndocker-compose up -d --build\n```\n\n## üêõ Troubleshooting\n\n### Issue: Server not connecting in Cursor\n\n**Solutions:**\n1. Verify Docker container is running: `docker ps | grep claude-vision`\n2. Check container logs: `docker logs claude-vision-mcp-server`\n3. Restart Cursor IDE completely\n4. Verify API key in URL is correct\n\n### Issue: API key errors\n\n**Solutions:**\n1. Ensure key starts with `sk-ant-`\n2. Test key at: https://console.anthropic.com/\n3. Check environment variables in container\n4. Verify URL parameter format\n\n### Issue: Container won't start\n\n**Solutions:**\n```bash\n# Check logs\ndocker logs claude-vision-mcp-server\n\n# Verify .env file\ncat .env\n\n# Rebuild from scratch\ndocker-compose down -v\ndocker-compose up -d --build\n```\n\n## üí° Performance\n\nWith Bun runtime:\n- ‚ö° 4x faster package installs\n- ‚ö° 3-4x faster script execution  \n- üì¶ Smaller Docker images\n- üöÄ Faster cold starts\n\n## üìñ Comprehensive Guides\n\n- **[PROACTIVE_THINKING_WORKFLOW.md](./PROACTIVE_THINKING_WORKFLOW.md)** - Complete workflow guide with before/after examples\n- **[THINK_TOOL_EXAMPLES.md](./THINK_TOOL_EXAMPLES.md)** - 10 real-world usage examples\n- **[.cursorrules](./.cursorrules)** - Cursor IDE rules for automatic think-before-code pattern\n\n## üí° Why This Workflow?\n\n### Without Think Tool:\n```\n1. User reports error\n2. AI writes quick fix\n3. Fix creates new bug\n4. Multiple iterations needed\n‚è±Ô∏è Total: 30 minutes, 3 iterations\n```\n\n### With Claude_Think Tool:\n```\n1. User reports error\n2. AI analyzes with claude_think tool (20s)\n3. AI writes comprehensive fix\n4.",
        "start_pos": 5469,
        "end_pos": 7299,
        "token_count_estimate": 457,
        "source_type": "readme",
        "agent_id": "410ec0b56ba40c4e"
      },
      {
        "chunk_id": 4,
        "text": "irst time\n‚è±Ô∏è Total: 5 minutes, 1 iteration\n```\n\n**Result**: 6x faster, better quality, fewer bugs! üéâ\n\n## üìÑ License\n\nMIT\n\n## ü§ù Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\n## üìû Support\n\nFor issues or questions:\n- Open an issue on GitHub\n- Check the [MCP Documentation](https://modelcontextprotocol.io/)\n- Read the workflow guides in this repository\n\n## üôè Acknowledgments\n\n- Built with [Anthropic Claude API](https://www.anthropic.com/)\n- Powered by [Model Context Protocol](https://modelcontextprotocol.io/)\n- Containerized with [Bun](https://bun.sh/)",
        "start_pos": 7317,
        "end_pos": 7907,
        "token_count_estimate": 147,
        "source_type": "readme",
        "agent_id": "410ec0b56ba40c4e"
      }
    ]
  },
  {
    "agent_id": "bb15b078a39953aa",
    "name": "ai.smithery/infranodus-mcp-server-infranodus",
    "source": "mcp",
    "source_url": "https://github.com/infranodus/mcp-server-infranodus",
    "description": "Map text into knowledge graphs to create a structured representation of conceptual relations and t‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-04T08:08:11.981455Z",
    "indexed_at": "2026-02-18T04:07:06.476627",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# InfraNodus MCP Server\n\nA Model Context Protocol (MCP) server that integrates InfraNodus knowledge graph and text network analysis capabilities into LLM workflows and AI assistants like Claude Desktop.\n\n## Overview\n\nInfraNodus MCP Server enables LLM workflows and AI assistants to analyze text using advanced network science algorithms, generate knowledge graphs, detect content gaps, and identify key topics and concepts. It transforms unstructured text into structured insights using graph theory and network analysis.\n\n![InfraNodus MCP Server](https://infranodus.com/images/front/infranodus-overview.jpg)\n\n## Features\n\n### You Can Use It To\n\n- Connect your existing InfraNodus knowledge graphs to your LLM workflows and AI chats\n- Identify the main topical clusters in discourse without missing the important nuances (works better than standard LLM workflows)\n- Identify the content gaps in any discourse (helpful for content creation and research)\n- Generate new knowledge graphs from any text and use them to augment your LLM responses\n- Save and retrieve entities and relations from memory using the knowledge graphs\n\n### Available Tools\n\n1. **generate_knowledge_graph**\n\n   - Convert any text into a visual knowledge graph\n   - Extract topics, concepts, and their relationships\n   - Identify structural patterns and clusters\n   - Apply AI-powered topic naming\n   - Perform entity detection for cleaner graphs\n\n2. **analyze_existing_graph_by_name**\n\n   - Retrieve and analyze existing graphs from your InfraNodus account\n   - Access previously saved analyses\n   - Export graph data with full statistics\n\n3. **generate_content_gaps**\n\n   - Detect missing connections in discourse\n   - Identify underexplored topics\n   - Generate research questions\n   - Suggest content development opportunities\n\n4. **generate_topical_clusters**\n\n   - Generate topics and clusters of keywords from text using knowledge graph analysis\n   - Make sure to beyond genetic insights and detect smaller topics\n   - Use the topical clusters to establish topical authority for SEO\n\n5. **generate_contextual_hint**\n\n   - Generate a topical overview of a text and provide insights for LLMs to generate better responses\n   - Use it to get a high-level understanding of a text\n   - Use it to augment prompts in your LLM workflows and AI assistants\n\n6. **generate_research_questions**\n\n   - Generate research questions that bridge content gaps\n   - Use them as prompts in your LLM models and AI workflows\n   - Use any AI model (included in InfraNodus API)\n   - Content gaps are identified based on topical clustering\n\n7. **generate_research_ideas**\n\n   - Generate innovative research ideas based on content gaps identified in the text\n   - Get actionable ideas to improve the text and develop the discourse\n   - Use any AI model (included in InfraNodus API)\n   - Ideas are generated from gaps between topical clusters\n\n8. **research_questions_from_graph**\n\n   - Generate research questions based on an existing InfraNodus graph\n   - Use them as prompts in your LLM models\n   - Use any AI model (included in InfraNodus API)\n   - Content gaps are identified based on topical clustering\n\n9. **generate_responses_from_graph**\n\n   - Generate responses based on an existing InfraNodus graph\n   - Integrate them into your LLM workflows and AI assistants\n   - Use any AI model (included in InfraNodus API)\n   - Use any prompt\n\n10. **develop_conceptual_bridges**\n\n    - Analyze text and develop latent ideas based on concepts that connect this text to a broader discourse\n    - Discover hidden themes and patterns that link your text to wider contexts\n    - Use any AI model (included in InfraNodus API)\n    - Generate insights that help develop the discourse\n\n11. **develop_latent_topics**\n\n    - Analyze text and extract underdeveloped topics with ideas on how to develop them\n    - Identify topics that need more attention and elaboration\n    - Use any AI model (included in InfraNodus API)\n    - Get actionable suggestions for content expansion\n\n12. **develop_text_tool**\n\n    - Comprehensive text analysis combining content gap ideas, latent topics, and conceptual bridges\n    - Executes multiple analyses in sequence with progress tracking\n    - Generates research ideas based on content gaps\n    - Identifies latent topics and conceptual bridges to develop\n    - Finds content gaps for deeper exploration\n\n13. **create_knowledge_graph**\n\n    - Create a knowledge graph in InfraNodus from text and provide a link to it\n    - Use it to create a knowledge graph in InfraNodus from text\n\n14. **overlap_between_texts**\n\n    - Create knowledge graphs from two or more texts and find the overlap (similarities) between them\n    - Use it to find similar topics and keywords across different texts\n\n15. **difference_between_texts**\n\n    - Compare knowledge graphs from two or more texts and find what's not present in the first graph that's present in the others\n    - Use it to find how one text can be enriched with the others\n\n16. **analyze_google_search_results**\n\n    - Generate a graph with keywords and topics for Google search results for a certain query\n    - Use it to understand the current informational supply (what people find)\n\n17. **analyze_related_search_queries**\n\n    - Generate a graph from the search queries suggested by Google for a certain query\n    - Use it to understand the current informational demand (what people are looking for)\n\n18. **search_queries_vs_search_results**\n\n    - Generate a graph of keyword combinations and topics people tend to search for that do not readily appear in the search results for the same queries\n    - Use it to understand what people search for but don't yet find\n\n19. **generate_seo_report**\n\n    - Analyze content for SEO optimization by comparing it with Google search results and search queries\n    - Identify content gaps and opportunities for better search visibility\n    - Get comprehensive analysis of what's in search results but not in your text\n    - Discover what people search for but don't find in current results\n\n20. **memory_add_relations**\n\n    - Add relations to the InfraNodus memory from text\n    - Automatically detect entities or use [[wikilinks]] syntax to mark them\n    - Save memory to a specified graph name for future retrieval\n    - Support automatic entity extraction or manual entity marking\n    - Provide links to created memory graphs for easy access\n\n21. **memory_get_relations**\n\n    - Retrieve relations from InfraNodus memory for specific entities\n    - Search for entity relations using [[wikilinks]] syntax\n    - Query specific memory contexts or search across all memory graphs\n    - Extract statements and relationships from stored knowledge graphs\n    - Support both entity-specific searches and full context retrieval\n\n22. **retrieve_from_knowledge_base**\n\n    - Retrieve context from an existing InfraNodus knowledge graph using GraphRAG\n    - Query your knowledge base with a natural language prompt to get relevant statements\n    - Include graph summaries for quick overviews of the knowledge structure\n    - Optionally retrieve the full graph, statements, or extended analysis\n    - Ideal for augmenting LLM responses with domain-specific knowledge\n\n23. **search**\n\n    - Search through existing InfraNodus graphs\n    - Also use it to search through the public graphs of a specific user\n    - Compatible with ChatGPT Deep Research mode via Developer Mode > Connectors\n\n24. **fetch**\n\n    - Fetch a specific search result for a graph\n    - Can be used in ChatGPT Deep Research mode via Developer Mode > Connectors\n\n_More capabilites coming soon!_\n\n### Key Capabilities\n\n- **Topic Modeling**: Automatic clustering and categorization of concepts\n- **Content Gap Detection**: Find missing links between concept clusters\n- **Entity Recognition**: Clean extraction of names, places, and organizations\n- **AI Enhancement**: Optional AI-powered topic naming and analysis\n- **Structural Analysis**: Identify influential nodes and community structures\n- **Network Structure Statistics**: Modularity, centrality, betweenness, and other graph metrics\n- **Knowledge Graph Memory**: Save and retrieve knowledge graph memories and analyze them to retrieve key nodes, clusters, and connectors\n\n## Knowledge Graph Memory Use Advice\n\nInfraNodus represents any text as a network graph in order to identify the main clusters of ideas and gaps between them. This helps generate advanced insights based on the text's structure. The network is effectively a knowledge graph that can also be used to retrieve complex ontological relations between different entities and concepts. This process is automated in InfraNodus using the `search` and `fetch` tools along with the other tools that analyze the underlying network.\n\nHowever, you can also easily use InfraNodus as a more traditional memory server to save and retrieve relations. We use [[wikilinks]] to highlight entities in your text to make your content and graphs compatible with markup syntax and PKM tools such as Obsidian. By default, InfraNodus will generate the name of the memory graph for you based on the context of the conversation. However, you can modify this default behavior by adding a **system prompt** or **project instruction** into your LLM client.\n\nSpecifically you can specify to always use a speciic knowlege graph for memories to store everything in one place:\n\n```\nSave all memories in the `my-memories` graph in InfraNodus.\n```\n\nOr you can ask InfraNodus to only save certain entities, e.g. for building social networks:\n\n```\nWhen generating entities, only extract people, companies, and organizations. Ignore everything else.\n```\n\n## Installation\n\nThe easiest and the fastest way to launch the InfraNodus MCP server is to either use our server URL `https://mcp.infranodus.com` for the remote / web applications or to add a manual configuration to your LLM apps if you're running them locally.\n\nYou can also install the server locally, so you have more control over it. In this case, you can also edit the source files and even create your tools based on the [InfraNodus API](https://infranodus.com/api).\n\nBelow we describe the two different ways to set up your InfraNodus MCP server.\n\n### 1. Easiest Setup: InfraNodus MCP Server (via HTTP/SSE)\n\n0. **Prerequisites**\n\n- Create an account on [InfraNodus](https://infranodus.com) if you don't have it already and get your [InfraNodus API Key](https://infranodus.com/api-access). We offer 14-day free trials.\n\n1. **Get the URL**\n\n- We currently use the following URL for our MCP server deployed in our infrastructure:\n\n```bash\nhttps://mcp.infranodus.com\n```\n\n2. **Add the MCP server URL to the Client Tool Where You Want to Use InfraNodus**\n\n- Once you add the URL above to your tool, it will automatically prompt you to authenticate using OAuth in order to be able to access the InfraNodus MCP hosted on it.\n\n4. **Using InfraNodus Tools in Your Calls**\n\n- To use InfraNodus, see the tools available and simply call them through the chat interface (e.g. \"show me the graphs where I talk about this topic\" or \"get the content gaps from the document I uploaded\")\n\n- If your client is not using InfraNodus for some actions, add the instruction to use InfraNodus explicitly.\n\n### 2. Manual Setup: via NPX\n\nYou can deploy the InfraNodus server manually via `npx` ‚Äî a package that allows to execute local and remote Node.Js packages on your computer.\n\nThe InfraNodus MCP server is available as an npm package at [https://www.npmjs.com/package/infranodus-mcp-server](https://www.npmjs.com/package/infranodus-mcp-server) from where you can launch it remotely on your local computer with npx. It will expose its tools to the MCP client that will be using this command to launch the server\n\n#### For Claude Desktop / Cursor IDE:\n\nJust add this in your Claude's configuration file (Settings > Developer > Edit Config), inside the `\"mcpServers\"` object where the different servers are listed:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"infranodus-mcp-server\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"YOUR_INFRANODUS_API_KEY\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n#### For Claude Code\n\nTo connect the InfraNodus MCP server to your Claude code, you can use this command. Make sure to provide the correct InfraNodus API key for your account:\n\n```bash\nclaude mcp add infranodus -s user \\\n\t-- env INFRANODUS_API_KEY=YOUR_INRANODUS_KEY \\\n\t\tnpx -y infranodus-mcp-server\n```\n\n### 3. Manual Setup: Launching MCP as a Local Server (for inspection & development)\n\n0. **Prerequisites**\n\n- Node.js 18+ installed\n- InfraNodus API key (get yours at [https://infranodus.com/api-access](https://infranodus.com/api-access))\n\n1. **Clone and build the server:**\n\n   ```bash\n   git clone https://github.com/yourusername/mcp-server-infranodus.git\n   cd mcp-server-infranodus\n   npm install\n   npm run build:inspect\n   ```\n\nNote that `build:inspect` will generate the `dist/index.js` file which you will then use in your server setup. The standard `npm run build` command will only build a Smithery file.\n\n2. **Set up your API key:**\n\n   Create a `.env` file in the project root:\n\n   ```\n   INFRANODUS_API_KEY=your-api-key-here\n   ```\n\n3. **Inspect the MCP:**\n\n   ```bash\n   npm run inspect\n   ```\n\n### Claude Desktop Configuration (macOS)\n\n1. Open your Claude Desktop configuration file:\n\n   ```bash\n   open ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n2. Add the InfraNodus server configuration:\n\na. remote launch via `npx`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"infranodus-mcp-server\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"YOUR_INFRANODUS_API_KEY\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nb. launch this repo with `node`, specify the absolute path to the repo + `/dist/index.js`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"/absolute/path/to/mcp-server-infranodus/dist/index.js\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"your-api-key-here\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n**Note:** you can leave the `INFRANODUS_API_KEY` empty in which case you can make 70 free requests after which you will hit quota and will need to add your API key.\n\n3. Restart Claude Desktop to load the new server.\n\n### Claude Desktop Configuration (Windows)\n\n1. Open your Claude Desktop configuration file:\n\n   ```\n   %APPDATA%\\Claude\\claude_desktop_config.json\n   ```\n\n2. Add the InfraNodus server configuration:\n\na. remote launch via `npx`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"infranodus-mcp-server\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"YOUR_INFRANODUS_API_KEY\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nb. launch this repo with `node`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"C:\\\\path\\\\to\\\\mcp-server-infranodus\\\\dist\\\\index.js\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"your-api-key-here\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n3. Restart Claude Desktop.\n\n### Cursor Configuration\n\n### Other MCP-Compatible Applications\n\nFor other applications supporting MCP, use the following command to start the server via npx:\n\n```bash\nINFRANODUS_API_KEY=your-api-key npx -y infranodus-mcp-server\n```\n\nor locally\n\n```bash\nINFRANODUS_API_KEY=your-api-key node /path/to/mcp-server-infranodus/dist/index.js\n```\n\nThe server communicates via stdio, so configure your application to run this command and communicate through standard input/output.\n\n### Legacy Setup via Smithery\n\nInfraNodus server is also available through Smithery: a repository of MCP servers that has an easy-to-follow installation process for most LLM clients. You will need a separate accout at Smithery though.\n\n- Create an account on [Smithery.Ai](https://smithery.ai/) (it's free and you can use your Google or GitHub login)\n\n- Then go to the [Smithery InfraNodus Server](https://smithery.ai/server/@infranodus/mcp-server-infranodus), click \"Configure\" at the top right, and add your InfraNodus API key there.\n\n- Go to [Smithery InfraNodus Server](https://smithery.ai/server/@infranodus/mcp-server-infranodus) and get the URL link from Smithery [https://server.smithery.ai/@infranodus/mcp-server-infranodus/mcp](https://server.smithery.ai/@infranodus/mcp-server-infranodus/mcp) for the server or use one of their automatic setup tools for Claude or Cursor.\n\n- You may need to get your separate Smithery API key and Smithery proile link to make this work.\n\n##### For Cursor:\n\n```json\n// e.g. Cursor will access directly the server via Smithery\n\"mcpServers\": {\n    \"mcp-server-infranodus\": {\n      \"type\": \"http\",\n      \"url\": \"https://server.smithery.ai/@infranodus/mcp-server-infranodus/mcp?api_key=YOUR_SMITHERY_KEY&profile=YOUR_SMITHERY_PROFILE\",\n      \"headers\": {}\n    }\n  }\n```\n\n#### For Claude:\n\n```json\n// Claude uses a slightly different implementation\n// Fot this, it launches the MCP server on your local machine\n\"mcpServers\": {\n   \"mcp-server-infranodus\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\n\t\t\t\t\"-y\",\n\t\t\t\t\"@smithery/cli@latest\",\n\t\t\t\t\"run\",\n\t\t\t\t\"@infranodus/mcp-server-infranodus\",\n\t\t\t\t\"--key\",\n\t\t\t\t\"YOUR_SMITHERY_KEY\",\n\t\t\t\t\"--profile\",\n\t\t\t\t\"YOUR_SMITHERY_PROFILE\"\n\t\t\t]\n\t\t}\n  }\n```\n\n**Note**, in both cases, you'll automatically get the `YOUR_SMITHERY_KEY` and `YOUR_SMITHERY_PROFILE` values from Smithery when you copy the URL with credentials. These are not your InfraNodus API keys. You can use the InfraNodus API server without the API for the first 70 calls. Then you can add it to your Smithery profile and it will automatically connect to your account using the link above.\n\n## Usage Examples\n\nOnce installed, you can ask Claude to:\n\n- \"Use InfraNodus to analyze this text and show me the main topics\"\n- \"Generate a knowledge graph from this document\"\n- \"Find content gaps in this article\"\n- \"Retrieve my existing graph called 'Research Notes' from InfraNodus\"\n- \"What are the structural gaps in this text?\"\n- \"Identify the most influential concepts in this content\"\n\n## Development\n\n### Running in Development Mode\n\n```bash\nnpm run dev\n```\n\n### Using the MCP Inspector\n\nTest the server with the MCP Inspector:\n\n```bash\n\nnpm run build:inspect\nnpm run inspect\n```\n\n### Building from Source\n\n```bash\nnpm run build\n```\n\n### Watching for Changes\n\n```bash\nnpm run watch\n```\n\n## API Documentation\n\n### generate_knowledge_graph\n\nAnalyzes text and generates a knowledge graph.\n\n**Parameters:**\n\n- `text` (string, required): The text to analyze\n- `includeStatements` (boolean): Include original statements in response\n- `modifyAnalyzedText` (string): Text modification options (\"none\", \"entities\", \"lemmatize\")\n\n### analyze_existing_graph_by_name\n\nRetrieves and analyzes an existing graph from your InfraNodus account.\n\n**Parameters:**\n\n- `graphName` (string, required): Name of the existing graph\n- `includeStatements` (boolean): Include statements in response\n- `includeGraphSummary` (boolean): Include graph summary\n\n### generate_content_gaps\n\nIdentifies content gaps and missing connections in text.\n\n**Parameters:**\n\n- `text` (string, required): The text to analyze for gaps\n\n## Progress Notifications\n\nFor long-running operations (like SEO analysis), the MCP server supports **real-time progress notifications** that provide intermediary feedback to AI agents. This allows agents to:\n\n- Track the progress of multi-step operations\n- Display status messages to users\n- Understand what's happening during lengthy analyses\n\n### Implementation\n\nThe server implements MCP progress notifications using:\n\n1. **ToolHandlerContext**: All tool handlers can receive an optional context parameter containing the server instance and progress token\n2. **ProgressReporter**: A utility class that simplifies sending progress updates with percentages and messages\n3. **Wrapped Handlers**: Tool registration automatically injects the server context into handlers\n\n### Example Usage in Tools\n\n```typescript\nimport { ProgressReporter } from \"../utils/progress.js\";\nimport { ToolHandlerContext } from \"../types/index.js\";\n\nhandler: async (params: ParamType, context: ToolHandlerContext = {}) => {\n\tconst progress = new ProgressReporter(context);\n\n\tawait progress.report(25, \"Fetching data from API...\");\n\t// Do work\n\n\tawait progress.report(75, \"Analyzing results...\");\n\t// More work\n\n\tawait progress.report(100, \"Complete!\");\n\treturn results;\n};\n```\n\nThe `generate_seo_report` tool demonstrates this pattern with 6 major progress checkpoints that provide detailed status updates throughout the multi-step analysis process.\n\n## Troubleshooting\n\n### Server doesn't appear in Claude\n\n1. Verify the configuration file path is correct\n2. Check that the API key is valid\n3. Ensure Node.js is in your system PATH\n4. Restart Claude Desktop completely\n\n### API Key Issues\n\n- Get your API key at: [https://infranodus.com/api-access](https://infranodus.com/api-access)\n- Ensure the key is correctly set in the configuration\n- Check that the key has not expired\n\n### Build Errors\n\n```bash\n# Clean install\nrm -rf node_modules package-lock.json\nnpm install\nnpm run build\n```\n\n## Resources\n\n- [InfraNodus Website](https://infranodus.com)\n- [InfraNodus MCP](https://infranodus.com/mcp)\n- [InfraNodus API Documentation](https://infranodus.com/api-access)\n- [MCP Protocol Documentation](https://modelcontextprotocol.io)\n- [Graph Theory Concepts](https://noduslabs.com/research/)\n\n## License\n\nMIT\n\n## Support\n\nFor issues related to:\n\n- This MCP server: Open an issue in this repository\n- InfraNodus API: Contact support@infranodus.com\n- MCP Protocol: Visit the [MCP community](https://modelcontextprotocol.io)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect InfraNodus knowledge graphs to LLM workflows and AI chats",
        "Identify main topical clusters and nuances in discourse",
        "Detect content gaps and underexplored topics in text",
        "Generate knowledge graphs from text with topic and entity extraction",
        "Save and retrieve entities and relations from knowledge graph memory",
        "Generate research questions and ideas based on content gaps",
        "Analyze and compare multiple texts via knowledge graph overlaps and differences",
        "Analyze Google search results and related queries for SEO insights",
        "Generate SEO reports identifying content gaps and search visibility opportunities",
        "Search and fetch existing InfraNodus graphs for integration with LLMs"
      ],
      "limitations": [
        "Requires InfraNodus account and API key for access",
        "Hosted MCP server requires OAuth authentication",
        "Some advanced AI model usage depends on InfraNodus API availability",
        "No mention of support for non-textual data inputs",
        "Limited to text and graph-based knowledge representations"
      ],
      "requirements": [
        "InfraNodus account with API key (14-day free trial available)",
        "OAuth authentication for hosted MCP server usage",
        "Node.js environment for manual local server deployment via npx",
        "LLM client or AI assistant capable of integrating MCP servers"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with usage contexts, explicit requirements, and outlines limitations, making it highly informative and practical.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# InfraNodus MCP Server\n\nA Model Context Protocol (MCP) server that integrates InfraNodus knowledge graph and text network analysis capabilities into LLM workflows and AI assistants like Claude Desktop.\n\n## Overview\n\nInfraNodus MCP Server enables LLM workflows and AI assistants to analyze text using advanced network science algorithms, generate knowledge graphs, detect content gaps, and identify key topics and concepts. It transforms unstructured text into structured insights using graph theory and network analysis.\n\n![InfraNodus MCP Server](https://infranodus.com/images/front/infranodus-overview.jpg)\n\n## Features\n\n### You Can Use It To\n\n- Connect your existing InfraNodus knowledge graphs to your LLM workflows and AI chats\n- Identify the main topical clusters in discourse without missing the important nuances (works better than standard LLM workflows)\n- Identify the content gaps in any discourse (helpful for content creation and research)\n- Generate new knowledge graphs from any text and use them to augment your LLM responses\n- Save and retrieve entities and relations from memory using the knowledge graphs\n\n### Available Tools\n\n1. **generate_knowledge_graph**\n\n   - Convert any text into a visual knowledge graph\n   - Extract topics, concepts, and their relationships\n   - Identify structural patterns and clusters\n   - Apply AI-powered topic naming\n   - Perform entity detection for cleaner graphs\n\n2. **analyze_existing_graph_by_name**\n\n   - Retrieve and analyze existing graphs from your InfraNodus account\n   - Access previously saved analyses\n   - Export graph data with full statistics\n\n3. **generate_content_gaps**\n\n   - Detect missing connections in discourse\n   - Identify underexplored topics\n   - Generate research questions\n   - Suggest content development opportunities\n\n4.",
        "start_pos": 0,
        "end_pos": 1804,
        "token_count_estimate": 451,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 1,
        "text": "e topics and clusters of keywords from text using knowledge graph analysis\n   - Make sure to beyond genetic insights and detect smaller topics\n   - Use the topical clusters to establish topical authority for SEO\n\n5. **generate_contextual_hint**\n\n   - Generate a topical overview of a text and provide insights for LLMs to generate better responses\n   - Use it to get a high-level understanding of a text\n   - Use it to augment prompts in your LLM workflows and AI assistants\n\n6. **generate_research_questions**\n\n   - Generate research questions that bridge content gaps\n   - Use them as prompts in your LLM models and AI workflows\n   - Use any AI model (included in InfraNodus API)\n   - Content gaps are identified based on topical clustering\n\n7. **generate_research_ideas**\n\n   - Generate innovative research ideas based on content gaps identified in the text\n   - Get actionable ideas to improve the text and develop the discourse\n   - Use any AI model (included in InfraNodus API)\n   - Ideas are generated from gaps between topical clusters\n\n8. **research_questions_from_graph**\n\n   - Generate research questions based on an existing InfraNodus graph\n   - Use them as prompts in your LLM models\n   - Use any AI model (included in InfraNodus API)\n   - Content gaps are identified based on topical clustering\n\n9. **generate_responses_from_graph**\n\n   - Generate responses based on an existing InfraNodus graph\n   - Integrate them into your LLM workflows and AI assistants\n   - Use any AI model (included in InfraNodus API)\n   - Use any prompt\n\n10. **develop_conceptual_bridges**\n\n    - Analyze text and develop latent ideas based on concepts that connect this text to a broader discourse\n    - Discover hidden themes and patterns that link your text to wider contexts\n    - Use any AI model (included in InfraNodus API)\n    - Generate insights that help develop the discourse\n\n11.",
        "start_pos": 1848,
        "end_pos": 3729,
        "token_count_estimate": 470,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 2,
        "text": "scourse\n    - Discover hidden themes and patterns that link your text to wider contexts\n    - Use any AI model (included in InfraNodus API)\n    - Generate insights that help develop the discourse\n\n11. **develop_latent_topics**\n\n    - Analyze text and extract underdeveloped topics with ideas on how to develop them\n    - Identify topics that need more attention and elaboration\n    - Use any AI model (included in InfraNodus API)\n    - Get actionable suggestions for content expansion\n\n12. **develop_text_tool**\n\n    - Comprehensive text analysis combining content gap ideas, latent topics, and conceptual bridges\n    - Executes multiple analyses in sequence with progress tracking\n    - Generates research ideas based on content gaps\n    - Identifies latent topics and conceptual bridges to develop\n    - Finds content gaps for deeper exploration\n\n13. **create_knowledge_graph**\n\n    - Create a knowledge graph in InfraNodus from text and provide a link to it\n    - Use it to create a knowledge graph in InfraNodus from text\n\n14. **overlap_between_texts**\n\n    - Create knowledge graphs from two or more texts and find the overlap (similarities) between them\n    - Use it to find similar topics and keywords across different texts\n\n15. **difference_between_texts**\n\n    - Compare knowledge graphs from two or more texts and find what's not present in the first graph that's present in the others\n    - Use it to find how one text can be enriched with the others\n\n16. **analyze_google_search_results**\n\n    - Generate a graph with keywords and topics for Google search results for a certain query\n    - Use it to understand the current informational supply (what people find)\n\n17. **analyze_related_search_queries**\n\n    - Generate a graph from the search queries suggested by Google for a certain query\n    - Use it to understand the current informational demand (what people are looking for)\n\n18.",
        "start_pos": 3529,
        "end_pos": 5427,
        "token_count_estimate": 474,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 3,
        "text": "_search_queries**\n\n    - Generate a graph from the search queries suggested by Google for a certain query\n    - Use it to understand the current informational demand (what people are looking for)\n\n18. **search_queries_vs_search_results**\n\n    - Generate a graph of keyword combinations and topics people tend to search for that do not readily appear in the search results for the same queries\n    - Use it to understand what people search for but don't yet find\n\n19. **generate_seo_report**\n\n    - Analyze content for SEO optimization by comparing it with Google search results and search queries\n    - Identify content gaps and opportunities for better search visibility\n    - Get comprehensive analysis of what's in search results but not in your text\n    - Discover what people search for but don't find in current results\n\n20. **memory_add_relations**\n\n    - Add relations to the InfraNodus memory from text\n    - Automatically detect entities or use [[wikilinks]] syntax to mark them\n    - Save memory to a specified graph name for future retrieval\n    - Support automatic entity extraction or manual entity marking\n    - Provide links to created memory graphs for easy access\n\n21. **memory_get_relations**\n\n    - Retrieve relations from InfraNodus memory for specific entities\n    - Search for entity relations using [[wikilinks]] syntax\n    - Query specific memory contexts or search across all memory graphs\n    - Extract statements and relationships from stored knowledge graphs\n    - Support both entity-specific searches and full context retrieval\n\n22. **retrieve_from_knowledge_base**\n\n    - Retrieve context from an existing InfraNodus knowledge graph using GraphRAG\n    - Query your knowledge base with a natural language prompt to get relevant statements\n    - Include graph summaries for quick overviews of the knowledge structure\n    - Optionally retrieve the full graph, statements, or extended analysis\n    - Ideal for augmenting LLM responses with domain-specific knowledge\n\n23.",
        "start_pos": 5227,
        "end_pos": 7225,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 4,
        "text": "s for quick overviews of the knowledge structure\n    - Optionally retrieve the full graph, statements, or extended analysis\n    - Ideal for augmenting LLM responses with domain-specific knowledge\n\n23. **search**\n\n    - Search through existing InfraNodus graphs\n    - Also use it to search through the public graphs of a specific user\n    - Compatible with ChatGPT Deep Research mode via Developer Mode > Connectors\n\n24. **fetch**\n\n    - Fetch a specific search result for a graph\n    - Can be used in ChatGPT Deep Research mode via Developer Mode > Connectors\n\n_More capabilites coming soon!_\n\n### Key Capabilities\n\n- **Topic Modeling**: Automatic clustering and categorization of concepts\n- **Content Gap Detection**: Find missing links between concept clusters\n- **Entity Recognition**: Clean extraction of names, places, and organizations\n- **AI Enhancement**: Optional AI-powered topic naming and analysis\n- **Structural Analysis**: Identify influential nodes and community structures\n- **Network Structure Statistics**: Modularity, centrality, betweenness, and other graph metrics\n- **Knowledge Graph Memory**: Save and retrieve knowledge graph memories and analyze them to retrieve key nodes, clusters, and connectors\n\n## Knowledge Graph Memory Use Advice\n\nInfraNodus represents any text as a network graph in order to identify the main clusters of ideas and gaps between them. This helps generate advanced insights based on the text's structure. The network is effectively a knowledge graph that can also be used to retrieve complex ontological relations between different entities and concepts. This process is automated in InfraNodus using the `search` and `fetch` tools along with the other tools that analyze the underlying network.\n\nHowever, you can also easily use InfraNodus as a more traditional memory server to save and retrieve relations. We use [[wikilinks]] to highlight entities in your text to make your content and graphs compatible with markup syntax and PKM tools such as Obsidian.",
        "start_pos": 7025,
        "end_pos": 9031,
        "token_count_estimate": 501,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 5,
        "text": "onal memory server to save and retrieve relations. We use [[wikilinks]] to highlight entities in your text to make your content and graphs compatible with markup syntax and PKM tools such as Obsidian. By default, InfraNodus will generate the name of the memory graph for you based on the context of the conversation. However, you can modify this default behavior by adding a **system prompt** or **project instruction** into your LLM client.\n\nSpecifically you can specify to always use a speciic knowlege graph for memories to store everything in one place:\n\n```\nSave all memories in the `my-memories` graph in InfraNodus.\n```\n\nOr you can ask InfraNodus to only save certain entities, e.g. for building social networks:\n\n```\nWhen generating entities, only extract people, companies, and organizations. Ignore everything else.\n```\n\n## Installation\n\nThe easiest and the fastest way to launch the InfraNodus MCP server is to either use our server URL `https://mcp.infranodus.com` for the remote / web applications or to add a manual configuration to your LLM apps if you're running them locally.\n\nYou can also install the server locally, so you have more control over it. In this case, you can also edit the source files and even create your tools based on the [InfraNodus API](https://infranodus.com/api).\n\nBelow we describe the two different ways to set up your InfraNodus MCP server.\n\n### 1. Easiest Setup: InfraNodus MCP Server (via HTTP/SSE)\n\n0. **Prerequisites**\n\n- Create an account on [InfraNodus](https://infranodus.com) if you don't have it already and get your [InfraNodus API Key](https://infranodus.com/api-access). We offer 14-day free trials.\n\n1. **Get the URL**\n\n- We currently use the following URL for our MCP server deployed in our infrastructure:\n\n```bash\nhttps://mcp.infranodus.com\n```\n\n2.",
        "start_pos": 8831,
        "end_pos": 10638,
        "token_count_estimate": 451,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 6,
        "text": "ool Where You Want to Use InfraNodus**\n\n- Once you add the URL above to your tool, it will automatically prompt you to authenticate using OAuth in order to be able to access the InfraNodus MCP hosted on it.\n\n4. **Using InfraNodus Tools in Your Calls**\n\n- To use InfraNodus, see the tools available and simply call them through the chat interface (e.g. \"show me the graphs where I talk about this topic\" or \"get the content gaps from the document I uploaded\")\n\n- If your client is not using InfraNodus for some actions, add the instruction to use InfraNodus explicitly.\n\n### 2. Manual Setup: via NPX\n\nYou can deploy the InfraNodus server manually via `npx` ‚Äî a package that allows to execute local and remote Node.Js packages on your computer.\n\nThe InfraNodus MCP server is available as an npm package at [https://www.npmjs.com/package/infranodus-mcp-server](https://www.npmjs.com/package/infranodus-mcp-server) from where you can launch it remotely on your local computer with npx. It will expose its tools to the MCP client that will be using this command to launch the server\n\n#### For Claude Desktop / Cursor IDE:\n\nJust add this in your Claude's configuration file (Settings > Developer > Edit Config), inside the `\"mcpServers\"` object where the different servers are listed:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"infranodus-mcp-server\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"YOUR_INFRANODUS_API_KEY\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n#### For Claude Code\n\nTo connect the InfraNodus MCP server to your Claude code, you can use this command. Make sure to provide the correct InfraNodus API key for your account:\n\n```bash\nclaude mcp add infranodus -s user \\\n\t-- env INFRANODUS_API_KEY=YOUR_INRANODUS_KEY \\\n\t\tnpx -y infranodus-mcp-server\n```\n\n### 3. Manual Setup: Launching MCP as a Local Server (for inspection & development)\n\n0. **Prerequisites**\n\n- Node.js 18+ installed\n- InfraNodus API key (get yours at [https://infranodus.com/api-access](https://infranodus.com/api-access))\n\n1.",
        "start_pos": 10679,
        "end_pos": 12698,
        "token_count_estimate": 504,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 7,
        "text": "ocal Server (for inspection & development)\n\n0. **Prerequisites**\n\n- Node.js 18+ installed\n- InfraNodus API key (get yours at [https://infranodus.com/api-access](https://infranodus.com/api-access))\n\n1. **Clone and build the server:**\n\n   ```bash\n   git clone https://github.com/yourusername/mcp-server-infranodus.git\n   cd mcp-server-infranodus\n   npm install\n   npm run build:inspect\n   ```\n\nNote that `build:inspect` will generate the `dist/index.js` file which you will then use in your server setup. The standard `npm run build` command will only build a Smithery file.\n\n2. **Set up your API key:**\n\n   Create a `.env` file in the project root:\n\n   ```\n   INFRANODUS_API_KEY=your-api-key-here\n   ```\n\n3. **Inspect the MCP:**\n\n   ```bash\n   npm run inspect\n   ```\n\n### Claude Desktop Configuration (macOS)\n\n1. Open your Claude Desktop configuration file:\n\n   ```bash\n   open ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n   ```\n\n2. Add the InfraNodus server configuration:\n\na. remote launch via `npx`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"infranodus-mcp-server\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"YOUR_INFRANODUS_API_KEY\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nb. launch this repo with `node`, specify the absolute path to the repo + `/dist/index.js`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"/absolute/path/to/mcp-server-infranodus/dist/index.js\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"your-api-key-here\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n**Note:** you can leave the `INFRANODUS_API_KEY` empty in which case you can make 70 free requests after which you will hit quota and will need to add your API key.\n\n3. Restart Claude Desktop to load the new server.\n\n### Claude Desktop Configuration (Windows)\n\n1. Open your Claude Desktop configuration file:\n\n   ```\n   %APPDATA%\\Claude\\claude_desktop_config.json\n   ```\n\n2. Add the InfraNodus server configuration:\n\na.",
        "start_pos": 12498,
        "end_pos": 14440,
        "token_count_estimate": 485,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 8,
        "text": "# Claude Desktop Configuration (Windows)\n\n1. Open your Claude Desktop configuration file:\n\n   ```\n   %APPDATA%\\Claude\\claude_desktop_config.json\n   ```\n\n2. Add the InfraNodus server configuration:\n\na. remote launch via `npx`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\"-y\", \"infranodus-mcp-server\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"YOUR_INFRANODUS_API_KEY\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nb. launch this repo with `node`:\n\n```json\n{\n\t\"mcpServers\": {\n\t\t\"infranodus\": {\n\t\t\t\"command\": \"node\",\n\t\t\t\"args\": [\"C:\\\\path\\\\to\\\\mcp-server-infranodus\\\\dist\\\\index.js\"],\n\t\t\t\"env\": {\n\t\t\t\t\"INFRANODUS_API_KEY\": \"your-api-key-here\"\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\n3. Restart Claude Desktop.\n\n### Cursor Configuration\n\n### Other MCP-Compatible Applications\n\nFor other applications supporting MCP, use the following command to start the server via npx:\n\n```bash\nINFRANODUS_API_KEY=your-api-key npx -y infranodus-mcp-server\n```\n\nor locally\n\n```bash\nINFRANODUS_API_KEY=your-api-key node /path/to/mcp-server-infranodus/dist/index.js\n```\n\nThe server communicates via stdio, so configure your application to run this command and communicate through standard input/output.\n\n### Legacy Setup via Smithery\n\nInfraNodus server is also available through Smithery: a repository of MCP servers that has an easy-to-follow installation process for most LLM clients. You will need a separate accout at Smithery though.\n\n- Create an account on [Smithery.Ai](https://smithery.ai/) (it's free and you can use your Google or GitHub login)\n\n- Then go to the [Smithery InfraNodus Server](https://smithery.ai/server/@infranodus/mcp-server-infranodus), click \"Configure\" at the top right, and add your InfraNodus API key there.",
        "start_pos": 14240,
        "end_pos": 15943,
        "token_count_estimate": 425,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 9,
        "text": "server.smithery.ai/@infranodus/mcp-server-infranodus/mcp](https://server.smithery.ai/@infranodus/mcp-server-infranodus/mcp) for the server or use one of their automatic setup tools for Claude or Cursor.\n\n- You may need to get your separate Smithery API key and Smithery proile link to make this work.\n\n##### For Cursor:\n\n```json\n// e.g. Cursor will access directly the server via Smithery\n\"mcpServers\": {\n    \"mcp-server-infranodus\": {\n      \"type\": \"http\",\n      \"url\": \"https://server.smithery.ai/@infranodus/mcp-server-infranodus/mcp?api_key=YOUR_SMITHERY_KEY&profile=YOUR_SMITHERY_PROFILE\",\n      \"headers\": {}\n    }\n  }\n```\n\n#### For Claude:\n\n```json\n// Claude uses a slightly different implementation\n// Fot this, it launches the MCP server on your local machine\n\"mcpServers\": {\n   \"mcp-server-infranodus\": {\n\t\t\t\"command\": \"npx\",\n\t\t\t\"args\": [\n\t\t\t\t\"-y\",\n\t\t\t\t\"@smithery/cli@latest\",\n\t\t\t\t\"run\",\n\t\t\t\t\"@infranodus/mcp-server-infranodus\",\n\t\t\t\t\"--key\",\n\t\t\t\t\"YOUR_SMITHERY_KEY\",\n\t\t\t\t\"--profile\",\n\t\t\t\t\"YOUR_SMITHERY_PROFILE\"\n\t\t\t]\n\t\t}\n  }\n```\n\n**Note**, in both cases, you'll automatically get the `YOUR_SMITHERY_KEY` and `YOUR_SMITHERY_PROFILE` values from Smithery when you copy the URL with credentials. These are not your InfraNodus API keys. You can use the InfraNodus API server without the API for the first 70 calls. Then you can add it to your Smithery profile and it will automatically connect to your account using the link above.",
        "start_pos": 16088,
        "end_pos": 17525,
        "token_count_estimate": 359,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 10,
        "text": "### Running in Development Mode\n\n```bash\nnpm run dev\n```\n\n### Using the MCP Inspector\n\nTest the server with the MCP Inspector:\n\n```bash\n\nnpm run build:inspect\nnpm run inspect\n```\n\n### Building from Source\n\n```bash\nnpm run build\n```\n\n### Watching for Changes\n\n```bash\nnpm run watch\n```\n\n## API Documentation\n\n### generate_knowledge_graph\n\nAnalyzes text and generates a knowledge graph.\n\n**Parameters:**\n\n- `text` (string, required): The text to analyze\n- `includeStatements` (boolean): Include original statements in response\n- `modifyAnalyzedText` (string): Text modification options (\"none\", \"entities\", \"lemmatize\")\n\n### analyze_existing_graph_by_name\n\nRetrieves and analyzes an existing graph from your InfraNodus account.\n\n**Parameters:**\n\n- `graphName` (string, required): Name of the existing graph\n- `includeStatements` (boolean): Include statements in response\n- `includeGraphSummary` (boolean): Include graph summary\n\n### generate_content_gaps\n\nIdentifies content gaps and missing connections in text.\n\n**Parameters:**\n\n- `text` (string, required): The text to analyze for gaps\n\n## Progress Notifications\n\nFor long-running operations (like SEO analysis), the MCP server supports **real-time progress notifications** that provide intermediary feedback to AI agents. This allows agents to:\n\n- Track the progress of multi-step operations\n- Display status messages to users\n- Understand what's happening during lengthy analyses\n\n### Implementation\n\nThe server implements MCP progress notifications using:\n\n1. **ToolHandlerContext**: All tool handlers can receive an optional context parameter containing the server instance and progress token\n2. **ProgressReporter**: A utility class that simplifies sending progress updates with percentages and messages\n3.",
        "start_pos": 17936,
        "end_pos": 19698,
        "token_count_estimate": 440,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      },
      {
        "chunk_id": 11,
        "text": "handlers\n\n### Example Usage in Tools\n\n```typescript\nimport { ProgressReporter } from \"../utils/progress.js\";\nimport { ToolHandlerContext } from \"../types/index.js\";\n\nhandler: async (params: ParamType, context: ToolHandlerContext = {}) => {\n\tconst progress = new ProgressReporter(context);\n\n\tawait progress.report(25, \"Fetching data from API...\");\n\t// Do work\n\n\tawait progress.report(75, \"Analyzing results...\");\n\t// More work\n\n\tawait progress.report(100, \"Complete!\");\n\treturn results;\n};\n```\n\nThe `generate_seo_report` tool demonstrates this pattern with 6 major progress checkpoints that provide detailed status updates throughout the multi-step analysis process.\n\n## Troubleshooting\n\n### Server doesn't appear in Claude\n\n1. Verify the configuration file path is correct\n2. Check that the API key is valid\n3. Ensure Node.js is in your system PATH\n4. Restart Claude Desktop completely\n\n### API Key Issues\n\n- Get your API key at: [https://infranodus.com/api-access](https://infranodus.com/api-access)\n- Ensure the key is correctly set in the configuration\n- Check that the key has not expired\n\n### Build Errors\n\n```bash\n# Clean install\nrm -rf node_modules package-lock.json\nnpm install\nnpm run build\n```\n\n## Resources\n\n- [InfraNodus Website](https://infranodus.com)\n- [InfraNodus MCP](https://infranodus.com/mcp)\n- [InfraNodus API Documentation](https://infranodus.com/api-access)\n- [MCP Protocol Documentation](https://modelcontextprotocol.io)\n- [Graph Theory Concepts](https://noduslabs.com/research/)\n\n## License\n\nMIT\n\n## Support\n\nFor issues related to:\n\n- This MCP server: Open an issue in this repository\n- InfraNodus API: Contact support@infranodus.com\n- MCP Protocol: Visit the [MCP community](https://modelcontextprotocol.io)",
        "start_pos": 19784,
        "end_pos": 21519,
        "token_count_estimate": 433,
        "source_type": "readme",
        "agent_id": "bb15b078a39953aa"
      }
    ]
  },
  {
    "agent_id": "53726c54421f08b5",
    "name": "ai.smithery/isnow890-data4library-mcp",
    "source": "mcp",
    "source_url": "https://github.com/isnow890/data4library-mcp",
    "description": "Ï±Ö Ïã´Ïñ¥ÌïòÎäî Ï†úÍ∞Ä Ï±ÖÏóê ÎåÄÌï¥ ÏïÑÎäîÏ≤ôÌïòÍ≥† Ïã∂Ïñ¥ÏÑú ÎßåÎì§ÏóàÏäµÎãàÎã§.. ÎÇ¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥Ä Ïã§ÏãúÍ∞Ñ ÎåÄÏ∂ú ÌôïÏù∏ ÏùΩÍ≥† Ïã∂ÏùÄ Ï±ÖÏùÑ Í≤ÄÏÉâÌïòÎ©¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥Ä ÎåÄÏ∂ú Í∞ÄÎä• Ïó¨Î∂ÄÎ•º Ï¶âÏãú ÌôïÏù∏ Íµ≥Ïù¥ ÎèÑÏÑúÍ¥Ä‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T10:45:21.467985Z",
    "indexed_at": "2026-02-18T04:07:08.631784",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![smithery badge](https://smithery.ai/badge/@isnow890/data4library-mcp)](https://smithery.ai/server/@isnow890/data4library-mcp)\n[![MSeeP.ai Security Assessment Badge](https://mseep.net/pr/isnow890-data4library-mcp-badge.png)](https://mseep.ai/app/isnow890-data4library-mcp)\n[![smithery badge](https://smithery.ai/badge/@isnow890/data4library-mcp)](https://smithery.ai/server/@isnow890/data4library-mcp)\n\n<div align=\"center\">\n  <img src=\"https://firebasestorage.googleapis.com/v0/b/rottenbridge-e6efa.appspot.com/o/logo.jpg?alt=media&token=68d16fd2-799f-4aba-8c1e-da6977e2949e\" alt=\"Data4Library MCP Server Logo\" width=\"300\"/>\n</div>\n\n# üìö Ï†ïÎ≥¥ÎÇòÎ£® ÎèÑÏÑúÍ≤ÄÏÉâ MCP (data4library-mcp)\n\n**ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® MCP**Îäî **Íµ≠Î¶ΩÏ§ëÏïôÎèÑÏÑúÍ¥Ä**ÏóêÏÑú Ï†úÍ≥µÌïòÎäî **ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® API**Î•º ÏôÑÏ†ÑÌûà ÌôúÏö©Ìï† Ïàò ÏûàÎèÑÎ°ù Í∞úÎ∞úÎêú Ìè¨Í¥ÑÏ†ÅÏù∏ **MCP(Model Context Protocol) ÏÑúÎ≤Ñ**ÏûÖÎãàÎã§. ÌïúÍµ≠Ïùò Î™®Îì† Í≥µÍ≥µÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥, ÎèÑÏÑú Í≤ÄÏÉâ, ÎåÄÏ∂ú ÌòÑÌô©, ÎèÖÏÑú ÌÜµÍ≥Ñ Îì±ÏùÑ AI Î™®Îç∏ÏóêÏÑú ÏâΩÍ≤å Ï†ëÍ∑ºÌïòÍ≥† ÌôúÏö©Ìï† Ïàò ÏûàÍ≤å Ìï¥Ï§çÎãàÎã§.\n\n> üá∫üá∏ **English Documentation**: [README-en.md](README-en.md)\n\n## üöÄ Îπ†Î•∏ ÏÑ§Ïπò (Smithery Í∂åÏû•)\n\n**Í∞ÄÏû• Ïâ¨Ïö¥ ÏÑ§Ïπò Î∞©Î≤ï**ÏùÄ [Smithery](https://smithery.ai)Î•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏûÖÎãàÎã§:\n\n**üîó [SmitheryÏóêÏÑú ÏÑ§ÏπòÌïòÍ∏∞](https://smithery.ai/server/@isnow890/data4library-mcp)**\n\n1. ÏúÑ ÎßÅÌÅ¨Î•º ÌÅ¥Î¶≠ÌïòÏó¨ Smithery ÌéòÏù¥ÏßÄÎ°ú Ïù¥Îèô\n2. **\"Install\"** Î≤ÑÌäº ÌÅ¥Î¶≠\n3. API ÌÇ§ ÏûÖÎ†• (ÏïÑÎûò API ÌÇ§ Î∞úÍ∏â Î∞©Î≤ï Ï∞∏Ï°∞)\n4. Claude DesktopÏóêÏÑú Î∞îÎ°ú ÏÇ¨Ïö© Í∞ÄÎä•!\n\n## üåü ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®(Data4Library)ÎûÄ?\n\n[ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®](https://www.data4library.kr/)Îäî **Íµ≠Î¶ΩÏ§ëÏïôÎèÑÏÑúÍ¥Ä**Ïù¥ Ïö¥ÏòÅÌïòÎäî **Ï†ÑÍµ≠ Í≥µÍ≥µÎèÑÏÑúÍ¥Ä ÌÜµÌï© Ï†ïÎ≥¥ ÏÑúÎπÑÏä§**ÏûÖÎãàÎã§. Ï†ÑÍµ≠ 1,000Ïó¨ Í∞ú Í≥µÍ≥µÎèÑÏÑúÍ¥ÄÏùò Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞Î•º ÌÜµÌï©ÌïòÏó¨ Ï†úÍ≥µÌïòÎ©∞, Îã§ÏùåÍ≥º Í∞ôÏùÄ Ï†ïÎ≥¥Î•º Ìè¨Ìï®Ìï©ÎãàÎã§:\n\n- üìç **Ï†ÑÍµ≠ Í≥µÍ≥µÎèÑÏÑúÍ¥Ä ÏúÑÏπò Î∞è Ïö¥ÏòÅÏ†ïÎ≥¥** (1,000+ Í∞úÍ¥Ä)\n- üìñ **ÎèÑÏÑú ÏÜåÏû• Î∞è ÎåÄÏ∂ú ÌòÑÌô©** (Ïã§ÏãúÍ∞Ñ)\n- üìä **ÎåÄÏ∂ú ÌÜµÍ≥Ñ Î∞è Ìä∏Î†åÎìú Î∂ÑÏÑù**\n- üî• **Ïù∏Í∏∞ÎèÑÏÑú Î∞è Í∏âÏÉÅÏäπ ÎèÑÏÑú**\n- üìà **ÏßÄÏó≠Î≥Ñ/Ïó∞Î†πÎ≥Ñ ÎèÖÏÑúÎüâ ÌÜµÍ≥Ñ**\n- üÜï **Ïã†Ï∞©ÎèÑÏÑú Ï†ïÎ≥¥**\n\n## üéØ Ï£ºÏöî Í∏∞Îä• (25Í∞ú ÎèÑÍµ¨)\n\n### üìö ÎèÑÏÑúÍ¥Ä & ÎèÑÏÑú Í≤ÄÏÉâ\n\n- **üèõÔ∏è Ï†ÑÍµ≠ Í≥µÍ≥µÎèÑÏÑúÍ¥Ä Í≤ÄÏÉâ** (`search_libraries`): ÏßÄÏó≠Î≥Ñ, ÎèÑÏÑúÍ¥ÄÎ™ÖÏúºÎ°ú Í≤ÄÏÉâ\n- **üìñ ÎèÑÏÑú ÌÜµÌï© Í≤ÄÏÉâ** (`search_books`): Ï†úÎ™©, Ï†ÄÏûê, Ï∂úÌåêÏÇ¨, Ï£ºÏ†úÎ≥Ñ ÎèÑÏÑú Í≤ÄÏÉâ\n- **üîç ÎèÑÏÑúÍ¥ÄÎ≥Ñ ÏÜåÏû•ÎèÑÏÑú Í≤ÄÏÉâ** (`search_libraries_by_book`): ÌäπÏ†ï ÎèÑÏÑúÎ•º ÏÜåÏû•Ìïú ÎèÑÏÑúÍ¥Ä Ï∞æÍ∏∞\n- **üìã ÎèÑÏÑú ÏÉÅÏÑ∏Ï†ïÎ≥¥** (`get_book_detail`): ISBNÏúºÎ°ú ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Ï°∞Ìöå\n- **‚úÖ ÎåÄÏ∂ú Í∞ÄÎä• Ïó¨Î∂Ä** (`check_book_availability`): Ïã§ÏãúÍ∞Ñ ÎåÄÏ∂ú Í∞ÄÎä• ÏÉÅÌÉú ÌôïÏù∏\n\n### üìä Ïù∏Í∏∞ÎèÑÏÑú & Ìä∏Î†åÎìú Î∂ÑÏÑù\n\n- **üî• Ïù∏Í∏∞ ÎåÄÏ∂úÎèÑÏÑú** (`search_popular_books`): Ï†ÑÍµ≠/ÏßÄÏó≠Î≥Ñ Î≤†Ïä§Ìä∏ÏÖÄÎü¨\n- **üèÜ ÎèÑÏÑúÍ¥ÄÎ≥Ñ Ïù∏Í∏∞ÎèÑÏÑú** (`search_popular_books_by_library`): ÌäπÏ†ï ÎèÑÏÑúÍ¥ÄÏùò Ïù∏Í∏∞ÎèÑÏÑú\n- **üìà ÎåÄÏ∂ú Í∏âÏÉÅÏäπ ÎèÑÏÑú** (`get_hot_trend`): Ìä∏Î†åÎî© ÎèÑÏÑú Ïã§ÏãúÍ∞Ñ Ï°∞Ìöå\n- **üÜï Ïã†Ï∞©ÎèÑÏÑú** (`get_new_arrival_books`): ÎèÑÏÑúÍ¥ÄÎ≥Ñ ÏÉàÎ°ú Îì§Ïñ¥Ïò® ÎèÑÏÑú\n- **üè∑Ô∏è Ïù¥Îã¨Ïùò ÌÇ§ÏõåÎìú** (`get_monthly_keywords`): ÎèÖÏÑú Ìä∏Î†åÎìú ÌÇ§ÏõåÎìú\n\n### üìà ÌÜµÍ≥Ñ & Î∂ÑÏÑù ÎèÑÍµ¨\n\n- **üìä ÎåÄÏ∂úÎ∞òÎÇ© Ï∂îÏù¥** (`get_usage_trend`): ÎèÑÏÑúÍ¥ÄÎ≥Ñ Ïù¥Ïö© ÌÜµÍ≥Ñ Í∑∏ÎûòÌîÑ\n- **üåç ÏßÄÏó≠Î≥Ñ ÎèÖÏÑúÎüâ** (`get_reading_quantity`): ÎèÖÏÑúÏú® Î∞è ÎèÖÏÑúÎüâ ÎπÑÍµê\n- **üìö Ïû•ÏÑú/ÎåÄÏ∂ú Îç∞Ïù¥ÌÑ∞** (`search_items`): ÎèÑÏÑúÍ¥ÄÎ≥Ñ ÏÉÅÏÑ∏ ÌÜµÍ≥Ñ\n- **üìñ ÎèÑÏÑú Ïù¥Ïö© Î∂ÑÏÑù** (`get_book_usage_analysis`): ÌäπÏ†ï ÎèÑÏÑúÏùò Ïù¥Ïö© Ìå®ÌÑ¥\n\n### üéØ Í∞úÏù∏Ìôî Ï∂îÏ≤ú ÏãúÏä§ÌÖú\n\n- **üéì ÎßàÎãàÏïÑ Ï∂îÏ≤úÎèÑÏÑú** (`get_mania_recommendations`): Ï†ÑÎ¨∏Í∞ÄÏö© Ïã¨Ìôî ÎèÑÏÑú\n- **üìö Îã§ÎèÖÏûê Ï∂îÏ≤úÎèÑÏÑú** (`get_reader_recommendations`): Ïó∞ÏÜç ÎèÖÏÑúÏö© ÎèÑÏÑú\n- **üè∑Ô∏è ÎèÑÏÑú ÌÇ§ÏõåÎìú Î∂ÑÏÑù** (`get_book_keywords`): ÎèÑÏÑúÎ≥Ñ ÌïµÏã¨ ÌÇ§ÏõåÎìú\n\n### üó∫Ô∏è ÏúÑÏπò Í∏∞Î∞ò ÏÑúÎπÑÏä§ (ÎèÖÏûêÏ†Å Íµ¨ÌòÑ)\n\n- **üìç ÎÇ¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥Ä Í≤ÄÏÉâ** (`search_nearby_libraries`): GPS Í∏∞Î∞ò Í∞ÄÍπåÏö¥ ÎèÑÏÑúÍ¥Ä ÏûêÎèô Í≤ÄÏÉâ\n  - **Í±∞Î¶¨Ïàú Ï†ïÎ†¨**: Ïã§ÏãúÍ∞Ñ Í±∞Î¶¨ Í≥ÑÏÇ∞ Î∞è Í∞ÄÍπåÏö¥ ÏàúÏÑúÎ°ú Ï†ïÎ†¨\n  - **ÏÉÅÏÑ∏ Í±∞Î¶¨ Ï†ïÎ≥¥**: Í∞Å ÎèÑÏÑúÍ¥ÄÍπåÏßÄÏùò Ï†ïÌôïÌïú Í±∞Î¶¨(km) ÌëúÏãú\n\n### üîß ÏΩîÎìú Í≤ÄÏÉâ ÎèÑÍµ¨ (API Ïó∞Îèô ÏßÄÏõê)\n\n- **üèõÔ∏è ÎèÑÏÑúÍ¥Ä ÏΩîÎìú Í≤ÄÏÉâ** (`search_library_codes`): ÎèÑÏÑúÍ¥ÄÎ™ÖÏúºÎ°ú libCode Ï∞æÍ∏∞\n- **üåç ÏßÄÏó≠ÏΩîÎìú Ï°∞Ìöå** (`get_region_codes`, `get_detailed_region_codes`): Ï†ÑÍµ≠/ÏÑ∏Î∂Ä ÏßÄÏó≠ÏΩîÎìú\n- **üìö Ï£ºÏ†úÎ∂ÑÎ•òÏΩîÎìú** (`get_subject_codes`, `get_detailed_subject_codes`): KDC ÎåÄ/ÏÑ∏Î∂ÑÎ•ò\n- **üìä ÌÜµÌï©Ï†ïÎ≥¥ Ï°∞Ìöå** (`get_library_info`, `get_popular_books_by_library`): ÎèÑÏÑúÍ¥ÄÎ≥Ñ Ï¢ÖÌï© Ï†ïÎ≥¥\n\n### üõ†Ô∏è ÏÑ∏ÏÖò Í¥ÄÎ¶¨\n\n- **üìä ÏÇ¨Ïö©Îüâ ÌÜµÍ≥Ñ** (`session_stats`): Ïã§ÏãúÍ∞Ñ ÎèÑÍµ¨ Ìò∏Ï∂ú ÌÜµÍ≥Ñ Î∞è ÏÑ∏ÏÖò Ï†ïÎ≥¥\n\n## üí° Ïã§Ï†ú ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§\n\n### üîç ÎèÑÏÑúÍ¥Ä Ï∞æÍ∏∞\n\n- **\"ÎÇ¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥Ä Ïñ¥Îîî ÏûàÏñ¥?\"** ‚Üí `search_nearby_libraries` ÏÇ¨Ïö©\n- **\"ÏÑúÏö∏ Í∞ïÎÇ®Íµ¨ ÎèÑÏÑúÍ¥Ä Ï∞æÏïÑÏ§ò\"** ‚Üí `search_detailed_region_codes` + `search_libraries` ÏàúÏ∞® ÏÇ¨Ïö©\n\n### üìñ ÎèÑÏÑú Í≤ÄÏÉâ\n\n- **\"Ìï¥Î¶¨Ìè¨ÌÑ∞ Ï±Ö Ïñ¥ÎîîÏÑú ÎπåÎ¶¥ Ïàò ÏûàÏñ¥?\"** ‚Üí `search_books` + `search_libraries_by_book` Ïó∞Í≥Ñ\n- **\"ÍπÄÏòÅÌïò ÏûëÍ∞Ä ÏÜåÏÑ§ Ï∞æÏïÑÏ§ò\"** ‚Üí `search_books` (Ï†ÄÏûêÎ™Ö Í≤ÄÏÉâ)\n\n### üìä Ìä∏Î†åÎìú Î∂ÑÏÑù\n\n- **\"ÏöîÏ¶ò Ïù∏Í∏∞ ÏûàÎäî Ï±Ö Î≠êÏïº?\"** ‚Üí `search_popular_books` ÎòêÎäî `get_hot_trend`\n- **\"Í∞ïÎÇ®ÎèÑÏÑúÍ¥ÄÏóêÏÑú Ïù∏Í∏∞ ÏûàÎäî Ï±Ö\"** ‚Üí `search_library_codes` + `search_popular_books_by_library`\n\n### üìà ÌÜµÍ≥Ñ Î∂ÑÏÑù\n\n- **\"ÏÑúÏö∏ ÏÇ¨ÎûåÎì§Ïù¥ Ï±ÖÏùÑ ÏñºÎßàÎÇò ÎßéÏù¥ ÏùΩÏñ¥?\"** ‚Üí `get_reading_quantity`\n- **\"ÎèÑÏÑúÍ¥Ä Ïù¥Ïö© Í∑∏ÎûòÌîÑ Î≥¥Ïó¨Ï§ò\"** ‚Üí `get_usage_trend`\n\n### üéØ Î≥µÌï© ÏøºÎ¶¨ ÏòàÏãú\n\n- **\"ÎÇ¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥ÄÎì§Ïùò Ïã†Í∞Ñ ÎèÑÏÑú ÌòÑÌô©\"**\n\n  1. `search_nearby_libraries` (ÏúÑÏπò Í∏∞Î∞ò ÎèÑÏÑúÍ¥Ä Í≤ÄÏÉâ)\n  2. `get_new_arrival_books` (Í∞Å ÎèÑÏÑúÍ¥ÄÎ≥ÑÎ°ú Ïã†Í∞Ñ Ï°∞Ìöå)\n\n- **\"Í∞ïÎÇ®Íµ¨ ÎèÑÏÑúÍ¥ÄÏóêÏÑú Í≤ΩÏ†úÏÑúÏ†Å Ïù∏Í∏∞ ÏàúÏúÑ Î≥¥Ïó¨Ï§ò\"**\n  1. `search_detailed_region_codes` (Í∞ïÎÇ®Íµ¨ ÏΩîÎìú Ï°∞Ìöå)\n  2. `get_subject_codes` (Í≤ΩÏ†ú Î∂ÑÏïº ÏΩîÎìú Ï°∞Ìöå)\n  3. `search_popular_books_by_library` (ÌïÑÌÑ∞ Ï†ÅÏö©ÌïòÏó¨ Í≤ÄÏÉâ)\n\n## üöÄ Í∏∞Ïà†Ï†Å ÌäπÏßï\n\n- **‚úÖ ÏôÑÏ†ÑÌïú API ÎûòÌïë**: ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® API 25Í∞ú ÏóîÎìúÌè¨Ïù∏Ìä∏ Ï†ÑÏ≤¥ ÏßÄÏõê\n- **üîó Ïä§ÎßàÌä∏ Ï≤¥Ïù¥Îãù**: ÎèÑÍµ¨ Í∞Ñ ÏûêÎèô Ïó∞Í≥ÑÎ°ú Î≥µÏû°Ìïú ÏøºÎ¶¨ Ï≤òÎ¶¨\n- **‚ö° Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞**: ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®ÏôÄ Ïã§ÏãúÍ∞Ñ ÎèôÍ∏∞Ìôî\n- **üó∫Ô∏è ÏúÑÏπò Í∏∞Î∞ò ÏïåÍ≥†Î¶¨Ï¶ò**: ÏûêÏ≤¥ Íµ¨ÌòÑÌïú Haversine Í≥µÏãù Í∏∞Î∞ò Í±∞Î¶¨ Í≥ÑÏÇ∞ Î∞è Ï†ïÎ†¨\n- **üõ°Ô∏è Zod Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù**: Î™®Îì† ÏûÖÎ†•Í∞í ÌÉÄÏûÖ ÏïàÏ†ÑÏÑ± Î≥¥Ïû•\n- **üìä ÏÑ∏ÏÖò ÌÜµÍ≥Ñ**: ÎèÑÍµ¨ ÏÇ¨Ïö©Îüâ Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ\n- **üîß Ïò§Î•ò Ï≤òÎ¶¨**: ÏÉÅÏÑ∏Ìïú Î°úÍπÖ Î∞è ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥\n- **üéØ ÏãúÎÇòÎ¶¨Ïò§ Í∏∞Î∞ò ÏÑ§Î™Ö**: LLMÏù¥ ÏÉÅÌô©Ïóê ÎßûÎäî ÎèÑÍµ¨Î•º ÏâΩÍ≤å ÏÑ†ÌÉùÌï† Ïàò ÏûàÎèÑÎ°ù Íµ¨Ï≤¥Ï†ÅÏù∏ ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§ Ï†úÍ≥µ\n\n## üé¨ ÌôúÏö© ÏÇ¨Î°Ä\n\n### üèõÔ∏è ÏãúÎØº/Í∏∞Í¥Ä Ìè¨ÌÑ∏\n\n- ÎèôÎÑ§ ÎèÑÏÑúÍ¥Ä ÌòÑÌô© Î∞è Ïö¥ÏòÅ Ï†ïÎ≥¥ Ï±óÎ¥á\n- Ïã†Í∞Ñ/Ïù∏Í∏∞ÎèÑÏÑú ÏïåÎ¶º ÏÑúÎπÑÏä§\n\n### üéì ÍµêÏú°/Ïó∞Íµ¨\n\n- KDC Ï£ºÏ†úÎ≥Ñ ÎèÖÏÑú Ìä∏Î†åÎìú Î∂ÑÏÑù\n- Ïó∞Î†π/ÏßÄÏó≠Î≥Ñ ÎèÖÏÑú ÌÜµÍ≥Ñ Î¶¨Ìè¨Ìä∏\n\n### üìà Ï∂úÌåê/ÎßàÏºÄÌåÖ\n\n- Ïù∏Í∏∞ Ïû•Î•¥/ÎèÑÏÑú Î∞úÍµ¥ (Ïó∞Î†π/ÏÑ±Î≥Ñ/ÏßÄÏó≠Î≥Ñ)\n- Ìä∏Î†åÎìú Î≥ÄÌôî Î™®ÎãàÌÑ∞ÎßÅ\n\n### üì± Ïï±/ÏÑúÎπÑÏä§\n\n- ISBN Í∏∞Î∞ò Ïã§ÏãúÍ∞Ñ ÏÜåÏû•/ÎåÄÏ∂ú Í∞ÄÎä• Ïó¨Î∂Ä UX\n- ÏúÑÏπò Í∏∞Î∞ò ÎèÑÏÑúÍ¥Ä Ï∂îÏ≤ú\n\n## üöÄ ÏãúÏûëÌïòÍ∏∞\n\n### 1Ô∏è‚É£ Ï†ÑÏ†ú Ï°∞Í±¥\n\n- Node.js 18+\n- ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® API ÌÇ§\n\n### 2Ô∏è‚É£ API ÌÇ§ Î∞úÍ∏â Î∞©Î≤ï\n\n1. [ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®](https://www.data4library.kr/) ÌöåÏõêÍ∞ÄÏûÖ\n2. Î°úÍ∑∏Ïù∏ ÌõÑ Ïö∞ÏÉÅÎã® **[ÎßàÏù¥ÌéòÏù¥ÏßÄ]** ÌÅ¥Î¶≠\n3. ÎßàÏù¥ÌéòÏù¥ÏßÄ Î©îÎâ¥ÏóêÏÑú **Ïù∏Ï¶ùÌÇ§** ÏÑ†ÌÉù\n4. Ï†ÅÏ†àÌïú **Ïù¥Ïö©Î™©Ï†Å** Ï≤¥ÌÅ¨ Î∞è **Í∞úÏù∏Ï†ïÎ≥¥ ÏàòÏßë Ïù¥Ïö© ÎèôÏùò** Ï≤¥ÌÅ¨\n5. **ÏàòÏ†ïÏôÑÎ£å** Î≤ÑÌäº ÌÅ¥Î¶≠\n6. ÏÉÅÌÉúÍ∞Ä **ÏäπÏù∏ÎåÄÍ∏∞Ï§ë**ÏúºÎ°ú ÌëúÏãú - ÏäπÏù∏ÍπåÏßÄ ÏãúÍ∞Ñ ÏÜåÏöî\n7. ÏäπÏù∏ ÌõÑ Î∞úÍ∏âÎêú API ÌÇ§Î•º Î≥µÏÇ¨ÌïòÏó¨ ÌôòÍ≤ΩÎ≥ÄÏàòÏóê Ï†ÄÏû•\n\nüí° **Ï∞∏Í≥†**: ÏäπÏù∏ Ï≤òÎ¶¨Ïóê ÏãúÍ∞ÑÏù¥ Í±∏Î¶¥ Ïàò ÏûàÏäµÎãàÎã§. Î≥¥ÌÜµ Ïã†Ï≤≠ ÌõÑ ÏùµÏùº Ïò§Ï†ÑÏóê ÏäπÏù∏Îê©ÎãàÎã§.\n\n### üìä API Ìò∏Ï∂ú Ï†úÌïú\n\n- **Í∏∞Î≥∏**: ÌïòÎ£® 500Ìöå Ï†úÌïú\n- **IP Îì±Î°ù ÌõÑ**: ÌïòÎ£® 30,000Ìöå Ï†úÌïú\n\n**IP Îì±Î°ù Î∞©Î≤ï**: ÎßàÏù¥ÌéòÏù¥ÏßÄ ‚Üí Ïù∏Ï¶ùÌÇ§ Í¥ÄÎ¶¨ÏóêÏÑú **ÏÑúÎ≤ÑIP ÌïÑÎìú**Ïóê MCP ÏÑúÎ≤ÑÍ∞Ä Ïã§ÌñâÎê† Ïª¥Ìì®ÌÑ∞Ïùò IP Ï£ºÏÜåÎ•º ÏûÖÎ†•ÌïòÎ©¥ Ìò∏Ï∂ú Ï†úÌïúÏù¥ 500ÌöåÏóêÏÑú 30,000ÌöåÎ°ú ÌôïÎåÄÎê©ÎãàÎã§.\n\n‚ö†Ô∏è **Ï§ëÏöî**: 2023ÎÖÑ 11Ïõî 20ÏùºÎ∂ÄÌÑ∞ Î¨¥Ï†úÌïú Ìò∏Ï∂úÏù¥ Ï§ëÎã®ÎêòÏóàÏúºÎ©∞ ÏµúÎåÄ Ï†úÌïúÏùÄ ÌïòÎ£® 30,000ÌöåÏûÖÎãàÎã§.\n\n### 3Ô∏è‚É£ ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï\n\n- **LIBRARY_API_KEY** (ÌïÑÏàò): ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®ÏóêÏÑú Î∞úÍ∏âÎ∞õÏùÄ API ÌÇ§\n\nWindows PowerShell (ÌòÑÏû¨ ÏÑ∏ÏÖòÏö©):\n\n```powershell\n$env:LIBRARY_API_KEY=\"your-api-key\"\n```\n\nmacOS/Linux:\n\n```bash\nexport LIBRARY_API_KEY=\"your-api-key\"\n```\n\n## üì¶ ÏÑ§Ïπò Î∞©Î≤ï\n\n### Installing via Smithery\n\nTo install data4library-mcp automatically via [Smithery](https://smithery.ai/server/@isnow890/data4library-mcp):\n\n```bash\nnpx -y @smithery/cli install @isnow890/data4library-mcp\n```\n### Î∞©Î≤ï 1: NPX ÏÑ§Ïπò (Í∂åÏû•)\n\n\nÍ∞ÄÏû• Ïâ¨Ïö¥ Î∞©Î≤ïÏùÄ NPXÎ•º ÌÜµÌïú ÏÑ§ÏπòÏûÖÎãàÎã§. ÏûêÏÑ∏Ìïú Ìå®ÌÇ§ÏßÄ Ï†ïÎ≥¥Îäî [NPM Ìå®ÌÇ§ÏßÄ ÌéòÏù¥ÏßÄ](https://www.npmjs.com/package/@isnow890/data4library-mcp)Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.\n\n#### Claude Desktop ÏÑ§Ï†ï\n\nClaude Desktop ÏÑ§Ï†ï ÌååÏùº (Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`, macOS/Linux: `~/Library/Application Support/Claude/claude_desktop_config.json`)Ïóê Îã§ÏùåÏùÑ Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"data4library-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/data4library-mcp\"],\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor AI ÏÑ§Ï†ï\n\n`mcp.json`Ïóê Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"data4library-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/data4library-mcp\"],\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n### Î∞©Î≤ï 2: Î°úÏª¨ ÏÑ§Ïπò\n\nÎ°úÏª¨ Í∞úÎ∞úÏù¥ÎÇò Ïª§Ïä§ÌÖÄ ÏàòÏ†ïÏùÑ ÏúÑÌïú ÏÑ§Ïπò:\n\n#### Step 1: ÏÜåÏä§ÏΩîÎìú Îã§Ïö¥Î°úÎìú Î∞è ÎπåÎìú\n\n##### GitÏúºÎ°ú ÌÅ¥Î°†\n\n```bash\ngit clone https://github.com/isnow890/data4library-mcp.git\ncd data4library-mcp\nnpm install\nnpm run build\n```\n\n##### ZIP ÌååÏùº Îã§Ïö¥Î°úÎìú\n\n1. [GitHub Releases ÌéòÏù¥ÏßÄ](https://github.com/isnow890/data4library-mcp/releases)ÏóêÏÑú ÏµúÏã† Î≤ÑÏ†Ñ Îã§Ïö¥Î°úÎìú\n2. ZIP ÌååÏùºÏùÑ ÏõêÌïòÎäî ÏúÑÏπòÏóê ÏïïÏ∂ï Ìï¥Ï†ú\n3. ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏïïÏ∂ï Ìï¥Ï†úÎêú Ìè¥ÎçîÎ°ú Ïù¥Îèô:\n\n```bash\ncd /path/to/data4library-mcp\nnpm install\nnpm run build\n```\n\n‚ö†Ô∏è **Ï§ëÏöî**: ÏÑ§Ïπò ÌõÑ Î∞òÎìúÏãú `npm run build`Î•º Ïã§ÌñâÌïòÏó¨ `dist` Ìè¥ÎçîÏóê Ïª¥ÌååÏùºÎêú JavaScript ÌååÏùºÎì§ÏùÑ ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§.\n\n#### Step 2: Claude Desktop ÏÑ§Ï†ï\n\nÎπåÎìú ÏôÑÎ£å ÌõÑ ÌïÑÏöîÌïú Í≤ÉÎì§:\n\n- **LIBRARY_API_KEY**: ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®ÏóêÏÑú Î∞úÍ∏âÎ∞õÏùÄ API ÌÇ§\n- **ÏÑ§Ïπò Í≤ΩÎ°ú**: Îã§Ïö¥Î°úÎìúÌïú Ìè¥ÎçîÏùò Ï†àÎåÄ Í≤ΩÎ°ú\n\n##### Windows ÏÑ§Ï†ï\n\nClaude Desktop ÏÑ§Ï†ï ÌååÏùº (`%APPDATA%\\Claude\\claude_desktop_config.json`)Ïóê Îã§ÏùåÏùÑ Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"data4library-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"node\",\n        \"C:\\\\path\\\\to\\\\data4library-mcp\\\\dist\\\\src\\\\index.js\"\n      ],\n      \"cwd\": \"C:\\\\path\\\\to\\\\data4library-mcp\",\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n##### macOS/Linux ÏÑ§Ï†ï\n\nClaude Desktop ÏÑ§Ï†ï ÌååÏùº (`~/Library/Application Support/Claude/claude_desktop_config.json`)Ïóê Îã§ÏùåÏùÑ Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"data4library-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"node\",\n      \"args\": [\"/path/to/data4library-mcp/dist/src/index.js\"],\n      \"cwd\": \"/path/to/data4library-mcp\",\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n##### Í≤ΩÎ°ú ÏÑ§Ï†ï Ï£ºÏùòÏÇ¨Ìï≠\n\n‚ö†Ô∏è **Ï§ëÏöî**: ÏúÑ ÏÑ§Ï†ïÏóêÏÑú Îã§Ïùå Í≤ΩÎ°úÎì§ÏùÑ Ïã§Ï†ú ÏÑ§Ïπò Í≤ΩÎ°úÎ°ú Î∞îÍøîÏ£ºÏÑ∏Ïöî:\n\n- **Windows**: `C:\\\\path\\\\to\\\\data4library-mcp`Î•º Ïã§Ï†ú Îã§Ïö¥Î°úÎìúÌïú Ìè¥Îçî Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω\n- **macOS/Linux**: `/path/to/data4library-mcp`Î•º Ïã§Ï†ú Îã§Ïö¥Î°úÎìúÌïú Ìè¥Îçî Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω\n- **ÎπåÎìú Í≤ΩÎ°ú**: Í≤ΩÎ°úÍ∞Ä `dist/src/index.js`Î•º Í∞ÄÎ¶¨ÌÇ§ÎèÑÎ°ù ÌôïÏù∏ (Îã®ÏàúÌûà `index.js`Í∞Ä ÏïÑÎãò)\n\nÍ≤ΩÎ°ú Ï∞æÍ∏∞:\n\n```bash\n# ÌòÑÏû¨ ÏúÑÏπò ÌôïÏù∏\npwd\n\n# Ï†àÎåÄ Í≤ΩÎ°ú ÏòàÏãú\n# Windows: C:\\Users\\YourName\\Downloads\\data4library-mcp\n# macOS: /Users/YourName/Downloads/data4library-mcp\n# Linux: /home/YourName/Downloads\\data4library-mcp\n```\n\n#### Step 3: Claude Desktop Ïû¨ÏãúÏûë\n\nÏÑ§Ï†ï ÏôÑÎ£å ÌõÑ Claude DesktopÏùÑ ÏôÑÏ†ÑÌûà Ï¢ÖÎ£åÌïòÍ≥† Ïû¨ÏãúÏûëÌïòÏó¨ Data4Library MCP ÏÑúÎ≤ÑÎ•º ÌôúÏÑ±ÌôîÌïòÏÑ∏Ïöî.\n\n## üîß Î°úÏª¨ Ïã§Ìñâ (Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö©)\n\nClaude Desktop ÌÜµÌï© ÏóÜÏù¥ ÏßÅÏ†ë Ïã§ÌñâÌïòÍ∏∞:\n\n```bash\nnpm start\n# ÎòêÎäî\nnode dist/src/index.js\n```\n\nDocker (ÏÑ†ÌÉùÏÇ¨Ìï≠):\n\n```bash\ndocker build -t data4library-mcp .\ndocker run -i --rm -e LIBRARY_API_KEY=$LIBRARY_API_KEY data4library-mcp\n```\n\nMCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÌÜµÌï© (.mcp.json ÏòàÏãú, Î°úÏª¨ Ïã§Ìñâ):\n\n```json\n{\n  \"mcpServers\": {\n    \"data4library-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"node\",\n      \"args\": [\"dist/src/index.js\"],\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n## üí° ÏÇ¨Ïö© ÌåÅ\n\n- **ÌçºÏßÄ Í≤ÄÏÉâ**: `search_library_codes`Î•º ÏÇ¨Ïö©ÌïòÏó¨ Î∂ÄÎ∂Ñ Ïù¥Î¶Ñ/Ï£ºÏÜåÎ°ú ÎèÑÏÑúÍ¥Ä Ï∞æÍ∏∞\n- **ÏΩîÎìú Ìó¨Ìçº**: `get_subject_codes`, `search_detailed_kdc_codes`, `search_detailed_region_codes`Î°ú ÌïÑÏöîÌïú ÌååÎùºÎØ∏ÌÑ∞ ÏΩîÎìú Ï°∞Ìöå\n- **ÏÑ∏ÏÖò Î™®ÎãàÌÑ∞ÎßÅ**: `session_stats`Î°ú ÏÑ∏ÏÖòÎ≥Ñ ÎèÑÍµ¨ ÏÇ¨Ïö©Îüâ/Ï†úÌïú ÌôïÏù∏\n- **ÎèÑÍµ¨ Ïó∞Í≥Ñ**: Î≥µÏû°Ìïú ÏøºÎ¶¨Îäî Ïó¨Îü¨ ÎèÑÍµ¨Î•º ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©ÌïòÏó¨ Ìï¥Í≤∞\n\n## üìù ÎùºÏù¥ÏÑ†Ïä§ Î∞è Í≥†ÏßÄÏÇ¨Ìï≠\n\n- **ÎùºÏù¥ÏÑ†Ïä§**: MIT (LICENSE ÌååÏùº Ï∞∏Ï°∞)\n- **Îç∞Ïù¥ÌÑ∞ Ï∂úÏ≤ò**: ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® Í≥µÍ≥µ API\n- **ÏÇ¨Ïö©Î≤ï**: Í≥µÍ≥µ API Ï†ïÏ±Ö/Ìï†ÎãπÎüâÏùÑ Ï§ÄÏàòÌïòÏÑ∏Ïöî. Í∞úÏù∏Ï†ïÎ≥¥Î•º Ï†ÄÏû•/ÎÖ∏Ï∂úÌïòÏßÄ ÎßàÏÑ∏Ïöî.\n\n---\n\nüí¨ **ÏßàÎ¨∏Ïù¥ÎÇò ÌîºÎìúÎ∞±Ïù¥ ÏûàÏúºÏãúÎ©¥** GitHub Ïù¥ÏäàÎ•º Ïó¥Ïñ¥Ï£ºÏÑ∏Ïöî!\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search public libraries nationwide by region or name",
        "Search books by title, author, publisher, or subject",
        "Find libraries holding a specific book",
        "Retrieve detailed book information by ISBN",
        "Check real-time book loan availability",
        "Retrieve popular and trending books nationally or by region/library",
        "Get new arrival books per library",
        "Analyze loan and return trends with usage statistics",
        "Provide reading quantity and rate statistics by region and age",
        "Recommend personalized books for experts and avid readers",
        "Search nearby libraries based on GPS location with distance sorting",
        "Lookup library, region, and subject classification codes",
        "Monitor real-time session and tool usage statistics",
        "Support smart chaining of tools for complex queries",
        "Validate input data types with Zod schema for safety"
      ],
      "limitations": [
        "Daily API call limit of 500 by default, extendable to 30,000 after IP registration",
        "No unlimited API calls allowed since November 20, 2023",
        "Requires approval process for API key issuance which may take up to a day",
        "Does not store or expose personal data, must comply with public API policies"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "API key issued by Data4Library after registration and approval",
        "Environment variable LIBRARY_API_KEY set with the issued API key",
        "For higher API limits, server IP must be registered with Data4Library",
        "Claude Desktop or compatible MCP client for integration"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, a full list of supported tools, technical features, limitations, and environment requirements, making it excellent for users.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![smithery badge](https://smithery.ai/badge/@isnow890/data4library-mcp)](https://smithery.ai/server/@isnow890/data4library-mcp)\n[![MSeeP.ai Security Assessment Badge](https://mseep.net/pr/isnow890-data4library-mcp-badge.png)](https://mseep.ai/app/isnow890-data4library-mcp)\n[![smithery badge](https://smithery.ai/badge/@isnow890/data4library-mcp)](https://smithery.ai/server/@isnow890/data4library-mcp)\n\n<div align=\"center\">\n  <img src=\"https://firebasestorage.googleapis.com/v0/b/rottenbridge-e6efa.appspot.com/o/logo.jpg?alt=media&token=68d16fd2-799f-4aba-8c1e-da6977e2949e\" alt=\"Data4Library MCP Server Logo\" width=\"300\"/>\n</div>\n\n# üìö Ï†ïÎ≥¥ÎÇòÎ£® ÎèÑÏÑúÍ≤ÄÏÉâ MCP (data4library-mcp)\n\n**ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® MCP**Îäî **Íµ≠Î¶ΩÏ§ëÏïôÎèÑÏÑúÍ¥Ä**ÏóêÏÑú Ï†úÍ≥µÌïòÎäî **ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® API**Î•º ÏôÑÏ†ÑÌûà ÌôúÏö©Ìï† Ïàò ÏûàÎèÑÎ°ù Í∞úÎ∞úÎêú Ìè¨Í¥ÑÏ†ÅÏù∏ **MCP(Model Context Protocol) ÏÑúÎ≤Ñ**ÏûÖÎãàÎã§. ÌïúÍµ≠Ïùò Î™®Îì† Í≥µÍ≥µÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥, ÎèÑÏÑú Í≤ÄÏÉâ, ÎåÄÏ∂ú ÌòÑÌô©, ÎèÖÏÑú ÌÜµÍ≥Ñ Îì±ÏùÑ AI Î™®Îç∏ÏóêÏÑú ÏâΩÍ≤å Ï†ëÍ∑ºÌïòÍ≥† ÌôúÏö©Ìï† Ïàò ÏûàÍ≤å Ìï¥Ï§çÎãàÎã§.\n\n> üá∫üá∏ **English Documentation**: [README-en.md](README-en.md)\n\n## üöÄ Îπ†Î•∏ ÏÑ§Ïπò (Smithery Í∂åÏû•)\n\n**Í∞ÄÏû• Ïâ¨Ïö¥ ÏÑ§Ïπò Î∞©Î≤ï**ÏùÄ [Smithery](https://smithery.ai)Î•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏûÖÎãàÎã§:\n\n**üîó [SmitheryÏóêÏÑú ÏÑ§ÏπòÌïòÍ∏∞](https://smithery.ai/server/@isnow890/data4library-mcp)**\n\n1. ÏúÑ ÎßÅÌÅ¨Î•º ÌÅ¥Î¶≠ÌïòÏó¨ Smithery ÌéòÏù¥ÏßÄÎ°ú Ïù¥Îèô\n2. **\"Install\"** Î≤ÑÌäº ÌÅ¥Î¶≠\n3. API ÌÇ§ ÏûÖÎ†• (ÏïÑÎûò API ÌÇ§ Î∞úÍ∏â Î∞©Î≤ï Ï∞∏Ï°∞)\n4. Claude DesktopÏóêÏÑú Î∞îÎ°ú ÏÇ¨Ïö© Í∞ÄÎä•!\n\n## üåü ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®(Data4Library)ÎûÄ?\n\n[ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®](https://www.data4library.kr/)Îäî **Íµ≠Î¶ΩÏ§ëÏïôÎèÑÏÑúÍ¥Ä**Ïù¥ Ïö¥ÏòÅÌïòÎäî **Ï†ÑÍµ≠ Í≥µÍ≥µÎèÑÏÑúÍ¥Ä ÌÜµÌï© Ï†ïÎ≥¥ ÏÑúÎπÑÏä§**ÏûÖÎãàÎã§.",
        "start_pos": 0,
        "end_pos": 1326,
        "token_count_estimate": 331,
        "source_type": "readme",
        "agent_id": "53726c54421f08b5"
      },
      {
        "chunk_id": 1,
        "text": "ilability`): Ïã§ÏãúÍ∞Ñ ÎåÄÏ∂ú Í∞ÄÎä• ÏÉÅÌÉú ÌôïÏù∏\n\n### üìä Ïù∏Í∏∞ÎèÑÏÑú & Ìä∏Î†åÎìú Î∂ÑÏÑù\n\n- **üî• Ïù∏Í∏∞ ÎåÄÏ∂úÎèÑÏÑú** (`search_popular_books`): Ï†ÑÍµ≠/ÏßÄÏó≠Î≥Ñ Î≤†Ïä§Ìä∏ÏÖÄÎü¨\n- **üèÜ ÎèÑÏÑúÍ¥ÄÎ≥Ñ Ïù∏Í∏∞ÎèÑÏÑú** (`search_popular_books_by_library`): ÌäπÏ†ï ÎèÑÏÑúÍ¥ÄÏùò Ïù∏Í∏∞ÎèÑÏÑú\n- **üìà ÎåÄÏ∂ú Í∏âÏÉÅÏäπ ÎèÑÏÑú** (`get_hot_trend`): Ìä∏Î†åÎî© ÎèÑÏÑú Ïã§ÏãúÍ∞Ñ Ï°∞Ìöå\n- **üÜï Ïã†Ï∞©ÎèÑÏÑú** (`get_new_arrival_books`): ÎèÑÏÑúÍ¥ÄÎ≥Ñ ÏÉàÎ°ú Îì§Ïñ¥Ïò® ÎèÑÏÑú\n- **üè∑Ô∏è Ïù¥Îã¨Ïùò ÌÇ§ÏõåÎìú** (`get_monthly_keywords`): ÎèÖÏÑú Ìä∏Î†åÎìú ÌÇ§ÏõåÎìú\n\n### üìà ÌÜµÍ≥Ñ & Î∂ÑÏÑù ÎèÑÍµ¨\n\n- **üìä ÎåÄÏ∂úÎ∞òÎÇ© Ï∂îÏù¥** (`get_usage_trend`): ÎèÑÏÑúÍ¥ÄÎ≥Ñ Ïù¥Ïö© ÌÜµÍ≥Ñ Í∑∏ÎûòÌîÑ\n- **üåç ÏßÄÏó≠Î≥Ñ ÎèÖÏÑúÎüâ** (`get_reading_quantity`): ÎèÖÏÑúÏú® Î∞è ÎèÖÏÑúÎüâ ÎπÑÍµê\n- **üìö Ïû•ÏÑú/ÎåÄÏ∂ú Îç∞Ïù¥ÌÑ∞** (`search_items`): ÎèÑÏÑúÍ¥ÄÎ≥Ñ ÏÉÅÏÑ∏ ÌÜµÍ≥Ñ\n- **üìñ ÎèÑÏÑú Ïù¥Ïö© Î∂ÑÏÑù** (`get_book_usage_analysis`): ÌäπÏ†ï ÎèÑÏÑúÏùò Ïù¥Ïö© Ìå®ÌÑ¥\n\n### üéØ Í∞úÏù∏Ìôî Ï∂îÏ≤ú ÏãúÏä§ÌÖú\n\n- **üéì ÎßàÎãàÏïÑ Ï∂îÏ≤úÎèÑÏÑú** (`get_mania_recommendations`): Ï†ÑÎ¨∏Í∞ÄÏö© Ïã¨Ìôî ÎèÑÏÑú\n- **üìö Îã§ÎèÖÏûê Ï∂îÏ≤úÎèÑÏÑú** (`get_reader_recommendations`): Ïó∞ÏÜç ÎèÖÏÑúÏö© ÎèÑÏÑú\n- **üè∑Ô∏è ÎèÑÏÑú ÌÇ§ÏõåÎìú Î∂ÑÏÑù** (`get_book_keywords`): ÎèÑÏÑúÎ≥Ñ ÌïµÏã¨ ÌÇ§ÏõåÎìú\n\n### üó∫Ô∏è ÏúÑÏπò Í∏∞Î∞ò ÏÑúÎπÑÏä§ (ÎèÖÏûêÏ†Å Íµ¨ÌòÑ)\n\n- **üìç ÎÇ¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥Ä Í≤ÄÏÉâ** (`search_nearby_libraries`): GPS Í∏∞Î∞ò Í∞ÄÍπåÏö¥ ÎèÑÏÑúÍ¥Ä ÏûêÎèô Í≤ÄÏÉâ\n  - **Í±∞Î¶¨Ïàú Ï†ïÎ†¨**: Ïã§ÏãúÍ∞Ñ Í±∞Î¶¨ Í≥ÑÏÇ∞ Î∞è Í∞ÄÍπåÏö¥ ÏàúÏÑúÎ°ú Ï†ïÎ†¨\n  - **ÏÉÅÏÑ∏ Í±∞Î¶¨ Ï†ïÎ≥¥**: Í∞Å ÎèÑÏÑúÍ¥ÄÍπåÏßÄÏùò Ï†ïÌôïÌïú Í±∞Î¶¨(km) ÌëúÏãú\n\n### üîß ÏΩîÎìú Í≤ÄÏÉâ ÎèÑÍµ¨ (API Ïó∞Îèô ÏßÄÏõê)\n\n- **üèõÔ∏è ÎèÑÏÑúÍ¥Ä ÏΩîÎìú Í≤ÄÏÉâ** (`search_library_codes`): ÎèÑÏÑúÍ¥ÄÎ™ÖÏúºÎ°ú libCode Ï∞æÍ∏∞\n- **üåç ÏßÄÏó≠ÏΩîÎìú Ï°∞Ìöå** (`get_region_codes`, `get_detailed_region_codes`): Ï†ÑÍµ≠/ÏÑ∏Î∂Ä ÏßÄÏó≠ÏΩîÎìú\n- **üìö Ï£ºÏ†úÎ∂ÑÎ•òÏΩîÎìú** (`get_subject_codes`, `get_detailed_subject_codes`): KDC ÎåÄ/ÏÑ∏Î∂ÑÎ•ò\n- **üìä ÌÜµÌï©Ï†ïÎ≥¥ Ï°∞Ìöå** (`get_library_info`, `get_popular_books_by_library`): ÎèÑÏÑúÍ¥ÄÎ≥Ñ Ï¢ÖÌï© Ï†ïÎ≥¥\n\n### üõ†Ô∏è ÏÑ∏ÏÖò Í¥ÄÎ¶¨\n\n- **üìä ÏÇ¨Ïö©Îüâ ÌÜµÍ≥Ñ** (`session_stats`): Ïã§ÏãúÍ∞Ñ ÎèÑÍµ¨ Ìò∏Ï∂ú ÌÜµÍ≥Ñ Î∞è ÏÑ∏ÏÖò Ï†ïÎ≥¥\n\n## üí° Ïã§Ï†ú ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§\n\n### üîç ÎèÑÏÑúÍ¥Ä Ï∞æÍ∏∞\n\n- **\"ÎÇ¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥Ä Ïñ¥Îîî ÏûàÏñ¥?\"** ‚Üí `search_nearby_libraries` ÏÇ¨Ïö©\n- **\"ÏÑúÏö∏ Í∞ïÎÇ®Íµ¨ ÎèÑÏÑúÍ¥Ä Ï∞æÏïÑÏ§ò\"** ‚Üí `search_detailed_region_codes` + `search_libraries` ÏàúÏ∞® ÏÇ¨Ïö©\n\n### üìñ ÎèÑÏÑú Í≤ÄÏÉâ\n\n- **\"Ìï¥Î¶¨Ìè¨ÌÑ∞ Ï±Ö Ïñ¥ÎîîÏÑú ÎπåÎ¶¥ Ïàò ÏûàÏñ¥?\"** ‚Üí `search_books` + `search_libraries_by_book` Ïó∞Í≥Ñ\n- **\"ÍπÄÏòÅÌïò ÏûëÍ∞Ä ÏÜåÏÑ§ Ï∞æÏïÑÏ§ò\"** ‚Üí `search_books` (Ï†ÄÏûêÎ™Ö Í≤ÄÏÉâ)\n\n### üìä Ìä∏Î†åÎìú Î∂ÑÏÑù\n\n- **\"ÏöîÏ¶ò Ïù∏Í∏∞ ÏûàÎäî Ï±Ö Î≠êÏïº?\"** ‚Üí `search_popular_books` ÎòêÎäî `get_hot_trend`\n- **\"Í∞ïÎÇ®ÎèÑÏÑúÍ¥ÄÏóêÏÑú Ïù∏Í∏∞ ÏûàÎäî Ï±Ö\"** ‚Üí `search_library_codes` + `search_popular_books_by_library`\n\n### üìà ÌÜµÍ≥Ñ Î∂ÑÏÑù\n\n- **\"ÏÑúÏö∏ ÏÇ¨ÎûåÎì§Ïù¥ Ï±ÖÏùÑ ÏñºÎßàÎÇò ÎßéÏù¥ ÏùΩÏñ¥?\"** ‚Üí `get_reading_quantity`\n- **\"ÎèÑÏÑúÍ¥Ä Ïù¥Ïö© Í∑∏ÎûòÌîÑ Î≥¥Ïó¨Ï§ò\"** ‚Üí `get_usage_trend`\n\n### üéØ Î≥µÌï© ÏøºÎ¶¨ ÏòàÏãú\n\n- **\"ÎÇ¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥ÄÎì§Ïùò Ïã†Í∞Ñ ÎèÑÏÑú ÌòÑÌô©\"**\n\n  1. `search_nearby_libraries` (ÏúÑÏπò Í∏∞Î∞ò ÎèÑÏÑúÍ¥Ä Í≤ÄÏÉâ)\n  2.",
        "start_pos": 1848,
        "end_pos": 3865,
        "token_count_estimate": 504,
        "source_type": "readme",
        "agent_id": "53726c54421f08b5"
      },
      {
        "chunk_id": 2,
        "text": "Í≥Ñ Î∂ÑÏÑù\n\n- **\"ÏÑúÏö∏ ÏÇ¨ÎûåÎì§Ïù¥ Ï±ÖÏùÑ ÏñºÎßàÎÇò ÎßéÏù¥ ÏùΩÏñ¥?\"** ‚Üí `get_reading_quantity`\n- **\"ÎèÑÏÑúÍ¥Ä Ïù¥Ïö© Í∑∏ÎûòÌîÑ Î≥¥Ïó¨Ï§ò\"** ‚Üí `get_usage_trend`\n\n### üéØ Î≥µÌï© ÏøºÎ¶¨ ÏòàÏãú\n\n- **\"ÎÇ¥ Ï£ºÎ≥Ä ÎèÑÏÑúÍ¥ÄÎì§Ïùò Ïã†Í∞Ñ ÎèÑÏÑú ÌòÑÌô©\"**\n\n  1. `search_nearby_libraries` (ÏúÑÏπò Í∏∞Î∞ò ÎèÑÏÑúÍ¥Ä Í≤ÄÏÉâ)\n  2. `get_new_arrival_books` (Í∞Å ÎèÑÏÑúÍ¥ÄÎ≥ÑÎ°ú Ïã†Í∞Ñ Ï°∞Ìöå)\n\n- **\"Í∞ïÎÇ®Íµ¨ ÎèÑÏÑúÍ¥ÄÏóêÏÑú Í≤ΩÏ†úÏÑúÏ†Å Ïù∏Í∏∞ ÏàúÏúÑ Î≥¥Ïó¨Ï§ò\"**\n  1. `search_detailed_region_codes` (Í∞ïÎÇ®Íµ¨ ÏΩîÎìú Ï°∞Ìöå)\n  2. `get_subject_codes` (Í≤ΩÏ†ú Î∂ÑÏïº ÏΩîÎìú Ï°∞Ìöå)\n  3. `search_popular_books_by_library` (ÌïÑÌÑ∞ Ï†ÅÏö©ÌïòÏó¨ Í≤ÄÏÉâ)\n\n## üöÄ Í∏∞Ïà†Ï†Å ÌäπÏßï\n\n- **‚úÖ ÏôÑÏ†ÑÌïú API ÎûòÌïë**: ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® API 25Í∞ú ÏóîÎìúÌè¨Ïù∏Ìä∏ Ï†ÑÏ≤¥ ÏßÄÏõê\n- **üîó Ïä§ÎßàÌä∏ Ï≤¥Ïù¥Îãù**: ÎèÑÍµ¨ Í∞Ñ ÏûêÎèô Ïó∞Í≥ÑÎ°ú Î≥µÏû°Ìïú ÏøºÎ¶¨ Ï≤òÎ¶¨\n- **‚ö° Ïã§ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞**: ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®ÏôÄ Ïã§ÏãúÍ∞Ñ ÎèôÍ∏∞Ìôî\n- **üó∫Ô∏è ÏúÑÏπò Í∏∞Î∞ò ÏïåÍ≥†Î¶¨Ï¶ò**: ÏûêÏ≤¥ Íµ¨ÌòÑÌïú Haversine Í≥µÏãù Í∏∞Î∞ò Í±∞Î¶¨ Í≥ÑÏÇ∞ Î∞è Ï†ïÎ†¨\n- **üõ°Ô∏è Zod Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù**: Î™®Îì† ÏûÖÎ†•Í∞í ÌÉÄÏûÖ ÏïàÏ†ÑÏÑ± Î≥¥Ïû•\n- **üìä ÏÑ∏ÏÖò ÌÜµÍ≥Ñ**: ÎèÑÍµ¨ ÏÇ¨Ïö©Îüâ Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ\n- **üîß Ïò§Î•ò Ï≤òÎ¶¨**: ÏÉÅÏÑ∏Ìïú Î°úÍπÖ Î∞è ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥\n- **üéØ ÏãúÎÇòÎ¶¨Ïò§ Í∏∞Î∞ò ÏÑ§Î™Ö**: LLMÏù¥ ÏÉÅÌô©Ïóê ÎßûÎäî ÎèÑÍµ¨Î•º ÏâΩÍ≤å ÏÑ†ÌÉùÌï† Ïàò ÏûàÎèÑÎ°ù Íµ¨Ï≤¥Ï†ÅÏù∏ ÏÇ¨Ïö© ÏãúÎÇòÎ¶¨Ïò§ Ï†úÍ≥µ\n\n## üé¨ ÌôúÏö© ÏÇ¨Î°Ä\n\n### üèõÔ∏è ÏãúÎØº/Í∏∞Í¥Ä Ìè¨ÌÑ∏\n\n- ÎèôÎÑ§ ÎèÑÏÑúÍ¥Ä ÌòÑÌô© Î∞è Ïö¥ÏòÅ Ï†ïÎ≥¥ Ï±óÎ¥á\n- Ïã†Í∞Ñ/Ïù∏Í∏∞ÎèÑÏÑú ÏïåÎ¶º ÏÑúÎπÑÏä§\n\n### üéì ÍµêÏú°/Ïó∞Íµ¨\n\n- KDC Ï£ºÏ†úÎ≥Ñ ÎèÖÏÑú Ìä∏Î†åÎìú Î∂ÑÏÑù\n- Ïó∞Î†π/ÏßÄÏó≠Î≥Ñ ÎèÖÏÑú ÌÜµÍ≥Ñ Î¶¨Ìè¨Ìä∏\n\n### üìà Ï∂úÌåê/ÎßàÏºÄÌåÖ\n\n- Ïù∏Í∏∞ Ïû•Î•¥/ÎèÑÏÑú Î∞úÍµ¥ (Ïó∞Î†π/ÏÑ±Î≥Ñ/ÏßÄÏó≠Î≥Ñ)\n- Ìä∏Î†åÎìú Î≥ÄÌôî Î™®ÎãàÌÑ∞ÎßÅ\n\n### üì± Ïï±/ÏÑúÎπÑÏä§\n\n- ISBN Í∏∞Î∞ò Ïã§ÏãúÍ∞Ñ ÏÜåÏû•/ÎåÄÏ∂ú Í∞ÄÎä• Ïó¨Î∂Ä UX\n- ÏúÑÏπò Í∏∞Î∞ò ÎèÑÏÑúÍ¥Ä Ï∂îÏ≤ú\n\n## üöÄ ÏãúÏûëÌïòÍ∏∞\n\n### 1Ô∏è‚É£ Ï†ÑÏ†ú Ï°∞Í±¥\n\n- Node.js 18+\n- ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® API ÌÇ§\n\n### 2Ô∏è‚É£ API ÌÇ§ Î∞úÍ∏â Î∞©Î≤ï\n\n1. [ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£®](https://www.data4library.kr/) ÌöåÏõêÍ∞ÄÏûÖ\n2. Î°úÍ∑∏Ïù∏ ÌõÑ Ïö∞ÏÉÅÎã® **[ÎßàÏù¥ÌéòÏù¥ÏßÄ]** ÌÅ¥Î¶≠\n3. ÎßàÏù¥ÌéòÏù¥ÏßÄ Î©îÎâ¥ÏóêÏÑú **Ïù∏Ï¶ùÌÇ§** ÏÑ†ÌÉù\n4. Ï†ÅÏ†àÌïú **Ïù¥Ïö©Î™©Ï†Å** Ï≤¥ÌÅ¨ Î∞è **Í∞úÏù∏Ï†ïÎ≥¥ ÏàòÏßë Ïù¥Ïö© ÎèôÏùò** Ï≤¥ÌÅ¨\n5. **ÏàòÏ†ïÏôÑÎ£å** Î≤ÑÌäº ÌÅ¥Î¶≠\n6. ÏÉÅÌÉúÍ∞Ä **ÏäπÏù∏ÎåÄÍ∏∞Ï§ë**ÏúºÎ°ú ÌëúÏãú - ÏäπÏù∏ÍπåÏßÄ ÏãúÍ∞Ñ ÏÜåÏöî\n7. ÏäπÏù∏ ÌõÑ Î∞úÍ∏âÎêú API ÌÇ§Î•º Î≥µÏÇ¨ÌïòÏó¨ ÌôòÍ≤ΩÎ≥ÄÏàòÏóê Ï†ÄÏû•\n\nüí° **Ï∞∏Í≥†**: ÏäπÏù∏ Ï≤òÎ¶¨Ïóê ÏãúÍ∞ÑÏù¥ Í±∏Î¶¥ Ïàò ÏûàÏäµÎãàÎã§. Î≥¥ÌÜµ Ïã†Ï≤≠ ÌõÑ ÏùµÏùº Ïò§Ï†ÑÏóê ÏäπÏù∏Îê©ÎãàÎã§.\n\n### üìä API Ìò∏Ï∂ú Ï†úÌïú\n\n- **Í∏∞Î≥∏**: ÌïòÎ£® 500Ìöå Ï†úÌïú\n- **IP Îì±Î°ù ÌõÑ**: ÌïòÎ£® 30,000Ìöå Ï†úÌïú\n\n**IP Îì±Î°ù Î∞©Î≤ï**: ÎßàÏù¥ÌéòÏù¥ÏßÄ ‚Üí Ïù∏Ï¶ùÌÇ§ Í¥ÄÎ¶¨ÏóêÏÑú **ÏÑúÎ≤ÑIP ÌïÑÎìú**Ïóê MCP ÏÑúÎ≤ÑÍ∞Ä Ïã§ÌñâÎê† Ïª¥Ìì®ÌÑ∞Ïùò IP Ï£ºÏÜåÎ•º ÏûÖÎ†•ÌïòÎ©¥ Ìò∏Ï∂ú Ï†úÌïúÏù¥ 500ÌöåÏóêÏÑú 30,000ÌöåÎ°ú ÌôïÎåÄÎê©ÎãàÎã§.\n\n‚ö†Ô∏è **Ï§ëÏöî**: 2023ÎÖÑ 11Ïõî 20ÏùºÎ∂ÄÌÑ∞ Î¨¥Ï†úÌïú Ìò∏Ï∂úÏù¥ Ï§ëÎã®ÎêòÏóàÏúºÎ©∞ ÏµúÎåÄ Ï†úÌïúÏùÄ ÌïòÎ£® 30,000ÌöåÏûÖÎãàÎã§.",
        "start_pos": 3665,
        "end_pos": 5273,
        "token_count_estimate": 402,
        "source_type": "readme",
        "agent_id": "53726c54421f08b5"
      },
      {
        "chunk_id": 3,
        "text": "talling via Smithery\n\nTo install data4library-mcp automatically via [Smithery](https://smithery.ai/server/@isnow890/data4library-mcp):\n\n```bash\nnpx -y @smithery/cli install @isnow890/data4library-mcp\n```\n### Î∞©Î≤ï 1: NPX ÏÑ§Ïπò (Í∂åÏû•)\n\n\nÍ∞ÄÏû• Ïâ¨Ïö¥ Î∞©Î≤ïÏùÄ NPXÎ•º ÌÜµÌïú ÏÑ§ÏπòÏûÖÎãàÎã§. ÏûêÏÑ∏Ìïú Ìå®ÌÇ§ÏßÄ Ï†ïÎ≥¥Îäî [NPM Ìå®ÌÇ§ÏßÄ ÌéòÏù¥ÏßÄ](https://www.npmjs.com/package/@isnow890/data4library-mcp)Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.\n\n#### Claude Desktop ÏÑ§Ï†ï\n\nClaude Desktop ÏÑ§Ï†ï ÌååÏùº (Windows: `%APPDATA%\\Claude\\claude_desktop_config.json`, macOS/Linux: `~/Library/Application Support/Claude/claude_desktop_config.json`)Ïóê Îã§ÏùåÏùÑ Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"data4library-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/data4library-mcp\"],\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor AI ÏÑ§Ï†ï\n\n`mcp.json`Ïóê Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"data4library-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@isnow890/data4library-mcp\"],\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n### Î∞©Î≤ï 2: Î°úÏª¨ ÏÑ§Ïπò\n\nÎ°úÏª¨ Í∞úÎ∞úÏù¥ÎÇò Ïª§Ïä§ÌÖÄ ÏàòÏ†ïÏùÑ ÏúÑÌïú ÏÑ§Ïπò:\n\n#### Step 1: ÏÜåÏä§ÏΩîÎìú Îã§Ïö¥Î°úÎìú Î∞è ÎπåÎìú\n\n##### GitÏúºÎ°ú ÌÅ¥Î°†\n\n```bash\ngit clone https://github.com/isnow890/data4library-mcp.git\ncd data4library-mcp\nnpm install\nnpm run build\n```\n\n##### ZIP ÌååÏùº Îã§Ïö¥Î°úÎìú\n\n1. [GitHub Releases ÌéòÏù¥ÏßÄ](https://github.com/isnow890/data4library-mcp/releases)ÏóêÏÑú ÏµúÏã† Î≤ÑÏ†Ñ Îã§Ïö¥Î°úÎìú\n2. ZIP ÌååÏùºÏùÑ ÏõêÌïòÎäî ÏúÑÏπòÏóê ÏïïÏ∂ï Ìï¥Ï†ú\n3. ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏïïÏ∂ï Ìï¥Ï†úÎêú Ìè¥ÎçîÎ°ú Ïù¥Îèô:\n\n```bash\ncd /path/to/data4library-mcp\nnpm install\nnpm run build\n```\n\n‚ö†Ô∏è **Ï§ëÏöî**: ÏÑ§Ïπò ÌõÑ Î∞òÎìúÏãú `npm run build`Î•º Ïã§ÌñâÌïòÏó¨ `dist` Ìè¥ÎçîÏóê Ïª¥ÌååÏùºÎêú JavaScript ÌååÏùºÎì§ÏùÑ ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§.",
        "start_pos": 5513,
        "end_pos": 7028,
        "token_count_estimate": 378,
        "source_type": "readme",
        "agent_id": "53726c54421f08b5"
      },
      {
        "chunk_id": 4,
        "text": "gs\": [\n        \"/c\",\n        \"node\",\n        \"C:\\\\path\\\\to\\\\data4library-mcp\\\\dist\\\\src\\\\index.js\"\n      ],\n      \"cwd\": \"C:\\\\path\\\\to\\\\data4library-mcp\",\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n##### macOS/Linux ÏÑ§Ï†ï\n\nClaude Desktop ÏÑ§Ï†ï ÌååÏùº (`~/Library/Application Support/Claude/claude_desktop_config.json`)Ïóê Îã§ÏùåÏùÑ Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"data4library-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"node\",\n      \"args\": [\"/path/to/data4library-mcp/dist/src/index.js\"],\n      \"cwd\": \"/path/to/data4library-mcp\",\n      \"env\": {\n        \"LIBRARY_API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n```\n\n##### Í≤ΩÎ°ú ÏÑ§Ï†ï Ï£ºÏùòÏÇ¨Ìï≠\n\n‚ö†Ô∏è **Ï§ëÏöî**: ÏúÑ ÏÑ§Ï†ïÏóêÏÑú Îã§Ïùå Í≤ΩÎ°úÎì§ÏùÑ Ïã§Ï†ú ÏÑ§Ïπò Í≤ΩÎ°úÎ°ú Î∞îÍøîÏ£ºÏÑ∏Ïöî:\n\n- **Windows**: `C:\\\\path\\\\to\\\\data4library-mcp`Î•º Ïã§Ï†ú Îã§Ïö¥Î°úÎìúÌïú Ìè¥Îçî Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω\n- **macOS/Linux**: `/path/to/data4library-mcp`Î•º Ïã§Ï†ú Îã§Ïö¥Î°úÎìúÌïú Ìè¥Îçî Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω\n- **ÎπåÎìú Í≤ΩÎ°ú**: Í≤ΩÎ°úÍ∞Ä `dist/src/index.js`Î•º Í∞ÄÎ¶¨ÌÇ§ÎèÑÎ°ù ÌôïÏù∏ (Îã®ÏàúÌûà `index.js`Í∞Ä ÏïÑÎãò)\n\nÍ≤ΩÎ°ú Ï∞æÍ∏∞:\n\n```bash\n# ÌòÑÏû¨ ÏúÑÏπò ÌôïÏù∏\npwd\n\n# Ï†àÎåÄ Í≤ΩÎ°ú ÏòàÏãú\n# Windows: C:\\Users\\YourName\\Downloads\\data4library-mcp\n# macOS: /Users/YourName/Downloads/data4library-mcp\n# Linux: /home/YourName/Downloads\\data4library-mcp\n```\n\n#### Step 3: Claude Desktop Ïû¨ÏãúÏûë\n\nÏÑ§Ï†ï ÏôÑÎ£å ÌõÑ Claude DesktopÏùÑ ÏôÑÏ†ÑÌûà Ï¢ÖÎ£åÌïòÍ≥† Ïû¨ÏãúÏûëÌïòÏó¨ Data4Library MCP ÏÑúÎ≤ÑÎ•º ÌôúÏÑ±ÌôîÌïòÏÑ∏Ïöî.\n\n## üîß Î°úÏª¨ Ïã§Ìñâ (Í∞úÎ∞ú/ÌÖåÏä§Ìä∏Ïö©)\n\nClaude Desktop ÌÜµÌï© ÏóÜÏù¥ ÏßÅÏ†ë Ïã§ÌñâÌïòÍ∏∞:\n\n```bash\nnpm start\n# ÎòêÎäî\nnode dist/src/index.js\n```\n\nDocker (ÏÑ†ÌÉùÏÇ¨Ìï≠):\n\n```bash\ndocker build -t data4library-mcp .",
        "start_pos": 7361,
        "end_pos": 8745,
        "token_count_estimate": 346,
        "source_type": "readme",
        "agent_id": "53726c54421f08b5"
      },
      {
        "chunk_id": 5,
        "text": "es`, `search_detailed_region_codes`Î°ú ÌïÑÏöîÌïú ÌååÎùºÎØ∏ÌÑ∞ ÏΩîÎìú Ï°∞Ìöå\n- **ÏÑ∏ÏÖò Î™®ÎãàÌÑ∞ÎßÅ**: `session_stats`Î°ú ÏÑ∏ÏÖòÎ≥Ñ ÎèÑÍµ¨ ÏÇ¨Ïö©Îüâ/Ï†úÌïú ÌôïÏù∏\n- **ÎèÑÍµ¨ Ïó∞Í≥Ñ**: Î≥µÏû°Ìïú ÏøºÎ¶¨Îäî Ïó¨Îü¨ ÎèÑÍµ¨Î•º ÏàúÏ∞®Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©ÌïòÏó¨ Ìï¥Í≤∞\n\n## üìù ÎùºÏù¥ÏÑ†Ïä§ Î∞è Í≥†ÏßÄÏÇ¨Ìï≠\n\n- **ÎùºÏù¥ÏÑ†Ïä§**: MIT (LICENSE ÌååÏùº Ï∞∏Ï°∞)\n- **Îç∞Ïù¥ÌÑ∞ Ï∂úÏ≤ò**: ÎèÑÏÑúÍ¥Ä Ï†ïÎ≥¥ÎÇòÎ£® Í≥µÍ≥µ API\n- **ÏÇ¨Ïö©Î≤ï**: Í≥µÍ≥µ API Ï†ïÏ±Ö/Ìï†ÎãπÎüâÏùÑ Ï§ÄÏàòÌïòÏÑ∏Ïöî. Í∞úÏù∏Ï†ïÎ≥¥Î•º Ï†ÄÏû•/ÎÖ∏Ï∂úÌïòÏßÄ ÎßàÏÑ∏Ïöî.\n\n---\n\nüí¨ **ÏßàÎ¨∏Ïù¥ÎÇò ÌîºÎìúÎ∞±Ïù¥ ÏûàÏúºÏãúÎ©¥** GitHub Ïù¥ÏäàÎ•º Ïó¥Ïñ¥Ï£ºÏÑ∏Ïöî!",
        "start_pos": 9209,
        "end_pos": 9530,
        "token_count_estimate": 80,
        "source_type": "readme",
        "agent_id": "53726c54421f08b5"
      }
    ]
  },
  {
    "agent_id": "8daab80a59ad6ed6",
    "name": "ai.smithery/jekakos-mcp-user-data-enrichment",
    "source": "mcp",
    "source_url": "https://github.com/jekakos/mcp-user-data-enrichment",
    "description": "Enrich user data by adding social network links based on provided personal information. Integrate‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T12:50:51.350056Z",
    "indexed_at": "2026-02-18T04:07:10.131968",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# MCP User Data Enrichment Server\n\nA Model Context Protocol (MCP) server that enriches user data by adding social network links. This server can be integrated with AI platforms like [Smithery.ai](https://smithery.ai/) to provide social media link discovery capabilities.\n\n## Features\n\n- **User Data Enrichment**: Takes user information (name, birth date) and returns social media links\n- **Mock Data Support**: Includes pre-configured social links for demonstration\n- **Dynamic Generation**: Automatically generates social links for new users\n- **MCP Protocol**: Standard MCP implementation via stdio\n- **HTTP Wrapper**: Optional HTTP API for remote access\n- **Smithery Integration**: Ready for integration with Smithery.ai\n\n## Installation\n\n```bash\nnpm install mcp-user-data-enrichment\n```\n\n## Usage\n\n### As MCP Server (Recommended for Smithery)\n\n```bash\n# Direct stdio usage\nnode src/mcp-server.js\n\n# Or via npm script\nnpm run mcp\n```\n\n### As HTTP Server\n\n```bash\n# Start HTTP server on port 3000\nnpm start\n```\n\n## API Endpoints\n\n### HTTP API (when running as server)\n\n- `GET /status` - Server status\n- `GET /tools` - List available tools\n- `POST /tools/call` - Call any tool\n- `POST /enrich-user` - Enrich user data\n\n### MCP Protocol\n\nThe server provides one tool: `enrich_user_data`\n\n**Input Schema:**\n```json\n{\n  \"firstName\": \"string\",\n  \"lastName\": \"string\", \n  \"birthDate\": \"string (YYYY-MM-DD)\"\n}\n```\n\n**Output:**\n```json\n{\n  \"user\": {\n    \"firstName\": \"John\",\n    \"lastName\": \"Smith\",\n    \"birthDate\": \"1990-01-01\"\n  },\n  \"socialLinks\": {\n    \"instagram\": \"https://instagram.com/john_smith\",\n    \"facebook\": \"https://facebook.com/john.smith\",\n    \"twitter\": \"https://twitter.com/john_smith\",\n    \"linkedin\": \"https://linkedin.com/in/john_smith\"\n  }\n}\n```\n\n## Smithery.ai Integration\n\nThis MCP server is designed to work with [Smithery.ai](https://smithery.ai/), a platform for AI agent orchestration.\n\n### Setup in Smithery\n\n1. **Deploy your server** to a public repository on GitHub\n2. **Configure MCP connection** in Smithery:\n   ```json\n   {\n     \"mcpServers\": {\n       \"user-data-enrichment\": {\n         \"command\": \"node\",\n         \"args\": [\"path/to/mcp-server.js\"]\n       }\n     }\n   }\n   ```\n3. **Use the tool** in your AI agent workflows\n\n### Example Smithery Usage\n\n```javascript\n// In your Smithery agent\nconst result = await mcp.callTool('enrich_user_data', {\n  firstName: 'John',\n  lastName: 'Smith', \n  birthDate: '1990-01-01'\n});\n\nconsole.log(result.content[0].text);\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Test MCP server directly\necho '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\"}' | node src/mcp-server.js\n```\n\n## Testing\n\n```bash\n# Run test client\nnode test-client.js\n\n# Test with curl\ncurl -X POST http://localhost:3000/enrich-user \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"firstName\": \"John\", \"lastName\": \"Smith\", \"birthDate\": \"1990-01-01\"}'\n```\n\n## Mock Data\n\nThe server includes mock social links for these users:\n- John Smith\n- Sarah Johnson  \n- Michael Brown\n\nFor other users, links are generated automatically based on the name.\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests if applicable\n5. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details\n\n## Deployment Files\n\n- `Dockerfile` - Docker configuration for containerized deployment\n- `smithery.yaml` - Smithery.ai configuration file\n- `.dockerignore` - Docker ignore file for optimized builds\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Smithery.ai](https://smithery.ai/) - AI Agent Orchestration Platform\n- [MCP Inspector](https://github.com/modelcontextprotocol/inspector) - MCP Testing Tool\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Enrich user data by adding social network links based on name and birth date",
        "Provide mock social media links for demonstration users",
        "Dynamically generate social media links for new users",
        "Serve as an MCP protocol server via stdio",
        "Expose an optional HTTP API for remote access",
        "Integrate seamlessly with Smithery.ai for AI agent orchestration",
        "Support deployment via Docker for containerized environments",
        "Allow listing and calling tools through HTTP endpoints"
      ],
      "limitations": [
        "Social media links are generated based on name heuristics and may not reflect actual user profiles",
        "Mock data is limited to a predefined set of users (John Smith, Sarah Johnson, Michael Brown)",
        "No explicit rate limiting or concurrency constraints documented",
        "No authentication or authorization mechanisms described for HTTP API"
      ],
      "requirements": [
        "Node.js environment to run the server",
        "NPM package 'mcp-user-data-enrichment' installed",
        "Smithery.ai account and configuration for integration",
        "Public repository deployment for Smithery integration",
        "Port 3000 open if using HTTP server mode"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples for both MCP and HTTP modes, detailed API schemas, integration steps with Smithery.ai, development and testing commands, and notes on mock data and deployment, covering capabilities, limitations, and requirements clearly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# MCP User Data Enrichment Server\n\nA Model Context Protocol (MCP) server that enriches user data by adding social network links. This server can be integrated with AI platforms like [Smithery.ai](https://smithery.ai/) to provide social media link discovery capabilities.\n\n## Features\n\n- **User Data Enrichment**: Takes user information (name, birth date) and returns social media links\n- **Mock Data Support**: Includes pre-configured social links for demonstration\n- **Dynamic Generation**: Automatically generates social links for new users\n- **MCP Protocol**: Standard MCP implementation via stdio\n- **HTTP Wrapper**: Optional HTTP API for remote access\n- **Smithery Integration**: Ready for integration with Smithery.ai\n\n## Installation\n\n```bash\nnpm install mcp-user-data-enrichment\n```\n\n## Usage\n\n### As MCP Server (Recommended for Smithery)\n\n```bash\n# Direct stdio usage\nnode src/mcp-server.js\n\n# Or via npm script\nnpm run mcp\n```\n\n### As HTTP Server\n\n```bash\n# Start HTTP server on port 3000\nnpm start\n```\n\n## API Endpoints\n\n### HTTP API (when running as server)\n\n- `GET /status` - Server status\n- `GET /tools` - List available tools\n- `POST /tools/call` - Call any tool\n- `POST /enrich-user` - Enrich user data\n\n### MCP Protocol\n\nThe server provides one tool: `enrich_user_data`\n\n**Input Schema:**\n```json\n{\n  \"firstName\": \"string\",\n  \"lastName\": \"string\", \n  \"birthDate\": \"string (YYYY-MM-DD)\"\n}\n```\n\n**Output:**\n```json\n{\n  \"user\": {\n    \"firstName\": \"John\",\n    \"lastName\": \"Smith\",\n    \"birthDate\": \"1990-01-01\"\n  },\n  \"socialLinks\": {\n    \"instagram\": \"https://instagram.com/john_smith\",\n    \"facebook\": \"https://facebook.com/john.smith\",\n    \"twitter\": \"https://twitter.com/john_smith\",\n    \"linkedin\": \"https://linkedin.com/in/john_smith\"\n  }\n}\n```\n\n## Smithery.ai Integration\n\nThis MCP server is designed to work with [Smithery.ai](https://smithery.ai/), a platform for AI agent orchestration.\n\n### Setup in Smithery\n\n1. **Deploy your server** to a public repository on GitHub\n2.",
        "start_pos": 0,
        "end_pos": 1995,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "8daab80a59ad6ed6"
      },
      {
        "chunk_id": 1,
        "text": "is MCP server is designed to work with [Smithery.ai](https://smithery.ai/), a platform for AI agent orchestration.\n\n### Setup in Smithery\n\n1. **Deploy your server** to a public repository on GitHub\n2. **Configure MCP connection** in Smithery:\n   ```json\n   {\n     \"mcpServers\": {\n       \"user-data-enrichment\": {\n         \"command\": \"node\",\n         \"args\": [\"path/to/mcp-server.js\"]\n       }\n     }\n   }\n   ```\n3. **Use the tool** in your AI agent workflows\n\n### Example Smithery Usage\n\n```javascript\n// In your Smithery agent\nconst result = await mcp.callTool('enrich_user_data', {\n  firstName: 'John',\n  lastName: 'Smith', \n  birthDate: '1990-01-01'\n});\n\nconsole.log(result.content[0].text);\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Run in development mode\nnpm run dev\n\n# Test MCP server directly\necho '{\"jsonrpc\": \"2.0\", \"id\": 1, \"method\": \"tools/list\"}' | node src/mcp-server.js\n```\n\n## Testing\n\n```bash\n# Run test client\nnode test-client.js\n\n# Test with curl\ncurl -X POST http://localhost:3000/enrich-user \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"firstName\": \"John\", \"lastName\": \"Smith\", \"birthDate\": \"1990-01-01\"}'\n```\n\n## Mock Data\n\nThe server includes mock social links for these users:\n- John Smith\n- Sarah Johnson  \n- Michael Brown\n\nFor other users, links are generated automatically based on the name.\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests if applicable\n5. Submit a pull request\n\n## License\n\nMIT License - see LICENSE file for details\n\n## Deployment Files\n\n- `Dockerfile` - Docker configuration for containerized deployment\n- `smithery.yaml` - Smithery.ai configuration file\n- `.dockerignore` - Docker ignore file for optimized builds\n\n## Related Links\n\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n- [Smithery.ai](https://smithery.ai/) - AI Agent Orchestration Platform\n- [MCP Inspector](https://github.com/modelcontextprotocol/inspector) - MCP Testing Tool",
        "start_pos": 1795,
        "end_pos": 3782,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "8daab80a59ad6ed6"
      },
      {
        "chunk_id": 2,
        "text": "col](https://modelcontextprotocol.io/)\n- [Smithery.ai](https://smithery.ai/) - AI Agent Orchestration Platform\n- [MCP Inspector](https://github.com/modelcontextprotocol/inspector) - MCP Testing Tool",
        "start_pos": 3582,
        "end_pos": 3782,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "8daab80a59ad6ed6"
      }
    ]
  },
  {
    "agent_id": "e0627023240012d9",
    "name": "ai.smithery/jenniferjiang0511-mit-ai-studio-hw3",
    "source": "mcp",
    "source_url": "https://github.com/jenniferjiang0511/MIT-AI-studio-HW3",
    "description": "Greet people by name and check local forecasts and weather alerts across the U.S. Switch to a play‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T03:01:55.290688Z",
    "indexed_at": "2026-02-18T04:07:12.258019",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# weather-app\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n\n# Functions of the tools that are being connected\nThis weather app has two functions: get alert and get forecast. Get alert gets any weather alert within the U.S. state, and get forecast gets the weather forecast for that location.\n\nTo make things more interesting, if you ask the it to speak in a pirate like tone, it can answer your requests with such a tone.\n\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started with running the server\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development of the server\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploying the server\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Get weather alerts within a U.S. state",
        "Get weather forecasts for a specified location",
        "Respond in a pirate-like tone upon request"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai/account/api-keys",
        "Smithery CLI for building and deploying the server",
        "GitHub account for repository hosting and deployment"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "The documentation provides installation steps, usage examples, tool descriptions, and deployment instructions but lacks explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# weather-app\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n\n# Functions of the tools that are being connected\nThis weather app has two functions: get alert and get forecast. Get alert gets any weather alert within the U.S. state, and get forecast gets the weather forecast for that location.\n\nTo make things more interesting, if you ask the it to speak in a pirate like tone, it can answer your requests with such a tone.\n\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started with running the server\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development of the server\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploying the server\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 1406,
        "token_count_estimate": 351,
        "source_type": "readme",
        "agent_id": "e0627023240012d9"
      }
    ]
  },
  {
    "agent_id": "12b3ccef86eb6893",
    "name": "ai.smithery/jessicayanwang-test",
    "source": "mcp",
    "source_url": "https://github.com/jessicayanwang/frankfurtermcp",
    "description": "Fetch latest and historical currency exchange rates from Frankfurter. Convert amounts between curr‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T02:03:00.046026Z",
    "indexed_at": "2026-02-18T04:07:13.848960",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue?logo=python&logoColor=3776ab&labelColor=e4e4e4)](https://www.python.org/downloads/release/python-3120/) [![pytest](https://github.com/anirbanbasu/frankfurtermcp/actions/workflows/uv-pytest.yml/badge.svg)](https://github.com/anirbanbasu/frankfurtermcp/actions/workflows/uv-pytest.yml) ![GitHub commits since latest release](https://img.shields.io/github/commits-since/anirbanbasu/frankfurtermcp/latest)\n [![PyPI](https://img.shields.io/pypi/v/frankfurtermcp?label=pypi%20package)](https://pypi.org/project/frankfurtermcp/#history)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/frankfurtermcp?label=pypi%20downloads)](https://pypi.org/project/frankfurtermcp/)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/c6527bdb-9b60-430d-9ed6-cb3c8b9a2b54) [![smithery badge](https://smithery.ai/badge/@anirbanbasu/frankfurtermcp)](https://smithery.ai/server/@anirbanbasu/frankfurtermcp)\n\n# Frankfurter MCP\n\n[Frankfurter](https://frankfurter.dev/) is a useful API for latest currency exchange rates, historical data, or time series published by sources such as the European Central Bank. Should you have to access the Frankfurter API as tools for language model agents exposed over the Model Context Protocol (MCP), Frankfurter MCP is what you need.\n\n# Installation\n\n_If your objective is to use the tools available on this MCP server, please refer to the usage > client sub-section below_.\n\nThe directory where you clone this repository will be referred to as the _working directory_ or _WD_ hereinafter.\n\nInstall [uv](https://docs.astral.sh/uv/getting-started/installation/). To install the project with its minimal dependencies in a virtual environment, run the following in the _WD_. To install all non-essential dependencies (_which are required for developing and testing_), replace the `--no-dev` with the `--all-groups` flag in the following command.\n\n```bash\nuv sync --no-dev\n```\n\n## Environment variables\n\nFollowing is a list of environment variables that can be used to configure the application. A template of environment variables is provided in the file `.env.template`. _Note that the default values listed in the table below are not always the same as those in the `.env.template` file_.\n\nThe following environment variables can be specified, prefixed with `FASTMCP_`: `HOST`, `PORT`, `DEBUG` and `LOG_LEVEL`. See [global configuration options](https://gofastmcp.com/servers/server#global-settings) for FastMCP. Note that `on_duplicate_` prefixed options specified as environment variables _will be ignored_.\n\nThe underlying HTTP client also respects some environment variables, as documented in [the HTTPX library](https://www.python-httpx.org/environment_variables/). In addition, `SSL_CERT_FILE` and `SSL_CERT_DIR` can be configured to use self-signed certificates of hosted API endpoint or intermediate HTTP(S) proxy server(s).\n\n| Variable |  [Default value] and description   |\n|--------------|----------------|\n| `LOG_LEVEL` | [INFO] The level for logging. Changing this level also affects the log output of other dependent libraries that may use the same environment variable. See valid values at [Python logging documentation](https://docs.python.org/3/library/logging.html#logging-levels). |\n| `HTTPX_TIMEOUT` | [5.0] The time for the underlying HTTP client to wait, in seconds, for a response from the Frankfurter API. |\n| `HTTPX_VERIFY_SSL` | [True] This variable can be set to False to turn off SSL certificate verification, if, for instance, you are using a proxy server with a self-signed certificate. However, setting this to False _is advised against_: instead, use the `SSL_CERT_FILE` and `SSL_CERT_DIR` variables to properly configure self-signed certificates. |\n| `FAST_MCP_HOST` | [localhost] This variable specifies which host the MCP server must bind to unless the server transport (see below) is set to `stdio`. |\n| `FAST_MCP_PORT` | [8000] This variable specifies which port the MCP server must listen on unless the server transport (see below) is set to `stdio`. |\n| `MCP_SERVER_TRANSPORT` | [stdio] The acceptable options are `stdio`, `sse` or `streamable-http`. However, in the `.env.template`, the default value is set to `stdio`. |\n| `MCP_SERVER_INCLUDE_METADATA_IN_RESPONSE` | [True] This specifies if additional metadata will be included with the MCP type `TextContent` that wraps the response data from each tool call. The additional metadata, for example, will include the API URL of the Frankfurter server, amongst others, that is used to obtain the responses. |\n| `FRANKFURTER_API_URL` | [https://api.frankfurter.dev/v1] If you are [self-hosting the Frankfurter API](https://hub.docker.com/r/lineofflight/frankfurter), you should change this to the API endpoint address of your deployment. |\n\n# Usage\n\nThe following sub-sections illustrate how to run the Frankfurter MCP as a server and how to access it from MCP clients.\n\n## Server\nWhile running the server, you have the choice to use `stdio` transport or HTTP options (`sse` or the newer `streamable-http`).\n\nUsing default settings and `MCP_SERVER_TRANSPORT` set to `sse` or `streamable-http`, the MCP endpoint will be available over HTTP at [http://localhost:8000/sse](http://localhost:8000/sse) for the Server Sent Events (SSE) transport, or [http://localhost:8000/mcp](http://localhost:8000/mcp) for the streamable HTTP transport.\n\nIf you want to run Frankfurter MCP with `stdio` transport and the default parameters, execute the commands below without using the `.env.template` file.\n\n### Server with `uv`\n\n_Optional_: Copy the `.env.template` file to a `.env` file in the _WD_, to modify the aforementioned environment variables, if you want to use anything other than the default settings. Or, on your shell, you can export the environment variables that you wish to modify.\n\nRun the following in the _WD_ to start the MCP server.\n\n```bash\nuv run frankfurtermcp\n```\n\n### Server with `pip` from PyPI package\n\nAdd this package from PyPI using `pip` in a virtual environment (possibly managed by `uv`, `pyenv` or `conda`) and then start the server by running the following.\n\n_Optional_: Add a `.env` file with the contents of the `.env.template` file if you wish to modify the default values of the aforementioned environment variables. Or, on your shell, you can export the environment variables that you wish to modify.\n\n```bash\npip install frankfurtermcp\npython -m frankfurtermcp.server\n```\n\n### Server using Docker\n\nThere are two Dockerfiles provided in this repository.\n\n - `local.dockerfile` for containerising the Frankfurter MCP server.\n - `smithery.dockerfile` for deploying to [Smithery AI](https://smithery.ai/), which you do not have to use.\n\nTo build the image, create the container and start it, run the following in _WD_. _Choose shorter names for the image and container if you prefer._\n\nIf you change the port to anything other than 8000 in `.env.template`, _do remember to change the port number references in the following command_. Instead of passing all the environment variables using the `--env-file` option, you can also pass individual environment variables using the `-e` option.\n\n```bash\ndocker build -t frankfurtermcp -f local.dockerfile .\ndocker create -p 8000:8000/tcp --env-file .env.template --expose 8000 --name frankfurtermcp-container frankfurtermcp\ndocker start frankfurtermcp-container\n```\n\nUpon successful build and container start, the MCP server will be available over HTTP at [http://localhost:8000/sse](http://localhost:8000/sse) for the Server Sent Events (SSE) transport, or [http://localhost:8000/mcp](http://localhost:8000/mcp) for the streamable HTTP transport.\n\n### Cloud hosted servers\n\nThe currently available cloud hosted options are as follows.\n\n - FastMCP Cloud: https://frankfurtermcp.fastmcp.app/mcp\n - Glama.AI: https://glama.ai/mcp/servers/@anirbanbasu/frankfurtermcp\n - Smithery.AI: https://smithery.ai/server/@anirbanbasu/frankfurtermcp\n\n\n## Client access\n\nThis sub-section explains ways for a client to connect and test the FrankfurterMCP server.\n\n### The official MCP visual inspector\n\nThe [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is an _official_ Model Context Protocol tool that can be used by developers to test and debug MCP servers. This is the most comprehensive way to explore the MCP server.\n\nTo use it, you must have Node.js installed. The best way to install and manage `node` as well as packages such as the MCP Inspector is to use the [Node Version Manager (or, `nvm`)](https://github.com/nvm-sh/nvm). Once you have `nvm` installed, you can install and use the latest Long Term Release version of `node` by executing the following.\n\n```bash\nnvm install --lts\nnvm use --lts\n```\n\nFollowing that (install and) run the MCP Inspector by executing the following in the _WD_.\n\n```bash\nnpx @modelcontextprotocol/inspector uv run frankfurtermcp\n```\n\nThis will create a local URL at port 6274 with an authentication token, which you can copy and browse to on your browser. Once on the MCP Inspector UI, press _Connect_ to connect to the MCP server. Thereafter, you can explore the tools available on the server.\n\n### Claude Desktop, Visual Studio, and so on\n\nThe server entry to run with `stdio` transport that you can use with systems such as Claude Desktop, Visual Studio Code, and so on is as follows.\n\n```json\n{\n    \"command\": \"uv\",\n    \"args\": [\n        \"run\",\n        \"frankfurtermcp\"\n    ]\n}\n```\n\nInstead of having `frankfurtermcp` as the last item in the list of `args`, you may need to specify the full path to the script, e.g., _WD_`/.venv/bin/frankfurtermcp`. Likewise, instead of using `uv`, you could also have the following JSON configuration with the path properly substituted for `python3.12`, for instance such as _WD_`/.venv/bin/python3.12`.\n\n```json\n{\n    \"command\": \"python3.12\",\n    \"args\": [\n        \"-m\",\n        \"frankfurtermcp.server\"\n    ]\n}\n```\n\n# List of available MCP features\n\nFrankfurterMCP has the following MCP features.\n\n## Tools\n\nThe following table lists the names of the tools as exposed by the FrankfurterMCP server. The descriptions shown here are for documentation purposes, which may differ from the actual descriptions exposed over the model context protocol.\n\n| Name         |  Description   |\n|--------------|----------------|\n| `get_supported_currencies` | Get a list of currencies supported by the Frankfurter API. |\n| `get_latest_exchange_rates` | Get latest exchange rates in specific currencies for a given base currency. |\n| `convert_currency_latest` | Convert an amount from one currency to another using the latest exchange rates. |\n| `get_historical_exchange_rates` | Get historical exchange rates for a specific date or date range in specific currencies for a given base currency. |\n| `convert_currency_specific_date` | Convert an amount from one currency to another using the exchange rates for a specific date. |\n\nThe required and optional arguments for each tool are not listed in the following table for brevity but are available to the MCP client over the protocol.\n\n# Contributing\n\nInstall [`pre-commit`](https://pre-commit.com/) for Git and [`ruff`](https://docs.astral.sh/ruff/installation/). Then enable `pre-commit` by running the following in the _WD_.\n\n```bash\npre-commit install\n```\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n# Testing and coverage\n\nTo run the provided test cases, execute the following. Add the flag `--capture=tee-sys` to the command to display further console output.\n\n```bash\nuv run --group test pytest tests/\n```\n\nThere is a handy testing script _WD_`/run-tests.sh`, which will run all the tests and generate a coverage report as follows. It can also accept arguments and parameters to be passed to `pytest`, such as `-k` for filtering the tests to run. If all tests are run, the generated coverage report may look like the one below.\n\n```bash\nName                             Stmts   Miss  Cover\n----------------------------------------------------\nsrc/frankfurtermcp/__init__.py      10      0   100%\nsrc/frankfurtermcp/common.py        23      0   100%\nsrc/frankfurtermcp/mixin.py         52      4    92%\nsrc/frankfurtermcp/model.py         17      0   100%\nsrc/frankfurtermcp/server.py       111     20    82%\ntests/__init__.py                    0      0   100%\ntests/test_data_models.py           60      0   100%\ntests/test_server.py                71      0   100%\n----------------------------------------------------\nTOTAL                              344     24    93%\n```\n\n# License\n\n[MIT](https://choosealicense.com/licenses/mit/).\n\n# Project status\n\nFollowing is a table of some updates regarding the project status. Note that these do not correspond to specific commits or milestones.\n\n| Date     |  Status   |  Notes or observations   |\n|----------|:-------------:|----------------------|\n| September 6, 2025 |  active |  Code refactoring and cleanup. |\n| June 27, 2025 |  active |  Successful remote deployments on Glama.AI and Smithery.AI. |\n| June 9, 2025 |  active |  Added containerisation, support for self-signed proxies. |\n| June 7, 2025 |  active |  Project started. Added tools to cover all the functionalities of the Frankfurter API. |\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide latest currency exchange rates for specific currencies and base currency",
        "Retrieve historical exchange rates for specific dates or date ranges",
        "Convert currency amounts using latest exchange rates",
        "Convert currency amounts using exchange rates for a specific date",
        "List all currencies supported by the Frankfurter API",
        "Serve MCP endpoints over multiple transports including stdio, SSE, and streamable HTTP",
        "Allow configuration of server behavior via environment variables",
        "Support deployment via Docker and cloud hosting platforms",
        "Enable client access and testing through official MCP Inspector and other tools"
      ],
      "limitations": [
        "Does not list detailed required and optional arguments for tools in documentation (available only over MCP protocol)",
        "SSL certificate verification can be disabled but is discouraged",
        "Environment variables with prefix 'on_duplicate_' are ignored",
        "Default server transport is stdio but can be changed; some clients may require specific transport configurations"
      ],
      "requirements": [
        "Python 3.12 or higher",
        "Installation of 'uv' tool for running the server",
        "Optional: Node.js and nvm for using MCP Inspector client",
        "Optional: Docker for containerized deployment",
        "Environment variables prefixed with FASTMCP_ for server configuration",
        "Network access to Frankfurter API endpoint or self-hosted equivalent",
        "Pre-commit and ruff for development and testing contributions"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples for server and client, detailed environment variable configuration, tool descriptions, deployment options, and testing guidance.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue?logo=python&logoColor=3776ab&labelColor=e4e4e4)](https://www.python.org/downloads/release/python-3120/) [![pytest](https://github.com/anirbanbasu/frankfurtermcp/actions/workflows/uv-pytest.yml/badge.svg)](https://github.com/anirbanbasu/frankfurtermcp/actions/workflows/uv-pytest.yml) ![GitHub commits since latest release](https://img.shields.io/github/commits-since/anirbanbasu/frankfurtermcp/latest)\n [![PyPI](https://img.shields.io/pypi/v/frankfurtermcp?label=pypi%20package)](https://pypi.org/project/frankfurtermcp/#history)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/frankfurtermcp?label=pypi%20downloads)](https://pypi.org/project/frankfurtermcp/)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/c6527bdb-9b60-430d-9ed6-cb3c8b9a2b54) [![smithery badge](https://smithery.ai/badge/@anirbanbasu/frankfurtermcp)](https://smithery.ai/server/@anirbanbasu/frankfurtermcp)\n\n# Frankfurter MCP\n\n[Frankfurter](https://frankfurter.dev/) is a useful API for latest currency exchange rates, historical data, or time series published by sources such as the European Central Bank. Should you have to access the Frankfurter API as tools for language model agents exposed over the Model Context Protocol (MCP), Frankfurter MCP is what you need.\n\n# Installation\n\n_If your objective is to use the tools available on this MCP server, please refer to the usage > client sub-section below_.\n\nThe directory where you clone this repository will be referred to as the _working directory_ or _WD_ hereinafter.\n\nInstall [uv](https://docs.astral.sh/uv/getting-started/installation/). To install the project with its minimal dependencies in a virtual environment, run the following in the _WD_. To install all non-essential dependencies (_which are required for developing and testing_), replace the `--no-dev` with the `--all-groups` flag in the following command.",
        "start_pos": 0,
        "end_pos": 1939,
        "token_count_estimate": 484,
        "source_type": "readme",
        "agent_id": "12b3ccef86eb6893"
      },
      {
        "chunk_id": 1,
        "text": "un the following in the _WD_. To install all non-essential dependencies (_which are required for developing and testing_), replace the `--no-dev` with the `--all-groups` flag in the following command.\n\n```bash\nuv sync --no-dev\n```\n\n## Environment variables\n\nFollowing is a list of environment variables that can be used to configure the application. A template of environment variables is provided in the file `.env.template`. _Note that the default values listed in the table below are not always the same as those in the `.env.template` file_.\n\nThe following environment variables can be specified, prefixed with `FASTMCP_`: `HOST`, `PORT`, `DEBUG` and `LOG_LEVEL`. See [global configuration options](https://gofastmcp.com/servers/server#global-settings) for FastMCP. Note that `on_duplicate_` prefixed options specified as environment variables _will be ignored_.\n\nThe underlying HTTP client also respects some environment variables, as documented in [the HTTPX library](https://www.python-httpx.org/environment_variables/). In addition, `SSL_CERT_FILE` and `SSL_CERT_DIR` can be configured to use self-signed certificates of hosted API endpoint or intermediate HTTP(S) proxy server(s).\n\n| Variable |  [Default value] and description   |\n|--------------|----------------|\n| `LOG_LEVEL` | [INFO] The level for logging. Changing this level also affects the log output of other dependent libraries that may use the same environment variable. See valid values at [Python logging documentation](https://docs.python.org/3/library/logging.html#logging-levels). |\n| `HTTPX_TIMEOUT` | [5.0] The time for the underlying HTTP client to wait, in seconds, for a response from the Frankfurter API. |\n| `HTTPX_VERIFY_SSL` | [True] This variable can be set to False to turn off SSL certificate verification, if, for instance, you are using a proxy server with a self-signed certificate. However, setting this to False _is advised against_: instead, use the `SSL_CERT_FILE` and `SSL_CERT_DIR` variables to properly configure self-signed certificates.",
        "start_pos": 1739,
        "end_pos": 3775,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "12b3ccef86eb6893"
      },
      {
        "chunk_id": 2,
        "text": "erver with a self-signed certificate. However, setting this to False _is advised against_: instead, use the `SSL_CERT_FILE` and `SSL_CERT_DIR` variables to properly configure self-signed certificates. |\n| `FAST_MCP_HOST` | [localhost] This variable specifies which host the MCP server must bind to unless the server transport (see below) is set to `stdio`. |\n| `FAST_MCP_PORT` | [8000] This variable specifies which port the MCP server must listen on unless the server transport (see below) is set to `stdio`. |\n| `MCP_SERVER_TRANSPORT` | [stdio] The acceptable options are `stdio`, `sse` or `streamable-http`. However, in the `.env.template`, the default value is set to `stdio`. |\n| `MCP_SERVER_INCLUDE_METADATA_IN_RESPONSE` | [True] This specifies if additional metadata will be included with the MCP type `TextContent` that wraps the response data from each tool call. The additional metadata, for example, will include the API URL of the Frankfurter server, amongst others, that is used to obtain the responses. |\n| `FRANKFURTER_API_URL` | [https://api.frankfurter.dev/v1] If you are [self-hosting the Frankfurter API](https://hub.docker.com/r/lineofflight/frankfurter), you should change this to the API endpoint address of your deployment. |\n\n# Usage\n\nThe following sub-sections illustrate how to run the Frankfurter MCP as a server and how to access it from MCP clients.\n\n## Server\nWhile running the server, you have the choice to use `stdio` transport or HTTP options (`sse` or the newer `streamable-http`).\n\nUsing default settings and `MCP_SERVER_TRANSPORT` set to `sse` or `streamable-http`, the MCP endpoint will be available over HTTP at [http://localhost:8000/sse](http://localhost:8000/sse) for the Server Sent Events (SSE) transport, or [http://localhost:8000/mcp](http://localhost:8000/mcp) for the streamable HTTP transport.\n\nIf you want to run Frankfurter MCP with `stdio` transport and the default parameters, execute the commands below without using the `.env.template` file.",
        "start_pos": 3575,
        "end_pos": 5571,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "12b3ccef86eb6893"
      },
      {
        "chunk_id": 3,
        "text": "t:8000/mcp) for the streamable HTTP transport.\n\nIf you want to run Frankfurter MCP with `stdio` transport and the default parameters, execute the commands below without using the `.env.template` file.\n\n### Server with `uv`\n\n_Optional_: Copy the `.env.template` file to a `.env` file in the _WD_, to modify the aforementioned environment variables, if you want to use anything other than the default settings. Or, on your shell, you can export the environment variables that you wish to modify.\n\nRun the following in the _WD_ to start the MCP server.\n\n```bash\nuv run frankfurtermcp\n```\n\n### Server with `pip` from PyPI package\n\nAdd this package from PyPI using `pip` in a virtual environment (possibly managed by `uv`, `pyenv` or `conda`) and then start the server by running the following.\n\n_Optional_: Add a `.env` file with the contents of the `.env.template` file if you wish to modify the default values of the aforementioned environment variables. Or, on your shell, you can export the environment variables that you wish to modify.\n\n```bash\npip install frankfurtermcp\npython -m frankfurtermcp.server\n```\n\n### Server using Docker\n\nThere are two Dockerfiles provided in this repository.\n\n - `local.dockerfile` for containerising the Frankfurter MCP server.\n - `smithery.dockerfile` for deploying to [Smithery AI](https://smithery.ai/), which you do not have to use.\n\nTo build the image, create the container and start it, run the following in _WD_. _Choose shorter names for the image and container if you prefer._\n\nIf you change the port to anything other than 8000 in `.env.template`, _do remember to change the port number references in the following command_. Instead of passing all the environment variables using the `--env-file` option, you can also pass individual environment variables using the `-e` option.\n\n```bash\ndocker build -t frankfurtermcp -f local.dockerfile .",
        "start_pos": 5371,
        "end_pos": 7254,
        "token_count_estimate": 470,
        "source_type": "readme",
        "agent_id": "12b3ccef86eb6893"
      },
      {
        "chunk_id": 4,
        "text": "ing all the environment variables using the `--env-file` option, you can also pass individual environment variables using the `-e` option.\n\n```bash\ndocker build -t frankfurtermcp -f local.dockerfile .\ndocker create -p 8000:8000/tcp --env-file .env.template --expose 8000 --name frankfurtermcp-container frankfurtermcp\ndocker start frankfurtermcp-container\n```\n\nUpon successful build and container start, the MCP server will be available over HTTP at [http://localhost:8000/sse](http://localhost:8000/sse) for the Server Sent Events (SSE) transport, or [http://localhost:8000/mcp](http://localhost:8000/mcp) for the streamable HTTP transport.\n\n### Cloud hosted servers\n\nThe currently available cloud hosted options are as follows.\n\n - FastMCP Cloud: https://frankfurtermcp.fastmcp.app/mcp\n - Glama.AI: https://glama.ai/mcp/servers/@anirbanbasu/frankfurtermcp\n - Smithery.AI: https://smithery.ai/server/@anirbanbasu/frankfurtermcp\n\n\n## Client access\n\nThis sub-section explains ways for a client to connect and test the FrankfurterMCP server.\n\n### The official MCP visual inspector\n\nThe [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is an _official_ Model Context Protocol tool that can be used by developers to test and debug MCP servers. This is the most comprehensive way to explore the MCP server.\n\nTo use it, you must have Node.js installed. The best way to install and manage `node` as well as packages such as the MCP Inspector is to use the [Node Version Manager (or, `nvm`)](https://github.com/nvm-sh/nvm). Once you have `nvm` installed, you can install and use the latest Long Term Release version of `node` by executing the following.\n\n```bash\nnvm install --lts\nnvm use --lts\n```\n\nFollowing that (install and) run the MCP Inspector by executing the following in the _WD_.\n\n```bash\nnpx @modelcontextprotocol/inspector uv run frankfurtermcp\n```\n\nThis will create a local URL at port 6274 with an authentication token, which you can copy and browse to on your browser.",
        "start_pos": 7054,
        "end_pos": 9048,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "12b3ccef86eb6893"
      },
      {
        "chunk_id": 5,
        "text": "_WD_.\n\n```bash\nnpx @modelcontextprotocol/inspector uv run frankfurtermcp\n```\n\nThis will create a local URL at port 6274 with an authentication token, which you can copy and browse to on your browser. Once on the MCP Inspector UI, press _Connect_ to connect to the MCP server. Thereafter, you can explore the tools available on the server.\n\n### Claude Desktop, Visual Studio, and so on\n\nThe server entry to run with `stdio` transport that you can use with systems such as Claude Desktop, Visual Studio Code, and so on is as follows.\n\n```json\n{\n    \"command\": \"uv\",\n    \"args\": [\n        \"run\",\n        \"frankfurtermcp\"\n    ]\n}\n```\n\nInstead of having `frankfurtermcp` as the last item in the list of `args`, you may need to specify the full path to the script, e.g., _WD_`/.venv/bin/frankfurtermcp`. Likewise, instead of using `uv`, you could also have the following JSON configuration with the path properly substituted for `python3.12`, for instance such as _WD_`/.venv/bin/python3.12`.\n\n```json\n{\n    \"command\": \"python3.12\",\n    \"args\": [\n        \"-m\",\n        \"frankfurtermcp.server\"\n    ]\n}\n```\n\n# List of available MCP features\n\nFrankfurterMCP has the following MCP features.\n\n## Tools\n\nThe following table lists the names of the tools as exposed by the FrankfurterMCP server. The descriptions shown here are for documentation purposes, which may differ from the actual descriptions exposed over the model context protocol.\n\n| Name         |  Description   |\n|--------------|----------------|\n| `get_supported_currencies` | Get a list of currencies supported by the Frankfurter API. |\n| `get_latest_exchange_rates` | Get latest exchange rates in specific currencies for a given base currency. |\n| `convert_currency_latest` | Convert an amount from one currency to another using the latest exchange rates. |\n| `get_historical_exchange_rates` | Get historical exchange rates for a specific date or date range in specific currencies for a given base currency.",
        "start_pos": 8848,
        "end_pos": 10810,
        "token_count_estimate": 490,
        "source_type": "readme",
        "agent_id": "12b3ccef86eb6893"
      },
      {
        "chunk_id": 6,
        "text": "ency to another using the latest exchange rates. |\n| `get_historical_exchange_rates` | Get historical exchange rates for a specific date or date range in specific currencies for a given base currency. |\n| `convert_currency_specific_date` | Convert an amount from one currency to another using the exchange rates for a specific date. |\n\nThe required and optional arguments for each tool are not listed in the following table for brevity but are available to the MCP client over the protocol.\n\n# Contributing\n\nInstall [`pre-commit`](https://pre-commit.com/) for Git and [`ruff`](https://docs.astral.sh/ruff/installation/). Then enable `pre-commit` by running the following in the _WD_.\n\n```bash\npre-commit install\n```\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n# Testing and coverage\n\nTo run the provided test cases, execute the following. Add the flag `--capture=tee-sys` to the command to display further console output.\n\n```bash\nuv run --group test pytest tests/\n```\n\nThere is a handy testing script _WD_`/run-tests.sh`, which will run all the tests and generate a coverage report as follows. It can also accept arguments and parameters to be passed to `pytest`, such as `-k` for filtering the tests to run. If all tests are run, the generated coverage report may look like the one below.",
        "start_pos": 10610,
        "end_pos": 11972,
        "token_count_estimate": 340,
        "source_type": "readme",
        "agent_id": "12b3ccef86eb6893"
      },
      {
        "chunk_id": 7,
        "text": "tests/test_server.py                71      0   100%\n----------------------------------------------------\nTOTAL                              344     24    93%\n```\n\n# License\n\n[MIT](https://choosealicense.com/licenses/mit/).\n\n# Project status\n\nFollowing is a table of some updates regarding the project status. Note that these do not correspond to specific commits or milestones.\n\n| Date     |  Status   |  Notes or observations   |\n|----------|:-------------:|----------------------|\n| September 6, 2025 |  active |  Code refactoring and cleanup. |\n| June 27, 2025 |  active |  Successful remote deployments on Glama.AI and Smithery.AI. |\n| June 9, 2025 |  active |  Added containerisation, support for self-signed proxies. |\n| June 7, 2025 |  active |  Project started. Added tools to cover all the functionalities of the Frankfurter API. |",
        "start_pos": 12458,
        "end_pos": 13301,
        "token_count_estimate": 210,
        "source_type": "readme",
        "agent_id": "12b3ccef86eb6893"
      }
    ]
  },
  {
    "agent_id": "222a479a4ea614b6",
    "name": "ai.smithery/jirispilka-actors-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/jirispilka/actors-mcp-server",
    "description": "Greet anyone by name with friendly, personalized messages. Explore the origin of Hello, World thro‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T11:59:13.452963Z",
    "indexed_at": "2026-02-18T04:07:15.440984",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<h1 align=\"center\">\n    <a href=\"https://mcp.apify.com\">\n        <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/apify_mcp_server_dark_background.png\">\n            <img alt=\"Apify MCP Server\" src=\"docs/apify_mcp_server_white_background.png\" width=\"500\">\n        </picture>\n    </a>\n    <br>\n    <small><a href=\"https://mcp.apify.com\">mcp.apify.com</a></small>\n</h1>\n\n<p align=center>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" alt=\"NPM latest version\" data-canonical-src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml\"><img src=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://apify.com/apify/actors-mcp-server\"><img src=\"https://apify.com/actor-badge?actor=apify/actors-mcp-server\" alt=\"Actor runs\" style=\"max-width: 100%;\"></a>\n</p>\n\nThe Apify Model Context Protocol (MCP) Server at **mcp.apify.com** instantly connects AI applications and agents to thousands of ready‚Äëbuilt tools. It allows your AI assistant to use any [Apify Actor](https://apify.com/store) for web scraping, data extraction, and automation tasks in real time.\n\n> **üöÄ Try the hosted Apify MCP Server!**\n>\n> For the easiest setup and most powerful features, including the ability to find and use any Actor from Apify Store, connect your AI assistant to our hosted server:\n>\n> **[`https://mcp.apify.com`](https://mcp.apify.com)**\n>\n> It supports OAuth, so you can connect from clients like Claude.ai or Visual Studio Code with just the URL.\n\n![Actors-MCP-server](docs/actors-mcp-server.png)\n\n## Table of Contents\n- [üåê Introducing the Apify MCP server](#-introducing-the-apify-mcp-server)\n- [üöÄ Quickstart](#-quickstart)\n- [ü§ñ MCP clients and examples](#-mcp-clients-and-examples)\n- [ü™Ñ Try Apify MCP instantly](#-try-apify-mcp-instantly)\n- [üõ†Ô∏è Tools, resources, and prompts](#-tools-resources-and-prompts)\n- [üêõ Troubleshooting (local MCP server)](#-troubleshooting-local-mcp-server)\n- [‚öôÔ∏è Development](#-development)\n- [ü§ù Contributing](#-contributing)\n- [üìö Learn more](#-learn-more)\n \n\n# üåê Introducing the Apify MCP server\n\nThe Apify MCP Server allows an AI assistant to use any [Apify Actor](https://apify.com/store) as a tool to perform a specific task.\nFor example, it can:\n- Use [Facebook Posts Scraper](https://apify.com/apify/facebook-posts-scraper) to extract data from Facebook posts from multiple pages/profiles.\n- Use [Google Maps Email Extractor](https://apify.com/lukaskrivka/google-maps-with-contact-details) to extract contact details from Google Maps.\n- Use [Google Search Results Scraper](https://apify.com/apify/google-search-scraper) to scrape Google Search Engine Results Pages (SERPs).\n- Use [Instagram Scraper](https://apify.com/apify/instagram-scraper) to scrape Instagram posts, profiles, places, photos, and comments.\n- Use [RAG Web Browser](https://apify.com/apify/web-scraper) to search the web, scrape the top N URLs, and return their content.\n\n**Video tutorial: Integrate 5,000+ Apify Actors and Agents with Claude**\n\n[![Apify MCP Server Tutorial: Integrate 5,000+ Apify Actors and Agents with Claude](https://img.youtube.com/vi/BKu8H91uCTg/hqdefault.jpg)](https://www.youtube.com/watch?v=BKu8H91uCTg)\n\n# üöÄ Quickstart\n\nYou can use the Apify MCP Server in two ways:\n\n**HTTPS Endpoint (mcp.apify.com)**: Connect from your MCP client via OAuth or by including the `Authorization: Bearer <APIFY_TOKEN>` header in your requests. This is the recommended method for most use cases. Because it supports OAuth, you can connect from clients like [Claude.ai](https://claude.ai) or [Visual Studio Code](https://code.visualstudio.com/) using just the URL: `https://mcp.apify.com`.\n- `https://mcp.apify.com` (recommended) for streamable transport\n- `https://mcp.apify.com/sse` for legacy SSE transport\n\n**Standard Input/Output (stdio)**: Ideal for local integrations and command-line tools like the Claude for Desktop client.\n- Set the MCP client server command to `npx @apify/actors-mcp-server` and the `APIFY_TOKEN` environment variable to your Apify API token.\n- See `npx @apify/actors-mcp-server --help` for more options.\n\nYou can find detailed instructions for setting up the MCP server in the [Apify documentation](https://docs.apify.com/platform/integrations/mcp).\n\n# ü§ñ MCP clients and examples\n\nTo interact with the Apify MCP server, you can use various MCP clients, such as:\n- [Claude Desktop](https://claude.ai/download)\n- [Visual Studio Code](https://code.visualstudio.com/)\n- [LibreChat](https://www.librechat.ai/)\n- [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- Other clients at [https://modelcontextprotocol.io/clients](https://modelcontextprotocol.io/clients)\n- More clients at [https://glama.ai/mcp/clients](https://glama.ai/mcp/clients)\n\nWith MCP server integrated, you can ask your AI assistant things like:\n- \"Search the web and summarize recent trends in AI Agents.\"\n- \"Find the top 10 Italian restaurants in San Francisco.\"\n- \"Find and analyze the Instagram profile of The Rock.\"\n- \"Provide a step-by-step guide on using the Model Context Protocol, including source URLs.\"\n- \"What Apify Actors can I use?\"\n\n### Supported Clients Matrix\n\nThe following table outlines the tested MCP clients and their level of support for key features.\n\n| Client | Dynamic Tool Discovery | Notes |\n| --- | --- | --- |\n| **Claude.ai (web)** | ‚úÖ Full | |\n| **Claude Desktop** | üü° Partial | Tools may need to be reloaded manually in the client. |\n| **VS Code (Genie)** | ‚úÖ Full | |\n| **LibreChat** | ‚ùì Untested | |\n| **Apify Tester MCP Client** | ‚úÖ Full | Designed for testing Apify MCP servers. |\n\n*This matrix is a work in progress. If you have tested other clients, please consider contributing to this documentation.*\n\n# ü™Ñ Try Apify MCP Instantly\n\nWant to try Apify MCP without any setup?\n\nCheck out [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n\nThis interactive, chat-like interface provides an easy way to explore the capabilities of Apify MCP without any local setup.\nJust sign in with your Apify account and start experimenting with web scraping, data extraction, and automation tools!\n\nOr use the Anthropic Desktop extension file (dxt) for one-click installation: [Apify MCP server dxt file](https://github.com/apify/actors-mcp-server/releases/latest/download/actors-mcp-server.dxt)\n\n# üõ†Ô∏è Tools, resources, and prompts\n\nThe MCP server provides a set of tools for interacting with Apify Actors.\nSince the Apify Store is large and growing rapidly, the MCP server provides a way to dynamically discover and use new Actors.\n\n### Actors\n\nAny [Apify Actor](https://apify.com/store) can be used as a tool.\nBy default, the server is pre-configured with one Actor, `apify/rag-web-browser`, and several helper tools.\nThe MCP server loads an Actor's input schema and creates a corresponding MCP tool.\nThis allows the AI agent to know exactly what arguments to pass to the Actor and what to expect in return.\n\n\nFor example, for the `apify/rag-web-browser` Actor, the input parameters are:\n\n```json\n{\n  \"query\": \"restaurants in San Francisco\",\n  \"maxResults\": 3\n}\n```\nYou don't need to manually specify which Actor to call or its input parameters; the LLM handles this automatically.\nWhen a tool is called, the arguments are automatically passed to the Actor by the LLM.\nYou can refer to the specific Actor's documentation for a list of available arguments.\n\n### Helper tools\n\nOne of the most powerful features of using MCP with Apify is dynamic tool discovery.\nIt gives an AI agent the ability to find new tools (Actors) as needed and incorporate them.\nHere are some special MCP operations and how the Apify MCP Server supports them:\n\n- **Actor discovery and management**: Search for Actors, view their details, and dynamically add or remove them as available tools for the AI.\n- **Apify documentation**: Search the Apify documentation and fetch specific documents to provide context to the AI.\n- **Actor runs (*)**: Get lists of your Actor runs, inspect their details, and retrieve logs.\n- **Apify storage (*)**: Access data from your datasets and key-value stores.\n\n**Note**: Helper tool categories marked with (*) are not enabled by default in the MCP server and must be explicitly enabled using the `tools` argument (either the `--tools` command line argument for the stdio server or the `?tools` URL query parameter for the remote MCP server). The `tools` argument is a comma-separated list of categories with the following possible values:\n\n- `docs`: Search and fetch Apify documentation tools.\n- `runs`: Get Actor run lists, run details, and logs from a specific Actor run.\n- `storage`: Access datasets, key-value stores, and their records.\n- `preview`: Experimental tools in preview mode.\n\nFor example, to enable all tools, use `npx @apify/actors-mcp-server --tools docs,runs,storage,preview` or `https://mcp.apify.com/?tools=docs,runs,storage,preview`.\n\n### Overview of available tools\n\nHere is an overview list of all the tools provided by the Apify MCP Server.\n\n| Tool name | Category | Description | Enabled by default |\n| :--- | :--- | :--- | :---: |\n| `get-actor-details` | default | Retrieve detailed information about a specific Actor. | ‚úÖ |\n| `search-actors` | default | Search for Actors in the Apify Store. | ‚úÖ |\n| `add-actor` | default | Add an Actor as a new tool for the user to call. | ‚úÖ |\n| [`apify-slash-rag-web-browser`](https://apify.com/apify/rag-web-browser) | default | An Actor tool to browse the web. | ‚úÖ |\n| `search-apify-docs` | docs | Search the Apify documentation for relevant pages. | ‚úÖ |\n| `fetch-apify-docs` | docs | Fetch the full content of an Apify documentation page by its URL. | ‚úÖ |\n| `call-actor` | preview | Call an Actor and get its run results. |  |\n| `get-actor-run` | runs | Get detailed information about a specific Actor run. |  |\n| `get-actor-run-list` | runs | Get a list of an Actor's runs, filterable by status. |  |\n| `get-actor-log` | runs | Retrieve the logs for a specific Actor run. |  |\n| `get-dataset` | storage | Get metadata about a specific dataset. |  |\n| `get-dataset-items` | storage | Retrieve items from a dataset with support for filtering and pagination. |  |\n| `get-key-value-store` | storage | Get metadata about a specific key-value store. |  |\n| `get-key-value-store-keys`| storage | List the keys within a specific key-value store. |  |\n| `get-key-value-store-record`| storage | Get the value associated with a specific key in a key-value store. |  |\n| `get-dataset-list` | storage | List all available datasets for the user. |  |\n| `get-key-value-store-list`| storage | List all available key-value stores for the user. |  |\n\n### Prompts\n\nThe server provides a set of predefined example prompts to help you get started interacting with Apify through MCP. For example, there is a `GetLatestNewsOnTopic` prompt that allows you to easily retrieve the latest news on a specific topic using the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor.\n\n### Resources\n\nThe server does not yet provide any resources.\n\n### Debugging the NPM package\n\nTo debug the server, use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) tool:\n\n```shell\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector npx -y @apify/actors-mcp-server\n```\n\n# ‚öôÔ∏è Development\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/en) (v18 or higher)\n\nCreate an environment file, `.env`, with the following content:\n```text\nAPIFY_TOKEN=\"your-apify-token\"\n```\n\nBuild the `actor-mcp-server` package:\n\n```bash\nnpm run build\n```\n\n## Start HTTP streamable MCP server\n\nRun using Apify CLI:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nexport APIFY_META_ORIGIN=STANDBY\napify run -p\n```\n\nOnce the server is running, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to debug the server exposed at `http://localhost:3001`.\n\n## Start standard input/output (stdio) MCP server\n\nYou can launch the MCP Inspector with this command:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector node ./dist/stdio.js\n```\n\nUpon launching, the Inspector will display a URL that you can open in your browser to begin debugging.\n\n## üê¶ Canary PR releases\n\nDue to the current architecture where Apify MCP is split across two repositories, this one containing the core MCP logic and the private [apify-mcp-server](https://github.com/apify/apify-mcp-server) repository that handles the actual server implementation for [mcp.apify.com](https://mcp.apify.com), development can be challenging as changes need to be synchronized between both repositories.\n\nYou can create a canary release from your PR branch by adding the `beta` tag. This will test the code and publish the package to [pkg.pr.new](https://pkg.pr.new/) which you can then use, for example, in a staging environment to test before actually merging the changes. This way we do not need to create new NPM releases and keep the NPM versions cleaner. The workflow runs whenever you commit to a PR branch that has the `beta` tag or when you add the `beta` tag to an already existing PR. For more details check out [the workflow file](.github/workflows/pre_release.yaml).\n\n# üêõ Troubleshooting (local MCP server)\n\n- Make sure you have `node` installed by running `node -v`.\n- Make sure the `APIFY_TOKEN` environment variable is set.\n- Always use the latest version of the MCP server by using `@apify/actors-mcp-server@latest`.\n\n## üí° Limitations\n\nThe Actor input schema is processed to be compatible with most MCP clients while adhering to [JSON Schema](https://json-schema.org/) standards. The processing includes:\n- **Descriptions** are truncated to 500 characters (as defined in `MAX_DESCRIPTION_LENGTH`).\n- **Enum fields** are truncated to a maximum combined length of 200 characters for all elements (as defined in `ACTOR_ENUM_MAX_LENGTH`).\n- **Required fields** are explicitly marked with a `REQUIRED` prefix in their descriptions for compatibility with frameworks that may not handle the JSON schema properly.\n- **Nested properties** are built for special cases like proxy configuration and request list sources to ensure the correct input structure.\n- **Array item types** are inferred when not explicitly defined in the schema, using a priority order: explicit type in items > prefill type > default value type > editor type.\n- **Enum values and examples** are added to property descriptions to ensure visibility, even if the client doesn't fully support the JSON schema.\n- **Rental Actors** are only available for use with the hosted MCP server at https://mcp.apify.com. When running the server locally via stdio, you can only access Actors that are already added to your local toolset. To dynamically search for and use any Actor from the Apify Store‚Äîincluding rental Actors‚Äîconnect to the hosted endpoint.\n\n \n\n# ü§ù Contributing\n\nWe welcome contributions to improve the Apify MCP Server! Here's how you can help:\n\n- **üêõ Report issues**: Find a bug or have a feature request? [Open an issue](https://github.com/apify/actors-mcp-server/issues).\n- **üîß Submit pull requests**: Fork the repo and submit pull requests with enhancements or fixes.\n- **üìö Documentation**: Improvements to docs and examples are always welcome.\n- **üí° Share use cases**: Contribute examples to help other users.\n\nFor major changes, please open an issue first to discuss your proposal and ensure it aligns with the project's goals.\n\n# üìö Learn more\n\n- [Model Context Protocol](https://modelcontextprotocol.org/)\n- [What are AI Agents?](https://blog.apify.com/what-are-ai-agents/)\n- [What is MCP and why does it matter?](https://blog.apify.com/what-is-model-context-protocol/)\n- [How to use MCP with Apify Actors](https://blog.apify.com/how-to-use-mcp/)\n- [Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- [Webinar: Building and Monetizing MCP Servers on Apify](https://www.youtube.com/watch?v=w3AH3jIrXXo)\n- [MCP Client development guide](https://github.com/cyanheads/model-context-protocol-resources/blob/main/guides/mcp-client-development-guide.md)\n- [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect AI applications and agents to thousands of Apify Actors for web scraping, data extraction, and automation",
        "Dynamically discover and use new Apify Actors as tools",
        "Search for Apify Actors and view their details",
        "Add or remove Apify Actors as available tools for AI assistants",
        "Search and fetch Apify documentation pages",
        "Retrieve and inspect Actor run details and logs",
        "Access Apify storage including datasets and key-value stores",
        "Support OAuth authentication for secure connections",
        "Provide predefined example prompts for common tasks like retrieving latest news"
      ],
      "limitations": [
        "Helper tool categories for Actor runs, storage, and preview are not enabled by default and require explicit enabling",
        "Some MCP clients may have partial support or require manual tool reloads",
        "No resources are currently provided by the server",
        "Requires an Apify API token for authentication and usage"
      ],
      "requirements": [
        "Node.js version 18 or higher for local server usage",
        "Apify API token for authentication when connecting to the MCP server",
        "OAuth support for connecting from clients like Claude.ai or Visual Studio Code",
        "For local usage, environment variable APIFY_TOKEN must be set"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, client compatibility matrix, limitations, and requirements, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<h1 align=\"center\">\n    <a href=\"https://mcp.apify.com\">\n        <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/apify_mcp_server_dark_background.png\">\n            <img alt=\"Apify MCP Server\" src=\"docs/apify_mcp_server_white_background.png\" width=\"500\">\n        </picture>\n    </a>\n    <br>\n    <small><a href=\"https://mcp.apify.com\">mcp.apify.com</a></small>\n</h1>\n\n<p align=center>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" alt=\"NPM latest version\" data-canonical-src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml\"><img src=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://apify.com/apify/actors-mcp-server\"><img src=\"https://apify.com/actor-badge?actor=apify/actors-mcp-server\" alt=\"Actor runs\" style=\"max-width: 100%;\"></a>\n</p>\n\nThe Apify Model Context Protocol (MCP) Server at **mcp.apify.com** instantly connects AI applications and agents to thousands of ready‚Äëbuilt tools. It allows your AI assistant to use any [Apify Actor](https://apify.com/store) for web scraping, data extraction, and automation tasks in real time.",
        "start_pos": 0,
        "end_pos": 1700,
        "token_count_estimate": 425,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 1,
        "text": "m Apify Store, connect your AI assistant to our hosted server:\n>\n> **[`https://mcp.apify.com`](https://mcp.apify.com)**\n>\n> It supports OAuth, so you can connect from clients like Claude.ai or Visual Studio Code with just the URL.\n\n![Actors-MCP-server](docs/actors-mcp-server.png)\n\n## Table of Contents\n- [üåê Introducing the Apify MCP server](#-introducing-the-apify-mcp-server)\n- [üöÄ Quickstart](#-quickstart)\n- [ü§ñ MCP clients and examples](#-mcp-clients-and-examples)\n- [ü™Ñ Try Apify MCP instantly](#-try-apify-mcp-instantly)\n- [üõ†Ô∏è Tools, resources, and prompts](#-tools-resources-and-prompts)\n- [üêõ Troubleshooting (local MCP server)](#-troubleshooting-local-mcp-server)\n- [‚öôÔ∏è Development](#-development)\n- [ü§ù Contributing](#-contributing)\n- [üìö Learn more](#-learn-more)\n \n\n# üåê Introducing the Apify MCP server\n\nThe Apify MCP Server allows an AI assistant to use any [Apify Actor](https://apify.com/store) as a tool to perform a specific task.\nFor example, it can:\n- Use [Facebook Posts Scraper](https://apify.com/apify/facebook-posts-scraper) to extract data from Facebook posts from multiple pages/profiles.\n- Use [Google Maps Email Extractor](https://apify.com/lukaskrivka/google-maps-with-contact-details) to extract contact details from Google Maps.\n- Use [Google Search Results Scraper](https://apify.com/apify/google-search-scraper) to scrape Google Search Engine Results Pages (SERPs).\n- Use [Instagram Scraper](https://apify.com/apify/instagram-scraper) to scrape Instagram posts, profiles, places, photos, and comments.\n- Use [RAG Web Browser](https://apify.com/apify/web-scraper) to search the web, scrape the top N URLs, and return their content.",
        "start_pos": 1848,
        "end_pos": 3505,
        "token_count_estimate": 414,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 2,
        "text": "H91uCTg/hqdefault.jpg)](https://www.youtube.com/watch?v=BKu8H91uCTg)\n\n# üöÄ Quickstart\n\nYou can use the Apify MCP Server in two ways:\n\n**HTTPS Endpoint (mcp.apify.com)**: Connect from your MCP client via OAuth or by including the `Authorization: Bearer <APIFY_TOKEN>` header in your requests. This is the recommended method for most use cases. Because it supports OAuth, you can connect from clients like [Claude.ai](https://claude.ai) or [Visual Studio Code](https://code.visualstudio.com/) using just the URL: `https://mcp.apify.com`.\n- `https://mcp.apify.com` (recommended) for streamable transport\n- `https://mcp.apify.com/sse` for legacy SSE transport\n\n**Standard Input/Output (stdio)**: Ideal for local integrations and command-line tools like the Claude for Desktop client.\n- Set the MCP client server command to `npx @apify/actors-mcp-server` and the `APIFY_TOKEN` environment variable to your Apify API token.\n- See `npx @apify/actors-mcp-server --help` for more options.\n\nYou can find detailed instructions for setting up the MCP server in the [Apify documentation](https://docs.apify.com/platform/integrations/mcp).",
        "start_pos": 3696,
        "end_pos": 4820,
        "token_count_estimate": 281,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 3,
        "text": "the Instagram profile of The Rock.\"\n- \"Provide a step-by-step guide on using the Model Context Protocol, including source URLs.\"\n- \"What Apify Actors can I use?\"\n\n### Supported Clients Matrix\n\nThe following table outlines the tested MCP clients and their level of support for key features.\n\n| Client | Dynamic Tool Discovery | Notes |\n| --- | --- | --- |\n| **Claude.ai (web)** | ‚úÖ Full | |\n| **Claude Desktop** | üü° Partial | Tools may need to be reloaded manually in the client. |\n| **VS Code (Genie)** | ‚úÖ Full | |\n| **LibreChat** | ‚ùì Untested | |\n| **Apify Tester MCP Client** | ‚úÖ Full | Designed for testing Apify MCP servers. |\n\n*This matrix is a work in progress. If you have tested other clients, please consider contributing to this documentation.*\n\n# ü™Ñ Try Apify MCP Instantly\n\nWant to try Apify MCP without any setup?\n\nCheck out [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n\nThis interactive, chat-like interface provides an easy way to explore the capabilities of Apify MCP without any local setup.\nJust sign in with your Apify account and start experimenting with web scraping, data extraction, and automation tools!\n\nOr use the Anthropic Desktop extension file (dxt) for one-click installation: [Apify MCP server dxt file](https://github.com/apify/actors-mcp-server/releases/latest/download/actors-mcp-server.dxt)\n\n# üõ†Ô∏è Tools, resources, and prompts\n\nThe MCP server provides a set of tools for interacting with Apify Actors.\nSince the Apify Store is large and growing rapidly, the MCP server provides a way to dynamically discover and use new Actors.\n\n### Actors\n\nAny [Apify Actor](https://apify.com/store) can be used as a tool.\nBy default, the server is pre-configured with one Actor, `apify/rag-web-browser`, and several helper tools.\nThe MCP server loads an Actor's input schema and creates a corresponding MCP tool.\nThis allows the AI agent to know exactly what arguments to pass to the Actor and what to expect in return.",
        "start_pos": 5544,
        "end_pos": 7514,
        "token_count_estimate": 492,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 4,
        "text": "per tools.\nThe MCP server loads an Actor's input schema and creates a corresponding MCP tool.\nThis allows the AI agent to know exactly what arguments to pass to the Actor and what to expect in return.\n\n\nFor example, for the `apify/rag-web-browser` Actor, the input parameters are:\n\n```json\n{\n  \"query\": \"restaurants in San Francisco\",\n  \"maxResults\": 3\n}\n```\nYou don't need to manually specify which Actor to call or its input parameters; the LLM handles this automatically.\nWhen a tool is called, the arguments are automatically passed to the Actor by the LLM.\nYou can refer to the specific Actor's documentation for a list of available arguments.\n\n### Helper tools\n\nOne of the most powerful features of using MCP with Apify is dynamic tool discovery.\nIt gives an AI agent the ability to find new tools (Actors) as needed and incorporate them.\nHere are some special MCP operations and how the Apify MCP Server supports them:\n\n- **Actor discovery and management**: Search for Actors, view their details, and dynamically add or remove them as available tools for the AI.\n- **Apify documentation**: Search the Apify documentation and fetch specific documents to provide context to the AI.\n- **Actor runs (*)**: Get lists of your Actor runs, inspect their details, and retrieve logs.\n- **Apify storage (*)**: Access data from your datasets and key-value stores.\n\n**Note**: Helper tool categories marked with (*) are not enabled by default in the MCP server and must be explicitly enabled using the `tools` argument (either the `--tools` command line argument for the stdio server or the `?tools` URL query parameter for the remote MCP server). The `tools` argument is a comma-separated list of categories with the following possible values:\n\n- `docs`: Search and fetch Apify documentation tools.\n- `runs`: Get Actor run lists, run details, and logs from a specific Actor run.\n- `storage`: Access datasets, key-value stores, and their records.\n- `preview`: Experimental tools in preview mode.",
        "start_pos": 7314,
        "end_pos": 9302,
        "token_count_estimate": 497,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 5,
        "text": "ols.\n- `runs`: Get Actor run lists, run details, and logs from a specific Actor run.\n- `storage`: Access datasets, key-value stores, and their records.\n- `preview`: Experimental tools in preview mode.\n\nFor example, to enable all tools, use `npx @apify/actors-mcp-server --tools docs,runs,storage,preview` or `https://mcp.apify.com/?tools=docs,runs,storage,preview`.\n\n### Overview of available tools\n\nHere is an overview list of all the tools provided by the Apify MCP Server.\n\n| Tool name | Category | Description | Enabled by default |\n| :--- | :--- | :--- | :---: |\n| `get-actor-details` | default | Retrieve detailed information about a specific Actor. | ‚úÖ |\n| `search-actors` | default | Search for Actors in the Apify Store. | ‚úÖ |\n| `add-actor` | default | Add an Actor as a new tool for the user to call. | ‚úÖ |\n| [`apify-slash-rag-web-browser`](https://apify.com/apify/rag-web-browser) | default | An Actor tool to browse the web. | ‚úÖ |\n| `search-apify-docs` | docs | Search the Apify documentation for relevant pages. | ‚úÖ |\n| `fetch-apify-docs` | docs | Fetch the full content of an Apify documentation page by its URL. | ‚úÖ |\n| `call-actor` | preview | Call an Actor and get its run results. |  |\n| `get-actor-run` | runs | Get detailed information about a specific Actor run. |  |\n| `get-actor-run-list` | runs | Get a list of an Actor's runs, filterable by status. |  |\n| `get-actor-log` | runs | Retrieve the logs for a specific Actor run. |  |\n| `get-dataset` | storage | Get metadata about a specific dataset. |  |\n| `get-dataset-items` | storage | Retrieve items from a dataset with support for filtering and pagination. |  |\n| `get-key-value-store` | storage | Get metadata about a specific key-value store. |  |\n| `get-key-value-store-keys`| storage | List the keys within a specific key-value store. |  |\n| `get-key-value-store-record`| storage | Get the value associated with a specific key in a key-value store. |  |\n| `get-dataset-list` | storage | List all available datasets for the user.",
        "start_pos": 9102,
        "end_pos": 11111,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 6,
        "text": "store. |  |\n| `get-key-value-store-record`| storage | Get the value associated with a specific key in a key-value store. |  |\n| `get-dataset-list` | storage | List all available datasets for the user. |  |\n| `get-key-value-store-list`| storage | List all available key-value stores for the user. |  |\n\n### Prompts\n\nThe server provides a set of predefined example prompts to help you get started interacting with Apify through MCP. For example, there is a `GetLatestNewsOnTopic` prompt that allows you to easily retrieve the latest news on a specific topic using the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor.\n\n### Resources\n\nThe server does not yet provide any resources.\n\n### Debugging the NPM package\n\nTo debug the server, use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) tool:\n\n```shell\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector npx -y @apify/actors-mcp-server\n```\n\n# ‚öôÔ∏è Development\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/en) (v18 or higher)\n\nCreate an environment file, `.env`, with the following content:\n```text\nAPIFY_TOKEN=\"your-apify-token\"\n```\n\nBuild the `actor-mcp-server` package:\n\n```bash\nnpm run build\n```\n\n## Start HTTP streamable MCP server\n\nRun using Apify CLI:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nexport APIFY_META_ORIGIN=STANDBY\napify run -p\n```\n\nOnce the server is running, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to debug the server exposed at `http://localhost:3001`.\n\n## Start standard input/output (stdio) MCP server\n\nYou can launch the MCP Inspector with this command:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector node ./dist/stdio.js\n```\n\nUpon launching, the Inspector will display a URL that you can open in your browser to begin debugging.",
        "start_pos": 10911,
        "end_pos": 12761,
        "token_count_estimate": 462,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 7,
        "text": "port APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector node ./dist/stdio.js\n```\n\nUpon launching, the Inspector will display a URL that you can open in your browser to begin debugging.\n\n## üê¶ Canary PR releases\n\nDue to the current architecture where Apify MCP is split across two repositories, this one containing the core MCP logic and the private [apify-mcp-server](https://github.com/apify/apify-mcp-server) repository that handles the actual server implementation for [mcp.apify.com](https://mcp.apify.com), development can be challenging as changes need to be synchronized between both repositories.\n\nYou can create a canary release from your PR branch by adding the `beta` tag. This will test the code and publish the package to [pkg.pr.new](https://pkg.pr.new/) which you can then use, for example, in a staging environment to test before actually merging the changes. This way we do not need to create new NPM releases and keep the NPM versions cleaner. The workflow runs whenever you commit to a PR branch that has the `beta` tag or when you add the `beta` tag to an already existing PR. For more details check out [the workflow file](.github/workflows/pre_release.yaml).\n\n# üêõ Troubleshooting (local MCP server)\n\n- Make sure you have `node` installed by running `node -v`.\n- Make sure the `APIFY_TOKEN` environment variable is set.\n- Always use the latest version of the MCP server by using `@apify/actors-mcp-server@latest`.\n\n## üí° Limitations\n\nThe Actor input schema is processed to be compatible with most MCP clients while adhering to [JSON Schema](https://json-schema.org/) standards. The processing includes:\n- **Descriptions** are truncated to 500 characters (as defined in `MAX_DESCRIPTION_LENGTH`).\n- **Enum fields** are truncated to a maximum combined length of 200 characters for all elements (as defined in `ACTOR_ENUM_MAX_LENGTH`).\n- **Required fields** are explicitly marked with a `REQUIRED` prefix in their descriptions for compatibility with frameworks that may not handle the JSON schema properly.",
        "start_pos": 12561,
        "end_pos": 14599,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 8,
        "text": "in `ACTOR_ENUM_MAX_LENGTH`).\n- **Required fields** are explicitly marked with a `REQUIRED` prefix in their descriptions for compatibility with frameworks that may not handle the JSON schema properly.\n- **Nested properties** are built for special cases like proxy configuration and request list sources to ensure the correct input structure.\n- **Array item types** are inferred when not explicitly defined in the schema, using a priority order: explicit type in items > prefill type > default value type > editor type.\n- **Enum values and examples** are added to property descriptions to ensure visibility, even if the client doesn't fully support the JSON schema.\n- **Rental Actors** are only available for use with the hosted MCP server at https://mcp.apify.com. When running the server locally via stdio, you can only access Actors that are already added to your local toolset. To dynamically search for and use any Actor from the Apify Store‚Äîincluding rental Actors‚Äîconnect to the hosted endpoint.\n\n \n\n# ü§ù Contributing\n\nWe welcome contributions to improve the Apify MCP Server! Here's how you can help:\n\n- **üêõ Report issues**: Find a bug or have a feature request? [Open an issue](https://github.com/apify/actors-mcp-server/issues).\n- **üîß Submit pull requests**: Fork the repo and submit pull requests with enhancements or fixes.\n- **üìö Documentation**: Improvements to docs and examples are always welcome.\n- **üí° Share use cases**: Contribute examples to help other users.\n\nFor major changes, please open an issue first to discuss your proposal and ensure it aligns with the project's goals.",
        "start_pos": 14399,
        "end_pos": 15994,
        "token_count_estimate": 398,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      },
      {
        "chunk_id": 9,
        "text": "use MCP with Apify Actors](https://blog.apify.com/how-to-use-mcp/)\n- [Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- [Webinar: Building and Monetizing MCP Servers on Apify](https://www.youtube.com/watch?v=w3AH3jIrXXo)\n- [MCP Client development guide](https://github.com/cyanheads/model-context-protocol-resources/blob/main/guides/mcp-client-development-guide.md)\n- [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)",
        "start_pos": 16247,
        "end_pos": 16735,
        "token_count_estimate": 121,
        "source_type": "readme",
        "agent_id": "222a479a4ea614b6"
      }
    ]
  },
  {
    "agent_id": "ef74dc6dae592fe2",
    "name": "ai.smithery/jjlabsio-korea-stock-mcp",
    "source": "mcp",
    "source_url": "https://github.com/jjlabsio/korea-stock-mcp",
    "description": "Search company disclosures and financial statements from the Korean market. Retrieve stock profile‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-19T09:01:49.012754Z",
    "indexed_at": "2026-02-18T04:07:17.831744",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Korea Stock MCP Server\n\n[üá∞üá∑ ÌïúÍµ≠Ïñ¥](#korea-stock-mcp-server) | [üá∫üá∏ English](#english-version)\n\nÌïúÍµ≠ Ï£ºÏãù Î∂ÑÏÑùÏùÑ ÏúÑÌïú MCP ÏÑúÎ≤ÑÏûÖÎãàÎã§.  \nDART(Ï†ÑÏûêÍ≥µÏãúÏãúÏä§ÌÖú)ÏôÄ KRX(ÌïúÍµ≠Í±∞ÎûòÏÜå) Í≥µÏãù APIÎ•º ÌÜµÌï¥ Ï£ºÍ∞Ä Ï†ïÎ≥¥ÏôÄ Í≥µÏãú ÏûêÎ£å Í∏∞Î∞òÏùò AIÎ∂ÑÏÑùÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\n\n## üéØ Ï£ºÏöî Í∏∞Îä•\n\n- üîç **Í≥µÏãúÍ≤ÄÏÉâ** - ÌöåÏÇ¨Î≥Ñ, Í∏∞Í∞ÑÎ≥Ñ Í≥µÏãú Í≤ÄÏÉâ\n- üìä **Í≥µÏãú Îç∞Ïù¥ÌÑ∞** - Í≥µÏãúÎ≥¥Í≥†ÏÑú ÏõêÎ≥∏ÌååÏùº ÌååÏã±Ìïú Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ\n- üíº **Ïû¨Î¨¥Ï†úÌëú Î∂ÑÏÑù** - XBRL Í∏∞Î∞ò ÏÉÅÏÑ∏ Ïû¨Î¨¥ Îç∞Ïù¥ÌÑ∞\n- üìà **Ï£ºÏãù Îç∞Ïù¥ÌÑ∞** - KRX(ÏΩîÏä§Ìîº/ÏΩîÏä§Îã•) ÏùºÎ≥Ñ Ï£ºÍ∞ÄÏ†ïÎ≥¥, Ï¢ÖÎ™© Í∏∞Î≥∏Ï†ïÎ≥¥\n\n## ‚ö° Îπ†Î•∏ ÏãúÏûë\n\n### 1Ô∏è‚É£ API KEY Î∞úÍ∏â\n\nÎ®ºÏ†Ä DARTÏôÄ KRXÏùò API KEYÎ•º Î∞úÍ∏âÎ∞õÏïÑÏïº Ìï©ÎãàÎã§.\n\n#### üìù DART API KEY Î∞úÍ∏â\n\n1. **ÌöåÏõêÍ∞ÄÏûÖ**: [OPEN DART](https://opendart.fss.or.kr) ÌöåÏõêÍ∞ÄÏûÖ\n2. **ÌÇ§ Ïã†Ï≤≠**: [Ïù∏Ï¶ùÌÇ§ Ïã†Ï≤≠ ÌéòÏù¥ÏßÄ](https://opendart.fss.or.kr/uss/umt/EgovMberInsertView.do)ÏóêÏÑú API KEY Ïã†Ï≤≠\n3. **ÌÇ§ ÌôïÏù∏**: [Ïò§ÌîàAPI Ïù¥Ïö©ÌòÑÌô©](https://opendart.fss.or.kr/mng/apiUsageStatusView.do)ÏóêÏÑú Î∞úÍ∏âÎêú API KEY ÌôïÏù∏\n\n#### üìà KRX API KEY Î∞úÍ∏â\n\n1. **ÌöåÏõêÍ∞ÄÏûÖ**: [KRX OPEN API](https://openapi.krx.co.kr/contents/OPP/MAIN/main/index.cmd)ÏóêÏÑú ÌöåÏõêÍ∞ÄÏûÖ Î∞è Î°úÍ∑∏Ïù∏\n2. **ÌÇ§ Ïã†Ï≤≠**: ÎßàÏù¥ÌéòÏù¥ÏßÄ ‚Üí API Ïù∏Ï¶ùÌÇ§ Ïã†Ï≤≠ÏóêÏÑú Ïã†Ï≤≠\n3. **ÏÑúÎπÑÏä§ Ïã†Ï≤≠**: ÏäπÏù∏ ÌõÑ ÏÑúÎπÑÏä§Ïù¥Ïö© ‚Üí Ï£ºÏãù Î©îÎâ¥Î°ú Ïù¥Îèô\n4. **API Ïù¥Ïö©Ïã†Ï≤≠**: Îã§Ïùå 6Í∞ú Ìï≠Î™©ÏóêÏÑú Í∞ÅÍ∞Å \"API Ïù¥Ïö©Ïã†Ï≤≠\" ÌÅ¥Î¶≠\n\n   - Ïú†Í∞ÄÏ¶ùÍ∂å ÏùºÎ≥ÑÎß§Îß§Ï†ïÎ≥¥\n   - ÏΩîÏä§Îã• ÏùºÎ≥ÑÎß§Îß§Ï†ïÎ≥¥\n   - ÏΩîÎÑ•Ïä§ ÏùºÎ≥ÑÎß§Îß§Ï†ïÎ≥¥\n   - Ïú†Í∞ÄÏ¶ùÍ∂å Ï¢ÖÎ™©Í∏∞Î≥∏Ï†ïÎ≥¥\n   - ÏΩîÏä§Îã• Ï¢ÖÎ™©Í∏∞Î≥∏Ï†ïÎ≥¥\n   - ÏΩîÎÑ•Ïä§ Ï¢ÖÎ™©Í∏∞Î≥∏Ï†ïÎ≥¥\n\n   > ‚è±Ô∏è **ÏäπÏù∏ÍπåÏßÄ ÏïΩ 1Ïùº ÏÜåÏöîÎê©ÎãàÎã§.**\n\n5. **ÌÇ§ ÌôïÏù∏**: ÏäπÏù∏ ÌõÑ ÎßàÏù¥ÌéòÏù¥ÏßÄ ‚Üí API Ïù∏Ï¶ùÌÇ§ Î∞úÍ∏âÎÇ¥Ïó≠ÏóêÏÑú API KEY ÌôïÏù∏\n\n### 2Ô∏è‚É£ Claude Desktop ÏÑ§Ï†ï\n\n1. **Claude Desktop** Ïã§Ìñâ\n2. **ÏÑ§Ï†ï** ‚Üí **Í∞úÎ∞úÏûê** ‚Üí **Íµ¨ÏÑ±Ìé∏Ïßë** ÌÅ¥Î¶≠\n3. `claude_desktop_config.json` ÌååÏùºÏóê Îã§Ïùå ÎÇ¥Ïö© Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"korea-stock-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"korea-stock-mcp@latest\"],\n      \"env\": {\n        \"DART_API_KEY\": \"<YOUR_DART_API_KEY>\",\n        \"KRX_API_KEY\": \"<YOUR_KRX_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n4. **Ïû¨ÏãúÏûë**: Claude DesktopÏùÑ Ïû¨ÏãúÏûëÌïòÏó¨ ÏÑ§Ï†ï Ï†ÅÏö©\n\n> Ïù¥Ï†ú ClaudeÏóêÏÑú ÌïúÍµ≠ Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÏùÑ ÏãúÏûëÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨\n\n### DART (Ï†ÑÏûêÍ≥µÏãúÏãúÏä§ÌÖú)\n\n1. **get_disclosure_list** - Í≥µÏãúÍ≤ÄÏÉâ\n\n   - Í≥µÏãú Ïú†ÌòïÎ≥Ñ, ÌöåÏÇ¨Î≥Ñ, ÎÇ†ÏßúÎ≥Ñ Í≥µÏãúÎ≥¥Í≥†ÏÑú Í≤ÄÏÉâ\n\n2. **get_corp_code** - Í≥†Ïú†Î≤àÌò∏ Ï°∞Ìöå\n\n   - DART Îì±Î°ù Í≥µÏãúÎåÄÏÉÅÌöåÏÇ¨Ïùò Í≥†Ïú†Î≤àÌò∏, ÌöåÏÇ¨Î™Ö, Ï¢ÖÎ™©ÏΩîÎìú Ï†úÍ≥µ\n\n3. **get_disclosure** - Í≥µÏãúÎ≥¥Í≥†ÏÑú ÏõêÎ¨∏\n\n   - DART APIÎ•º ÌÜµÌïú Í≥µÏãúÎ≥¥Í≥†ÏÑú ÏõêÎ≥∏ÌååÏùº ÌååÏã±\n\n4. **get_financial_statement** - Ïû¨Î¨¥Ï†úÌëú\n   - ÏÉÅÏû•Î≤ïÏù∏ Î∞è Ï£ºÏöî ÎπÑÏÉÅÏû•Î≤ïÏù∏ XBRL Ïû¨Î¨¥Ï†úÌëú\n   - Ï†ïÍ∏∞Î≥¥Í≥†ÏÑú ÎÇ¥ Î™®Îì† Í≥ÑÏ†ïÍ≥ºÎ™© Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ\n\n### KRX (ÌïúÍµ≠Í±∞ÎûòÏÜå)\n\n1. **get_stock_base_info** - Ï¢ÖÎ™© Í∏∞Î≥∏Ï†ïÎ≥¥\n\n   - ÏΩîÏä§Ìîº, ÏΩîÏä§Îã•, ÏΩîÎÑ•Ïä§ ÏÉÅÏû• Ï¢ÖÎ™© Í∏∞Î≥∏ Ï†ïÎ≥¥\n   - Ï¢ÖÎ™©Î™Ö, Ï¢ÖÎ™©ÏΩîÎìú, ÏãúÏû•Íµ¨Î∂Ñ Îì± Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞\n\n2. **get_stock_trade_info** - ÏùºÎ≥Ñ Îß§Îß§Ï†ïÎ≥¥\n   - ÏΩîÏä§Ìîº, ÏΩîÏä§Îã•, ÏΩîÎÑ•Ïä§ Ï¢ÖÎ™©Î≥Ñ ÏùºÎ≥Ñ Í±∞Îûò Îç∞Ïù¥ÌÑ∞\n   - Ï£ºÍ∞Ä, Í±∞ÎûòÎüâ, ÏãúÍ∞ÄÏ¥ùÏï° Îì± ÏÉÅÏÑ∏ Í±∞Îûò Ï†ïÎ≥¥\n\n3. **get_market_type** - ÏãúÏû•Íµ¨Î∂Ñ Ï°∞Ìöå\n   - Ï¢ÖÎ™©ÏΩîÎìúÎ°ú Ìï¥Îãπ Ï¢ÖÎ™©Ïùò ÏãúÏû•Íµ¨Î∂Ñ(ÏΩîÏä§Ìîº/ÏΩîÏä§Îã•/ÏΩîÎÑ•Ïä§) Ï°∞Ìöå\n   - Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Ïãú ÌïÑÏöîÌïú ÏãúÏû• Ï†ïÎ≥¥ Ï†úÍ≥µ\n\n### Í∏∞ÌÉÄ ÎèÑÍµ¨\n\n1. **get_today_date** - Ïò§Îäò ÎÇ†Ïßú Ï°∞Ìöå\n   - ÌòÑÏû¨ ÎÇ†ÏßúÎ•º YYYYMMDD ÌòïÏãùÏúºÎ°ú Ï†úÍ≥µ\n   - AIÏùò Ï†ïÌôïÌïú ÎÇ†Ïßú Ï°∞ÌöåÎ•º ÏúÑÌïú ÎèÑÍµ¨\n\n## Ïã§Ï†ú ÏÇ¨Ïö© ÏòàÏãú\n\n### üìä Ïû¨Î¨¥ Î∂ÑÏÑù ÏòàÏ†ú\n\n**ÌîÑÎ°¨ÌîÑÌä∏**: \"ÏÇºÏñëÏãùÌíàÏùò 2023ÎÖÑ, 2024ÎÖÑ 1~4Î∂ÑÍ∏∞, 2025ÎÖÑ 1,2Î∂ÑÍ∏∞ Îß§Ï∂ú, ÏòÅÏóÖÏù¥Ïùµ Ï°∞ÏÇ¨Ìï¥Ï£ºÍ≥† ÏÑ±Ïû•Î•†ÎèÑ Ï°∞ÏÇ¨Ìï¥Ï§ò\"  \n‚Üí [ÏÇºÏñëÏãùÌíà Î∂ÑÏÑù Í≤∞Í≥º Î≥¥Í∏∞](./example/ÏÇºÏñëÏãùÌíà.md)\n\n**ÌîÑÎ°¨ÌîÑÌä∏**: \"ÏóêÏù¥ÌîºÏïåÏùò 23ÎÖÑ 1Î∂ÑÍ∏∞Î∂ÄÌÑ∞ 25ÎÖÑ 2Î∂ÑÍ∏∞ÍπåÏßÄÏùò Îß§Ï∂ú, ÏòÅÏóÖÏù¥Ïùµ ÏÑ±Ïû•Í≥º Ï£ºÍ∞Ä, ÏãúÍ∞ÄÏ¥ùÏï° ÌùêÎ¶ÑÏùÑ Ï°∞ÏÇ¨Ìï¥Ï§ò\"  \n‚Üí [ÏóêÏù¥ÌîºÏïå Î∂ÑÏÑù Í≤∞Í≥º Î≥¥Í∏∞](./example/ÏóêÏù¥ÌîºÏïå.md)\n\n### üè¢ Í∏∞ÏóÖ Î∂ÑÏÑù ÏòàÏ†ú\n\n**ÌîÑÎ°¨ÌîÑÌä∏**: \"HJÏ§ëÍ≥µÏóÖÏùÄ Î≠ò Ìï¥ÏÑú ÎèàÏùÑ Î≤ÑÎäî ÌöåÏÇ¨Ïù∏ÏßÄÎûë ÏÇ¨ÏóÖÎ∂ÄÎ¨∏Î≥Ñ Îß§Ï∂úÍπåÏßÄ Í∞ôÏù¥ ÏïåÎ†§Ï§ò\"  \n‚Üí [HJÏ§ëÍ≥µÏóÖ Î∂ÑÏÑù Í≤∞Í≥º Î≥¥Í∏∞](./example/HJÏ§ëÍ≥µÏóÖ.md)\n\n## API Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§\n\n- **DART (Ï†ÑÏûêÍ≥µÏãúÏãúÏä§ÌÖú)**: ÏÉÅÏû•Í∏∞ÏóÖ Í≥µÏãú Ï†ïÎ≥¥ Î∞è Ïû¨Î¨¥Ï†úÌëú\n- **KRX (ÌïúÍµ≠Í±∞ÎûòÏÜå)**: Ï¢ÖÎ™© Í∏∞Î≥∏Ï†ïÎ≥¥ Î∞è ÏùºÎ≥Ñ Îß§Îß§Ï†ïÎ≥¥\n\n## Í∏∞Ïó¨ÌïòÍ∏∞\n\nÍ∏∞Ïó¨Î•º ÌôòÏòÅÌï©ÎãàÎã§! Pull RequestÎ•º Î≥¥ÎÇ¥Ï£ºÏÑ∏Ïöî.\n\n1. Ïù¥ Ï†ÄÏû•ÏÜåÎ•º Ìè¨ÌÅ¨ÌïòÏÑ∏Ïöî\n2. Í∏∞Îä• Î∏åÎûúÏπòÎ•º ÎßåÎìúÏÑ∏Ïöî (`git checkout -b feature/AmazingFeature`)\n3. Î≥ÄÍ≤ΩÏÇ¨Ìï≠ÏùÑ Ïª§Î∞ãÌïòÏÑ∏Ïöî (`git commit -m 'Add some AmazingFeature'`)\n4. Î∏åÎûúÏπòÏóê Ìë∏ÏãúÌïòÏÑ∏Ïöî (`git push origin feature/AmazingFeature`)\n5. Pull RequestÎ•º Ïó¥Ïñ¥Ï£ºÏÑ∏Ïöî\n\n## ÎùºÏù¥ÏÑ†Ïä§\n\nISC ÎùºÏù¥ÏÑ†Ïä§\n\n## ÏßÄÏõê\n\n- üêõ Ïù¥ÏäàÍ∞Ä ÏûàÎã§Î©¥ GitHub IssuesÏóê Îì±Î°ùÌï¥Ï£ºÏÑ∏Ïöî\n- ‚≠ê Ïú†Ïö©ÌïòÎã§Î©¥ Ïä§ÌÉÄÎ•º ÎàåÎü¨Ï£ºÏÑ∏Ïöî!\n\n## Î©¥Ï±Ö Ï°∞Ìï≠\n\nÎ≥∏ ÎèÑÍµ¨Îäî Ï†ïÎ≥¥ Ï†úÍ≥µ Î™©Ï†ÅÏù¥Î©∞, Ìà¨Ïûê Ï°∞Ïñ∏Ïù¥ ÏïÑÎãôÎãàÎã§. Î™®Îì† Ìà¨Ïûê Í≤∞Ï†ïÏùÄ Î≥∏Ïù∏ Ï±ÖÏûÑÏûÖÎãàÎã§.\n\n---\n\n# English Version\n\nMCP Server for Korean stock analysis.  \nEnables AI-powered analysis of stock prices and disclosure data through official APIs from DART (Data Analysis, Retrieval and Transfer System) and KRX (Korea Exchange).\n\n## üéØ Key Features\n\n- üîç **Disclosure Search** - Search corporate disclosures by company and date\n- üìä **Disclosure Data** - Provides parsed data from original disclosure reports\n- üíº **Financial Statement Analysis** - Detailed financial data based on XBRL\n- üìà **Stock Data** - KRX (KOSPI/KOSDAQ) daily stock prices and basic stock information\n\n## ‚ö° Quick Start\n\n### 1Ô∏è‚É£ API KEY Registration\n\nYou need to obtain API KEYs from both DART and KRX.\n\n#### üìù DART API KEY Registration\n\n1. **Sign Up**: Register at [OPEN DART](https://opendart.fss.or.kr)\n2. **Request Key**: Apply for API KEY at [Authentication Key Application Page](https://opendart.fss.or.kr/uss/umt/EgovMberInsertView.do)\n3. **Check Key**: Verify issued API KEY at [Open API Usage Status](https://opendart.fss.or.kr/mng/apiUsageStatusView.do)\n\n#### üìà KRX API KEY Registration\n\n1. **Sign Up**: Register and login at [KRX OPEN API](https://openapi.krx.co.kr/contents/OPP/MAIN/main/index.cmd)\n2. **Request Key**: Apply for API authentication key in My Page ‚Üí API Authentication Key Application\n3. **Service Application**: After approval, go to Service Use ‚Üí Stock menu\n4. **API Usage Application**: Click \"API Usage Application\" for each of the following 6 items\n\n   - Securities Daily Trading Information\n   - KOSDAQ Daily Trading Information\n   - KONEX Daily Trading Information\n   - Securities Basic Information\n   - KOSDAQ Basic Information\n   - KONEX Basic Information\n\n   > ‚è±Ô∏è **Approval takes approximately 1 day.**\n\n5. **Key Verification**: After approval, check API KEY in My Page ‚Üí API Authentication Key Issuance History\n\n### 2Ô∏è‚É£ Claude Desktop Setup\n\n1. Launch **Claude Desktop**\n2. Go to **Settings** ‚Üí **Developer** ‚Üí **Edit Configuration**\n3. Add the following content to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"korea-stock-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"korea-stock-mcp@latest\"],\n      \"env\": {\n        \"DART_API_KEY\": \"<YOUR_DART_API_KEY>\",\n        \"KRX_API_KEY\": \"<YOUR_KRX_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n4. **Restart**: Restart Claude Desktop to apply settings\n\n> You can now start analyzing Korean stock data with Claude.\n\n## Available Tools\n\n### DART (Data Analysis, Retrieval and Transfer System)\n\n1. **get_disclosure_list** - Disclosure Search\n\n   - Search disclosure reports by type, company, and date\n\n2. **get_corp_code** - Corporate Code Inquiry\n\n   - Provides unique codes, company names, and stock codes of DART-registered disclosure companies\n\n3. **get_disclosure** - Disclosure Report Content\n\n   - Parse original disclosure report files through DART API\n\n4. **get_financial_statement** - Financial Statements\n   - XBRL financial statements for listed and major unlisted companies\n   - Provides all account data from periodic reports\n\n### KRX (Korea Exchange)\n\n1. **get_stock_base_info** - Basic Stock Information\n\n   - Basic information for KOSPI, KOSDAQ, and KONEX listed stocks\n   - Basic data including stock names, codes, and market classifications\n\n2. **get_stock_trade_info** - Daily Trading Information\n   - Daily trading data for KOSPI, KOSDAQ, and KONEX stocks\n   - Detailed trading information including stock prices, trading volume, and market capitalization\n\n3. **get_market_type** - Market Type Inquiry\n   - Query market classification (KOSPI/KOSDAQ/KONEX) by stock code\n   - Provides market information needed for stock data queries\n\n### Other Tools\n\n1. **get_today_date** - Today's Date Inquiry\n   - Provides current date in YYYYMMDD format\n   - Tool for AI's accurate date inquiry\n\n## Real Usage Examples\n\n### üìä Financial Analysis Examples\n\n**Prompt**: \"Investigate Samyang Foods's sales and operating profit for Q1-Q4 2023, Q1-Q4 2024, and Q1-Q2 2025, and also check growth rates\"  \n‚Üí [See Samyang Foods Analysis Results](./example/ÏÇºÏñëÏãùÌíà.md)\n\n**Prompt**: \"Investigate APR's sales and operating profit growth from Q1 2023 to Q2 2025, along with stock price and market cap trends\"  \n‚Üí [See APR Analysis Results](./example/ÏóêÏù¥ÌîºÏïå.md)\n\n### üè¢ Corporate Analysis Examples\n\n**Prompt**: \"Tell me what HJ SHIPBUILDING & CONSTRUCTION does to make money and include sales by business segment\"  \n‚Üí [See HJ SHIPBUILDING & CONSTRUCTION Analysis Results](./example/HJÏ§ëÍ≥µÏóÖ.md)\n\n## API Data Sources\n\n- **DART (Data Analysis, Retrieval and Transfer System)**: Listed company disclosure information and financial statements\n- **KRX (Korea Exchange)**: Basic stock information and daily trading information\n\n## Contributing\n\nContributions are welcome! Please send us a Pull Request.\n\n1. Fork this repository\n2. Create a feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## License\n\nISC License\n\n## Support\n\n- üêõ If you have issues, please register them in GitHub Issues\n- ‚≠ê If you find it useful, please give it a star!\n\n## Disclaimer\n\nThis tool is for informational purposes only and is not investment advice. All investment decisions are your own responsibility.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search corporate disclosures by company, type, and date using DART API",
        "Provide parsed data from original disclosure reports",
        "Analyze detailed financial statements based on XBRL data",
        "Retrieve daily stock prices and trading information from KRX for KOSPI, KOSDAQ, and KONEX markets",
        "Provide basic stock information including stock names, codes, and market classifications",
        "Query market classification of stocks by stock code",
        "Provide current date in YYYYMMDD format for accurate AI date referencing"
      ],
      "limitations": [
        "Requires separate API key registration and approval from DART and KRX, with KRX approval taking approximately one day",
        "Does not provide investment advice; all investment decisions are user responsibility",
        "Limited to Korean stock market data and disclosures from DART and KRX only"
      ],
      "requirements": [
        "API keys from DART (OPEN DART) and KRX (KRX OPEN API) with proper registration and approval",
        "Claude Desktop client configured with the MCP server and environment variables for DART_API_KEY and KRX_API_KEY",
        "Node.js environment to run the MCP server via npx"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation includes comprehensive installation instructions, detailed tool descriptions, usage examples, API key acquisition steps, environment setup, limitations, and disclaimers, providing an excellent level of detail.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Korea Stock MCP Server\n\n[üá∞üá∑ ÌïúÍµ≠Ïñ¥](#korea-stock-mcp-server) | [üá∫üá∏ English](#english-version)\n\nÌïúÍµ≠ Ï£ºÏãù Î∂ÑÏÑùÏùÑ ÏúÑÌïú MCP ÏÑúÎ≤ÑÏûÖÎãàÎã§.  \nDART(Ï†ÑÏûêÍ≥µÏãúÏãúÏä§ÌÖú)ÏôÄ KRX(ÌïúÍµ≠Í±∞ÎûòÏÜå) Í≥µÏãù APIÎ•º ÌÜµÌï¥ Ï£ºÍ∞Ä Ï†ïÎ≥¥ÏôÄ Í≥µÏãú ÏûêÎ£å Í∏∞Î∞òÏùò AIÎ∂ÑÏÑùÏù¥ Í∞ÄÎä•Ìï©ÎãàÎã§.\n\n## üéØ Ï£ºÏöî Í∏∞Îä•\n\n- üîç **Í≥µÏãúÍ≤ÄÏÉâ** - ÌöåÏÇ¨Î≥Ñ, Í∏∞Í∞ÑÎ≥Ñ Í≥µÏãú Í≤ÄÏÉâ\n- üìä **Í≥µÏãú Îç∞Ïù¥ÌÑ∞** - Í≥µÏãúÎ≥¥Í≥†ÏÑú ÏõêÎ≥∏ÌååÏùº ÌååÏã±Ìïú Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ\n- üíº **Ïû¨Î¨¥Ï†úÌëú Î∂ÑÏÑù** - XBRL Í∏∞Î∞ò ÏÉÅÏÑ∏ Ïû¨Î¨¥ Îç∞Ïù¥ÌÑ∞\n- üìà **Ï£ºÏãù Îç∞Ïù¥ÌÑ∞** - KRX(ÏΩîÏä§Ìîº/ÏΩîÏä§Îã•) ÏùºÎ≥Ñ Ï£ºÍ∞ÄÏ†ïÎ≥¥, Ï¢ÖÎ™© Í∏∞Î≥∏Ï†ïÎ≥¥\n\n## ‚ö° Îπ†Î•∏ ÏãúÏûë\n\n### 1Ô∏è‚É£ API KEY Î∞úÍ∏â\n\nÎ®ºÏ†Ä DARTÏôÄ KRXÏùò API KEYÎ•º Î∞úÍ∏âÎ∞õÏïÑÏïº Ìï©ÎãàÎã§.\n\n#### üìù DART API KEY Î∞úÍ∏â\n\n1. **ÌöåÏõêÍ∞ÄÏûÖ**: [OPEN DART](https://opendart.fss.or.kr) ÌöåÏõêÍ∞ÄÏûÖ\n2. **ÌÇ§ Ïã†Ï≤≠**: [Ïù∏Ï¶ùÌÇ§ Ïã†Ï≤≠ ÌéòÏù¥ÏßÄ](https://opendart.fss.or.kr/uss/umt/EgovMberInsertView.do)ÏóêÏÑú API KEY Ïã†Ï≤≠\n3. **ÌÇ§ ÌôïÏù∏**: [Ïò§ÌîàAPI Ïù¥Ïö©ÌòÑÌô©](https://opendart.fss.or.kr/mng/apiUsageStatusView.do)ÏóêÏÑú Î∞úÍ∏âÎêú API KEY ÌôïÏù∏\n\n#### üìà KRX API KEY Î∞úÍ∏â\n\n1. **ÌöåÏõêÍ∞ÄÏûÖ**: [KRX OPEN API](https://openapi.krx.co.kr/contents/OPP/MAIN/main/index.cmd)ÏóêÏÑú ÌöåÏõêÍ∞ÄÏûÖ Î∞è Î°úÍ∑∏Ïù∏\n2. **ÌÇ§ Ïã†Ï≤≠**: ÎßàÏù¥ÌéòÏù¥ÏßÄ ‚Üí API Ïù∏Ï¶ùÌÇ§ Ïã†Ï≤≠ÏóêÏÑú Ïã†Ï≤≠\n3. **ÏÑúÎπÑÏä§ Ïã†Ï≤≠**: ÏäπÏù∏ ÌõÑ ÏÑúÎπÑÏä§Ïù¥Ïö© ‚Üí Ï£ºÏãù Î©îÎâ¥Î°ú Ïù¥Îèô\n4. **API Ïù¥Ïö©Ïã†Ï≤≠**: Îã§Ïùå 6Í∞ú Ìï≠Î™©ÏóêÏÑú Í∞ÅÍ∞Å \"API Ïù¥Ïö©Ïã†Ï≤≠\" ÌÅ¥Î¶≠\n\n   - Ïú†Í∞ÄÏ¶ùÍ∂å ÏùºÎ≥ÑÎß§Îß§Ï†ïÎ≥¥\n   - ÏΩîÏä§Îã• ÏùºÎ≥ÑÎß§Îß§Ï†ïÎ≥¥\n   - ÏΩîÎÑ•Ïä§ ÏùºÎ≥ÑÎß§Îß§Ï†ïÎ≥¥\n   - Ïú†Í∞ÄÏ¶ùÍ∂å Ï¢ÖÎ™©Í∏∞Î≥∏Ï†ïÎ≥¥\n   - ÏΩîÏä§Îã• Ï¢ÖÎ™©Í∏∞Î≥∏Ï†ïÎ≥¥\n   - ÏΩîÎÑ•Ïä§ Ï¢ÖÎ™©Í∏∞Î≥∏Ï†ïÎ≥¥\n\n   > ‚è±Ô∏è **ÏäπÏù∏ÍπåÏßÄ ÏïΩ 1Ïùº ÏÜåÏöîÎê©ÎãàÎã§.**\n\n5. **ÌÇ§ ÌôïÏù∏**: ÏäπÏù∏ ÌõÑ ÎßàÏù¥ÌéòÏù¥ÏßÄ ‚Üí API Ïù∏Ï¶ùÌÇ§ Î∞úÍ∏âÎÇ¥Ïó≠ÏóêÏÑú API KEY ÌôïÏù∏\n\n### 2Ô∏è‚É£ Claude Desktop ÏÑ§Ï†ï\n\n1. **Claude Desktop** Ïã§Ìñâ\n2. **ÏÑ§Ï†ï** ‚Üí **Í∞úÎ∞úÏûê** ‚Üí **Íµ¨ÏÑ±Ìé∏Ïßë** ÌÅ¥Î¶≠\n3. `claude_desktop_config.json` ÌååÏùºÏóê Îã§Ïùå ÎÇ¥Ïö© Ï∂îÍ∞Ä:\n\n```json\n{\n  \"mcpServers\": {\n    \"korea-stock-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"korea-stock-mcp@latest\"],\n      \"env\": {\n        \"DART_API_KEY\": \"<YOUR_DART_API_KEY>\",\n        \"KRX_API_KEY\": \"<YOUR_KRX_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n4. **Ïû¨ÏãúÏûë**: Claude DesktopÏùÑ Ïû¨ÏãúÏûëÌïòÏó¨ ÏÑ§Ï†ï Ï†ÅÏö©\n\n> Ïù¥Ï†ú ClaudeÏóêÏÑú ÌïúÍµ≠ Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÏùÑ ÏãúÏûëÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n## ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨\n\n### DART (Ï†ÑÏûêÍ≥µÏãúÏãúÏä§ÌÖú)\n\n1. **get_disclosure_list** - Í≥µÏãúÍ≤ÄÏÉâ\n\n   - Í≥µÏãú Ïú†ÌòïÎ≥Ñ, ÌöåÏÇ¨Î≥Ñ, ÎÇ†ÏßúÎ≥Ñ Í≥µÏãúÎ≥¥Í≥†ÏÑú Í≤ÄÏÉâ\n\n2. **get_corp_code** - Í≥†Ïú†Î≤àÌò∏ Ï°∞Ìöå\n\n   - DART Îì±Î°ù Í≥µÏãúÎåÄÏÉÅÌöåÏÇ¨Ïùò Í≥†Ïú†Î≤àÌò∏, ÌöåÏÇ¨Î™Ö, Ï¢ÖÎ™©ÏΩîÎìú Ï†úÍ≥µ\n\n3. **get_disclosure** - Í≥µÏãúÎ≥¥Í≥†ÏÑú ÏõêÎ¨∏\n\n   - DART APIÎ•º ÌÜµÌïú Í≥µÏãúÎ≥¥Í≥†ÏÑú ÏõêÎ≥∏ÌååÏùº ÌååÏã±\n\n4. **get_financial_statement** - Ïû¨Î¨¥Ï†úÌëú\n   - ÏÉÅÏû•Î≤ïÏù∏ Î∞è Ï£ºÏöî ÎπÑÏÉÅÏû•Î≤ïÏù∏ XBRL Ïû¨Î¨¥Ï†úÌëú\n   - Ï†ïÍ∏∞Î≥¥Í≥†ÏÑú ÎÇ¥ Î™®Îì† Í≥ÑÏ†ïÍ≥ºÎ™© Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ\n\n### KRX (ÌïúÍµ≠Í±∞ÎûòÏÜå)\n\n1.",
        "start_pos": 0,
        "end_pos": 1953,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "ef74dc6dae592fe2"
      },
      {
        "chunk_id": 1,
        "text": ", ÌöåÏÇ¨Î™Ö, Ï¢ÖÎ™©ÏΩîÎìú Ï†úÍ≥µ\n\n3. **get_disclosure** - Í≥µÏãúÎ≥¥Í≥†ÏÑú ÏõêÎ¨∏\n\n   - DART APIÎ•º ÌÜµÌïú Í≥µÏãúÎ≥¥Í≥†ÏÑú ÏõêÎ≥∏ÌååÏùº ÌååÏã±\n\n4. **get_financial_statement** - Ïû¨Î¨¥Ï†úÌëú\n   - ÏÉÅÏû•Î≤ïÏù∏ Î∞è Ï£ºÏöî ÎπÑÏÉÅÏû•Î≤ïÏù∏ XBRL Ïû¨Î¨¥Ï†úÌëú\n   - Ï†ïÍ∏∞Î≥¥Í≥†ÏÑú ÎÇ¥ Î™®Îì† Í≥ÑÏ†ïÍ≥ºÎ™© Îç∞Ïù¥ÌÑ∞ Ï†úÍ≥µ\n\n### KRX (ÌïúÍµ≠Í±∞ÎûòÏÜå)\n\n1. **get_stock_base_info** - Ï¢ÖÎ™© Í∏∞Î≥∏Ï†ïÎ≥¥\n\n   - ÏΩîÏä§Ìîº, ÏΩîÏä§Îã•, ÏΩîÎÑ•Ïä§ ÏÉÅÏû• Ï¢ÖÎ™© Í∏∞Î≥∏ Ï†ïÎ≥¥\n   - Ï¢ÖÎ™©Î™Ö, Ï¢ÖÎ™©ÏΩîÎìú, ÏãúÏû•Íµ¨Î∂Ñ Îì± Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞\n\n2. **get_stock_trade_info** - ÏùºÎ≥Ñ Îß§Îß§Ï†ïÎ≥¥\n   - ÏΩîÏä§Ìîº, ÏΩîÏä§Îã•, ÏΩîÎÑ•Ïä§ Ï¢ÖÎ™©Î≥Ñ ÏùºÎ≥Ñ Í±∞Îûò Îç∞Ïù¥ÌÑ∞\n   - Ï£ºÍ∞Ä, Í±∞ÎûòÎüâ, ÏãúÍ∞ÄÏ¥ùÏï° Îì± ÏÉÅÏÑ∏ Í±∞Îûò Ï†ïÎ≥¥\n\n3. **get_market_type** - ÏãúÏû•Íµ¨Î∂Ñ Ï°∞Ìöå\n   - Ï¢ÖÎ™©ÏΩîÎìúÎ°ú Ìï¥Îãπ Ï¢ÖÎ™©Ïùò ÏãúÏû•Íµ¨Î∂Ñ(ÏΩîÏä§Ìîº/ÏΩîÏä§Îã•/ÏΩîÎÑ•Ïä§) Ï°∞Ìöå\n   - Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Ïãú ÌïÑÏöîÌïú ÏãúÏû• Ï†ïÎ≥¥ Ï†úÍ≥µ\n\n### Í∏∞ÌÉÄ ÎèÑÍµ¨\n\n1. **get_today_date** - Ïò§Îäò ÎÇ†Ïßú Ï°∞Ìöå\n   - ÌòÑÏû¨ ÎÇ†ÏßúÎ•º YYYYMMDD ÌòïÏãùÏúºÎ°ú Ï†úÍ≥µ\n   - AIÏùò Ï†ïÌôïÌïú ÎÇ†Ïßú Ï°∞ÌöåÎ•º ÏúÑÌïú ÎèÑÍµ¨\n\n## Ïã§Ï†ú ÏÇ¨Ïö© ÏòàÏãú\n\n### üìä Ïû¨Î¨¥ Î∂ÑÏÑù ÏòàÏ†ú\n\n**ÌîÑÎ°¨ÌîÑÌä∏**: \"ÏÇºÏñëÏãùÌíàÏùò 2023ÎÖÑ, 2024ÎÖÑ 1~4Î∂ÑÍ∏∞, 2025ÎÖÑ 1,2Î∂ÑÍ∏∞ Îß§Ï∂ú, ÏòÅÏóÖÏù¥Ïùµ Ï°∞ÏÇ¨Ìï¥Ï£ºÍ≥† ÏÑ±Ïû•Î•†ÎèÑ Ï°∞ÏÇ¨Ìï¥Ï§ò\"  \n‚Üí [ÏÇºÏñëÏãùÌíà Î∂ÑÏÑù Í≤∞Í≥º Î≥¥Í∏∞](./example/ÏÇºÏñëÏãùÌíà.md)\n\n**ÌîÑÎ°¨ÌîÑÌä∏**: \"ÏóêÏù¥ÌîºÏïåÏùò 23ÎÖÑ 1Î∂ÑÍ∏∞Î∂ÄÌÑ∞ 25ÎÖÑ 2Î∂ÑÍ∏∞ÍπåÏßÄÏùò Îß§Ï∂ú, ÏòÅÏóÖÏù¥Ïùµ ÏÑ±Ïû•Í≥º Ï£ºÍ∞Ä, ÏãúÍ∞ÄÏ¥ùÏï° ÌùêÎ¶ÑÏùÑ Ï°∞ÏÇ¨Ìï¥Ï§ò\"  \n‚Üí [ÏóêÏù¥ÌîºÏïå Î∂ÑÏÑù Í≤∞Í≥º Î≥¥Í∏∞](./example/ÏóêÏù¥ÌîºÏïå.md)\n\n### üè¢ Í∏∞ÏóÖ Î∂ÑÏÑù ÏòàÏ†ú\n\n**ÌîÑÎ°¨ÌîÑÌä∏**: \"HJÏ§ëÍ≥µÏóÖÏùÄ Î≠ò Ìï¥ÏÑú ÎèàÏùÑ Î≤ÑÎäî ÌöåÏÇ¨Ïù∏ÏßÄÎûë ÏÇ¨ÏóÖÎ∂ÄÎ¨∏Î≥Ñ Îß§Ï∂úÍπåÏßÄ Í∞ôÏù¥ ÏïåÎ†§Ï§ò\"  \n‚Üí [HJÏ§ëÍ≥µÏóÖ Î∂ÑÏÑù Í≤∞Í≥º Î≥¥Í∏∞](./example/HJÏ§ëÍ≥µÏóÖ.md)\n\n## API Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§\n\n- **DART (Ï†ÑÏûêÍ≥µÏãúÏãúÏä§ÌÖú)**: ÏÉÅÏû•Í∏∞ÏóÖ Í≥µÏãú Ï†ïÎ≥¥ Î∞è Ïû¨Î¨¥Ï†úÌëú\n- **KRX (ÌïúÍµ≠Í±∞ÎûòÏÜå)**: Ï¢ÖÎ™© Í∏∞Î≥∏Ï†ïÎ≥¥ Î∞è ÏùºÎ≥Ñ Îß§Îß§Ï†ïÎ≥¥\n\n## Í∏∞Ïó¨ÌïòÍ∏∞\n\nÍ∏∞Ïó¨Î•º ÌôòÏòÅÌï©ÎãàÎã§! Pull RequestÎ•º Î≥¥ÎÇ¥Ï£ºÏÑ∏Ïöî.\n\n1. Ïù¥ Ï†ÄÏû•ÏÜåÎ•º Ìè¨ÌÅ¨ÌïòÏÑ∏Ïöî\n2. Í∏∞Îä• Î∏åÎûúÏπòÎ•º ÎßåÎìúÏÑ∏Ïöî (`git checkout -b feature/AmazingFeature`)\n3. Î≥ÄÍ≤ΩÏÇ¨Ìï≠ÏùÑ Ïª§Î∞ãÌïòÏÑ∏Ïöî (`git commit -m 'Add some AmazingFeature'`)\n4. Î∏åÎûúÏπòÏóê Ìë∏ÏãúÌïòÏÑ∏Ïöî (`git push origin feature/AmazingFeature`)\n5. Pull RequestÎ•º Ïó¥Ïñ¥Ï£ºÏÑ∏Ïöî\n\n## ÎùºÏù¥ÏÑ†Ïä§\n\nISC ÎùºÏù¥ÏÑ†Ïä§\n\n## ÏßÄÏõê\n\n- üêõ Ïù¥ÏäàÍ∞Ä ÏûàÎã§Î©¥ GitHub IssuesÏóê Îì±Î°ùÌï¥Ï£ºÏÑ∏Ïöî\n- ‚≠ê Ïú†Ïö©ÌïòÎã§Î©¥ Ïä§ÌÉÄÎ•º ÎàåÎü¨Ï£ºÏÑ∏Ïöî!\n\n## Î©¥Ï±Ö Ï°∞Ìï≠\n\nÎ≥∏ ÎèÑÍµ¨Îäî Ï†ïÎ≥¥ Ï†úÍ≥µ Î™©Ï†ÅÏù¥Î©∞, Ìà¨Ïûê Ï°∞Ïñ∏Ïù¥ ÏïÑÎãôÎãàÎã§. Î™®Îì† Ìà¨Ïûê Í≤∞Ï†ïÏùÄ Î≥∏Ïù∏ Ï±ÖÏûÑÏûÖÎãàÎã§.\n\n---\n\n# English Version\n\nMCP Server for Korean stock analysis.  \nEnables AI-powered analysis of stock prices and disclosure data through official APIs from DART (Data Analysis, Retrieval and Transfer System) and KRX (Korea Exchange).",
        "start_pos": 1753,
        "end_pos": 3448,
        "token_count_estimate": 423,
        "source_type": "readme",
        "agent_id": "ef74dc6dae592fe2"
      },
      {
        "chunk_id": 2,
        "text": "ginal disclosure reports\n- üíº **Financial Statement Analysis** - Detailed financial data based on XBRL\n- üìà **Stock Data** - KRX (KOSPI/KOSDAQ) daily stock prices and basic stock information\n\n## ‚ö° Quick Start\n\n### 1Ô∏è‚É£ API KEY Registration\n\nYou need to obtain API KEYs from both DART and KRX.\n\n#### üìù DART API KEY Registration\n\n1. **Sign Up**: Register at [OPEN DART](https://opendart.fss.or.kr)\n2. **Request Key**: Apply for API KEY at [Authentication Key Application Page](https://opendart.fss.or.kr/uss/umt/EgovMberInsertView.do)\n3. **Check Key**: Verify issued API KEY at [Open API Usage Status](https://opendart.fss.or.kr/mng/apiUsageStatusView.do)\n\n#### üìà KRX API KEY Registration\n\n1. **Sign Up**: Register and login at [KRX OPEN API](https://openapi.krx.co.kr/contents/OPP/MAIN/main/index.cmd)\n2. **Request Key**: Apply for API authentication key in My Page ‚Üí API Authentication Key Application\n3. **Service Application**: After approval, go to Service Use ‚Üí Stock menu\n4. **API Usage Application**: Click \"API Usage Application\" for each of the following 6 items\n\n   - Securities Daily Trading Information\n   - KOSDAQ Daily Trading Information\n   - KONEX Daily Trading Information\n   - Securities Basic Information\n   - KOSDAQ Basic Information\n   - KONEX Basic Information\n\n   > ‚è±Ô∏è **Approval takes approximately 1 day.**\n\n5. **Key Verification**: After approval, check API KEY in My Page ‚Üí API Authentication Key Issuance History\n\n### 2Ô∏è‚É£ Claude Desktop Setup\n\n1. Launch **Claude Desktop**\n2. Go to **Settings** ‚Üí **Developer** ‚Üí **Edit Configuration**\n3. Add the following content to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"korea-stock-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"korea-stock-mcp@latest\"],\n      \"env\": {\n        \"DART_API_KEY\": \"<YOUR_DART_API_KEY>\",\n        \"KRX_API_KEY\": \"<YOUR_KRX_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n4. **Restart**: Restart Claude Desktop to apply settings\n\n> You can now start analyzing Korean stock data with Claude.",
        "start_pos": 3601,
        "end_pos": 5598,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "ef74dc6dae592fe2"
      },
      {
        "chunk_id": 3,
        "text": "T_API_KEY>\",\n        \"KRX_API_KEY\": \"<YOUR_KRX_API_KEY>\"\n      }\n    }\n  }\n}\n```\n\n4. **Restart**: Restart Claude Desktop to apply settings\n\n> You can now start analyzing Korean stock data with Claude.\n\n## Available Tools\n\n### DART (Data Analysis, Retrieval and Transfer System)\n\n1. **get_disclosure_list** - Disclosure Search\n\n   - Search disclosure reports by type, company, and date\n\n2. **get_corp_code** - Corporate Code Inquiry\n\n   - Provides unique codes, company names, and stock codes of DART-registered disclosure companies\n\n3. **get_disclosure** - Disclosure Report Content\n\n   - Parse original disclosure report files through DART API\n\n4. **get_financial_statement** - Financial Statements\n   - XBRL financial statements for listed and major unlisted companies\n   - Provides all account data from periodic reports\n\n### KRX (Korea Exchange)\n\n1. **get_stock_base_info** - Basic Stock Information\n\n   - Basic information for KOSPI, KOSDAQ, and KONEX listed stocks\n   - Basic data including stock names, codes, and market classifications\n\n2. **get_stock_trade_info** - Daily Trading Information\n   - Daily trading data for KOSPI, KOSDAQ, and KONEX stocks\n   - Detailed trading information including stock prices, trading volume, and market capitalization\n\n3. **get_market_type** - Market Type Inquiry\n   - Query market classification (KOSPI/KOSDAQ/KONEX) by stock code\n   - Provides market information needed for stock data queries\n\n### Other Tools\n\n1.",
        "start_pos": 5398,
        "end_pos": 6856,
        "token_count_estimate": 364,
        "source_type": "readme",
        "agent_id": "ef74dc6dae592fe2"
      },
      {
        "chunk_id": 4,
        "text": "**Prompt**: \"Investigate APR's sales and operating profit growth from Q1 2023 to Q2 2025, along with stock price and market cap trends\"  \n‚Üí [See APR Analysis Results](./example/ÏóêÏù¥ÌîºÏïå.md)\n\n### üè¢ Corporate Analysis Examples\n\n**Prompt**: \"Tell me what HJ SHIPBUILDING & CONSTRUCTION does to make money and include sales by business segment\"  \n‚Üí [See HJ SHIPBUILDING & CONSTRUCTION Analysis Results](./example/HJÏ§ëÍ≥µÏóÖ.md)\n\n## API Data Sources\n\n- **DART (Data Analysis, Retrieval and Transfer System)**: Listed company disclosure information and financial statements\n- **KRX (Korea Exchange)**: Basic stock information and daily trading information\n\n## Contributing\n\nContributions are welcome! Please send us a Pull Request.\n\n1. Fork this repository\n2. Create a feature branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## License\n\nISC License\n\n## Support\n\n- üêõ If you have issues, please register them in GitHub Issues\n- ‚≠ê If you find it useful, please give it a star!\n\n## Disclaimer\n\nThis tool is for informational purposes only and is not investment advice. All investment decisions are your own responsibility.",
        "start_pos": 7246,
        "end_pos": 8510,
        "token_count_estimate": 315,
        "source_type": "readme",
        "agent_id": "ef74dc6dae592fe2"
      }
    ]
  },
  {
    "agent_id": "4a46b9c1b824b3b8",
    "name": "ai.smithery/jweingardt12-mlb_mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@jweingardt12/mlb_mcp/mcp",
    "description": "Provides easy access to MLB, Baseball Savant, Statcast, and Fangraphs baseball data. Query detaile‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-06T14:52:36.727607Z",
    "indexed_at": "2026-02-18T04:07:19.733635",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to MLB baseball data",
        "Provide access to Baseball Savant data",
        "Provide access to Statcast baseball data",
        "Provide access to Fangraphs baseball data",
        "Allow querying of detailed baseball statistics"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence that outlines data sources and querying capability but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "ce8111aeb26af1f9",
    "name": "ai.smithery/kaszek-kaszek-attio-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@kaszek/kaszek-attio-mcp/mcp",
    "description": "Automate Attio CRM workflows with fast search and bulk operations across companies, people, deals,‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T23:26:13.627724Z",
    "indexed_at": "2026-02-18T04:07:21.501806",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Automate Attio CRM workflows",
        "Perform fast search across companies, people, and deals",
        "Execute bulk operations on companies, people, and deals"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of capabilities but lacks detailed examples, structure, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "13659e54f63c2c84",
    "name": "ai.smithery/keithah-hostex-mcp",
    "source": "mcp",
    "source_url": "https://github.com/keithah/hostex-mcp",
    "description": "Manage your Hostex vacation rentals‚Äîproperties, reservations, availability, listings, and guest me‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T04:10:38.519183Z",
    "indexed_at": "2026-02-18T04:07:23.018690",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# hostex-mcp\n\n[![Release](https://github.com/keithah/hostex-mcp/actions/workflows/release.yml/badge.svg)](https://github.com/keithah/hostex-mcp/actions/workflows/release.yml)\n[![Check hostex-ts Updates](https://github.com/keithah/hostex-mcp/actions/workflows/check-npm-updates.yml/badge.svg)](https://github.com/keithah/hostex-mcp/actions/workflows/check-npm-updates.yml)\n[![Smithery](https://smithery.ai/badge/@keithah/hostex-mcp)](https://smithery.ai/server/@keithah/hostex-mcp)\n\nModel Context Protocol server for the Hostex property management API. Manage your vacation rental properties, reservations, guest communications, and more through Claude and other MCP clients.\n\nBuilt on [hostex-ts](https://www.npmjs.com/package/hostex-ts) - TypeScript client library for Hostex API v3.0.0.\n\n## Features\n\n- üè† **Properties** - Property and room type management\n- üìÖ **Reservations** - CRUD operations, custom fields, lock codes\n- üìä **Availability** - Property availability calendars\n- üìã **Listings** - Channel listings, pricing, inventory\n- üí¨ **Messaging** - Guest communication and messaging\n- ‚≠ê **Reviews** - Review management and responses\n- üîó **Webhooks** - Real-time event notifications\n- ‚öôÔ∏è **Utilities** - Custom channels and income methods\n\n### Available Tools (25+)\n\n- `hostex_list_properties` - List all properties\n- `hostex_list_room_types` - List room types\n- `hostex_list_reservations` - Search and filter reservations\n- `hostex_create_reservation` - Create direct bookings\n- `hostex_cancel_reservation` - Cancel reservations\n- `hostex_update_lock_code` - Update stay lock codes\n- `hostex_get_custom_fields` - Get custom field values\n- `hostex_update_custom_fields` - Update custom fields\n- `hostex_list_availabilities` - Check property availability\n- `hostex_update_availabilities` - Block/open dates\n- `hostex_list_conversations` - List guest conversations\n- `hostex_get_conversation` - Get conversation details\n- `hostex_send_message` - Send messages to guests\n- `hostex_list_reviews` - Query reviews\n- `hostex_create_review` - Leave reviews or replies\n- `hostex_list_webhooks` - List configured webhooks\n- `hostex_create_webhook` - Register new webhooks\n- `hostex_delete_webhook` - Remove webhooks\n- `hostex_get_listing_calendar` - Get listing calendars\n- `hostex_update_listing_prices` - Update channel prices\n- `hostex_list_custom_channels` - List custom channels\n- `hostex_list_income_methods` - List income methods\n\n## Installation\n\n### Option 1: Via Smithery (Recommended)\n\nInstall directly from Smithery:\n\n```bash\nnpx -y @smithery/cli install @keithah/hostex-mcp --client claude\n```\n\nOr add the hosted server URL to your MCP client:\n\n```\nhttps://server.smithery.ai/@keithah/hostex-mcp/mcp\n```\n\nWhen prompted, provide your Hostex API access token.\n\n### Option 2: MCPB Package\n\n1. Download the latest `.mcpb` file from [Releases](https://github.com/keithah/hostex-mcp/releases)\n2. Double-click the file to install in your MCP client\n3. Configure your Hostex API token when prompted\n\n### Option 3: Manual Installation (Claude Desktop)\n\nAdd to your Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS or `~/.config/Claude/claude_desktop_config.json` on Linux):\n\n```json\n{\n  \"mcpServers\": {\n    \"hostex\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"hostex-mcp\"],\n      \"env\": {\n        \"HOSTEX_ACCESS_TOKEN\": \"your_hostex_api_token\"\n      }\n    }\n  }\n}\n```\n\nOr use the Smithery hosted server:\n\n```json\n{\n  \"mcpServers\": {\n    \"hostex\": {\n      \"url\": \"https://server.smithery.ai/@keithah/hostex-mcp/mcp\",\n      \"env\": {\n        \"HOSTEX_ACCESS_TOKEN\": \"your_hostex_api_token\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\nYou need a Hostex API access token. Get yours from your Hostex account settings at https://www.hostex.io/\n\nThe server accepts configuration through the `configSchema`:\n\n- `accessToken` (required): Your Hostex API access token\n\n## Usage Examples\n\nOnce installed, you can ask Claude natural language questions like:\n\n- \"Show me all my Hostex properties\"\n- \"List reservations checking in this week\"\n- \"Block property 12345 for next weekend\"\n- \"Send a welcome message to the guest in conversation ABC123\"\n- \"What reviews have I received this month?\"\n- \"Create a direct booking for property XYZ\"\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18\n- npm\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/keithah/hostex-mcp.git\ncd hostex-mcp\n\n# Install dependencies\nnpm install\n\n# Build for stdio transport\nnpm run build:stdio\n\n# Build for streamable HTTP transport\nnpm run build:shttp\n\n# Build both transports\nnpm run build:all\n```\n\n### Testing Locally\n\n```bash\n# Start dev server with Smithery\nnpm run dev\n```\n\n## Architecture\n\nThis MCP server uses:\n\n- **[@modelcontextprotocol/sdk](https://www.npmjs.com/package/@modelcontextprotocol/sdk)** - MCP protocol implementation\n- **[@smithery/sdk](https://www.npmjs.com/package/@smithery/sdk)** - Multi-transport support\n- **[hostex-ts](https://www.npmjs.com/package/hostex-ts)** - Hostex API client library\n- **[zod](https://www.npmjs.com/package/zod)** - Schema validation\n\n### Transports\n\nThe server supports two transport protocols:\n\n- **stdio** - Standard input/output (for local MCP clients like Claude Desktop)\n- **shttp** - Streamable HTTP (for remote/web-based MCP clients)\n\nBoth are built using [Smithery](https://smithery.ai) for seamless multi-transport support.\n\n## Automatic Updates\n\nThis repository includes a GitHub Actions workflow that:\n\n1. Checks hourly for new releases of `hostex-ts` on npm\n2. Automatically creates a PR to update the dependency\n3. Bumps the patch version\n4. Rebuilds the MCP bundles\n\nWhen you merge the PR and create a new release tag, the MCPB package is automatically built and attached to the GitHub release.\n\n## Requirements\n\n- Node.js >= 18\n- MCP-compatible client (Claude Desktop v0.10.0+, or any MCP client)\n- Hostex account with API access\n\n## Documentation\n\n- [Hostex API Documentation](https://docs.hostex.io/)\n- [hostex-ts npm package](https://www.npmjs.com/package/hostex-ts)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n\n## License\n\nMIT\n\n## Author\n\nKeith Hadfield\n\n## Support\n\n- **Issues**: [GitHub Issues](https://github.com/keithah/hostex-mcp/issues)\n- **Hostex API**: https://docs.hostex.io/\n- **MCP Documentation**: https://modelcontextprotocol.io/\n\n---\n\n**Note**: This is an unofficial community project and is not officially supported by Hostex."
    },
    "llm_extracted": {
      "capabilities": [
        "Manage vacation rental properties and room types",
        "Perform CRUD operations on reservations including creating and canceling bookings",
        "Manage property availability calendars and update availability",
        "Handle channel listings, pricing, and inventory updates",
        "Send and manage guest communications and messaging",
        "Manage and respond to guest reviews",
        "Configure and manage real-time event webhooks",
        "List and update custom fields and income methods",
        "Support multiple transport protocols including stdio and streamable HTTP"
      ],
      "limitations": [
        "Does not officially support Hostex beyond community project status",
        "Requires Hostex API access token for all operations",
        "Limited to Hostex API v3.0.0 capabilities as exposed by hostex-ts library"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "MCP-compatible client such as Claude Desktop v0.10.0+",
        "Hostex account with API access and valid API access token",
        "npm for development and building from source"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, a full list of available tools, configuration requirements, development setup, architecture overview, and known limitations, making it excellent for users and developers.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# hostex-mcp\n\n[![Release](https://github.com/keithah/hostex-mcp/actions/workflows/release.yml/badge.svg)](https://github.com/keithah/hostex-mcp/actions/workflows/release.yml)\n[![Check hostex-ts Updates](https://github.com/keithah/hostex-mcp/actions/workflows/check-npm-updates.yml/badge.svg)](https://github.com/keithah/hostex-mcp/actions/workflows/check-npm-updates.yml)\n[![Smithery](https://smithery.ai/badge/@keithah/hostex-mcp)](https://smithery.ai/server/@keithah/hostex-mcp)\n\nModel Context Protocol server for the Hostex property management API. Manage your vacation rental properties, reservations, guest communications, and more through Claude and other MCP clients.\n\nBuilt on [hostex-ts](https://www.npmjs.com/package/hostex-ts) - TypeScript client library for Hostex API v3.0.0.",
        "start_pos": 0,
        "end_pos": 788,
        "token_count_estimate": 197,
        "source_type": "readme",
        "agent_id": "13659e54f63c2c84"
      },
      {
        "chunk_id": 1,
        "text": "guest conversations\n- `hostex_get_conversation` - Get conversation details\n- `hostex_send_message` - Send messages to guests\n- `hostex_list_reviews` - Query reviews\n- `hostex_create_review` - Leave reviews or replies\n- `hostex_list_webhooks` - List configured webhooks\n- `hostex_create_webhook` - Register new webhooks\n- `hostex_delete_webhook` - Remove webhooks\n- `hostex_get_listing_calendar` - Get listing calendars\n- `hostex_update_listing_prices` - Update channel prices\n- `hostex_list_custom_channels` - List custom channels\n- `hostex_list_income_methods` - List income methods\n\n## Installation\n\n### Option 1: Via Smithery (Recommended)\n\nInstall directly from Smithery:\n\n```bash\nnpx -y @smithery/cli install @keithah/hostex-mcp --client claude\n```\n\nOr add the hosted server URL to your MCP client:\n\n```\nhttps://server.smithery.ai/@keithah/hostex-mcp/mcp\n```\n\nWhen prompted, provide your Hostex API access token.\n\n### Option 2: MCPB Package\n\n1. Download the latest `.mcpb` file from [Releases](https://github.com/keithah/hostex-mcp/releases)\n2. Double-click the file to install in your MCP client\n3. Configure your Hostex API token when prompted\n\n### Option 3: Manual Installation (Claude Desktop)\n\nAdd to your Claude Desktop config (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS or `~/.config/Claude/claude_desktop_config.json` on Linux):\n\n```json\n{\n  \"mcpServers\": {\n    \"hostex\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"hostex-mcp\"],\n      \"env\": {\n        \"HOSTEX_ACCESS_TOKEN\": \"your_hostex_api_token\"\n      }\n    }\n  }\n}\n```\n\nOr use the Smithery hosted server:\n\n```json\n{\n  \"mcpServers\": {\n    \"hostex\": {\n      \"url\": \"https://server.smithery.ai/@keithah/hostex-mcp/mcp\",\n      \"env\": {\n        \"HOSTEX_ACCESS_TOKEN\": \"your_hostex_api_token\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\nYou need a Hostex API access token.",
        "start_pos": 1848,
        "end_pos": 3717,
        "token_count_estimate": 467,
        "source_type": "readme",
        "agent_id": "13659e54f63c2c84"
      },
      {
        "chunk_id": 2,
        "text": "ttps://server.smithery.ai/@keithah/hostex-mcp/mcp\",\n      \"env\": {\n        \"HOSTEX_ACCESS_TOKEN\": \"your_hostex_api_token\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\nYou need a Hostex API access token. Get yours from your Hostex account settings at https://www.hostex.io/\n\nThe server accepts configuration through the `configSchema`:\n\n- `accessToken` (required): Your Hostex API access token\n\n## Usage Examples\n\nOnce installed, you can ask Claude natural language questions like:\n\n- \"Show me all my Hostex properties\"\n- \"List reservations checking in this week\"\n- \"Block property 12345 for next weekend\"\n- \"Send a welcome message to the guest in conversation ABC123\"\n- \"What reviews have I received this month?\"\n- \"Create a direct booking for property XYZ\"\n\n## Development\n\n### Prerequisites\n\n- Node.js >= 18\n- npm\n\n### Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/keithah/hostex-mcp.git\ncd hostex-mcp\n\n# Install dependencies\nnpm install\n\n# Build for stdio transport\nnpm run build:stdio\n\n# Build for streamable HTTP transport\nnpm run build:shttp\n\n# Build both transports\nnpm run build:all\n```\n\n### Testing Locally\n\n```bash\n# Start dev server with Smithery\nnpm run dev\n```\n\n## Architecture\n\nThis MCP server uses:\n\n- **[@modelcontextprotocol/sdk](https://www.npmjs.com/package/@modelcontextprotocol/sdk)** - MCP protocol implementation\n- **[@smithery/sdk](https://www.npmjs.com/package/@smithery/sdk)** - Multi-transport support\n- **[hostex-ts](https://www.npmjs.com/package/hostex-ts)** - Hostex API client library\n- **[zod](https://www.npmjs.com/package/zod)** - Schema validation\n\n### Transports\n\nThe server supports two transport protocols:\n\n- **stdio** - Standard input/output (for local MCP clients like Claude Desktop)\n- **shttp** - Streamable HTTP (for remote/web-based MCP clients)\n\nBoth are built using [Smithery](https://smithery.ai) for seamless multi-transport support.\n\n## Automatic Updates\n\nThis repository includes a GitHub Actions workflow that:\n\n1. Checks hourly for new releases of `hostex-ts` on npm\n2.",
        "start_pos": 3517,
        "end_pos": 5558,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "13659e54f63c2c84"
      },
      {
        "chunk_id": 3,
        "text": "](https://smithery.ai) for seamless multi-transport support.\n\n## Automatic Updates\n\nThis repository includes a GitHub Actions workflow that:\n\n1. Checks hourly for new releases of `hostex-ts` on npm\n2. Automatically creates a PR to update the dependency\n3. Bumps the patch version\n4. Rebuilds the MCP bundles\n\nWhen you merge the PR and create a new release tag, the MCPB package is automatically built and attached to the GitHub release.\n\n## Requirements\n\n- Node.js >= 18\n- MCP-compatible client (Claude Desktop v0.10.0+, or any MCP client)\n- Hostex account with API access\n\n## Documentation\n\n- [Hostex API Documentation](https://docs.hostex.io/)\n- [hostex-ts npm package](https://www.npmjs.com/package/hostex-ts)\n- [Model Context Protocol](https://modelcontextprotocol.io/)\n\n## License\n\nMIT\n\n## Author\n\nKeith Hadfield\n\n## Support\n\n- **Issues**: [GitHub Issues](https://github.com/keithah/hostex-mcp/issues)\n- **Hostex API**: https://docs.hostex.io/\n- **MCP Documentation**: https://modelcontextprotocol.io/\n\n---\n\n**Note**: This is an unofficial community project and is not officially supported by Hostex.",
        "start_pos": 5358,
        "end_pos": 6463,
        "token_count_estimate": 276,
        "source_type": "readme",
        "agent_id": "13659e54f63c2c84"
      }
    ]
  },
  {
    "agent_id": "a51e2df910aedbcc",
    "name": "ai.smithery/keithah-tessie-mcp",
    "source": "mcp",
    "source_url": "https://github.com/keithah/tessie-mcp",
    "description": "Unofficial integration! ## ‚ú® Key Features ### üí∞ Financial Intelligence - **Smart Charging Cost An‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T04:22:17.289426Z",
    "indexed_at": "2026-02-18T04:07:24.469767",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Tessie MCP Server v2\n\nMCP server rebuilt on the latest developer.tessie.com API. Summary-first tools, composite commands, and live-tested smoke scripts.\n\n## Quickstart\n- Install (Smithery recommended): `npx -y @smithery/cli install @keithah/tessie-mcp`\n- Set `TESSIE_API_KEY` (<https://dash.tessie.com/settings/api>) in your MCP client or `.env`. In Smithery UI the field appears as `accessToken`.\n- Try in a client: ‚ÄúList my vehicles‚Äù ‚Üí `get_active_context`, ‚ÄúLock VIN ...‚Äù ‚Üí `manage_vehicle_command` with `confirm: true`.\n\n## Tools\n- `get_active_context` ‚Äî vehicle roster with next-step guidance.\n- `fetch_vehicle_state` ‚Äî locks, climate, battery, location snapshot.\n- `fetch_vehicle_battery` ‚Äî charging-focused battery view.\n- `search_drives` ‚Äî recent drives with optional date range.\n- `get_driving_path` ‚Äî coordinate series for mapping/analysis.\n- `manage_vehicle_command` ‚Äî lock/unlock, charging, climate, speed limit, sentry, cabin overheat, seat heat/cool, flash/honk, wake.\n\n### Command safety\nDestructive operations require `params.confirm: true`.\n```json\n{\n  \"vin\": \"YOUR_VIN\",\n  \"operation\": \"lock\",\n  \"params\": { \"confirm\": true }\n}\n```\nNon-destructive actions like `flash_lights` / `honk` skip confirmation.\n\n## Local dev & tests\n- Build stdio: `npm run build:stdio`\n- Build shttp: `npm run build:shttp` or `npm run build:all`\n- Tests: `npm test` (includes command validation)\n- Smoke with live Tessie token: `npm run smoke` (raw client), `npm run smoke:tools` (MCP tools)\n\n## Smithery\n- Playground/dev tunnel: `npm run dev` or `npx @smithery/cli dev`\n- Transports: stdio (`npm run build:stdio`), shttp (`npm run build:shttp`, default for publish)\n- Docs index: <https://smithery.ai/docs/llms.txt> ; TS quickstart: `npx create-smithery@latest`\n- Config schema: `.well-known/mcp-config` (expects `TESSIE_API_KEY`). Server card: `.well-known/mcp.json` (aliases in `.well-known/mcp-server.json` and `.well-known/mcp/server.json`).\n- Publish/update: `npm run build:shttp` ‚Üí `npx @smithery/cli publish` (uses `manifest.json`). Ensure `TESSIE_API_KEY` is provided in user config.\n\n## Notes\n- API references cached in `docs/llms-full.txt` and `docs/tessie-api-metadata.json` for offline context.\n- Uses TypeScript MCP SDK and Tessie HTTPS API; all state stays in Tessie. Undo/confirmation is enforced in `manage_vehicle_command`.\n- MCP design references: see `docs/glama-links.md` for glama.ai best-practice articles.\n- Speed-limit operations accept `speed_limit_pin` (sensitive); avoid logging or sharing it.\n- Optional debug logging: set `TESSIE_MCP_DEBUG=1` (or `true`) to emit request failures with URLs/status only (no headers/API keys); retry/backoff is built-in for 429/5xx responses.\n- Tessie client caches read requests (vehicles, state, battery, drives, paths, historical states) per client instance with short TTLs (15-30s), capped size (200 entries), and VIN-scoped invalidation after commands to avoid stale state while keeping token usage low.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Retrieve vehicle roster with next-step guidance",
        "Fetch vehicle state including locks, climate, battery, and location snapshot",
        "Fetch detailed battery charging status",
        "Search recent drives with optional date range filtering",
        "Get driving path coordinates for mapping or analysis",
        "Manage vehicle commands such as lock/unlock, charging, climate control, speed limit, sentry mode, cabin overheat protection, seat heating/cooling, flash lights, honk horn, and wake vehicle"
      ],
      "limitations": [
        "Destructive vehicle commands require explicit confirmation parameter to execute",
        "Speed-limit operations require sensitive speed_limit_pin which should not be logged or shared",
        "Rate limiting and server errors are handled with retry/backoff but may affect request timing",
        "Client-side caching with short TTLs and capped size may cause brief stale state between commands"
      ],
      "requirements": [
        "TESSIE_API_KEY from https://dash.tessie.com/settings/api for authentication",
        "MCP client or environment supporting setting TESSIE_API_KEY or accessToken",
        "Node.js environment for local development and testing with npm scripts",
        "Smithery CLI recommended for installation, development, and publishing"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with usage examples, command safety notes, development and testing guidance, and explicit requirements and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Tessie MCP Server v2\n\nMCP server rebuilt on the latest developer.tessie.com API. Summary-first tools, composite commands, and live-tested smoke scripts.\n\n## Quickstart\n- Install (Smithery recommended): `npx -y @smithery/cli install @keithah/tessie-mcp`\n- Set `TESSIE_API_KEY` (<https://dash.tessie.com/settings/api>) in your MCP client or `.env`. In Smithery UI the field appears as `accessToken`.\n- Try in a client: ‚ÄúList my vehicles‚Äù ‚Üí `get_active_context`, ‚ÄúLock VIN ...‚Äù ‚Üí `manage_vehicle_command` with `confirm: true`.\n\n## Tools\n- `get_active_context` ‚Äî vehicle roster with next-step guidance.\n- `fetch_vehicle_state` ‚Äî locks, climate, battery, location snapshot.\n- `fetch_vehicle_battery` ‚Äî charging-focused battery view.\n- `search_drives` ‚Äî recent drives with optional date range.\n- `get_driving_path` ‚Äî coordinate series for mapping/analysis.\n- `manage_vehicle_command` ‚Äî lock/unlock, charging, climate, speed limit, sentry, cabin overheat, seat heat/cool, flash/honk, wake.\n\n### Command safety\nDestructive operations require `params.confirm: true`.\n```json\n{\n  \"vin\": \"YOUR_VIN\",\n  \"operation\": \"lock\",\n  \"params\": { \"confirm\": true }\n}\n```\nNon-destructive actions like `flash_lights` / `honk` skip confirmation.\n\n## Local dev & tests\n- Build stdio: `npm run build:stdio`\n- Build shttp: `npm run build:shttp` or `npm run build:all`\n- Tests: `npm test` (includes command validation)\n- Smoke with live Tessie token: `npm run smoke` (raw client), `npm run smoke:tools` (MCP tools)\n\n## Smithery\n- Playground/dev tunnel: `npm run dev` or `npx @smithery/cli dev`\n- Transports: stdio (`npm run build:stdio`), shttp (`npm run build:shttp`, default for publish)\n- Docs index: <https://smithery.ai/docs/llms.txt> ; TS quickstart: `npx create-smithery@latest`\n- Config schema: `.well-known/mcp-config` (expects `TESSIE_API_KEY`). Server card: `.well-known/mcp.json` (aliases in `.well-known/mcp-server.json` and `.well-known/mcp/server.json`).\n- Publish/update: `npm run build:shttp` ‚Üí `npx @smithery/cli publish` (uses `manifest.json`).",
        "start_pos": 0,
        "end_pos": 2037,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "a51e2df910aedbcc"
      },
      {
        "chunk_id": 1,
        "text": "card: `.well-known/mcp.json` (aliases in `.well-known/mcp-server.json` and `.well-known/mcp/server.json`).\n- Publish/update: `npm run build:shttp` ‚Üí `npx @smithery/cli publish` (uses `manifest.json`). Ensure `TESSIE_API_KEY` is provided in user config.\n\n## Notes\n- API references cached in `docs/llms-full.txt` and `docs/tessie-api-metadata.json` for offline context.\n- Uses TypeScript MCP SDK and Tessie HTTPS API; all state stays in Tessie. Undo/confirmation is enforced in `manage_vehicle_command`.\n- MCP design references: see `docs/glama-links.md` for glama.ai best-practice articles.\n- Speed-limit operations accept `speed_limit_pin` (sensitive); avoid logging or sharing it.\n- Optional debug logging: set `TESSIE_MCP_DEBUG=1` (or `true`) to emit request failures with URLs/status only (no headers/API keys); retry/backoff is built-in for 429/5xx responses.\n- Tessie client caches read requests (vehicles, state, battery, drives, paths, historical states) per client instance with short TTLs (15-30s), capped size (200 entries), and VIN-scoped invalidation after commands to avoid stale state while keeping token usage low.",
        "start_pos": 1837,
        "end_pos": 2967,
        "token_count_estimate": 282,
        "source_type": "readme",
        "agent_id": "a51e2df910aedbcc"
      }
    ]
  },
  {
    "agent_id": "72adf9245f732f64",
    "name": "ai.smithery/keremurat-json",
    "source": "mcp",
    "source_url": "https://github.com/keremurat/mcp",
    "description": "Compare two JSON files deeply without worrying about key or array order. Detect missing, extra, an‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T13:28:40.425657Z",
    "indexed_at": "2026-02-18T04:07:26.311276",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# üîç JSON Compare MCP Server\n\n<div align=\"center\">\n\n![Python](https://img.shields.io/badge/python-3.11+-blue.svg)\n![MCP](https://img.shields.io/badge/MCP-Compatible-green.svg)\n![Docker](https://img.shields.io/badge/docker-ready-blue.svg)\n![Smithery](https://img.shields.io/badge/Smithery-Deploy%20Ready-orange.svg)\n\n**Derinlemesine ve sƒ±ra-baƒüƒ±msƒ±z JSON kar≈üƒ±la≈ütƒ±rma MCP tool'u**\n\n*ƒ∞ki JSON dosyasƒ±nƒ± akƒ±llƒ±ca kar≈üƒ±la≈ütƒ±rƒ±n, farklƒ±lƒ±klarƒ± tespit edin!*\n\n[üéØ Hƒ±zlƒ± Ba≈ülangƒ±√ß](#-hƒ±zlƒ±-ba≈ülangƒ±√ß) ‚Ä¢ [üì¶ √ñzellikler](#-√∂zellikler) ‚Ä¢ [üöÄ Kullanƒ±m](#-kullanƒ±m) ‚Ä¢ [üß™ Test](#-test)\n\n</div>\n\n---\n\n## ‚ú® √ñzellikler\n\n- üîç **Derinlemesine Kar≈üƒ±la≈ütƒ±rma** - Nested objeler ve array'ler dahil t√ºm seviyeler\n- üîÄ **Sƒ±ra-Baƒüƒ±msƒ±z** - Property ve obje sƒ±ralamasƒ± √∂nemli deƒüil\n- üìä **Detaylƒ± Raporlama** - Eksik, fazla ve farklƒ± deƒüerlerin detaylƒ± raporu\n- üéØ **Path Tracking** - Her farkƒ±n tam konumu (√∂rn: `root.user.settings.theme`)\n- üè∑Ô∏è **Tip Kontrol√º** - Deƒüer tiplerini de kar≈üƒ±la≈ütƒ±rƒ±r\n- ‚ö° **Hƒ±zlƒ± ve Verimli** - Optimize edilmi≈ü recursive algoritma\n\n## üéØ Hƒ±zlƒ± Ba≈ülangƒ±√ß\n\n### 1. Kurulum\n\n```bash\ngit clone https://github.com/yourusername/json-compare-mcp.git\ncd json-compare-mcp\npip install -r requirements.txt\n```\n\n### 2. MCP Server'ƒ± Ba≈ülat\n\n```bash\npython server.py\n```\n\n### 3. Test Et\n\n```bash\npython test_compare.py\n```\n\n## üì¶ Ne ƒ∞√ßeriyor?\n\n```\njson-compare-mcp/\n‚îú‚îÄ‚îÄ üêç app.py              # JSON kar≈üƒ±la≈ütƒ±rma implementasyonu\n‚îú‚îÄ‚îÄ üöÄ server.py           # MCP server yapƒ±landƒ±rmasƒ±\n‚îú‚îÄ‚îÄ üß™ test_compare.py     # Test suite\n‚îú‚îÄ‚îÄ üìã requirements.txt    # Python baƒüƒ±mlƒ±lƒ±klarƒ±\n‚îú‚îÄ‚îÄ üê≥ Dockerfile          # Container yapƒ±landƒ±rmasƒ±\n‚îú‚îÄ‚îÄ ‚öôÔ∏è smithery.yaml       # Smithery deployment config\n‚îî‚îÄ‚îÄ üìÅ test_samples/       # √ñrnek JSON dosyalarƒ±\n    ‚îú‚îÄ‚îÄ file1.json\n    ‚îú‚îÄ‚îÄ file2.json\n    ‚îî‚îÄ‚îÄ file3_different.json\n```\n\n## üöÄ Kullanƒ±m\n\n### MCP Tool: `compare_json`\n\n```python\n# ƒ∞ki JSON string'ini kar≈üƒ±la≈ütƒ±r\ncompare_json(\n    json1='{\"name\": \"Ahmet\", \"age\": 30, \"city\": \"Istanbul\"}',\n    json2='{\"age\": 30, \"city\": \"Istanbul\", \"name\": \"Ahmet\"}'\n)\n```\n\n**Not:** Artƒ±k dosya yolu deƒüil, direkt JSON string kullanƒ±n!\n\n### √áƒ±ktƒ± Formatƒ±\n\n```json\n{\n  \"status\": \"different\",\n  \"total_differences\": 11,\n  \"differences\": [\n    {\n      \"type\": \"missing_key\",\n      \"path\": \"root.user.settings.language\",\n      \"key\": \"language\",\n      \"file1_value\": \"tr\",\n      \"message\": \"Key 'language' exists in file1 but missing in file2\"\n    },\n    {\n      \"type\": \"value_mismatch\",\n      \"path\": \"root.user.name\",\n      \"file1_value\": \"Ahmet Yƒ±lmaz\",\n      \"file2_value\": \"Mehmet Demir\",\n      \"message\": \"Value mismatch at 'root.user.name'\"\n    }\n  ],\n  \"summary\": {\n    \"missing_keys\": 1,\n    \"extra_keys\": 2,\n    \"value_mismatches\": 8,\n    \"type_mismatches\": 0\n  }\n}\n```\n\n## üîç Kar≈üƒ±la≈ütƒ±rma Mantƒ±ƒüƒ±\n\n### Seviye 1: √úst Seviye Obje Kar≈üƒ±la≈ütƒ±rma\n\n- ‚úÖ Her iki JSON'daki √ºst seviye objeleri/anahtarlarƒ± tespit et\n- ‚úÖ **SIRALAMA √ñNEMLƒ∞ DEƒûƒ∞L** - Objeler farklƒ± sƒ±rada olabilir\n- ‚úÖ Eksik/fazla objeleri tespit et ve uyar\n- ‚úÖ E≈üle≈üen objeler i√ßin Seviye 2'ye ge√ß\n\n### Seviye 2: ƒ∞√ß Nesne/Property Kar≈üƒ±la≈ütƒ±rma\n\n- ‚úÖ T√ºm property'leri kar≈üƒ±la≈ütƒ±r\n- ‚úÖ **SIRALAMA √ñNEMLƒ∞ DEƒûƒ∞L** - Property'ler farklƒ± sƒ±rada olabilir\n- ‚úÖ Property varlƒ±ƒüƒ±, tip ve i√ßerik kontrol√º\n- ‚úÖ Array'ler i√ßin sƒ±ra-baƒüƒ±msƒ±z kar≈üƒ±la≈ütƒ±rma\n\n### Fark Tipleri\n\n| Tip | A√ßƒ±klama |\n|-----|----------|\n| `missing_key` | Anahtar file1'de var, file2'de yok |\n| `extra_key` | Anahtar file2'de var, file1'de yok |\n| `value_mismatch` | Deƒüerler farklƒ± |\n| `type_mismatch` | Veri tipleri farklƒ± |\n\n## üß™ Test\n\n### Test Suite'i √áalƒ±≈ütƒ±r\n\n```bash\npython test_compare.py\n```\n\n### Test Senaryolarƒ±\n\n1. **Sƒ±ra-baƒüƒ±msƒ±z test**: Aynƒ± i√ßerik, farklƒ± sƒ±ralama ‚Üí `identical`\n2. **Farklƒ±lƒ±k tespiti**: Farklƒ± deƒüerler ‚Üí detaylƒ± fark raporu\n3. **Kendisi ile**: Aynƒ± dosya ‚Üí `identical`\n\n### Test √áƒ±ktƒ±sƒ±\n\n```\n‚úÖ Test 1 - Sƒ±ra-baƒüƒ±msƒ±z: BA≈ûARILI\n‚úÖ Test 2 - Farklƒ±lƒ±k tespiti: BA≈ûARILI\n‚úÖ Test 3 - Aynƒ± dosya: BA≈ûARILI\n\nüîç Test 2 Detaylarƒ±:\n   - Eksik anahtarlar: 1\n   - Fazla anahtarlar: 2\n   - Deƒüer uyu≈ümazlƒ±klarƒ±: 8\n   - Tip uyu≈ümazlƒ±klarƒ±: 0\n   - Toplam fark: 11\n```\n\n## üõ†Ô∏è Geli≈ütirme\n\n### Yeni Tool Eklemek\n\n[app.py](app.py) dosyasƒ±na yeni fonksiyon ekle:\n\n```python\ndef myNewTool(param: str) -> str:\n    # Your logic\n    return result\n```\n\n[server.py](server.py) dosyasƒ±na MCP tool olarak kaydet:\n\n```python\n@mcp.tool()\nasync def my_new_tool(param: str) -> str:\n    \"\"\"Tool a√ßƒ±klamasƒ±\"\"\"\n    return myNewTool(param)\n```\n\n## üöÄ Deployment\n\n### Smithery Deployment\n\n1. Repository'yi GitHub'a push et\n2. Smithery'de repository'yi baƒüla\n3. Tek tƒ±kla deploy et! üéâ\n\n### Docker Deployment\n\n```bash\ndocker build -t json-compare-mcp .\ndocker run -p 8000:8000 json-compare-mcp\n```\n\n### Manuel Deployment\n\n```bash\npip install -r requirements.txt\npython server.py\n```\n\n## üìã Gereksinimler\n\n- Python 3.11+\n- mcp\n- requests\n\n## ü§ù Katkƒ±da Bulunma\n\n1. üç¥ Fork et\n2. üå± Feature branch olu≈ütur\n3. üíª Deƒüi≈üiklikleri yap\n4. üß™ Test et\n5. üìù Pull request g√∂nder\n\n## üìÑ Lisans\n\nMIT License - Detaylar i√ßin [LICENSE](LICENSE) dosyasƒ±na bakƒ±n.\n\n---\n\n<div align=\"center\">\n\n**Made with ‚ù§Ô∏è for better JSON comparison**\n\n*Sƒ±ra-baƒüƒ±msƒ±z, derinlemesine, g√ºvenilir* üöÄ\n\n</div>\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Compare two JSON strings deeply including nested objects and arrays",
        "Perform order-independent comparison of JSON properties and objects",
        "Generate detailed reports of differences including missing, extra, and mismatched values",
        "Track and report the exact JSON path of each difference",
        "Compare data types of values in JSON objects",
        "Run as an MCP server with a callable tool interface",
        "Support deployment via Docker and Smithery",
        "Provide a test suite to validate comparison functionality"
      ],
      "limitations": [
        "Cannot compare JSON files directly, only JSON strings are accepted",
        "Comparison is limited to JSON data structures; no support for other data formats",
        "No explicit mention of rate limits or concurrency handling",
        "No support for partial or fuzzy matching beyond exact value/type checks"
      ],
      "requirements": [
        "Python version 3.11 or higher",
        "mcp Python package",
        "requests Python package"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation includes installation instructions, detailed feature descriptions, usage examples with output format, testing procedures, deployment options, and explicit requirements, providing comprehensive guidance.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# üîç JSON Compare MCP Server\n\n<div align=\"center\">\n\n![Python](https://img.shields.io/badge/python-3.11+-blue.svg)\n![MCP](https://img.shields.io/badge/MCP-Compatible-green.svg)\n![Docker](https://img.shields.io/badge/docker-ready-blue.svg)\n![Smithery](https://img.shields.io/badge/Smithery-Deploy%20Ready-orange.svg)\n\n**Derinlemesine ve sƒ±ra-baƒüƒ±msƒ±z JSON kar≈üƒ±la≈ütƒ±rma MCP tool'u**\n\n*ƒ∞ki JSON dosyasƒ±nƒ± akƒ±llƒ±ca kar≈üƒ±la≈ütƒ±rƒ±n, farklƒ±lƒ±klarƒ± tespit edin!*\n\n[üéØ Hƒ±zlƒ± Ba≈ülangƒ±√ß](#-hƒ±zlƒ±-ba≈ülangƒ±√ß) ‚Ä¢ [üì¶ √ñzellikler](#-√∂zellikler) ‚Ä¢ [üöÄ Kullanƒ±m](#-kullanƒ±m) ‚Ä¢ [üß™ Test](#-test)\n\n</div>\n\n---\n\n## ‚ú® √ñzellikler\n\n- üîç **Derinlemesine Kar≈üƒ±la≈ütƒ±rma** - Nested objeler ve array'ler dahil t√ºm seviyeler\n- üîÄ **Sƒ±ra-Baƒüƒ±msƒ±z** - Property ve obje sƒ±ralamasƒ± √∂nemli deƒüil\n- üìä **Detaylƒ± Raporlama** - Eksik, fazla ve farklƒ± deƒüerlerin detaylƒ± raporu\n- üéØ **Path Tracking** - Her farkƒ±n tam konumu (√∂rn: `root.user.settings.theme`)\n- üè∑Ô∏è **Tip Kontrol√º** - Deƒüer tiplerini de kar≈üƒ±la≈ütƒ±rƒ±r\n- ‚ö° **Hƒ±zlƒ± ve Verimli** - Optimize edilmi≈ü recursive algoritma\n\n## üéØ Hƒ±zlƒ± Ba≈ülangƒ±√ß\n\n### 1. Kurulum\n\n```bash\ngit clone https://github.com/yourusername/json-compare-mcp.git\ncd json-compare-mcp\npip install -r requirements.txt\n```\n\n### 2. MCP Server'ƒ± Ba≈ülat\n\n```bash\npython server.py\n```\n\n### 3. Test Et\n\n```bash\npython test_compare.py\n```\n\n## üì¶ Ne ƒ∞√ßeriyor?",
        "start_pos": 0,
        "end_pos": 1324,
        "token_count_estimate": 331,
        "source_type": "readme",
        "agent_id": "72adf9245f732f64"
      },
      {
        "chunk_id": 1,
        "text": "N string'ini kar≈üƒ±la≈ütƒ±r\ncompare_json(\n    json1='{\"name\": \"Ahmet\", \"age\": 30, \"city\": \"Istanbul\"}',\n    json2='{\"age\": 30, \"city\": \"Istanbul\", \"name\": \"Ahmet\"}'\n)\n```\n\n**Not:** Artƒ±k dosya yolu deƒüil, direkt JSON string kullanƒ±n!\n\n### √áƒ±ktƒ± Formatƒ±\n\n```json\n{\n  \"status\": \"different\",\n  \"total_differences\": 11,\n  \"differences\": [\n    {\n      \"type\": \"missing_key\",\n      \"path\": \"root.user.settings.language\",\n      \"key\": \"language\",\n      \"file1_value\": \"tr\",\n      \"message\": \"Key 'language' exists in file1 but missing in file2\"\n    },\n    {\n      \"type\": \"value_mismatch\",\n      \"path\": \"root.user.name\",\n      \"file1_value\": \"Ahmet Yƒ±lmaz\",\n      \"file2_value\": \"Mehmet Demir\",\n      \"message\": \"Value mismatch at 'root.user.name'\"\n    }\n  ],\n  \"summary\": {\n    \"missing_keys\": 1,\n    \"extra_keys\": 2,\n    \"value_mismatches\": 8,\n    \"type_mismatches\": 0\n  }\n}\n```\n\n## üîç Kar≈üƒ±la≈ütƒ±rma Mantƒ±ƒüƒ±\n\n### Seviye 1: √úst Seviye Obje Kar≈üƒ±la≈ütƒ±rma\n\n- ‚úÖ Her iki JSON'daki √ºst seviye objeleri/anahtarlarƒ± tespit et\n- ‚úÖ **SIRALAMA √ñNEMLƒ∞ DEƒûƒ∞L** - Objeler farklƒ± sƒ±rada olabilir\n- ‚úÖ Eksik/fazla objeleri tespit et ve uyar\n- ‚úÖ E≈üle≈üen objeler i√ßin Seviye 2'ye ge√ß\n\n### Seviye 2: ƒ∞√ß Nesne/Property Kar≈üƒ±la≈ütƒ±rma\n\n- ‚úÖ T√ºm property'leri kar≈üƒ±la≈ütƒ±r\n- ‚úÖ **SIRALAMA √ñNEMLƒ∞ DEƒûƒ∞L** - Property'ler farklƒ± sƒ±rada olabilir\n- ‚úÖ Property varlƒ±ƒüƒ±, tip ve i√ßerik kontrol√º\n- ‚úÖ Array'ler i√ßin sƒ±ra-baƒüƒ±msƒ±z kar≈üƒ±la≈ütƒ±rma\n\n### Fark Tipleri\n\n| Tip | A√ßƒ±klama |\n|-----|----------|\n| `missing_key` | Anahtar file1'de var, file2'de yok |\n| `extra_key` | Anahtar file2'de var, file1'de yok |\n| `value_mismatch` | Deƒüerler farklƒ± |\n| `type_mismatch` | Veri tipleri farklƒ± |\n\n## üß™ Test\n\n### Test Suite'i √áalƒ±≈ütƒ±r\n\n```bash\npython test_compare.py\n```\n\n### Test Senaryolarƒ±\n\n1. **Sƒ±ra-baƒüƒ±msƒ±z test**: Aynƒ± i√ßerik, farklƒ± sƒ±ralama ‚Üí `identical`\n2. **Farklƒ±lƒ±k tespiti**: Farklƒ± deƒüerler ‚Üí detaylƒ± fark raporu\n3.",
        "start_pos": 1848,
        "end_pos": 3726,
        "token_count_estimate": 469,
        "source_type": "readme",
        "agent_id": "72adf9245f732f64"
      },
      {
        "chunk_id": 2,
        "text": "≈ütƒ±r\n\n```bash\npython test_compare.py\n```\n\n### Test Senaryolarƒ±\n\n1. **Sƒ±ra-baƒüƒ±msƒ±z test**: Aynƒ± i√ßerik, farklƒ± sƒ±ralama ‚Üí `identical`\n2. **Farklƒ±lƒ±k tespiti**: Farklƒ± deƒüerler ‚Üí detaylƒ± fark raporu\n3. **Kendisi ile**: Aynƒ± dosya ‚Üí `identical`\n\n### Test √áƒ±ktƒ±sƒ±\n\n```\n‚úÖ Test 1 - Sƒ±ra-baƒüƒ±msƒ±z: BA≈ûARILI\n‚úÖ Test 2 - Farklƒ±lƒ±k tespiti: BA≈ûARILI\n‚úÖ Test 3 - Aynƒ± dosya: BA≈ûARILI\n\nüîç Test 2 Detaylarƒ±:\n   - Eksik anahtarlar: 1\n   - Fazla anahtarlar: 2\n   - Deƒüer uyu≈ümazlƒ±klarƒ±: 8\n   - Tip uyu≈ümazlƒ±klarƒ±: 0\n   - Toplam fark: 11\n```\n\n## üõ†Ô∏è Geli≈ütirme\n\n### Yeni Tool Eklemek\n\n[app.py](app.py) dosyasƒ±na yeni fonksiyon ekle:\n\n```python\ndef myNewTool(param: str) -> str:\n    # Your logic\n    return result\n```\n\n[server.py](server.py) dosyasƒ±na MCP tool olarak kaydet:\n\n```python\n@mcp.tool()\nasync def my_new_tool(param: str) -> str:\n    \"\"\"Tool a√ßƒ±klamasƒ±\"\"\"\n    return myNewTool(param)\n```\n\n## üöÄ Deployment\n\n### Smithery Deployment\n\n1. Repository'yi GitHub'a push et\n2. Smithery'de repository'yi baƒüla\n3. Tek tƒ±kla deploy et! üéâ\n\n### Docker Deployment\n\n```bash\ndocker build -t json-compare-mcp .\ndocker run -p 8000:8000 json-compare-mcp\n```\n\n### Manuel Deployment\n\n```bash\npip install -r requirements.txt\npython server.py\n```\n\n## üìã Gereksinimler\n\n- Python 3.11+\n- mcp\n- requests\n\n## ü§ù Katkƒ±da Bulunma\n\n1. üç¥ Fork et\n2. üå± Feature branch olu≈ütur\n3. üíª Deƒüi≈üiklikleri yap\n4. üß™ Test et\n5. üìù Pull request g√∂nder\n\n## üìÑ Lisans\n\nMIT License - Detaylar i√ßin [LICENSE](LICENSE) dosyasƒ±na bakƒ±n.\n\n---\n\n<div align=\"center\">\n\n**Made with ‚ù§Ô∏è for better JSON comparison**\n\n*Sƒ±ra-baƒüƒ±msƒ±z, derinlemesine, g√ºvenilir* üöÄ\n\n</div>",
        "start_pos": 3526,
        "end_pos": 5122,
        "token_count_estimate": 398,
        "source_type": "readme",
        "agent_id": "72adf9245f732f64"
      }
    ]
  },
  {
    "agent_id": "9f69645c47b867e4",
    "name": "ai.smithery/kesslerio-attio-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/kesslerio/attio-mcp-server",
    "description": "Enable AI assistants to interact directly with your Attio CRM data through natural language querie‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-10T17:52:23.110358Z",
    "indexed_at": "2026-02-18T04:07:27.980097",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Attio MCP Server\n\n[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n[![npm version](https://badge.fury.io/js/attio-mcp.svg)](https://badge.fury.io/js/attio-mcp)\n[![Node.js Version](https://img.shields.io/badge/node-%3E%3D20.0.0-brightgreen.svg)](https://nodejs.org/)\n[![GitHub Release](https://img.shields.io/github/v/release/kesslerio/attio-mcp-server)](https://github.com/kesslerio/attio-mcp-server/releases)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/kesslerio/attio-mcp-server)\n\nA comprehensive Model Context Protocol (MCP) server for [Attio](https://attio.com/), providing **complete CRM surface coverage**. This server enables AI assistants like Claude and ChatGPT to interact directly with your entire Attio workspace through natural language‚Äîmanage Deals, Tasks, Lists, People, Companies, Records, and Notes without falling back to raw API calls.\n\n## üéØ What is Attio MCP Server?\n\nTransform your CRM workflows with AI-powered automation. Instead of clicking through multiple screens, simply ask Claude or ChatGPT to find prospects, update records, manage pipelines, and analyze your data using natural language commands.\n\n**üéâ v1.0.0 Milestone**: Complete Attio CRM surface coverage with full ChatGPT Developer Mode integration.\n\n> \"Find all AI companies with 50+ employees that we haven't contacted in 30 days and add them to our Q1 outreach list\"\n\n## üöÄ **ChatGPT Developer Mode Integration**\n\n> **‚ö†Ô∏è Smithery Temporarily Unavailable**: Smithery has changed their deployment model to require external hosting. We're working on Cloudflare Worker hosting for ChatGPT users. In the meantime, use [Tier 4 (Cloudflare Worker)](#tier-4-cloudflare-worker-remote-deployment) for ChatGPT/remote access.\n\nChatGPT Pro/Plus users can access the Attio toolset through natural language using a self-hosted Cloudflare Worker:\n\n- **üîê Built-in Approval Flows**: MCP safety annotations auto-approve read operations, request approval for writes\n- **üåê OAuth Integration**: Self-hosted OAuth via Cloudflare Worker deployment\n- **üí¨ Natural Language CRM**: Manage your entire Attio workspace through conversational AI\n- **üìñ Setup Guide**: See [ChatGPT Developer Mode docs](./docs/chatgpt-developer-mode.md) and [Cloudflare Worker Guide](./examples/cloudflare-mcp-server/README.md)\n\n## ‚ú® Core Features & Implementation Status\n\n### üéØ **Universal Tools Architecture** (14 Tools)\n\n**68% Tool Reduction**: Consolidated 40+ resource-specific tools into 14 universal operations for consistent, powerful CRM management.\n\n- **High Performance**: 89.7% speed improvement with 227KB memory reduction (PR #483)\n- **Enterprise Quality**: 97.15/100 production readiness score with zero breaking changes\n- **Clean Architecture**: Complete production-test separation with mock factory pattern\n\n### üìä **Feature Implementation Status**\n\n#### ‚úÖ **Complete CRM Surface Coverage**\n\n- **Companies**: Search, Create, Update, Delete, Advanced Search, Relationship Search\n- **People**: Search, Create, Update, Delete, Advanced Search, Relationship Search\n- **Deals**: Full CRUD operations with intelligent field mapping and stage validation\n- **Tasks**: Create, Update, Delete, Search with multi-assignee support\n- **Lists**: Full CRUD operations, filtering, advanced filtering, entry management\n- **Notes**: Create and list operations for all record types\n- **Records**: Universal CRUD operations across all resource types\n- **Batch Operations**: Create, Update, Delete with chunking and error handling\n- **Content Search**: Universal search capabilities across notes, tasks, and lists\n- **Relationship Navigation**: Bidirectional company‚Üîperson‚Üîdeal relationships\n- **Advanced Filtering**: Sophisticated query capabilities with intelligent field mapping\n\n### üìä **Company Management**\n\n- **Universal Search**: Find companies with `search_records` and `search_records_advanced`\n- **Full CRUD**: Create, read, update, and delete with universal record operations\n- **Relationship Discovery**: Find companies through `search_records_by_relationship`\n- **Batch Operations**: Process hundreds of companies with `batch_records`\n- **Detailed Information**: Get contact, business, and social info with `get_record_info`\n\n### üë• **People Management**\n\n- **Universal Contact Search**: Find people by any criteria using universal search tools\n- **Relationship Tracking**: Link people to companies with `search_records_by_relationship`\n- **Activity Timeline**: Track interactions with `search_records_by_content` and `search_records_by_timeframe`\n- **Advanced Filtering**: Multi-attribute search with universal filtering\n- **Bulk Operations**: Efficiently manage contacts with universal batch operations\n\n### üìã **Lists & Pipeline Management** (4 Tools + 8 Deprecated)\n\n- **Active Tools**: 4 consolidated tools with auto-mode detection ([Migration Guide](./docs/migration/v2-list-tools.md))\n  - `filter-list-entries` - Unified filtering with 4 modes\n  - `manage-list-entry` - Unified entry management with 3 modes\n  - `get-list-entries` - Retrieve list entries\n  - `get-record-list-memberships` - Find record's list memberships\n- **Deprecated (v2.0.0 removal)**: 8 legacy tools replaced by consolidated versions\n- **Pipeline Operations**: Move deals through sales stages\n- **Smart Segmentation**: Create and manage targeted contact lists\n- **Advanced Filtering**: Complex multi-condition filtering with AND/OR logic\n- **Entry Management**: Add, remove, and update list memberships\n- **Deal Tracking**: Monitor opportunities and revenue pipeline\n- **Deal Defaults**: Configurable default stage, owner, and currency for streamlined deal creation\n\n### ‚úÖ **Task Management**\n\n- **Universal Task Operations**: Create, update, and manage tasks with universal tools\n- **Record Linking**: Associate tasks with any record type using `resource_type` parameter\n- **Progress Tracking**: Monitor completion with universal search and filtering\n- **Team Coordination**: Streamline follow-ups with consistent universal operations\n\n### üîß **Advanced Capabilities**\n\n- **Batch Processing**: Handle bulk operations with error tracking\n- **Enhanced Filtering**: Text, numeric, date, boolean, and relationship filters with timeframe search (Issue #475)\n- **Data Export**: JSON serialization for integrations\n- **Real-time Updates**: Live data synchronization with Attio\n\n### üß† **Claude Skills**\n\nSupercharge Claude's Attio knowledge with pre-built skills that prevent common errors and teach best practices.\n\n| Skill                      | Purpose                                        | Setup                                           |\n| -------------------------- | ---------------------------------------------- | ----------------------------------------------- |\n| **attio-mcp-usage**        | Error prevention + universal workflow patterns | Bundled - just use it                           |\n| **attio-workspace-schema** | YOUR workspace's exact field names and options | `npx attio-discover generate-skill --all --zip` |\n| **attio-skill-generator**  | Create custom workflow skills (advanced)       | Python + prompting                              |\n\n**Quick Start** (solves \"wrong field name\" errors):\n\n```bash\nnpx attio-discover generate-skill --all --zip\n# Import ZIP into Claude Desktop: Settings > Skills > Install Skill\n```\n\nSee [Skills Documentation](./docs/usage/skills/README.md) for complete setup and usage guides.\n\n### üí¨ **Pre-Built Prompts** (10 Prompts)\n\nIntelligent shortcuts that help Claude work faster with your CRM data:\n\n- **Search & Find** (5): people_search, company_search, deal_search, meeting_prep, pipeline_health\n- **Take Actions** (4): log_activity, create_task, advance_deal, add_to_list with dry-run safety\n- **Research & Qualify** (1): qualify_lead with automated web research and BANT/CHAMP frameworks\n- **Token-efficient**: 300-700 tokens per prompt with consistent formatting\n- **Discoverable**: Claude automatically suggests relevant prompts for your tasks\n\nSee [Using Out-of-the-Box Prompts](#-using-out-of-the-box-prompts) for detailed documentation and examples.\n\n## üéØ **Using Out-of-the-Box Prompts**\n\n**NEW**: 10 pre-built MCP prompts for common Sales workflows. No setup required‚Äîjust use them!\n\n### Available Prompts\n\n| Prompt               | Description                                     | Key Arguments                                    | Example                            |\n| -------------------- | ----------------------------------------------- | ------------------------------------------------ | ---------------------------------- |\n| `people_search.v1`   | Find people by title, company, territory        | `query`, `limit`, `format`                       | Find AE in fintech, SF             |\n| `company_search.v1`  | Query companies by domain, segment, plan        | `query`, `limit`, `format`                       | Find SaaS companies >100 employees |\n| `deal_search.v1`     | Filter deals by owner, stage, value, close date | `query`, `limit`, `format`                       | Find deals >$50k closing Q1        |\n| `log_activity.v1`    | Log calls/meetings/emails to records            | `target`, `type`, `summary`, `dry_run`           | Log call with Nina at Acme         |\n| `create_task.v1`     | Create tasks with natural language due dates    | `title`, `content`, `due_date`, `dry_run`        | Create task: Follow up tomorrow    |\n| `advance_deal.v1`    | Move deal to target stage with next action      | `deal`, `target_stage`, `create_task`, `dry_run` | Advance deal to \"Proposal Sent\"    |\n| `add_to_list.v1`     | Add records to a List by name or ID             | `records`, `list`, `dry_run`                     | Add 5 companies to Q1 Outreach     |\n| `qualify_lead.v1`    | Research lead with web + BANT/CHAMP scoring     | `target`, `framework`, `limit_web`, `dry_run`    | Qualify Acme Corp with BANT        |\n| `meeting_prep.v1`    | 360¬∞ prep: notes, tasks, deals, agenda          | `target`, `format`, `verbosity`                  | Prep for meeting with Acme CEO     |\n| `pipeline_health.v1` | Weekly snapshot: created/won/slipped + risks    | `owner`, `timeframe`, `segment`                  | Pipeline health for @me last 30d   |\n\n### Quick Examples\n\n```bash\n# Search for prospects\n\"Use people_search.v1: Find Account Executives in San Francisco at fintech companies, limit 25\"\n\n# Log activity\n\"Use log_activity.v1: Log a call with Nina at Acme Corp, discussed Q1 pricing, create follow-up task\"\n\n# Qualify a lead (with web research)\n\"Use qualify_lead.v1: Qualify Acme Corp using BANT framework, dry run mode\"\n\n# Meeting prep\n\"Use meeting_prep.v1: Prepare for meeting with contact at Acme Corp\"\n```\n\n### Universal Arguments\n\n**All read prompts** support:\n\n- `format`: `table` | `json` | `ids` (default: `table`)\n- `fields_preset`: `sales_short` | `full` (default: `sales_short`)\n- `verbosity`: `brief` | `normal` (default: `brief`)\n\n**All write prompts** support:\n\n- `dry_run`: `true` | `false` (default: `false`) - Preview changes without executing\n\n### Token Awareness Features\n\nPrompts include built-in token optimization:\n\n- **Budget Guards**: Prompts stay within token limits (people_search <500, qualify_lead <400)\n- **Dev Metadata**: Set `MCP_DEV_META=true` for token counts in responses\n- **Telemetry**: Set `PROMPT_TELEMETRY_ENABLED=true` for usage logging\n- **Configurable Limits**: Override with `MAX_PROMPT_TOKENS` environment variable\n\nFor complete prompt documentation, see [docs/prompts/v1-catalog.md](./docs/prompts/v1-catalog.md).\n\n## ‚ö†Ô∏è **Known Limitations & Important Notes**\n\n### **Current Limitations**\n\n- **Field Parameter Filtering**: Tasks endpoint `/objects/tasks/attributes` has limitations, handled with fallback patterns\n- **Pagination**: Tasks pagination uses in-memory handling due to API constraints\n\n### **API Compatibility**\n\n- **Universal Tools**: Primary interface (14 tools) - recommended for all new integrations\n- **Legacy Tools**: Available via `DISABLE_UNIVERSAL_TOOLS=true` environment variable (deprecated)\n- **Lists API**: Fully functional with complete CRUD operations (contrary to some outdated documentation)\n\n### ü§ù **OpenAI MCP Compatibility**\n\n- **Developer Mode Ready**: Every tool now publishes MCP safety annotations (`readOnlyHint`, `destructiveHint`) so OpenAI Developer Mode can auto-approve reads and request confirmation for writes.\n- **Full Tool Access (Default)**: All 35 tools are exposed by default (21 universal + 11 list + 3 workspace member). Do NOT set `ATTIO_MCP_TOOL_MODE` in Smithery configuration for full access.\n- **Search-Only Mode**: To restrict to read-only tools (`search`, `fetch`, `aaa-health-check`), explicitly configure `ATTIO_MCP_TOOL_MODE: 'search'` in Smithery dashboard when Developer Mode is unavailable.\n- **Detailed Guide**: See [docs/chatgpt-developer-mode.md](./docs/chatgpt-developer-mode.md) for environment variables, approval flows, and validation tips.\n- **User Documentation**: See the [ChatGPT Developer Mode docs](./docs/chatgpt-developer-mode.md) for a complete walkthrough of approval flows and setup instructions.\n\n### **Performance Considerations**\n\n- **Batch Operations**: Optimized with chunking, rate limiting, and error recovery\n- **Large Datasets**: Automatic pagination and field filtering for optimal performance\n- **Rate Limiting**: Built-in protection against API rate limits with exponential backoff\n\nFor detailed troubleshooting and solutions, see [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) and [GitHub Issues](https://github.com/kesslerio/attio-mcp-server/issues).\n\n## üéØ **Advanced Search Filters**\n\nBuild powerful CRM queries with multi-criteria AND/OR filtering. See the [Advanced Search Guide](./docs/usage/advanced-search.md) for complete examples and operator reference.\n\n## üöÄ Installation\n\n> ‚ö†Ô∏è **IMPORTANT: Correct Package Name**\n>\n> The npm package name is **`attio-mcp`** (not `attio-mcp-server`).\n> The GitHub repository is named `attio-mcp-server`, but the npm package was renamed to `attio-mcp` in June 2025.\n> Installing `attio-mcp-server` will give you an outdated v0.0.2 release with only 4 legacy tools.\n\n### Client Compatibility\n\n| Client             | Local Install (Tier 1-2) | Cloudflare Worker (Tier 3) |\n| ------------------ | ------------------------ | -------------------------- |\n| Claude Desktop     | ‚úÖ Recommended           | ‚úÖ Full support            |\n| Claude Web         | N/A                      | ‚úÖ Full support            |\n| ChatGPT (Pro/Plus) | N/A                      | ‚úÖ Recommended             |\n| Cursor IDE         | ‚úÖ Full support          | ‚úÖ Full support            |\n| Claude Code (CLI)  | ‚úÖ Recommended           | Partial                    |\n\n**Choose your installation method:**\n\n- **Most users**: Use [Tier 1 (Shell Installers)](#tier-1-shell-installers) - one command, automatic setup\n- **Power users**: Use [Tier 2 (Manual)](#tier-2-manual-configuration) - full control over configuration\n- **ChatGPT/Teams/Enterprise**: Use [Tier 3 (Cloudflare Worker)](#tier-3-cloudflare-worker-remote-deployment) - self-hosted, multi-user OAuth\n\n---\n\n### Tier 1: Shell Installers\n\n> **Best for**: Developers who prefer local installations with automatic configuration.\n\nOne-command scripts that install `attio-mcp` and configure your client automatically.\n\n#### Claude Desktop\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/kesslerio/attio-mcp-server/main/scripts/install-claude-desktop.sh | bash\n```\n\n#### Cursor IDE\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/kesslerio/attio-mcp-server/main/scripts/install-cursor.sh | bash\n```\n\n#### Claude Code (CLI)\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/kesslerio/attio-mcp-server/main/scripts/install-claude-code.sh | bash\n```\n\nThese scripts will:\n\n- Install `attio-mcp` npm package globally (if needed)\n- Backup existing configuration files\n- Prompt for your Attio API key\n- Configure the MCP server for your client\n- Print next steps and restart instructions\n\n---\n\n### Tier 2: Manual Configuration\n\n> **Best for**: Power users who prefer full control or use unsupported clients.\n\n<details>\n<summary><strong>Claude Desktop Manual Setup</strong></summary>\n\n#### Step 1: Install attio-mcp\n\n```bash\nnpm install -g attio-mcp\n```\n\n#### Step 2: Find your config file\n\n- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Linux**: `~/.config/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%\\Claude\\claude_desktop_config.json`\n\n#### Step 3: Add configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"attio-mcp\": {\n      \"command\": \"attio-mcp\",\n      \"env\": {\n        \"ATTIO_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n#### Step 4: Restart Claude Desktop completely (quit and reopen)\n\n</details>\n\n<details>\n<summary><strong>Cursor IDE Manual Setup</strong></summary>\n\n#### Step 1: Install attio-mcp\n\n```bash\nnpm install -g attio-mcp\n```\n\n#### Step 2: Edit config file\n\nLocation: `~/.cursor/mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"attio-mcp\": {\n      \"command\": \"attio-mcp\",\n      \"env\": {\n        \"ATTIO_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n#### Step 3: Restart Cursor\n\n</details>\n\n<details>\n<summary><strong>Claude Code (CLI) Manual Setup</strong></summary>\n\n#### Option A: Using Claude CLI command (recommended)\n\n```bash\necho '{\"command\":\"attio-mcp\",\"env\":{\"ATTIO_API_KEY\":\"your_key_here\"}}' | claude mcp add-json attio-mcp --stdin -s user\n```\n\n#### Option B: Manual config edit\n\nEdit `~/.claude/settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"attio-mcp\": {\n      \"command\": \"attio-mcp\",\n      \"env\": {\n        \"ATTIO_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><strong>Building from Source</strong></summary>\n\nFor development or custom deployments:\n\n```bash\ngit clone https://github.com/kesslerio/attio-mcp-server.git\ncd attio-mcp-server\nnpm install\nnpm run build\n```\n\nRun directly:\n\n```bash\nATTIO_API_KEY=your_key node dist/index.js\n```\n\n</details>\n\n<details>\n<summary><strong>NPM Global Install</strong></summary>\n\n```bash\n# Global installation for CLI usage\nnpm install -g attio-mcp\n\n# Or local installation for project integration\nnpm install attio-mcp\n```\n\n</details>\n\n---\n\n### Tier 3: Cloudflare Worker (Remote Deployment)\n\n> **Best for**: Teams needing centralized OAuth, multi-user access, mobile access, or running MCP without local installation.\n\nDeploy your own Attio MCP server on Cloudflare Workers with full OAuth 2.1 support.\n\n**Mobile Access**: With a remote MCP server, you can use Attio tools from:\n\n- ChatGPT mobile app (iOS/Android)\n- Claude mobile app (iOS/Android)\n- Any browser on any device\n\n#### Cloudflare Worker Features\n\n| Feature           | Cloudflare Worker |\n| ----------------- | ----------------- |\n| Setup complexity  | Medium            |\n| OAuth built-in    | ‚úÖ                |\n| Mobile app access | ‚úÖ                |\n| Multi-user access | ‚úÖ                |\n| Custom domain     | ‚úÖ                |\n| Self-hosted       | ‚úÖ                |\n| Team deployments  | ‚úÖ Full           |\n| Cost              | Free tier         |\n\n#### Quick Deploy\n\n```bash\ncd examples/cloudflare-mcp-server\nnpm install\nwrangler kv:namespace create \"TOKEN_STORE\"\n# Update wrangler.toml with the KV namespace ID\nwrangler secret put ATTIO_CLIENT_ID\nwrangler secret put ATTIO_CLIENT_SECRET\nwrangler secret put TOKEN_ENCRYPTION_KEY\nwrangler deploy\n```\n\n#### Client Configuration\n\nAfter deployment, configure your client with your Worker URL:\n\n- **Claude.ai**: Settings ‚Üí Connectors ‚Üí Add your Worker URL\n- **ChatGPT**: Settings ‚Üí Connectors ‚Üí Developer Mode ‚Üí Add Worker URL\n\nSee [Cloudflare Worker Deployment Guide](./examples/cloudflare-mcp-server/README.md) for:\n\n- Complete OAuth 2.1 setup with Attio\n- Token encryption configuration\n- Production deployment checklist\n- Troubleshooting guide\n\n---\n\n## üÜï What's New in v1.4.0\n\n### Major Features\n\n- **üéØ Workspace Schema Skill Generator** (#983) - Auto-generate Claude Skills from your Attio workspace schema for error-free field names and options\n- **üîç Select-field Transformer** (#1019) - Case-insensitive matching, partial matching, and UUID pass-through for select/status fields\n- **üõ†Ô∏è Attio Skill Generator Meta-skill** (#1020) - Meta-skill for automatic workspace documentation\n- **üìö Universal Usage Guide Skill** (#1018) - Hand-crafted workflow patterns and error prevention\n- **‚öôÔ∏è `get_record_attribute_options` tool** (#975) - Get valid options for select/status fields with enhanced error messages\n- **üìû Phone validation** (#951) - Built-in phone number validation support\n- **‚è±Ô∏è Configurable option fetch delay** - Rate limiting control via `--option-fetch-delay` flag\n\n### Major Enhancements\n\n- **üè∑Ô∏è MCP-compliant tool naming** (#1039) - All tools now use `snake_case`, verb-first naming (old names work via aliases until v2.0.0)\n- **üé® Custom object display names** (#1017) - Fetch display names directly from Attio API\n- **üìñ Split integration patterns** (#1023) - Progressive discovery patterns by use case\n- **üí° Enhanced attribute error messages** (#975) - Levenshtein distance suggestions for typos\n\n### Critical Fixes\n\n- üìù Note content line breaks preserved (#1052)\n- üë§ People search \"Unnamed\" display fixed (#1051)\n- ‚úÖ Select field persistence (#1045)\n- üîó Record-reference auto-transformation (#997)\n- üìä Multi-select array auto-transformation (#992)\n- üõ°Ô∏è Complex attribute validation (#991)\n- ‚ö†Ô∏è Field persistence false warnings (#995)\n- üì¶ SDK dependency pinning (#1025)\n- üíº Deal stage/UTM validation (#1043)\n- üìç Location field auto-normalization (#987)\n\n### Internal Improvements\n\n- Tool alias system refactoring (#1041) - Type-safe constants with pattern-based generation\n- Strategy Pattern for CRUD error handlers (#1001)\n- Consolidated metadata fetching (#984)\n- UniversalUpdateService modularization (#984)\n- Select transformation type rename (#1055) - `select_title_to_array` for clarity\n\n## üîÑ Migration Guide\n\n**Upgrading from v1.3.x or earlier?** Tool names have changed to follow MCP naming conventions.\n\n**Old names still work** via backward-compatible aliases, but will be removed in **v2.0.0 (Q1 2026)**.\n\n### Tool Name Changes\n\n| Old Name (Deprecated)            | New Name (MCP-compliant)         | Notes              |\n| -------------------------------- | -------------------------------- | ------------------ |\n| `records_search`                 | `search_records`                 | Verb-first pattern |\n| `records_get_details`            | `get_record_details`             | Verb-first pattern |\n| `records_get_attributes`         | `get_record_attributes`          | Verb-first pattern |\n| `records_discover_attributes`    | `discover_record_attributes`     | Verb-first pattern |\n| `records_search_advanced`        | `search_records_advanced`        | Verb-first pattern |\n| `records_search_by_relationship` | `search_records_by_relationship` | Verb-first pattern |\n| `records_search_by_content`      | `search_records_by_content`      | Verb-first pattern |\n| `records_search_by_timeframe`    | `search_records_by_timeframe`    | Verb-first pattern |\n| `records_batch`                  | `batch_records`                  | Verb-first pattern |\n| `search-records`                 | `search_records`                 | snake_case format  |\n| `get-record-details`             | `get_record_details`             | snake_case format  |\n| `create-record`                  | `create_record`                  | snake_case format  |\n| `update-record`                  | `update_record`                  | snake_case format  |\n| `delete-record`                  | `delete_record`                  | snake_case format  |\n| `create-note`                    | `create_note`                    | snake_case format  |\n| `list-notes`                     | `list_notes`                     | snake_case format  |\n| `smithery-debug-config`          | `smithery_debug_config`          | snake_case format  |\n\n**Action Required:** Update your integrations to use new tool names before Q1 2026. See [MIGRATION-GUIDE.md](docs/MIGRATION-GUIDE.md) for the complete migration table.\n\n---\n\n## ‚ö° Quick Start\n\n### Prerequisites\n\n- Node.js (v18 or higher)\n- Attio API Key ([Get one here](https://app.attio.com/settings/api)) **or** OAuth access token\n- Attio Workspace ID\n\n### üîê Authentication Options\n\nThe server supports two authentication methods‚Äîboth use the same Bearer token scheme:\n\n| Method                    | Environment Variable | Best For                             |\n| ------------------------- | -------------------- | ------------------------------------ |\n| **API Key** (recommended) | `ATTIO_API_KEY`      | Long-term integrations, personal use |\n| **OAuth Access Token**    | `ATTIO_ACCESS_TOKEN` | OAuth integrations, third-party apps |\n\n> **Note:** If both are set, `ATTIO_API_KEY` takes precedence.\n>\n> **OAuth Users:** For detailed setup including PKCE flow and token refresh, see [OAuth Authentication Guide](./docs/guides/oauth-authentication.md).\n\n### 1. Set Environment Variables\n\n```bash\n# Option 1: API Key (recommended for most users)\nexport ATTIO_API_KEY=\"your_api_key_here\"\n\n# Option 2: OAuth Access Token (for OAuth integrations)\n# export ATTIO_ACCESS_TOKEN=\"your_oauth_access_token_here\"\n\nexport ATTIO_WORKSPACE_ID=\"your_workspace_id_here\"\n\n# Optional: Deal defaults configuration\nexport ATTIO_DEFAULT_DEAL_STAGE=\"Interested\"           # Default stage for new deals\nexport ATTIO_DEFAULT_DEAL_OWNER=\"user@company.com\"     # Default owner email address (see below)\nexport ATTIO_DEFAULT_CURRENCY=\"USD\"                    # Default currency for deal values\n```\n\n### 2. Test the Installation\n\n```bash\n# Test the MCP server\nattio-mcp --help\n\n# Discover your Attio workspace attributes\nattio-discover attributes\n```\n\n### 3. üéØ **CRITICAL: Configure Field Mappings**\n\nThe MCP server uses **field mapping files** to translate between natural language and Attio's API field names. **This configuration is essential for proper operation.**\n\n#### **Quick Setup**\n\n```bash\n# 1. Copy the sample configuration to create your user config\ncp configs/runtime/mappings/sample.json configs/runtime/mappings/user.json\n\n# 2. Edit user.json to match your workspace's custom fields\n# Focus on the \"objects.companies\" and \"objects.people\" sections\n```\n\n#### **Configuration Files** (in `configs/runtime/mappings/`)\n\n- **`default.json`** - Standard Attio CRM fields (loaded first, don't edit)\n- **`sample.json`** - Examples with custom field templates (copy from this, not used at runtime)\n- **`user.json`** - **YOUR workspace-specific overrides** (merged on top of default.json)\n\n> üí° **Key Insight**: `user.json` is merged on top of `default.json`, so only include **overrides and additions**. Don't duplicate mappings that already exist in `default.json`.\n\n#### **How Configuration Merging Works**\n\nThe MCP server loads configuration in this order:\n\n1. **`default.json`** - Contains all standard Attio fields (Name, Description, Team, etc.)\n2. **`user.json`** - Your custom additions/overrides are **merged on top**\n\n**Example**: If `default.json` has `\"Name\": \"name\"` and your `user.json` also has `\"Name\": \"name\"`, that's wasted tokens. Only include fields that are:\n\n- **New custom fields** (not in default.json)\n- **Different mappings** (overriding default behavior)\n\n#### **Optimized user.json Example**\n\n```json\n{\n  \"mappings\": {\n    \"attributes\": {\n      \"objects\": {\n        \"companies\": {\n          \"// Only your custom fields - defaults are inherited\": \"\",\n          \"Lead Score\": \"lead_score\",\n          \"B2B Segment\": \"b2b_segment\",\n          \"Industry Vertical\": \"custom_industry_field\"\n        }\n      }\n    },\n    \"lists\": {\n      \"// Only your specific lists\": \"\",\n      \"Sales Pipeline\": \"your-pipeline-list-id\"\n    }\n  }\n}\n```\n\n**‚úÖ Good**: Only custom/override fields  \n**‚ùå Wasteful**: Duplicating standard fields from default.json\n\n> ‚ö†Ô∏è **Without proper mapping configuration, the MCP server may not work correctly with your custom fields and lists.**\n\n**Next:** Verify your field mappings work by testing with Claude:\n\n```\n\"Find companies in our pipeline with lead score > 80\"\n```\n\n### 4. Configure Claude Desktop\n\nAdd to your Claude Desktop MCP configuration:\n\n#### Finding Required IDs\n\n**Deal Owner Email** (for deal owner defaults):\nThe `ATTIO_DEFAULT_DEAL_OWNER` should be set to the email address of the workspace member who should own new deals by default. This is typically your own email address or the email address of your sales team lead.\n\n```bash\n# Example:\nexport ATTIO_DEFAULT_DEAL_OWNER=\"john.smith@company.com\"\n```\n\n**Note**: The system will automatically resolve email addresses to workspace member references when creating deals.\n\n**Deal Stages**:\nDeal stages are specific to your workspace. Check your Attio workspace settings or use the `discover-attributes` command to find available stages for deals.\n\n```json\n{\n  \"mcpServers\": {\n    \"attio-mcp\": {\n      \"command\": \"attio-mcp\",\n      \"env\": {\n        \"ATTIO_API_KEY\": \"your_api_key_here\",\n        \"ATTIO_WORKSPACE_ID\": \"your_workspace_id_here\",\n        \"ATTIO_DEFAULT_DEAL_STAGE\": \"Interested\",\n        \"ATTIO_DEFAULT_DEAL_OWNER\": \"user@company.com\",\n        \"ATTIO_DEFAULT_CURRENCY\": \"USD\"\n      }\n    }\n  }\n}\n```\n\n## üåü Example Use Cases\n\n### **For Sales Teams**\n\n```\n\"Find all companies in the AI space with 50+ employees that we haven't contacted in 30 days\"\n\"Show me all prospects added yesterday\"\n\"Find companies created in the last 7 days with revenue over $10M\"\n\"Create a task to follow up with Microsoft about the enterprise deal\"\n\"Add John Smith from Google to our Q1 prospect list\"\n```\n\n### **For Marketing Teams**\n\n```\n\"Create a list of all SaaS companies who opened our last 3 emails but haven't responded\"\n\"Show me engagement metrics for our outbound campaign this month\"\n\"Add all attendees from the conference to our nurture sequence\"\n```\n\n### **For Customer Success**\n\n```\n\"Show me all enterprise customers with upcoming renewal dates in Q1\"\n\"Create tasks for check-ins with accounts that haven't been contacted in 60 days\"\n\"Find all customers who mentioned pricing concerns in recent notes\"\n```\n\n### **For Data Operations**\n\n```\n\"Update all companies with missing industry data based on their domains\"\n\"Export all contacts added this quarter to CSV\"\n\"Merge duplicate company records for Acme Corporation\"\n```\n\n## üîê Security & Privacy\n\n- **Secure API Authentication**: Industry-standard API key authentication\n- **No Data Storage**: Direct API passthrough with no local data retention\n- **Open Source**: Full transparency with Apache 2.0 license\n- **Optional On-Premises**: Deploy in your own infrastructure\n\n## üìö Documentation\n\nComprehensive documentation is available in the [docs directory](./docs):\n\n### **Universal Tools (Recommended)**\n\n‚ö†Ô∏è **Note**: Universal tools documentation is currently being updated to match the latest implementation. Use the API directly or check the source code for the most accurate interface definitions.\n\n- [API Overview](./docs/api/api-overview.md) - High-level API concepts and patterns\n- [Universal Tools Source](./src/handlers/tool-configs/universal/) - Current implementation reference\n- [Tool Schemas](./src/handlers/tool-configs/universal/schemas.ts) - Parameter definitions and validation\n\n### **Getting Started**\n\n- [Installation & Setup](./docs/getting-started.md)\n- [Claude Desktop Configuration](./docs/claude-desktop-config.md)\n- [Troubleshooting Guide](./TROUBLESHOOTING.md)\n\n### **Configuration**\n\n- [Warning Filter Configuration](./docs/configuration/warning-filters.md) - Understanding cosmetic vs semantic mismatches, ESLint budgets, and suppression strategies\n- [Field Verification Configuration](./docs/configuration/field-verification.md) - Field persistence verification and validation settings\n- [Search Scoring Configuration](./docs/configuration/search-scoring.md) - Environment variables for relevance scoring, caching, and operator validation\n\n### **API Reference**\n\nüìã **Implementation Status**: These docs describe the Attio API endpoints. For MCP tool usage, refer to universal tools documentation above.\n\n- [API Overview](./docs/api/api-overview.md) - General Attio API concepts\n- [Companies API](./docs/api/companies-api.md) - Company record endpoints ‚úÖ Fully Implemented via Universal Tools\n- [People API](./docs/api/people-api.md) - Person record endpoints ‚úÖ Fully Implemented via Universal Tools\n- [Lists API](./docs/api/lists-api.md) - List management endpoints ‚úÖ Fully Implemented\n- [Notes API](./docs/api/notes-api.md) - Notes endpoints ‚úÖ Basic Implementation\n- [Tasks API](./docs/api/tasks-api.md) - Task endpoints ‚úÖ Implemented via Universal Tools\n\n### **Advanced Topics**\n\n- [Batch Operations](./docs/api/batch-operations.md) - Bulk operations ‚úÖ Implemented with chunking\n- [Advanced Filtering](./docs/api/advanced-filtering.md) - Complex queries ‚úÖ Implemented\n- [Error Handling](./docs/api/error-handling.md) - Error patterns ‚úÖ Enhanced error handling\n- [Extending MCP](./docs/api/extending-mcp.md) - Customization guide\n\n### **Deployment**\n\n- [Docker Guide](./docs/docker/docker-guide.md)\n- [Security Best Practices](./docs/docker/security-guide.md)\n\n## üõ† Development\n\n### **Setup Development Environment**\n\n```bash\ngit clone https://github.com/kesslerio/attio-mcp-server.git\ncd attio-mcp-server\nnpm install\nnpm run build\nnpm run test:offline\n```\n\n### **Smithery CLI Development**\n\nFor local development and testing with Smithery Playground:\n\n```bash\nnpm run dev  # Opens Smithery Playground with local server\n```\n\nSee [docs/deployment/smithery-cli-setup.md](./docs/deployment/smithery-cli-setup.md) for detailed Smithery CLI development setup.\n\n### **Testing**\n\n```bash\nnpm test                    # Run all tests\nnpm run test:offline        # Run only offline tests (no API required)\nnpm run test:integration    # Integration tests (requires ATTIO_API_KEY)\nnpm run e2e                 # E2E tests (requires ATTIO_API_KEY)\n```\n\nFor E2E/integration tests, create `.env` with your `ATTIO_API_KEY`. See the [Testing Guide](./docs/testing.md) for detailed setup.\n\n### **Available Scripts**\n\n```bash\nnpm run build          # Build TypeScript\nnpm run test           # Run all tests\nnpm run test:offline   # Run tests without API calls\nnpm run analyze:token-footprint # Generate baseline MCP token footprint report\nnpm run lint           # Check code style\nnpm run check          # Full quality check\nnpm run setup:test-data # Create test data for integration tests\n```\n\n## ü§ù Contributing\n\nWe welcome contributions! Please see our [Contributing Guidelines](./CONTRIB.md) for details on:\n\n- Adding new tools and features\n- Improving documentation\n- Reporting bugs and requesting features\n- Testing and quality assurance\n\n## üìà What's Next?\n\nThis initial release provides a solid foundation for CRM automation.\n\n## üîó Links\n\n- **NPM Package**: https://www.npmjs.com/package/attio-mcp\n- **GitHub Repository**: https://github.com/kesslerio/attio-mcp-server\n- **Issues & Support**: https://github.com/kesslerio/attio-mcp-server/issues\n- **Releases**: https://github.com/kesslerio/attio-mcp-server/releases\n- **Attio Documentation**: https://developers.attio.com/\n\n## üìÑ License\n\nThis project is licensed under the **Apache License 2.0** - see the [LICENSE](LICENSE) file for details.\n\n**Original Work Attribution**: This project is based on initial work by @hmk under BSD-3-Clause license, with substantial modifications and enhancements by @kesslerio. The original BSD license notice is preserved in the LICENSE file as required.\n\n---\n\n**Ready to transform your CRM workflow?** Install Attio MCP Server today and experience the future of CRM automation with AI!\n\n```bash\nnpm install -g attio-mcp\n```\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Manage entire Attio CRM workspace including Deals, Tasks, Lists, People, Companies, Records, and Notes via natural language",
        "Perform full CRUD operations on Companies, People, Deals, Tasks, Lists, Notes, and Records",
        "Execute advanced and universal search queries with filtering and relationship navigation",
        "Handle batch operations with chunking and error handling for bulk create, update, and delete",
        "Integrate with ChatGPT Developer Mode via self-hosted Cloudflare Worker for conversational AI access",
        "Use pre-built prompts for common sales workflows such as searching, logging activities, creating tasks, advancing deals, and qualifying leads",
        "Support OAuth integration and built-in approval flows for safe read/write operations",
        "Provide real-time updates and JSON data export for integrations",
        "Enable advanced filtering with text, numeric, date, boolean, and relationship filters including timeframe searches"
      ],
      "limitations": [
        "Tasks endpoint has filtering limitations requiring fallback patterns",
        "Tasks pagination is handled in-memory due to API constraints",
        "Smithery deployment temporarily unavailable requiring alternative Cloudflare Worker hosting for ChatGPT users"
      ],
      "requirements": [
        "Node.js version 20.0.0 or higher",
        "Self-hosted Cloudflare Worker for ChatGPT Developer Mode integration",
        "OAuth credentials for authentication",
        "Attio workspace account with API access",
        "Optional environment variables for token telemetry and prompt token limits (e.g., MCP_DEV_META, PROMPT_TELEMETRY_ENABLED, MAX_PROMPT_TOKENS)"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation requirements, detailed usage examples, tool descriptions, pre-built prompts, advanced features, and explicit limitations, making it excellent in coverage and clarity.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Attio MCP Server\n\n[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)\n[![npm version](https://badge.fury.io/js/attio-mcp.svg)](https://badge.fury.io/js/attio-mcp)\n[![Node.js Version](https://img.shields.io/badge/node-%3E%3D20.0.0-brightgreen.svg)](https://nodejs.org/)\n[![GitHub Release](https://img.shields.io/github/v/release/kesslerio/attio-mcp-server)](https://github.com/kesslerio/attio-mcp-server/releases)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/kesslerio/attio-mcp-server)\n\nA comprehensive Model Context Protocol (MCP) server for [Attio](https://attio.com/), providing **complete CRM surface coverage**. This server enables AI assistants like Claude and ChatGPT to interact directly with your entire Attio workspace through natural language‚Äîmanage Deals, Tasks, Lists, People, Companies, Records, and Notes without falling back to raw API calls.\n\n## üéØ What is Attio MCP Server?\n\nTransform your CRM workflows with AI-powered automation. Instead of clicking through multiple screens, simply ask Claude or ChatGPT to find prospects, update records, manage pipelines, and analyze your data using natural language commands.\n\n**üéâ v1.0.0 Milestone**: Complete Attio CRM surface coverage with full ChatGPT Developer Mode integration.\n\n> \"Find all AI companies with 50+ employees that we haven't contacted in 30 days and add them to our Q1 outreach list\"\n\n## üöÄ **ChatGPT Developer Mode Integration**\n\n> **‚ö†Ô∏è Smithery Temporarily Unavailable**: Smithery has changed their deployment model to require external hosting. We're working on Cloudflare Worker hosting for ChatGPT users. In the meantime, use [Tier 4 (Cloudflare Worker)](#tier-4-cloudflare-worker-remote-deployment) for ChatGPT/remote access.",
        "start_pos": 0,
        "end_pos": 1774,
        "token_count_estimate": 443,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 1,
        "text": "uage using a self-hosted Cloudflare Worker:\n\n- **üîê Built-in Approval Flows**: MCP safety annotations auto-approve read operations, request approval for writes\n- **üåê OAuth Integration**: Self-hosted OAuth via Cloudflare Worker deployment\n- **üí¨ Natural Language CRM**: Manage your entire Attio workspace through conversational AI\n- **üìñ Setup Guide**: See [ChatGPT Developer Mode docs](./docs/chatgpt-developer-mode.md) and [Cloudflare Worker Guide](./examples/cloudflare-mcp-server/README.md)\n\n## ‚ú® Core Features & Implementation Status\n\n### üéØ **Universal Tools Architecture** (14 Tools)\n\n**68% Tool Reduction**: Consolidated 40+ resource-specific tools into 14 universal operations for consistent, powerful CRM management.",
        "start_pos": 1848,
        "end_pos": 2569,
        "token_count_estimate": 180,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 2,
        "text": "ps\n- **Advanced Filtering**: Sophisticated query capabilities with intelligent field mapping\n\n### üìä **Company Management**\n\n- **Universal Search**: Find companies with `search_records` and `search_records_advanced`\n- **Full CRUD**: Create, read, update, and delete with universal record operations\n- **Relationship Discovery**: Find companies through `search_records_by_relationship`\n- **Batch Operations**: Process hundreds of companies with `batch_records`\n- **Detailed Information**: Get contact, business, and social info with `get_record_info`\n\n### üë• **People Management**\n\n- **Universal Contact Search**: Find people by any criteria using universal search tools\n- **Relationship Tracking**: Link people to companies with `search_records_by_relationship`\n- **Activity Timeline**: Track interactions with `search_records_by_content` and `search_records_by_timeframe`\n- **Advanced Filtering**: Multi-attribute search with universal filtering\n- **Bulk Operations**: Efficiently manage contacts with universal batch operations\n\n### üìã **Lists & Pipeline Management** (4 Tools + 8 Deprecated)\n\n- **Active Tools**: 4 consolidated tools with auto-mode detection ([Migration Guide](./docs/migration/v2-list-tools.md))\n  - `filter-list-entries` - Unified filtering with 4 modes\n  - `manage-list-entry` - Unified entry management with 3 modes\n  - `get-list-entries` - Retrieve list entries\n  - `get-record-list-memberships` - Find record's list memberships\n- **Deprecated (v2.0.0 removal)**: 8 legacy tools replaced by consolidated versions\n- **Pipeline Operations**: Move deals through sales stages\n- **Smart Segmentation**: Create and manage targeted contact lists\n- **Advanced Filtering**: Complex multi-condition filtering with AND/OR logic\n- **Entry Management**: Add, remove, and update list memberships\n- **Deal Tracking**: Monitor opportunities and revenue pipeline\n- **Deal Defaults**: Configurable default stage, owner, and currency for streamlined deal creation\n\n### ‚úÖ **Task Management**\n\n- **Universal Task Operations**: Create, update, and",
        "start_pos": 3696,
        "end_pos": 5744,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 3,
        "text": "nd revenue pipeline\n- **Deal Defaults**: Configurable default stage, owner, and currency for streamlined deal creation\n\n### ‚úÖ **Task Management**\n\n- **Universal Task Operations**: Create, update, and manage tasks with universal tools\n- **Record Linking**: Associate tasks with any record type using `resource_type` parameter\n- **Progress Tracking**: Monitor completion with universal search and filtering\n- **Team Coordination**: Streamline follow-ups with consistent universal operations\n\n### üîß **Advanced Capabilities**\n\n- **Batch Processing**: Handle bulk operations with error tracking\n- **Enhanced Filtering**: Text, numeric, date, boolean, and relationship filters with timeframe search (Issue #475)\n- **Data Export**: JSON serialization for integrations\n- **Real-time Updates**: Live data synchronization with Attio\n\n### üß† **Claude Skills**\n\nSupercharge Claude's Attio knowledge with pre-built skills that prevent common errors and teach best practices.\n\n| Skill                      | Purpose                                        | Setup                                           |\n| -------------------------- | ---------------------------------------------- | ----------------------------------------------- |\n| **attio-mcp-usage**        | Error prevention + universal workflow patterns | Bundled - just use it                           |\n| **attio-workspace-schema** | YOUR workspace's exact field names and options | `npx attio-discover generate-skill --all --zip` |\n| **attio-skill-generator**  | Create custom workflow skills (advanced)       | Python + prompting                              |\n\n**Quick Start** (solves \"wrong field name\" errors):\n\n```bash\nnpx attio-discover generate-skill --all --zip\n# Import ZIP into Claude Desktop: Settings > Skills > Install Skill\n```\n\nSee [Skills Documentation](./docs/usage/skills/README.md) for complete setup and usage guides.",
        "start_pos": 5544,
        "end_pos": 7431,
        "token_count_estimate": 471,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 4,
        "text": "cover generate-skill --all --zip\n# Import ZIP into Claude Desktop: Settings > Skills > Install Skill\n```\n\nSee [Skills Documentation](./docs/usage/skills/README.md) for complete setup and usage guides.\n\n### üí¨ **Pre-Built Prompts** (10 Prompts)\n\nIntelligent shortcuts that help Claude work faster with your CRM data:\n\n- **Search & Find** (5): people_search, company_search, deal_search, meeting_prep, pipeline_health\n- **Take Actions** (4): log_activity, create_task, advance_deal, add_to_list with dry-run safety\n- **Research & Qualify** (1): qualify_lead with automated web research and BANT/CHAMP frameworks\n- **Token-efficient**: 300-700 tokens per prompt with consistent formatting\n- **Discoverable**: Claude automatically suggests relevant prompts for your tasks\n\nSee [Using Out-of-the-Box Prompts](#-using-out-of-the-box-prompts) for detailed documentation and examples.\n\n## üéØ **Using Out-of-the-Box Prompts**\n\n**NEW**: 10 pre-built MCP prompts for common Sales workflows. No setup required‚Äîjust use them!",
        "start_pos": 7231,
        "end_pos": 8241,
        "token_count_estimate": 252,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 5,
        "text": "|\n| `log_activity.v1`    | Log calls/meetings/emails to records            | `target`, `type`, `summary`, `dry_run`           | Log call with Nina at Acme         |\n| `create_task.v1`     | Create tasks with natural language due dates    | `title`, `content`, `due_date`, `dry_run`        | Create task: Follow up tomorrow    |\n| `advance_deal.v1`    | Move deal to target stage with next action      | `deal`, `target_stage`, `create_task`, `dry_run` | Advance deal to \"Proposal Sent\"    |\n| `add_to_list.v1`     | Add records to a List by name or ID             | `records`, `list`, `dry_run`                     | Add 5 companies to Q1 Outreach     |\n| `qualify_lead.v1`    | Research lead with web + BANT/CHAMP scoring     | `target`, `framework`, `limit_web`, `dry_run`    | Qualify Acme Corp with BANT        |\n| `meeting_prep.v1`    | 360¬∞ prep: notes, tasks, deals, agenda          | `target`, `format`, `verbosity`                  | Prep for meeting with Acme CEO     |\n| `pipeline_health.v1` | Weekly snapshot: created/won/slipped + risks    | `owner`, `timeframe`, `segment`                  | Pipeline health for @me last 30d   |\n\n### Quick Examples\n\n```bash\n# Search for prospects\n\"Use people_search.v1: Find Account Executives in San Francisco at fintech companies, limit 25\"\n\n# Log activity\n\"Use log_activity.v1: Log a call with Nina at Acme Corp, discussed Q1 pricing, create follow-up task\"\n\n# Qualify a lead (with web research)\n\"Use qualify_lead.v1: Qualify Acme Corp using BANT framework, dry run mode\"\n\n# Meeting prep\n\"Use meeting_prep.v1: Prepare for meeting with contact at Acme Corp\"\n```\n\n### Universal Arguments\n\n**All read prompts** support:\n\n- `format`: `table` | `json` | `ids` (default: `table`)\n- `fields_preset`: `sales_short` | `full` (default: `sales_short`)\n- `verbosity`: `brief` | `normal` (default: `brief`)\n\n**All write prompts** support:\n\n- `dry_run`: `true` | `false` (default: `false`) - Preview changes without executing\n\n### Token Awareness Features\n\nPrompts include built-in token optimization:\n\n- **Budg",
        "start_pos": 9079,
        "end_pos": 11127,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 6,
        "text": "All write prompts** support:\n\n- `dry_run`: `true` | `false` (default: `false`) - Preview changes without executing\n\n### Token Awareness Features\n\nPrompts include built-in token optimization:\n\n- **Budget Guards**: Prompts stay within token limits (people_search <500, qualify_lead <400)\n- **Dev Metadata**: Set `MCP_DEV_META=true` for token counts in responses\n- **Telemetry**: Set `PROMPT_TELEMETRY_ENABLED=true` for usage logging\n- **Configurable Limits**: Override with `MAX_PROMPT_TOKENS` environment variable\n\nFor complete prompt documentation, see [docs/prompts/v1-catalog.md](./docs/prompts/v1-catalog.md).\n\n## ‚ö†Ô∏è **Known Limitations & Important Notes**\n\n### **Current Limitations**\n\n- **Field Parameter Filtering**: Tasks endpoint `/objects/tasks/attributes` has limitations, handled with fallback patterns\n- **Pagination**: Tasks pagination uses in-memory handling due to API constraints\n\n### **API Compatibility**\n\n- **Universal Tools**: Primary interface (14 tools) - recommended for all new integrations\n- **Legacy Tools**: Available via `DISABLE_UNIVERSAL_TOOLS=true` environment variable (deprecated)\n- **Lists API**: Fully functional with complete CRUD operations (contrary to some outdated documentation)\n\n### ü§ù **OpenAI MCP Compatibility**\n\n- **Developer Mode Ready**: Every tool now publishes MCP safety annotations (`readOnlyHint`, `destructiveHint`) so OpenAI Developer Mode can auto-approve reads and request confirmation for writes.\n- **Full Tool Access (Default)**: All 35 tools are exposed by default (21 universal + 11 list + 3 workspace member). Do NOT set `ATTIO_MCP_TOOL_MODE` in Smithery configuration for full access.\n- **Search-Only Mode**: To restrict to read-only tools (`search`, `fetch`, `aaa-health-check`), explicitly configure `ATTIO_MCP_TOOL_MODE: 'search'` in Smithery dashboard when Developer Mode is unavailable.\n- **Detailed Guide**: See [docs/chatgpt-developer-mode.md](./docs/chatgpt-developer-mode.md) for environment variables, approval flows, and validation tips.",
        "start_pos": 10927,
        "end_pos": 12937,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 7,
        "text": "shboard when Developer Mode is unavailable.\n- **Detailed Guide**: See [docs/chatgpt-developer-mode.md](./docs/chatgpt-developer-mode.md) for environment variables, approval flows, and validation tips.\n- **User Documentation**: See the [ChatGPT Developer Mode docs](./docs/chatgpt-developer-mode.md) for a complete walkthrough of approval flows and setup instructions.\n\n### **Performance Considerations**\n\n- **Batch Operations**: Optimized with chunking, rate limiting, and error recovery\n- **Large Datasets**: Automatic pagination and field filtering for optimal performance\n- **Rate Limiting**: Built-in protection against API rate limits with exponential backoff\n\nFor detailed troubleshooting and solutions, see [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) and [GitHub Issues](https://github.com/kesslerio/attio-mcp-server/issues).\n\n## üéØ **Advanced Search Filters**\n\nBuild powerful CRM queries with multi-criteria AND/OR filtering. See the [Advanced Search Guide](./docs/usage/advanced-search.md) for complete examples and operator reference.\n\n## üöÄ Installation\n\n> ‚ö†Ô∏è **IMPORTANT: Correct Package Name**\n>\n> The npm package name is **`attio-mcp`** (not `attio-mcp-server`).\n> The GitHub repository is named `attio-mcp-server`, but the npm package was renamed to `attio-mcp` in June 2025.\n> Installing `attio-mcp-server` will give you an outdated v0.0.2 release with only 4 legacy tools.",
        "start_pos": 12737,
        "end_pos": 14123,
        "token_count_estimate": 346,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 8,
        "text": "| ‚úÖ Full support            |\n| Claude Code (CLI)  | ‚úÖ Recommended           | Partial                    |\n\n**Choose your installation method:**\n\n- **Most users**: Use [Tier 1 (Shell Installers)](#tier-1-shell-installers) - one command, automatic setup\n- **Power users**: Use [Tier 2 (Manual)](#tier-2-manual-configuration) - full control over configuration\n- **ChatGPT/Teams/Enterprise**: Use [Tier 3 (Cloudflare Worker)](#tier-3-cloudflare-worker-remote-deployment) - self-hosted, multi-user OAuth\n\n---\n\n### Tier 1: Shell Installers\n\n> **Best for**: Developers who prefer local installations with automatic configuration.\n\nOne-command scripts that install `attio-mcp` and configure your client automatically.\n\n#### Claude Desktop\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/kesslerio/attio-mcp-server/main/scripts/install-claude-desktop.sh | bash\n```\n\n#### Cursor IDE\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/kesslerio/attio-mcp-server/main/scripts/install-cursor.sh | bash\n```\n\n#### Claude Code (CLI)\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/kesslerio/attio-mcp-server/main/scripts/install-claude-code.sh | bash\n```\n\nThese scripts will:\n\n- Install `attio-mcp` npm package globally (if needed)\n- Backup existing configuration files\n- Prompt for your Attio API key\n- Configure the MCP server for your client\n- Print next steps and restart instructions\n\n---\n\n### Tier 2: Manual Configuration\n\n> **Best for**: Power users who prefer full control or use unsupported clients.",
        "start_pos": 14585,
        "end_pos": 16098,
        "token_count_estimate": 377,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 9,
        "text": "`%APPDATA%\\Claude\\claude_desktop_config.json`\n\n#### Step 3: Add configuration\n\n```json\n{\n  \"mcpServers\": {\n    \"attio-mcp\": {\n      \"command\": \"attio-mcp\",\n      \"env\": {\n        \"ATTIO_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n#### Step 4: Restart Claude Desktop completely (quit and reopen)\n\n</details>\n\n<details>\n<summary><strong>Cursor IDE Manual Setup</strong></summary>\n\n#### Step 1: Install attio-mcp\n\n```bash\nnpm install -g attio-mcp\n```\n\n#### Step 2: Edit config file\n\nLocation: `~/.cursor/mcp.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"attio-mcp\": {\n      \"command\": \"attio-mcp\",\n      \"env\": {\n        \"ATTIO_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n#### Step 3: Restart Cursor\n\n</details>\n\n<details>\n<summary><strong>Claude Code (CLI) Manual Setup</strong></summary>\n\n#### Option A: Using Claude CLI command (recommended)\n\n```bash\necho '{\"command\":\"attio-mcp\",\"env\":{\"ATTIO_API_KEY\":\"your_key_here\"}}' | claude mcp add-json attio-mcp --stdin -s user\n```\n\n#### Option B: Manual config edit\n\nEdit `~/.claude/settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"attio-mcp\": {\n      \"command\": \"attio-mcp\",\n      \"env\": {\n        \"ATTIO_API_KEY\": \"your_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><strong>Building from Source</strong></summary>\n\nFor development or custom deployments:\n\n```bash\ngit clone https://github.com/kesslerio/attio-mcp-server.git\ncd attio-mcp-server\nnpm install\nnpm run build\n```\n\nRun directly:\n\n```bash\nATTIO_API_KEY=your_key node dist/index.js\n```\n\n</details>\n\n<details>\n<summary><strong>NPM Global Install</strong></summary>\n\n```bash\n# Global installation for CLI usage\nnpm install -g attio-mcp\n\n# Or local installation for project integration\nnpm install attio-mcp\n```\n\n</details>\n\n---\n\n### Tier 3: Cloudflare Worker (Remote Deployment)\n\n> **Best for**: Teams needing centralized OAuth, multi-user access, mobile access, or running MCP without local installation.\n\nDeploy your own Attio MCP server on Cloudflare Workers with full OAuth 2.1 support.",
        "start_pos": 16433,
        "end_pos": 18465,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 10,
        "text": "or**: Teams needing centralized OAuth, multi-user access, mobile access, or running MCP without local installation.\n\nDeploy your own Attio MCP server on Cloudflare Workers with full OAuth 2.1 support.",
        "start_pos": 18265,
        "end_pos": 18465,
        "token_count_estimate": 50,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 11,
        "text": "l matching, and UUID pass-through for select/status fields\n- **üõ†Ô∏è Attio Skill Generator Meta-skill** (#1020) - Meta-skill for automatic workspace documentation\n- **üìö Universal Usage Guide Skill** (#1018) - Hand-crafted workflow patterns and error prevention\n- **‚öôÔ∏è `get_record_attribute_options` tool** (#975) - Get valid options for select/status fields with enhanced error messages\n- **üìû Phone validation** (#951) - Built-in phone number validation support\n- **‚è±Ô∏è Configurable option fetch delay** - Rate limiting control via `--option-fetch-delay` flag\n\n### Major Enhancements\n\n- **üè∑Ô∏è MCP-compliant tool naming** (#1039) - All tools now use `snake_case`, verb-first naming (old names work via aliases until v2.0.0)\n- **üé® Custom object display names** (#1017) - Fetch display names directly from Attio API\n- **üìñ Split integration patterns** (#1023) - Progressive discovery patterns by use case\n- **üí° Enhanced attribute error messages** (#975) - Levenshtein distance suggestions for typos\n\n### Critical Fixes\n\n- üìù Note content line breaks preserved (#1052)\n- üë§ People search \"Unnamed\" display fixed (#1051)\n- ‚úÖ Select field persistence (#1045)\n- üîó Record-reference auto-transformation (#997)\n- üìä Multi-select array auto-transformation (#992)\n- üõ°Ô∏è Complex attribute validation (#991)\n- ‚ö†Ô∏è Field persistence false warnings (#995)\n- üì¶ SDK dependency pinning (#1025)\n- üíº Deal stage/UTM validation (#1043)\n- üìç Location field auto-normalization (#987)\n\n### Internal Improvements\n\n- Tool alias system refactoring (#1041) - Type-safe constants with pattern-based generation\n- Strategy Pattern for CRUD error handlers (#1001)\n- Consolidated metadata fetching (#984)\n- UniversalUpdateService modularization (#984)\n- Select transformation type rename (#1055) - `select_title_to_array` for clarity\n\n## üîÑ Migration Guide\n\n**Upgrading from v1.3.x or earlier?** Tool names have changed to follow MCP naming conventions.\n\n**Old names still work** via backward-compatible aliases, but will be removed in **v2.0.0 (Q1 2026)**.",
        "start_pos": 20113,
        "end_pos": 22122,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 12,
        "text": "**Upgrading from v1.3.x or earlier?** Tool names have changed to follow MCP naming conventions.\n\n**Old names still work** via backward-compatible aliases, but will be removed in **v2.0.0 (Q1 2026)**.",
        "start_pos": 21922,
        "end_pos": 22122,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 13,
        "text": "_notes`                     | snake_case format  |\n| `smithery-debug-config`          | `smithery_debug_config`          | snake_case format  |\n\n**Action Required:** Update your integrations to use new tool names before Q1 2026. See [MIGRATION-GUIDE.md](docs/MIGRATION-GUIDE.md) for the complete migration table.\n\n---\n\n## ‚ö° Quick Start\n\n### Prerequisites\n\n- Node.js (v18 or higher)\n- Attio API Key ([Get one here](https://app.attio.com/settings/api)) **or** OAuth access token\n- Attio Workspace ID\n\n### üîê Authentication Options\n\nThe server supports two authentication methods‚Äîboth use the same Bearer token scheme:\n\n| Method                    | Environment Variable | Best For                             |\n| ------------------------- | -------------------- | ------------------------------------ |\n| **API Key** (recommended) | `ATTIO_API_KEY`      | Long-term integrations, personal use |\n| **OAuth Access Token**    | `ATTIO_ACCESS_TOKEN` | OAuth integrations, third-party apps |\n\n> **Note:** If both are set, `ATTIO_API_KEY` takes precedence.\n>\n> **OAuth Users:** For detailed setup including PKCE flow and token refresh, see [OAuth Authentication Guide](./docs/guides/oauth-authentication.md).\n\n### 1. Set Environment Variables\n\n```bash\n# Option 1: API Key (recommended for most users)\nexport ATTIO_API_KEY=\"your_api_key_here\"\n\n# Option 2: OAuth Access Token (for OAuth integrations)\n# export ATTIO_ACCESS_TOKEN=\"your_oauth_access_token_here\"\n\nexport ATTIO_WORKSPACE_ID=\"your_workspace_id_here\"\n\n# Optional: Deal defaults configuration\nexport ATTIO_DEFAULT_DEAL_STAGE=\"Interested\"           # Default stage for new deals\nexport ATTIO_DEFAULT_DEAL_OWNER=\"user@company.com\"     # Default owner email address (see below)\nexport ATTIO_DEFAULT_CURRENCY=\"USD\"                    # Default currency for deal values\n```\n\n### 2. Test the Installation\n\n```bash\n# Test the MCP server\nattio-mcp --help\n\n# Discover your Attio workspace attributes\nattio-discover attributes\n```\n\n### 3.",
        "start_pos": 23770,
        "end_pos": 25747,
        "token_count_estimate": 494,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 14,
        "text": "# Default currency for deal values\n```\n\n### 2. Test the Installation\n\n```bash\n# Test the MCP server\nattio-mcp --help\n\n# Discover your Attio workspace attributes\nattio-discover attributes\n```\n\n### 3. üéØ **CRITICAL: Configure Field Mappings**\n\nThe MCP server uses **field mapping files** to translate between natural language and Attio's API field names. **This configuration is essential for proper operation.**\n\n#### **Quick Setup**\n\n```bash\n# 1. Copy the sample configuration to create your user config\ncp configs/runtime/mappings/sample.json configs/runtime/mappings/user.json\n\n# 2. Edit user.json to match your workspace's custom fields\n# Focus on the \"objects.companies\" and \"objects.people\" sections\n```\n\n#### **Configuration Files** (in `configs/runtime/mappings/`)\n\n- **`default.json`** - Standard Attio CRM fields (loaded first, don't edit)\n- **`sample.json`** - Examples with custom field templates (copy from this, not used at runtime)\n- **`user.json`** - **YOUR workspace-specific overrides** (merged on top of default.json)\n\n> üí° **Key Insight**: `user.json` is merged on top of `default.json`, so only include **overrides and additions**. Don't duplicate mappings that already exist in `default.json`.\n\n#### **How Configuration Merging Works**\n\nThe MCP server loads configuration in this order:\n\n1. **`default.json`** - Contains all standard Attio fields (Name, Description, Team, etc.)\n2. **`user.json`** - Your custom additions/overrides are **merged on top**\n\n**Example**: If `default.json` has `\"Name\": \"name\"` and your `user.json` also has `\"Name\": \"name\"`, that's wasted tokens.",
        "start_pos": 25547,
        "end_pos": 27144,
        "token_count_estimate": 398,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 15,
        "text": "nies\": {\n          \"// Only your custom fields - defaults are inherited\": \"\",\n          \"Lead Score\": \"lead_score\",\n          \"B2B Segment\": \"b2b_segment\",\n          \"Industry Vertical\": \"custom_industry_field\"\n        }\n      }\n    },\n    \"lists\": {\n      \"// Only your specific lists\": \"\",\n      \"Sales Pipeline\": \"your-pipeline-list-id\"\n    }\n  }\n}\n```\n\n**‚úÖ Good**: Only custom/override fields  \n**‚ùå Wasteful**: Duplicating standard fields from default.json\n\n> ‚ö†Ô∏è **Without proper mapping configuration, the MCP server may not work correctly with your custom fields and lists.**\n\n**Next:** Verify your field mappings work by testing with Claude:\n\n```\n\"Find companies in our pipeline with lead score > 80\"\n```\n\n### 4. Configure Claude Desktop\n\nAdd to your Claude Desktop MCP configuration:\n\n#### Finding Required IDs\n\n**Deal Owner Email** (for deal owner defaults):\nThe `ATTIO_DEFAULT_DEAL_OWNER` should be set to the email address of the workspace member who should own new deals by default. This is typically your own email address or the email address of your sales team lead.\n\n```bash\n# Example:\nexport ATTIO_DEFAULT_DEAL_OWNER=\"john.smith@company.com\"\n```\n\n**Note**: The system will automatically resolve email addresses to workspace member references when creating deals.\n\n**Deal Stages**:\nDeal stages are specific to your workspace. Check your Attio workspace settings or use the `discover-attributes` command to find available stages for deals.",
        "start_pos": 27395,
        "end_pos": 28849,
        "token_count_estimate": 363,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 16,
        "text": "**For Sales Teams**\n\n```\n\"Find all companies in the AI space with 50+ employees that we haven't contacted in 30 days\"\n\"Show me all prospects added yesterday\"\n\"Find companies created in the last 7 days with revenue over $10M\"\n\"Create a task to follow up with Microsoft about the enterprise deal\"\n\"Add John Smith from Google to our Q1 prospect list\"\n```\n\n### **For Marketing Teams**\n\n```\n\"Create a list of all SaaS companies who opened our last 3 emails but haven't responded\"\n\"Show me engagement metrics for our outbound campaign this month\"\n\"Add all attendees from the conference to our nurture sequence\"\n```\n\n### **For Customer Success**\n\n```\n\"Show me all enterprise customers with upcoming renewal dates in Q1\"\n\"Create tasks for check-ins with accounts that haven't been contacted in 60 days\"\n\"Find all customers who mentioned pricing concerns in recent notes\"\n```\n\n### **For Data Operations**\n\n```\n\"Update all companies with missing industry data based on their domains\"\n\"Export all contacts added this quarter to CSV\"\n\"Merge duplicate company records for Acme Corporation\"\n```\n\n## üîê Security & Privacy\n\n- **Secure API Authentication**: Industry-standard API key authentication\n- **No Data Storage**: Direct API passthrough with no local data retention\n- **Open Source**: Full transparency with Apache 2.0 license\n- **Optional On-Premises**: Deploy in your own infrastructure\n\n## üìö Documentation\n\nComprehensive documentation is available in the [docs directory](./docs):\n\n### **Universal Tools (Recommended)**\n\n‚ö†Ô∏è **Note**: Universal tools documentation is currently being updated to match the latest implementation. Use the API directly or check the source code for the most accurate interface definitions.",
        "start_pos": 29243,
        "end_pos": 30954,
        "token_count_estimate": 427,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 17,
        "text": "igs/universal/) - Current implementation reference\n- [Tool Schemas](./src/handlers/tool-configs/universal/schemas.ts) - Parameter definitions and validation\n\n### **Getting Started**\n\n- [Installation & Setup](./docs/getting-started.md)\n- [Claude Desktop Configuration](./docs/claude-desktop-config.md)\n- [Troubleshooting Guide](./TROUBLESHOOTING.md)\n\n### **Configuration**\n\n- [Warning Filter Configuration](./docs/configuration/warning-filters.md) - Understanding cosmetic vs semantic mismatches, ESLint budgets, and suppression strategies\n- [Field Verification Configuration](./docs/configuration/field-verification.md) - Field persistence verification and validation settings\n- [Search Scoring Configuration](./docs/configuration/search-scoring.md) - Environment variables for relevance scoring, caching, and operator validation\n\n### **API Reference**\n\nüìã **Implementation Status**: These docs describe the Attio API endpoints. For MCP tool usage, refer to universal tools documentation above.",
        "start_pos": 31091,
        "end_pos": 32084,
        "token_count_estimate": 248,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 18,
        "text": "ling\n- [Extending MCP](./docs/api/extending-mcp.md) - Customization guide\n\n### **Deployment**\n\n- [Docker Guide](./docs/docker/docker-guide.md)\n- [Security Best Practices](./docs/docker/security-guide.md)\n\n## üõ† Development\n\n### **Setup Development Environment**\n\n```bash\ngit clone https://github.com/kesslerio/attio-mcp-server.git\ncd attio-mcp-server\nnpm install\nnpm run build\nnpm run test:offline\n```\n\n### **Smithery CLI Development**\n\nFor local development and testing with Smithery Playground:\n\n```bash\nnpm run dev  # Opens Smithery Playground with local server\n```\n\nSee [docs/deployment/smithery-cli-setup.md](./docs/deployment/smithery-cli-setup.md) for detailed Smithery CLI development setup.\n\n### **Testing**\n\n```bash\nnpm test                    # Run all tests\nnpm run test:offline        # Run only offline tests (no API required)\nnpm run test:integration    # Integration tests (requires ATTIO_API_KEY)\nnpm run e2e                 # E2E tests (requires ATTIO_API_KEY)\n```\n\nFor E2E/integration tests, create `.env` with your `ATTIO_API_KEY`. See the [Testing Guide](./docs/testing.md) for detailed setup.\n\n### **Available Scripts**\n\n```bash\nnpm run build          # Build TypeScript\nnpm run test           # Run all tests\nnpm run test:offline   # Run tests without API calls\nnpm run analyze:token-footprint # Generate baseline MCP token footprint report\nnpm run lint           # Check code style\nnpm run check          # Full quality check\nnpm run setup:test-data # Create test data for integration tests\n```\n\n## ü§ù Contributing\n\nWe welcome contributions! Please see our [Contributing Guidelines](./CONTRIB.md) for details on:\n\n- Adding new tools and features\n- Improving documentation\n- Reporting bugs and requesting features\n- Testing and quality assurance\n\n## üìà What's Next?\n\nThis initial release provides a solid foundation for CRM automation.",
        "start_pos": 32939,
        "end_pos": 34794,
        "token_count_estimate": 463,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      },
      {
        "chunk_id": 19,
        "text": "and features\n- Improving documentation\n- Reporting bugs and requesting features\n- Testing and quality assurance\n\n## üìà What's Next?\n\nThis initial release provides a solid foundation for CRM automation.\n\n## üîó Links\n\n- **NPM Package**: https://www.npmjs.com/package/attio-mcp\n- **GitHub Repository**: https://github.com/kesslerio/attio-mcp-server\n- **Issues & Support**: https://github.com/kesslerio/attio-mcp-server/issues\n- **Releases**: https://github.com/kesslerio/attio-mcp-server/releases\n- **Attio Documentation**: https://developers.attio.com/\n\n## üìÑ License\n\nThis project is licensed under the **Apache License 2.0** - see the [LICENSE](LICENSE) file for details.\n\n**Original Work Attribution**: This project is based on initial work by @hmk under BSD-3-Clause license, with substantial modifications and enhancements by @kesslerio. The original BSD license notice is preserved in the LICENSE file as required.\n\n---\n\n**Ready to transform your CRM workflow?** Install Attio MCP Server today and experience the future of CRM automation with AI!\n\n```bash\nnpm install -g attio-mcp\n```",
        "start_pos": 34594,
        "end_pos": 35680,
        "token_count_estimate": 271,
        "source_type": "readme",
        "agent_id": "9f69645c47b867e4"
      }
    ]
  },
  {
    "agent_id": "964cd6faa82cb49e",
    "name": "ai.smithery/kirbah-mcp-youtube",
    "source": "mcp",
    "source_url": "https://github.com/kirbah/mcp-youtube",
    "description": "Provide token-optimized, structured YouTube data to enhance your LLM applications. Access efficien‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-04T16:31:58.771823Z",
    "indexed_at": "2026-02-18T04:07:30.277456",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# YouTube Data MCP Server (@kirbah/mcp-youtube)\n\n<!-- Badges Start -->\n<p align=\"left\">\n  <!-- GitHub Actions CI -->\n  <a href=\"https://github.com/kirbah/mcp-youtube/actions/workflows/ci.yml\">\n    <img src=\"https://github.com/kirbah/mcp-youtube/actions/workflows/ci.yml/badge.svg\" alt=\"CI Status\" />\n  </a>\n  <!-- Codecov -->\n  <a href=\"https://codecov.io/gh/kirbah/mcp-youtube\">\n    <img src=\"https://codecov.io/gh/kirbah/mcp-youtube/branch/main/graph/badge.svg?token=Y6B2E0T82P\" alt=\"Code Coverage\"/>\n  </a>\n  <!-- NPM Version -->\n  <a href=\"https://www.npmjs.com/package/@kirbah/mcp-youtube\">\n    <img src=\"https://img.shields.io/npm/v/@kirbah/mcp-youtube.svg\" alt=\"NPM Version\" />\n  </a>\n  <!-- NPM Downloads -->\n  <a href=\"https://www.npmjs.com/package/@kirbah/mcp-youtube\">\n    <img src=\"https://img.shields.io/npm/dt/@kirbah/mcp-youtube.svg\" alt=\"NPM Downloads\" />\n  </a>\n  <!-- Node Version -->\n  <a href=\"package.json\">\n    <img src=\"https://img.shields.io/node/v/@kirbah/mcp-youtube.svg\" alt=\"Node.js Version Support\" />\n  </a>\n</p>\n\n<a href=\"https://glama.ai/mcp/servers/@kirbah/mcp-youtube\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kirbah/mcp-youtube/badge\" />\n</a>\n<!-- Badges End -->\n\n**High-efficiency YouTube MCP server: Get token-optimized, structured data for your LLMs using the YouTube Data API v3.**\n\nThis Model Context Protocol (MCP) server empowers AI language models to seamlessly interact with YouTube. It's engineered to return **lean, structured data**, significantly **reducing token consumption** and making it ideal for cost-effective and performant LLM applications. Access a comprehensive suite of tools for video search, detail retrieval, transcript fetching, channel analysis, and trend discovery‚Äîall optimized for AI.\n\n**Built with MCP TypeScript Starter**\n\nThis project follows the architecture defined in the [MCP TypeScript Starter](https://github.com/kirbah/mcp-typescript-starter). If you are looking to build your own MCP server using these same patterns (Class-based Tools, Dependency Injection, and strict Type Safety), I recommend using that repository as your starting point.\n\n## Quick Start: Adding to an MCP Client\n\nThe easiest way to use `@kirbah/mcp-youtube` is with an MCP-compatible client application (like Claude Desktop or a custom client).\n\n1.  **Ensure you have a YouTube Data API v3 Key.**\n    - If you don't have one, follow the [YouTube API Setup](#youtube-api-setup) instructions below.\n\n2.  **MongoDB Connection String (Optional):** This server can use MongoDB to cache API responses and store analysis data, which significantly improves performance and reduces API quota usage. If you don't provide a connection string, the server will run without a database, but performance will be degraded, and you may hit API quota limits faster. You can get a free MongoDB Atlas cluster to obtain a connection string.\n\n    **Important:** If you use MongoDB, the server is hardcoded to use the database name `youtube_niche_analysis`. Your connection string must point to this database, and your user must have read/write permissions for it.\n\n3.  **Configure your MCP client:**\n    Add the following JSON configuration to your client, replacing `\"YOUR_YOUTUBE_API_KEY_HERE\"` with your actual API key.\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"youtube\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"@kirbah/mcp-youtube\"],\n          \"env\": {\n            \"YOUTUBE_API_KEY\": \"YOUR_YOUTUBE_API_KEY_HERE\",\n            \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://user:pass@cluster0.abc.mongodb.net/youtube_niche_analysis\"\n          }\n        }\n      }\n    }\n    ```\n\n    - **Windows PowerShell Users:** `npx` can sometimes cause issues directly. If you encounter problems, try modifying the command as follows:\n      ```json\n        \"command\": \"cmd\",\n        \"args\": [\"/k\", \"npx\", \"-y\", \"@kirbah/mcp-youtube\"],\n      ```\n\nThat's it! Your MCP client should now be able to leverage the YouTube tools provided by this server.\n\n## Why `@kirbah/mcp-youtube`?\n\nIn the world of Large Language Models, every token counts. `@kirbah/mcp-youtube` is designed from the ground up with this principle in mind:\n\n- üöÄ **Token Efficiency:** Get just the data you need, precisely structured to minimize overhead for your LLM prompts and responses.\n- üß† **LLM-Centric Design:** Tools and data formats are tailored for easy integration and consumption by AI models.\n- üìä **Comprehensive YouTube Toolkit:** Access a wide array of YouTube functionalities, from video details and transcripts to channel statistics and trending content.\n- üõ°Ô∏è **Robust & Reliable:** Built with strong input validation (Zod) and clear error handling.\n\n## Key Features\n\n- **Optimized Video Information:** Search videos with advanced filters. Retrieve detailed metadata, statistics (views, likes, etc.), and content details, all structured for minimal token footprint.\n- **Efficient Transcript Management:** Fetch video captions/subtitles with multi-language support, perfect for content analysis by LLMs.\n- **Insightful Channel Analysis:** Get concise channel statistics (subscribers, views, video count) and discover a channel's top-performing videos without data bloat.\n- **Lean Trend Discovery:** Find trending videos by region and category, and get lists of available video categories, optimized for quick AI processing.\n- **Structured for AI:** All responses are designed to be easily parsable and immediately useful for language models.\n- **Efficient Comment Retrieval:** Fetch video comments with fine-grained control over the number of results and replies, optimized for sentiment analysis and feedback extraction.\n\n## Available Tools\n\nThe server provides the following MCP tools, each designed to return token-optimized data:\n\n| Tool Name                       | Description                                                                                                                                  | Parameters (see details in tool schema)                                                                                                                   |\n| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `getVideoDetails`               | Retrieves detailed, **lean** information for multiple YouTube videos including metadata, statistics, engagement ratios, and content details. | `videoIds` (array of strings)                                                                                                                             |\n| `searchVideos`                  | Searches for videos or channels based on a query string with various filtering options, returning **concise** results.                       | `query` (string), `maxResults` (optional number), `order` (optional), `type` (optional), `channelId` (optional), etc.                                     |\n| `getTranscripts`                | Retrieves **token-efficient** transcripts (captions) for multiple videos, with options for full text or key segments (intro/outro).          | `videoIds` (array of strings), `lang` (optional string for language code), `format` (optional enum: 'full_text', 'key_segments' - default 'key_segments') |\n| `getChannelStatistics`          | Retrieves **lean** statistics for multiple channels (subscriber count, view count, video count, creation date).                              | `channelIds` (array of strings)                                                                                                                           |\n| `getChannelTopVideos`           | Retrieves a list of a channel's top-performing videos with **lean** details and engagement ratios.                                           | `channelId` (string), `maxResults` (optional number)                                                                                                      |\n| `getTrendingVideos`             | Retrieves a list of trending videos for a given region and optional category, with **lean** details and engagement ratios.                   | `regionCode` (optional string), `categoryId` (optional string), `maxResults` (optional number)                                                            |\n| `getVideoCategories`            | Retrieves available YouTube video categories (ID and title) for a specific region, providing **essential data only**.                        | `regionCode` (optional string)                                                                                                                            |\n| `getVideoComments`              | Retrieves comments for a YouTube video. Allows sorting, limiting results, and fetching a small number of replies per comment.                | `videoId` (string), `maxResults` (optional number), `order` (optional), `maxReplies` (optional number), `commentDetail` (optional string)                 |\n| `findConsistentOutlierChannels` | Identifies channels that consistently perform as outliers within a specific niche. **Requires a MongoDB connection.**                        | `niche` (string), `minVideos` (optional number), `maxChannels` (optional number)                                                                          |\n\n_For detailed input parameters and their descriptions, please refer to the `inputSchema` within each tool's configuration file in the `src/tools/` directory (e.g., `src/tools/video/getVideoDetails.ts`)._\n\n> _**Note on API Quota Costs:** Most tools are highly efficient. `getVideoDetails`, `getChannelStatistics`, and `getTrendingVideos` cost only **1 unit** per call. The `getTranscripts` tool has **0** API cost. The new `getVideoComments` tool has a variable cost: the base call is **1 unit**, but if you request replies (by setting `maxReplies > 0`), it costs an **additional 1 unit for each top-level comment** it fetches replies for. The search-based tools are the most expensive: `searchVideos` costs **100 units** and `getChannelTopVideos` costs **101 units**._\n\n## Advanced Usage & Local Development\n\nIf you wish to contribute, modify the server, or run it locally outside of an MCP client's managed environment:\n\n### Prerequisites\n\n- Node.js (version specified in `package.json` engines field - currently `>=20.0.0`)\n- npm (usually comes with Node.js)\n- A YouTube Data API v3 Key (see [YouTube API Setup](#youtube-api-setup))\n\n### Local Setup\n\n1.  **Clone the repository:**\n\n    ```bash\n    git clone https://github.com/kirbah/mcp-youtube.git\n    cd mcp-youtube\n    ```\n\n2.  **Install dependencies:**\n\n    ```bash\n    npm ci\n    ```\n\n3.  **Configure Environment:**\n    Create a `.env` file in the root by copying `.env.example`:\n    ```bash\n    cp .env.example .env\n    ```\n    Then, edit `.env` to add your `YOUTUBE_API_KEY`:\n    ```\n    YOUTUBE_API_KEY=your_youtube_api_key_here\n    MDB_MCP_CONNECTION_STRING=your_mongodb_connection_string_here\n    ```\n\n### Development Scripts\n\n```bash\n# Run in development mode with live reloading\nnpm run dev\n\n# Build for production\nnpm run build\n\n# Run the production build (after npm run build)\nnpm start\n\n# Lint files\nnpm run lint\n\n# Run tests\nnpm run test\nnpm run test -- --coverage # To generate coverage reports\n\n# Inspect MCP server using the Model Context Protocol Inspector\nnpm run inspector\n```\n\n### Local Development with an MCP Client\n\nTo have an MCP client run your _local development version_ (instead of the published NPM package):\n\n1.  Ensure you have a script in `package.json` for a non-watching start, e.g.:\n\n    ```json\n    \"scripts\": {\n      \"start:client\": \"tsx ./src/index.ts\"\n    }\n    ```\n\n2.  Configure your MCP client to spawn this local script:\n    ```json\n    {\n      \"mcpServers\": {\n        \"youtube_local_dev\": {\n          \"command\": \"npm\",\n          \"args\": [\"run\", \"start:client\"],\n          \"working_directory\": \"/absolute/path/to/your/cloned/mcp-youtube\",\n          \"env\": {\n            \"YOUTUBE_API_KEY\": \"YOUR_LOCAL_DEV_API_KEY_HERE\"\n          }\n        }\n      }\n    }\n    ```\n    _Note on the env block above: Setting YOUTUBE_API_KEY directly in the env block for the client configuration is one way to provide the API key. Alternatively, if your server correctly loads its .env file based on the working_directory, you might not need to specify it in the client's env block, as long as your local .env file in the project root contains the YOUTUBE_API_KEY. The working_directory path must be absolute and correct for the server to find its .env file._\n\n## YouTube API Setup\n\n1.  Go to the [Google Cloud Console](https://console.cloud.google.com/).\n2.  Create a new project or select an existing one.\n3.  In the navigation menu, go to \"APIs & Services\" > \"Library\".\n4.  Search for \"YouTube Data API v3\" and **Enable** it for your project.\n5.  Go to \"APIs & Services\" > \"Credentials\".\n6.  Click \"+ CREATE CREDENTIALS\" and choose \"API key\".\n7.  Copy the generated API key. This is your `YOUTUBE_API_KEY`.\n8.  **Important Security Step:** Restrict your API key to prevent unauthorized use. Click on the API key name, and under \"API restrictions,\" select \"Restrict key\" and choose \"YouTube Data API v3.\" You can also add \"Application restrictions\" (e.g., IP addresses) if applicable.\n\n## System Requirements\n\n- Node.js: `>=20.0.0` (as specified in `package.json`)\n- npm (for managing dependencies and running scripts)\n\n## Deep Dive: `findConsistentOutlierChannels` Tool\n\nThe `findConsistentOutlierChannels` tool is designed to identify emerging or established YouTube channels that consistently outperform their size within a specific niche. This tool is particularly useful for content creators, marketers, and analysts looking for high-potential channels.\n\n**Important Note:** This tool **requires a MongoDB connection** to store and analyze channel data. Without `MDB_MCP_CONNECTION_STRING` configured, this tool will not be available.\n\n### Internal Logic Overview\n\nThe tool operates through a multi-phase analysis process, leveraging both YouTube Data API and a MongoDB database:\n\n1.  **Candidate Search (Phase 1):**\n    - Uses the provided `query` to search for relevant videos and channels on YouTube.\n    - Filters initial results based on `videoCategoryId` and `regionCode` if specified.\n    - Collects a broad set of potential channels for deeper analysis.\n\n2.  **Channel Filtering (Phase 2):**\n    - Retrieves detailed statistics for candidate channels (subscribers, total views, video count).\n    - Filters channels based on `channelAge` (e.g., 'NEW' for channels under 6 months, 'ESTABLISHED' for 6-24 months).\n    - Ensures channels meet a minimum video count to be considered for consistency.\n\n3.  **Deep Analysis (Phase 3):**\n    - For each filtered channel, fetches their recent top-performing videos.\n    - Calculates a \"viral factor\" for each video (e.g., views relative to subscriber count).\n    - Assesses the `consistencyLevel` (e.g., 'MODERATE' for ~30% of videos showing outlier performance, 'HIGH' for ~50%).\n    - Determines `outlierMagnitude` (e.g., 'STANDARD' for views > subscribers, 'STRONG' for views > 3x subscribers).\n\n4.  **Ranking & Formatting (Phase 4):**\n    - Ranks channels based on their consistency, outlier magnitude, and overall performance within the niche.\n    - Formats the results into a token-optimized structure suitable for LLMs, including key channel metrics and examples of outlier videos.\n\n### Key Parameters Controlling the Flow\n\nThe behavior of this tool is primarily controlled by the following parameters:\n\n- `query` (string, required): The central topic or niche to analyze (e.g., \"DIY home repair\", \"quantum computing explained\").\n- `channelAge` (enum: \"NEW\", \"ESTABLISHED\", default: \"NEW\"): Focuses the search on emerging or more mature channels.\n- `consistencyLevel` (enum: \"MODERATE\", \"HIGH\", default: \"MODERATE\"): Sets the threshold for how consistently a channel's videos must perform as outliers.\n- `outlierMagnitude` (enum: \"STANDARD\", \"STRONG\", default: \"STANDARD\"): Defines how significantly a video's performance must exceed typical expectations (e.g., views vs. subscribers) to be considered an \"outlier.\"\n- `videoCategoryId` (string, optional): Narrows the search to a specific YouTube category ID.\n- `regionCode` (string, optional): Targets channels relevant to a particular geographical region.\n- `maxResults` (number, default: 10): Limits the number of top outlier channels returned.\n\n## Security Considerations\n\n- **API Key Security:** Your `YOUTUBE_API_KEY` is sensitive. Never commit it directly to your repository. Use environment variables (e.g., via a `.env` file which should be listed in `.gitignore`).\n- **API Quotas:** The YouTube Data API has a daily usage quota (default is 10,000 units). All tool calls deduct from this quota. Monitor your usage in the Google Cloud Console and be mindful of the cost of each tool. For a detailed breakdown of costs per API method, see the [official documentation](https://developers.google.com/youtube/v3/determine_quota_cost).\n- **Input Validation:** The server uses Zod for robust input validation for all tool parameters, enhancing security and reliability.\n\n## License\n\nThis project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Retrieve token-optimized, structured video details including metadata and engagement statistics",
        "Search YouTube videos and channels with advanced filtering options",
        "Fetch token-efficient video transcripts with multi-language and format support",
        "Obtain concise channel statistics and top-performing videos",
        "Discover trending videos by region and category with lean data",
        "Retrieve available YouTube video categories for specific regions",
        "Fetch video comments with control over sorting, limits, and replies",
        "Identify consistent outlier channels within niches using MongoDB caching"
      ],
      "limitations": [
        "Requires a valid YouTube Data API v3 key for all API interactions",
        "MongoDB connection is optional but recommended for caching and improved performance; without it, API quota limits may be hit faster",
        "API quota costs vary by tool, with search-based tools being the most expensive (e.g., searchVideos costs 100 units per call)",
        "The findConsistentOutlierChannels tool requires a MongoDB connection and cannot function without it",
        "Hardcoded MongoDB database name 'youtube_niche_analysis' must be used if MongoDB is enabled",
        "Windows PowerShell users may encounter issues running the server with npx and may need to adjust the command"
      ],
      "requirements": [
        "YouTube Data API v3 key (set as environment variable YOUTUBE_API_KEY)",
        "Node.js version >= 20.0.0",
        "npm package manager",
        "Optional MongoDB connection string with read/write permissions to database 'youtube_niche_analysis' (set as MDB_MCP_CONNECTION_STRING)",
        "For local development: cloning the repository, installing dependencies, and configuring .env file with required keys"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, a full list of tools with parameters, explicit limitations, and environment requirements, making it excellent for users and developers.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# YouTube Data MCP Server (@kirbah/mcp-youtube)\n\n<!-- Badges Start -->\n<p align=\"left\">\n  <!-- GitHub Actions CI -->\n  <a href=\"https://github.com/kirbah/mcp-youtube/actions/workflows/ci.yml\">\n    <img src=\"https://github.com/kirbah/mcp-youtube/actions/workflows/ci.yml/badge.svg\" alt=\"CI Status\" />\n  </a>\n  <!-- Codecov -->\n  <a href=\"https://codecov.io/gh/kirbah/mcp-youtube\">\n    <img src=\"https://codecov.io/gh/kirbah/mcp-youtube/branch/main/graph/badge.svg?token=Y6B2E0T82P\" alt=\"Code Coverage\"/>\n  </a>\n  <!-- NPM Version -->\n  <a href=\"https://www.npmjs.com/package/@kirbah/mcp-youtube\">\n    <img src=\"https://img.shields.io/npm/v/@kirbah/mcp-youtube.svg\" alt=\"NPM Version\" />\n  </a>\n  <!-- NPM Downloads -->\n  <a href=\"https://www.npmjs.com/package/@kirbah/mcp-youtube\">\n    <img src=\"https://img.shields.io/npm/dt/@kirbah/mcp-youtube.svg\" alt=\"NPM Downloads\" />\n  </a>\n  <!-- Node Version -->\n  <a href=\"package.json\">\n    <img src=\"https://img.shields.io/node/v/@kirbah/mcp-youtube.svg\" alt=\"Node.js Version Support\" />\n  </a>\n</p>\n\n<a href=\"https://glama.ai/mcp/servers/@kirbah/mcp-youtube\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@kirbah/mcp-youtube/badge\" />\n</a>\n<!-- Badges End -->\n\n**High-efficiency YouTube MCP server: Get token-optimized, structured data for your LLMs using the YouTube Data API v3.**\n\nThis Model Context Protocol (MCP) server empowers AI language models to seamlessly interact with YouTube. It's engineered to return **lean, structured data**, significantly **reducing token consumption** and making it ideal for cost-effective and performant LLM applications. Access a comprehensive suite of tools for video search, detail retrieval, transcript fetching, channel analysis, and trend discovery‚Äîall optimized for AI.\n\n**Built with MCP TypeScript Starter**\n\nThis project follows the architecture defined in the [MCP TypeScript Starter](https://github.com/kirbah/mcp-typescript-starter).",
        "start_pos": 0,
        "end_pos": 1948,
        "token_count_estimate": 487,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 1,
        "text": "discovery‚Äîall optimized for AI.\n\n**Built with MCP TypeScript Starter**\n\nThis project follows the architecture defined in the [MCP TypeScript Starter](https://github.com/kirbah/mcp-typescript-starter). If you are looking to build your own MCP server using these same patterns (Class-based Tools, Dependency Injection, and strict Type Safety), I recommend using that repository as your starting point.\n\n## Quick Start: Adding to an MCP Client\n\nThe easiest way to use `@kirbah/mcp-youtube` is with an MCP-compatible client application (like Claude Desktop or a custom client).\n\n1.  **Ensure you have a YouTube Data API v3 Key.**\n    - If you don't have one, follow the [YouTube API Setup](#youtube-api-setup) instructions below.\n\n2.  **MongoDB Connection String (Optional):** This server can use MongoDB to cache API responses and store analysis data, which significantly improves performance and reduces API quota usage. If you don't provide a connection string, the server will run without a database, but performance will be degraded, and you may hit API quota limits faster. You can get a free MongoDB Atlas cluster to obtain a connection string.\n\n    **Important:** If you use MongoDB, the server is hardcoded to use the database name `youtube_niche_analysis`. Your connection string must point to this database, and your user must have read/write permissions for it.\n\n3.  **Configure your MCP client:**\n    Add the following JSON configuration to your client, replacing `\"YOUR_YOUTUBE_API_KEY_HERE\"` with your actual API key.\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"youtube\": {\n          \"command\": \"npx\",\n          \"args\": [\"-y\", \"@kirbah/mcp-youtube\"],\n          \"env\": {\n            \"YOUTUBE_API_KEY\": \"YOUR_YOUTUBE_API_KEY_HERE\",\n            \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://user:pass@cluster0.abc.mongodb.net/youtube_niche_analysis\"\n          }\n        }\n      }\n    }\n    ```\n\n    - **Windows PowerShell Users:** `npx` can sometimes cause issues directly.",
        "start_pos": 1748,
        "end_pos": 3733,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 2,
        "text": "\": \"mongodb+srv://user:pass@cluster0.abc.mongodb.net/youtube_niche_analysis\"\n          }\n        }\n      }\n    }\n    ```\n\n    - **Windows PowerShell Users:** `npx` can sometimes cause issues directly. If you encounter problems, try modifying the command as follows:\n      ```json\n        \"command\": \"cmd\",\n        \"args\": [\"/k\", \"npx\", \"-y\", \"@kirbah/mcp-youtube\"],\n      ```\n\nThat's it! Your MCP client should now be able to leverage the YouTube tools provided by this server.\n\n## Why `@kirbah/mcp-youtube`?\n\nIn the world of Large Language Models, every token counts. `@kirbah/mcp-youtube` is designed from the ground up with this principle in mind:\n\n- üöÄ **Token Efficiency:** Get just the data you need, precisely structured to minimize overhead for your LLM prompts and responses.\n- üß† **LLM-Centric Design:** Tools and data formats are tailored for easy integration and consumption by AI models.\n- üìä **Comprehensive YouTube Toolkit:** Access a wide array of YouTube functionalities, from video details and transcripts to channel statistics and trending content.\n- üõ°Ô∏è **Robust & Reliable:** Built with strong input validation (Zod) and clear error handling.\n\n## Key Features\n\n- **Optimized Video Information:** Search videos with advanced filters. Retrieve detailed metadata, statistics (views, likes, etc.), and content details, all structured for minimal token footprint.\n- **Efficient Transcript Management:** Fetch video captions/subtitles with multi-language support, perfect for content analysis by LLMs.\n- **Insightful Channel Analysis:** Get concise channel statistics (subscribers, views, video count) and discover a channel's top-performing videos without data bloat.\n- **Lean Trend Discovery:** Find trending videos by region and category, and get lists of available video categories, optimized for quick AI processing.\n- **Structured for AI:** All responses are designed to be easily parsable and immediately useful for language models.",
        "start_pos": 3533,
        "end_pos": 5483,
        "token_count_estimate": 487,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 3,
        "text": "y, and get lists of available video categories, optimized for quick AI processing.\n- **Structured for AI:** All responses are designed to be easily parsable and immediately useful for language models.\n- **Efficient Comment Retrieval:** Fetch video comments with fine-grained control over the number of results and replies, optimized for sentiment analysis and feedback extraction.\n\n## Available Tools\n\nThe server provides the following MCP tools, each designed to return token-optimized data:\n\n| Tool Name                       | Description                                                                                                                                  | Parameters (see details in tool schema)                                                                                                                   |\n| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `getVideoDetails`               | Retrieves detailed, **lean** information for multiple YouTube videos including metadata, statistics, engagement ratios, and content details. | `videoIds` (array of strings)                                                                                                                             |\n| `searchVideos`                  | Searches for videos or channels based on a query string with various filtering options, returning **concise** results.                       | `query` (string), `maxResults` (optional number), `order` (optional), `type` (optional), `channelId` (optional), etc.                                     |\n| `getTranscripts`                | Retrieves **token-efficient** transcripts (captions) for multiple videos, with options for full text or key segments (intro/outro).",
        "start_pos": 5283,
        "end_pos": 7284,
        "token_count_estimate": 500,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 4,
        "text": "|\n| `getTranscripts`                | Retrieves **token-efficient** transcripts (captions) for multiple videos, with options for full text or key segments (intro/outro).          | `videoIds` (array of strings), `lang` (optional string for language code), `format` (optional enum: 'full_text', 'key_segments' - default 'key_segments') |\n| `getChannelStatistics`          | Retrieves **lean** statistics for multiple channels (subscriber count, view count, video count, creation date).                              | `channelIds` (array of strings)                                                                                                                           |\n| `getChannelTopVideos`           | Retrieves a list of a channel's top-performing videos with **lean** details and engagement ratios.                                           | `channelId` (string), `maxResults` (optional number)                                                                                                      |\n| `getTrendingVideos`             | Retrieves a list of trending videos for a given region and optional category, with **lean** details and engagement ratios.                   | `regionCode` (optional string), `categoryId` (optional string), `maxResults` (optional number)                                                            |\n| `getVideoCategories`            | Retrieves available YouTube video categories (ID and title) for a specific region, providing **essential data only**.                        | `regionCode` (optional string)                                                                                                                            |\n| `getVideoComments`              | Retrieves comments for a YouTube video. Allows sorting, limiting results, and fetching a small number of replies per comment.",
        "start_pos": 7084,
        "end_pos": 8953,
        "token_count_estimate": 459,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 5,
        "text": "|\n| `getVideoComments`              | Retrieves comments for a YouTube video. Allows sorting, limiting results, and fetching a small number of replies per comment.                | `videoId` (string), `maxResults` (optional number), `order` (optional), `maxReplies` (optional number), `commentDetail` (optional string)                 |\n| `findConsistentOutlierChannels` | Identifies channels that consistently perform as outliers within a specific niche. **Requires a MongoDB connection.**                        | `niche` (string), `minVideos` (optional number), `maxChannels` (optional number)                                                                          |\n\n_For detailed input parameters and their descriptions, please refer to the `inputSchema` within each tool's configuration file in the `src/tools/` directory (e.g., `src/tools/video/getVideoDetails.ts`)._\n\n> _**Note on API Quota Costs:** Most tools are highly efficient. `getVideoDetails`, `getChannelStatistics`, and `getTrendingVideos` cost only **1 unit** per call. The `getTranscripts` tool has **0** API cost. The new `getVideoComments` tool has a variable cost: the base call is **1 unit**, but if you request replies (by setting `maxReplies > 0`), it costs an **additional 1 unit for each top-level comment** it fetches replies for. The search-based tools are the most expensive: `searchVideos` costs **100 units** and `getChannelTopVideos` costs **101 units**._\n\n## Advanced Usage & Local Development\n\nIf you wish to contribute, modify the server, or run it locally outside of an MCP client's managed environment:\n\n### Prerequisites\n\n- Node.js (version specified in `package.json` engines field - currently `>=20.0.0`)\n- npm (usually comes with Node.js)\n- A YouTube Data API v3 Key (see [YouTube API Setup](#youtube-api-setup))\n\n### Local Setup\n\n1.  **Clone the repository:**\n\n    ```bash\n    git clone https://github.com/kirbah/mcp-youtube.git\n    cd mcp-youtube\n    ```\n\n2.",
        "start_pos": 8753,
        "end_pos": 10745,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 6,
        "text": "v3 Key (see [YouTube API Setup](#youtube-api-setup))\n\n### Local Setup\n\n1.  **Clone the repository:**\n\n    ```bash\n    git clone https://github.com/kirbah/mcp-youtube.git\n    cd mcp-youtube\n    ```\n\n2.  **Install dependencies:**\n\n    ```bash\n    npm ci\n    ```\n\n3.  **Configure Environment:**\n    Create a `.env` file in the root by copying `.env.example`:\n    ```bash\n    cp .env.example .env\n    ```\n    Then, edit `.env` to add your `YOUTUBE_API_KEY`:\n    ```\n    YOUTUBE_API_KEY=your_youtube_api_key_here\n    MDB_MCP_CONNECTION_STRING=your_mongodb_connection_string_here\n    ```\n\n### Development Scripts\n\n```bash\n# Run in development mode with live reloading\nnpm run dev\n\n# Build for production\nnpm run build\n\n# Run the production build (after npm run build)\nnpm start\n\n# Lint files\nnpm run lint\n\n# Run tests\nnpm run test\nnpm run test -- --coverage # To generate coverage reports\n\n# Inspect MCP server using the Model Context Protocol Inspector\nnpm run inspector\n```\n\n### Local Development with an MCP Client\n\nTo have an MCP client run your _local development version_ (instead of the published NPM package):\n\n1.  Ensure you have a script in `package.json` for a non-watching start, e.g.:\n\n    ```json\n    \"scripts\": {\n      \"start:client\": \"tsx ./src/index.ts\"\n    }\n    ```\n\n2.  Configure your MCP client to spawn this local script:\n    ```json\n    {\n      \"mcpServers\": {\n        \"youtube_local_dev\": {\n          \"command\": \"npm\",\n          \"args\": [\"run\", \"start:client\"],\n          \"working_directory\": \"/absolute/path/to/your/cloned/mcp-youtube\",\n          \"env\": {\n            \"YOUTUBE_API_KEY\": \"YOUR_LOCAL_DEV_API_KEY_HERE\"\n          }\n        }\n      }\n    }\n    ```\n    _Note on the env block above: Setting YOUTUBE_API_KEY directly in the env block for the client configuration is one way to provide the API key.",
        "start_pos": 10545,
        "end_pos": 12372,
        "token_count_estimate": 456,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 7,
        "text": "ur server correctly loads its .env file based on the working_directory, you might not need to specify it in the client's env block, as long as your local .env file in the project root contains the YOUTUBE_API_KEY. The working_directory path must be absolute and correct for the server to find its .env file._\n\n## YouTube API Setup\n\n1.  Go to the [Google Cloud Console](https://console.cloud.google.com/).\n2.  Create a new project or select an existing one.\n3.  In the navigation menu, go to \"APIs & Services\" > \"Library\".\n4.  Search for \"YouTube Data API v3\" and **Enable** it for your project.\n5.  Go to \"APIs & Services\" > \"Credentials\".\n6.  Click \"+ CREATE CREDENTIALS\" and choose \"API key\".\n7.  Copy the generated API key. This is your `YOUTUBE_API_KEY`.\n8.  **Important Security Step:** Restrict your API key to prevent unauthorized use. Click on the API key name, and under \"API restrictions,\" select \"Restrict key\" and choose \"YouTube Data API v3.\" You can also add \"Application restrictions\" (e.g., IP addresses) if applicable.\n\n## System Requirements\n\n- Node.js: `>=20.0.0` (as specified in `package.json`)\n- npm (for managing dependencies and running scripts)\n\n## Deep Dive: `findConsistentOutlierChannels` Tool\n\nThe `findConsistentOutlierChannels` tool is designed to identify emerging or established YouTube channels that consistently outperform their size within a specific niche. This tool is particularly useful for content creators, marketers, and analysts looking for high-potential channels.\n\n**Important Note:** This tool **requires a MongoDB connection** to store and analyze channel data. Without `MDB_MCP_CONNECTION_STRING` configured, this tool will not be available.\n\n### Internal Logic Overview\n\nThe tool operates through a multi-phase analysis process, leveraging both YouTube Data API and a MongoDB database:\n\n1.  **Candidate Search (Phase 1):**\n    - Uses the provided `query` to search for relevant videos and channels on YouTube.\n    - Filters initial results based on `videoCategoryId` and `regionCode` if specified.",
        "start_pos": 12393,
        "end_pos": 14440,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 8,
        "text": "idate Search (Phase 1):**\n    - Uses the provided `query` to search for relevant videos and channels on YouTube.\n    - Filters initial results based on `videoCategoryId` and `regionCode` if specified.\n    - Collects a broad set of potential channels for deeper analysis.\n\n2.  **Channel Filtering (Phase 2):**\n    - Retrieves detailed statistics for candidate channels (subscribers, total views, video count).\n    - Filters channels based on `channelAge` (e.g., 'NEW' for channels under 6 months, 'ESTABLISHED' for 6-24 months).\n    - Ensures channels meet a minimum video count to be considered for consistency.\n\n3.  **Deep Analysis (Phase 3):**\n    - For each filtered channel, fetches their recent top-performing videos.\n    - Calculates a \"viral factor\" for each video (e.g., views relative to subscriber count).\n    - Assesses the `consistencyLevel` (e.g., 'MODERATE' for ~30% of videos showing outlier performance, 'HIGH' for ~50%).\n    - Determines `outlierMagnitude` (e.g., 'STANDARD' for views > subscribers, 'STRONG' for views > 3x subscribers).\n\n4.  **Ranking & Formatting (Phase 4):**\n    - Ranks channels based on their consistency, outlier magnitude, and overall performance within the niche.\n    - Formats the results into a token-optimized structure suitable for LLMs, including key channel metrics and examples of outlier videos.\n\n### Key Parameters Controlling the Flow\n\nThe behavior of this tool is primarily controlled by the following parameters:\n\n- `query` (string, required): The central topic or niche to analyze (e.g., \"DIY home repair\", \"quantum computing explained\").\n- `channelAge` (enum: \"NEW\", \"ESTABLISHED\", default: \"NEW\"): Focuses the search on emerging or more mature channels.\n- `consistencyLevel` (enum: \"MODERATE\", \"HIGH\", default: \"MODERATE\"): Sets the threshold for how consistently a channel's videos must perform as outliers.\n- `outlierMagnitude` (enum: \"STANDARD\", \"STRONG\", default: \"STANDARD\"): Defines how significantly a video's performance must exceed typical expectations (e.g., views vs.",
        "start_pos": 14240,
        "end_pos": 16275,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      },
      {
        "chunk_id": 9,
        "text": "deos must perform as outliers.\n- `outlierMagnitude` (enum: \"STANDARD\", \"STRONG\", default: \"STANDARD\"): Defines how significantly a video's performance must exceed typical expectations (e.g., views vs. subscribers) to be considered an \"outlier.\"\n- `videoCategoryId` (string, optional): Narrows the search to a specific YouTube category ID.\n- `regionCode` (string, optional): Targets channels relevant to a particular geographical region.\n- `maxResults` (number, default: 10): Limits the number of top outlier channels returned.\n\n## Security Considerations\n\n- **API Key Security:** Your `YOUTUBE_API_KEY` is sensitive. Never commit it directly to your repository. Use environment variables (e.g., via a `.env` file which should be listed in `.gitignore`).\n- **API Quotas:** The YouTube Data API has a daily usage quota (default is 10,000 units). All tool calls deduct from this quota. Monitor your usage in the Google Cloud Console and be mindful of the cost of each tool. For a detailed breakdown of costs per API method, see the [official documentation](https://developers.google.com/youtube/v3/determine_quota_cost).\n- **Input Validation:** The server uses Zod for robust input validation for all tool parameters, enhancing security and reliability.\n\n## License\n\nThis project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.",
        "start_pos": 16075,
        "end_pos": 17436,
        "token_count_estimate": 340,
        "source_type": "readme",
        "agent_id": "964cd6faa82cb49e"
      }
    ]
  },
  {
    "agent_id": "329df63d9b7dd53a",
    "name": "ai.smithery/kkjdaniel-bgg-mcp",
    "source": "mcp",
    "source_url": "https://github.com/kkjdaniel/bgg-mcp",
    "description": "BGG MCP provides access to the BoardGameGeek API through the Model Context Protocol, enabling retr‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-16T00:14:29.642756Z",
    "indexed_at": "2026-02-18T04:07:32.023377",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<p align=\"center\">\n  <img src=\"images/bgg-mcp-logo.png\" width=\"200\" alt=\"BGG MCP Logo\">\n</p>\n<h1 align=\"center\">BGG MCP: BoardGameGeek MCP Server</h1>\n\n<p align=\"center\">\n  <a href=\"https://archestra.ai/mcp-catalog/kkjdaniel__bgg-mcp\"><img src=\"https://archestra.ai/mcp-catalog/api/badge/quality/kkjdaniel/bgg-mcp\" alt=\"trust score badge\"></a>\n  <a href=\"https://github.com/modelcontextprotocol/registry\"><img src=\"https://img.shields.io/badge/MCP_Registry-BGG_MCP-green\" alt=\"MCP Registry\"></a>\n  <br>\n  <a href=\"https://go.dev/\"><img src=\"https://img.shields.io/github/go-mod/go-version/kkjdaniel/bgg-mcp\" alt=\"Go Version\"></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/github/license/kkjdaniel/bgg-mcp\" alt=\"License\"></a>\n  <a href=\"https://modelcontextprotocol.io\"><img src=\"https://img.shields.io/badge/MCP-Protocol-blue\" alt=\"MCP Protocol\"></a>\n</p>\n\nBGG MCP provides access to the BoardGameGeek API through the [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol), enabling retrieval and filtering of board game data, user collections, and profiles. The server is implemented in Go, using the [GoGeek](https://github.com/kkjdaniel/gogeek) library, which helps ensure robust API interactions.\n\nPrice data is provided by [BoardGamePrices.co.uk](https://boardgameprices.co.uk), offering real-time pricing from multiple retailers.\n\nGame recommendations are powered by [Recommend.Games](https://recommend.games/), which provides algorithmic similarity recommendations based on BoardGameGeek data.\n\n<a href=\"https://boardgamegeek.com/\">\n  <img src=\"images/powered-bgg.webp\" width=\"160\" alt=\"Powered by BGG\">\n</a>\n\n## Demo\n\n<div align=\"center\">\n  \n  [![Rules Tool Demo Video](https://img.youtube.com/vi/cNX4WwVbFko/maxresdefault.jpg)](https://youtu.be/cNX4WwVbFko)\n  \n  **[‚ñ∂Ô∏è Watch the Rules Tool Demo Video](https://youtu.be/cNX4WwVbFko)**\n  \n</div>\n\n## Tools\n\n### Core Tools\n\n| Tool                 | Description                                                                 |\n| -------------------- | --------------------------------------------------------------------------- |\n| `bgg-search`         | Search for board games with type filtering (base games, expansions, or all) |\n| `bgg-details`        | Get detailed information about a specific board game                        |\n| `bgg-collection`     | Query and filter a user's game collection with extensive filtering options  |\n| `bgg-hot`            | Get the current BGG hotness list                                            |\n| `bgg-user`           | Get user profile information                                                |\n| `bgg-price`          | Get current prices from multiple retailers using BGG IDs                    |\n| `bgg-trade-finder`   | Find trading opportunities between two BGG users                            |\n| `bgg-recommender`    | Get game recommendations based on similarity to a specific game             |\n| `bgg-thread-details` | Get the full content of a specific BGG forum thread including all posts     |\n\n### üß™ Experimental Tools\n\n| Tool        | Description                                                                                |\n| ----------- | ------------------------------------------------------------------------------------------ |\n| `bgg-rules` | Answer rules questions by searching BGG forums for relevant discussions and clarifications |\n\n## Resources\n\nBGG MCP exposes resources that AI assistants can access directly for contextual information:\n\n| Resource            | URI                   | Description                                                      |\n| ------------------- | --------------------- | ---------------------------------------------------------------- |\n| `BGG Hotness`       | `bgg://hotness`       | Current BGG hotness list, always available                       |\n| `My BGG Collection` | `bgg://my-collection` | Your personal BGG collection (requires `BGG_USERNAME` to be set) |\n\n## Prompts\n\nBGG MCP includes pre-configured prompts for common workflows:\n\n| Prompt                   | Description                                                                          |\n| ------------------------ | ------------------------------------------------------------------------------------ |\n| `Trade Sales Post`       | Generate a formatted sales post for your BGG 'for trade' collection with discounted market prices |\n| `Game Recommendations`   | Get personalized game recommendations based on your BGG collection and preferences   |\n\n## Example Prompts\n\nHere are some example prompts you can use to interact with the BGG MCP tools:\n\n### üîç Search\n\n```\n\"Search for Wingspan on BGG\"\n\"How many expansions does Grand Austria Hotel have?\"\n\"Search for Wingspan expansions only\"\n```\n\n### üìä Game Details\n\n```\n\"Get details for Azul\"\n\"Show me information about game ID 224517\"\n\"What's the BGG rating for Gloomhaven?\"\n```\n\n### üìö Collection\n\n```\n\"Show me ZeeGarcia's game collection\"\n\"Show games rated 9+ in kkjdaniel's collection\"\n\"List unplayed games in rahdo's collection\"\n\"Find games for 6 players in kkjdaniel's collection\"\n\"Show me all the games rated 3 and below in my collection\"\n\"What games in my collection does rahdo want?\"\n\"What games does kkjdaniel have that I want?\"\n```\n\n### üî• Hotness\n\n```\n\"Show me the current BGG hotness list\"\n\"What's trending on BGG?\"\n```\n\n### üë§ User Profile\n\n```\n\"Show me details about BGG user rahdo\"\n\"When did user ZeeGarcia join BGG?\"\n\"How many buddies do I have on bgg?\"\n```\n\n### üí∞ Prices\n\n```\n\"Get the best price for Wingspan in GBP\"\n\"Show me the best UK price for Ark Nova\"\n\"Compare prices for: Wingspan & Ark Nova\"\n```\n\n### üéØ Recommendations\n\n```\n\"Recommend games similar to Wingspan\"\n\"What games are like Azul but with at least 1000 ratings?\"\n\"Find 5 games similar to Troyes\"\n```\n\n### üìñ Rules (Experimental)\n\n```\n\"[Your rules question about any board game] - use bgg-rules\"\n\"How does [game mechanic] work in [game name]? use bgg-rules\"\n\"Can I [specific action] in [game name]? use bgg-rules\"\n\"What happens when [situation] in [game name]? use bgg-rules\"\n```\n\nNote: Include \"use bgg-rules\" in your question to ensure the AI searches BGG forums for answers.\n\n## Installation\n\n> **Authentication Required**: Most BGG MCP tools require authentication to access BoardGameGeek's API. See the [Configuration section](#configuration) below for setup instructions.\n\n### A) Docker (Recommended)\n\nBGG MCP is published to [Docker Hub](https://hub.docker.com/r/kdaniel/bgg-mcp) and listed on the [MCP Registry](https://github.com/modelcontextprotocol/registry). Add the following to your `claude_desktop_config.json` (Claude Desktop) or `settings.json` (VS Code / Cursor):\n\n```json\n\"bgg\": {\n    \"command\": \"docker\",\n    \"args\": [\"run\", \"-i\", \"--rm\",\n        \"-e\", \"BGG_API_KEY\",\n        \"-e\", \"BGG_USERNAME\",\n        \"kdaniel/bgg-mcp\"\n    ],\n    \"env\": {\n        \"BGG_API_KEY\": \"your_api_key_here\",\n        \"BGG_USERNAME\": \"your_bgg_username\"\n    }\n}\n```\n\n> See [Configuration](#configuration) below for details on obtaining a BGG API key and setting up your username.\n\nFor more details on connecting MCP servers to your client, see the [official MCP guide](https://modelcontextprotocol.io/docs/develop/connect-local-servers).\n\n### B) Manual Setup\n\n#### 1. Install Go\n\nYou will need to have Go installed on your system to build binary. This can be easily [downloaded and setup here](https://go.dev/doc/install), or you can use the package manager that you prefer such as Brew.\n\n#### 2. Build\n\nThe project includes a Makefile to simplify building and managing the binary.\n\n```bash\n# Build the application (output goes to build/bgg-mcp)\nmake build\n\n# Clean build artifacts\nmake clean\n\n# Both clean and build\nmake all\n```\n\nOr you can simply build it directly with Go...\n\n```bash\ngo build -o build/bgg-mcp\n```\n\n#### 3. Add MCP Config\n\nIn the `settings.json` (VS Code / Cursor) or `claude_desktop_config.json` add the following to your list of servers, pointing it to the binary you created earlier, once you load up your AI tool you should see the tools provided by the server connected:\n\n```json\n\"bgg\": {\n    \"command\": \"path/to/build/bgg-mcp\",\n    \"args\": [\"-mode\", \"stdio\"]\n}\n```\n\nMore details for configuring Claude can be [found here](https://modelcontextprotocol.io/quickstart/user).\n\n## Configuration\n\n### Authentication\n\nBGG MCP v2.0+ uses the GoGeek v2.0 library which requires authentication for reliable access to BoardGameGeek's API.\n\nYou can configure authentication using **either** `BGG_API_KEY` (recommended) or `BGG_COOKIE`:\n\n#### Authentication Setup\n\n##### Option 1: API Key (Recommended)\n\nGet an API key from [BoardGameGeek's API application form](https://boardgamegeek.com/applications) and add it to your configuration:\n\n```json\n\"bgg\": {\n    \"env\": {\n        \"BGG_API_KEY\": \"your_api_key_here\"\n    }\n}\n```\n\n##### Option 2: Cookie Authentication\n\nAlternatively, you can use cookie-based authentication:\n\n```json\n\"bgg\": {\n    \"env\": {\n        \"BGG_COOKIE\": \"bggusername=user; bggpassword=pass; SessionID=xyz\"\n    }\n}\n```\n\n**Note**: If both are provided, `BGG_API_KEY` will be used by default.\n\n### Username Configuration\n\nYou can optionally set the `BGG_USERNAME` environment variable to enable \"me\" and \"my\" references in queries without needing to explicitly state your username:\n\n```json\n\"bgg\": {\n    \"env\": {\n        \"BGG_USERNAME\": \"your_bgg_username\",\n        \"BGG_API_KEY\": \"your_api_key_here\"\n    }\n}\n```\n\nThis enables:\n\n- **Collection queries**: \"Show my collection\" instead of specifying your username\n- **User queries**: \"Show my BGG profile\"\n- **AI assistance**: The AI can automatically use your username for comparisons and analysis\n\n**Note**: When you use self-references (me, my, I) without setting BGG_USERNAME, you'll get a clear error message.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search for board games with type filtering (base games, expansions, or all)",
        "Retrieve detailed information about specific board games",
        "Query and filter a user's game collection with extensive filtering options",
        "Get the current BoardGameGeek hotness list",
        "Retrieve user profile information",
        "Obtain current prices from multiple retailers using BoardGameGeek IDs",
        "Find trading opportunities between two BoardGameGeek users",
        "Provide game recommendations based on similarity to a specific game",
        "Retrieve full content of specific BoardGameGeek forum threads including all posts",
        "Answer rules questions by searching BoardGameGeek forums for relevant discussions (experimental)"
      ],
      "limitations": [
        "Requires authentication for most tools to access BoardGameGeek's API",
        "Experimental rules tool may have limited accuracy or coverage",
        "Self-reference queries require BGG_USERNAME to be set, otherwise return errors"
      ],
      "requirements": [
        "BoardGameGeek API key (recommended) or BGG_COOKIE for authentication",
        "Optional BGG_USERNAME environment variable for self-reference queries",
        "Go programming language installed for manual build",
        "Docker for recommended deployment",
        "Access to BoardGameGeek API via approved credentials"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions, usage examples, authentication requirements, and notes on limitations, making it excellent in quality.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<p align=\"center\">\n  <img src=\"images/bgg-mcp-logo.png\" width=\"200\" alt=\"BGG MCP Logo\">\n</p>\n<h1 align=\"center\">BGG MCP: BoardGameGeek MCP Server</h1>\n\n<p align=\"center\">\n  <a href=\"https://archestra.ai/mcp-catalog/kkjdaniel__bgg-mcp\"><img src=\"https://archestra.ai/mcp-catalog/api/badge/quality/kkjdaniel/bgg-mcp\" alt=\"trust score badge\"></a>\n  <a href=\"https://github.com/modelcontextprotocol/registry\"><img src=\"https://img.shields.io/badge/MCP_Registry-BGG_MCP-green\" alt=\"MCP Registry\"></a>\n  <br>\n  <a href=\"https://go.dev/\"><img src=\"https://img.shields.io/github/go-mod/go-version/kkjdaniel/bgg-mcp\" alt=\"Go Version\"></a>\n  <a href=\"LICENSE\"><img src=\"https://img.shields.io/github/license/kkjdaniel/bgg-mcp\" alt=\"License\"></a>\n  <a href=\"https://modelcontextprotocol.io\"><img src=\"https://img.shields.io/badge/MCP-Protocol-blue\" alt=\"MCP Protocol\"></a>\n</p>\n\nBGG MCP provides access to the BoardGameGeek API through the [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol), enabling retrieval and filtering of board game data, user collections, and profiles. The server is implemented in Go, using the [GoGeek](https://github.com/kkjdaniel/gogeek) library, which helps ensure robust API interactions.\n\nPrice data is provided by [BoardGamePrices.co.uk](https://boardgameprices.co.uk), offering real-time pricing from multiple retailers.\n\nGame recommendations are powered by [Recommend.Games](https://recommend.games/), which provides algorithmic similarity recommendations based on BoardGameGeek data.",
        "start_pos": 0,
        "end_pos": 1536,
        "token_count_estimate": 384,
        "source_type": "readme",
        "agent_id": "329df63d9b7dd53a"
      },
      {
        "chunk_id": 1,
        "text": "ttps://youtu.be/cNX4WwVbFko)**\n  \n</div>\n\n## Tools\n\n### Core Tools\n\n| Tool                 | Description                                                                 |\n| -------------------- | --------------------------------------------------------------------------- |\n| `bgg-search`         | Search for board games with type filtering (base games, expansions, or all) |\n| `bgg-details`        | Get detailed information about a specific board game                        |\n| `bgg-collection`     | Query and filter a user's game collection with extensive filtering options  |\n| `bgg-hot`            | Get the current BGG hotness list                                            |\n| `bgg-user`           | Get user profile information                                                |\n| `bgg-price`          | Get current prices from multiple retailers using BGG IDs                    |\n| `bgg-trade-finder`   | Find trading opportunities between two BGG users                            |\n| `bgg-recommender`    | Get game recommendations based on similarity to a specific game             |\n| `bgg-thread-details` | Get the full content of a specific BGG forum thread including all posts     |\n\n### üß™ Experimental Tools\n\n| Tool        | Description                                                                                |\n| ----------- | ------------------------------------------------------------------------------------------ |\n| `bgg-rules` | Answer rules questions by searching BGG forums for relevant discussions and clarifications |\n\n## Resources\n\nBGG MCP exposes resources that AI assistants can access directly for contextual information:\n\n| Resource            | URI                   | Description                                                      |\n| ------------------- | --------------------- | ---------------------------------------------------------------- |\n| `BGG Hotness`       | `bgg://hotness`       | Current BGG hotness list, always available                       |\n| `My BGG Collection` | `bgg://my-colle",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "329df63d9b7dd53a"
      },
      {
        "chunk_id": 2,
        "text": "------------------------------------------- |\n| `BGG Hotness`       | `bgg://hotness`       | Current BGG hotness list, always available                       |\n| `My BGG Collection` | `bgg://my-collection` | Your personal BGG collection (requires `BGG_USERNAME` to be set) |\n\n## Prompts\n\nBGG MCP includes pre-configured prompts for common workflows:\n\n| Prompt                   | Description                                                                          |\n| ------------------------ | ------------------------------------------------------------------------------------ |\n| `Trade Sales Post`       | Generate a formatted sales post for your BGG 'for trade' collection with discounted market prices |\n| `Game Recommendations`   | Get personalized game recommendations based on your BGG collection and preferences   |\n\n## Example Prompts\n\nHere are some example prompts you can use to interact with the BGG MCP tools:\n\n### üîç Search\n\n```\n\"Search for Wingspan on BGG\"\n\"How many expansions does Grand Austria Hotel have?\"\n\"Search for Wingspan expansions only\"\n```\n\n### üìä Game Details\n\n```\n\"Get details for Azul\"\n\"Show me information about game ID 224517\"\n\"What's the BGG rating for Gloomhaven?\"\n```\n\n### üìö Collection\n\n```\n\"Show me ZeeGarcia's game collection\"\n\"Show games rated 9+ in kkjdaniel's collection\"\n\"List unplayed games in rahdo's collection\"\n\"Find games for 6 players in kkjdaniel's collection\"\n\"Show me all the games rated 3 and below in my collection\"\n\"What games in my collection does rahdo want?\"\n\"What games does kkjdaniel have that I want?\"\n```\n\n### üî• Hotness\n\n```\n\"Show me the current BGG hotness list\"\n\"What's trending on BGG?\"\n```\n\n### üë§ User Profile\n\n```\n\"Show me details about BGG user rahdo\"\n\"When did user ZeeGarcia join BGG?\"\n\"How many buddies do I have on bgg?\"\n```\n\n### üí∞ Prices\n\n```\n\"Get the best price for Wingspan in GBP\"\n\"Show me the best UK price for Ark Nova\"\n\"Compare prices for: Wingspan & Ark Nova\"\n```\n\n### üéØ Recommendations\n\n```\n\"Recommend games similar to Wingspan\"\n\"What games are like Azul but with at",
        "start_pos": 3696,
        "end_pos": 5744,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "329df63d9b7dd53a"
      },
      {
        "chunk_id": 3,
        "text": "n in GBP\"\n\"Show me the best UK price for Ark Nova\"\n\"Compare prices for: Wingspan & Ark Nova\"\n```\n\n### üéØ Recommendations\n\n```\n\"Recommend games similar to Wingspan\"\n\"What games are like Azul but with at least 1000 ratings?\"\n\"Find 5 games similar to Troyes\"\n```\n\n### üìñ Rules (Experimental)\n\n```\n\"[Your rules question about any board game] - use bgg-rules\"\n\"How does [game mechanic] work in [game name]? use bgg-rules\"\n\"Can I [specific action] in [game name]? use bgg-rules\"\n\"What happens when [situation] in [game name]? use bgg-rules\"\n```\n\nNote: Include \"use bgg-rules\" in your question to ensure the AI searches BGG forums for answers.\n\n## Installation\n\n> **Authentication Required**: Most BGG MCP tools require authentication to access BoardGameGeek's API. See the [Configuration section](#configuration) below for setup instructions.\n\n### A) Docker (Recommended)\n\nBGG MCP is published to [Docker Hub](https://hub.docker.com/r/kdaniel/bgg-mcp) and listed on the [MCP Registry](https://github.com/modelcontextprotocol/registry). Add the following to your `claude_desktop_config.json` (Claude Desktop) or `settings.json` (VS Code / Cursor):\n\n```json\n\"bgg\": {\n    \"command\": \"docker\",\n    \"args\": [\"run\", \"-i\", \"--rm\",\n        \"-e\", \"BGG_API_KEY\",\n        \"-e\", \"BGG_USERNAME\",\n        \"kdaniel/bgg-mcp\"\n    ],\n    \"env\": {\n        \"BGG_API_KEY\": \"your_api_key_here\",\n        \"BGG_USERNAME\": \"your_bgg_username\"\n    }\n}\n```\n\n> See [Configuration](#configuration) below for details on obtaining a BGG API key and setting up your username.\n\nFor more details on connecting MCP servers to your client, see the [official MCP guide](https://modelcontextprotocol.io/docs/develop/connect-local-servers).\n\n### B) Manual Setup\n\n#### 1. Install Go\n\nYou will need to have Go installed on your system to build binary. This can be easily [downloaded and setup here](https://go.dev/doc/install), or you can use the package manager that you prefer such as Brew.\n\n#### 2. Build\n\nThe project includes a Makefile to simplify building and managing the binary.",
        "start_pos": 5544,
        "end_pos": 7580,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "329df63d9b7dd53a"
      },
      {
        "chunk_id": 4,
        "text": "setup here](https://go.dev/doc/install), or you can use the package manager that you prefer such as Brew.\n\n#### 2. Build\n\nThe project includes a Makefile to simplify building and managing the binary.\n\n```bash\n# Build the application (output goes to build/bgg-mcp)\nmake build\n\n# Clean build artifacts\nmake clean\n\n# Both clean and build\nmake all\n```\n\nOr you can simply build it directly with Go...\n\n```bash\ngo build -o build/bgg-mcp\n```\n\n#### 3. Add MCP Config\n\nIn the `settings.json` (VS Code / Cursor) or `claude_desktop_config.json` add the following to your list of servers, pointing it to the binary you created earlier, once you load up your AI tool you should see the tools provided by the server connected:\n\n```json\n\"bgg\": {\n    \"command\": \"path/to/build/bgg-mcp\",\n    \"args\": [\"-mode\", \"stdio\"]\n}\n```\n\nMore details for configuring Claude can be [found here](https://modelcontextprotocol.io/quickstart/user).\n\n## Configuration\n\n### Authentication\n\nBGG MCP v2.0+ uses the GoGeek v2.0 library which requires authentication for reliable access to BoardGameGeek's API.\n\nYou can configure authentication using **either** `BGG_API_KEY` (recommended) or `BGG_COOKIE`:\n\n#### Authentication Setup\n\n##### Option 1: API Key (Recommended)\n\nGet an API key from [BoardGameGeek's API application form](https://boardgamegeek.com/applications) and add it to your configuration:\n\n```json\n\"bgg\": {\n    \"env\": {\n        \"BGG_API_KEY\": \"your_api_key_here\"\n    }\n}\n```\n\n##### Option 2: Cookie Authentication\n\nAlternatively, you can use cookie-based authentication:\n\n```json\n\"bgg\": {\n    \"env\": {\n        \"BGG_COOKIE\": \"bggusername=user; bggpassword=pass; SessionID=xyz\"\n    }\n}\n```\n\n**Note**: If both are provided, `BGG_API_KEY` will be used by default.",
        "start_pos": 7380,
        "end_pos": 9118,
        "token_count_estimate": 434,
        "source_type": "readme",
        "agent_id": "329df63d9b7dd53a"
      },
      {
        "chunk_id": 5,
        "text": "d \"my\" references in queries without needing to explicitly state your username:\n\n```json\n\"bgg\": {\n    \"env\": {\n        \"BGG_USERNAME\": \"your_bgg_username\",\n        \"BGG_API_KEY\": \"your_api_key_here\"\n    }\n}\n```\n\nThis enables:\n\n- **Collection queries**: \"Show my collection\" instead of specifying your username\n- **User queries**: \"Show my BGG profile\"\n- **AI assistance**: The AI can automatically use your username for comparisons and analysis\n\n**Note**: When you use self-references (me, my, I) without setting BGG_USERNAME, you'll get a clear error message.",
        "start_pos": 9228,
        "end_pos": 9789,
        "token_count_estimate": 140,
        "source_type": "readme",
        "agent_id": "329df63d9b7dd53a"
      }
    ]
  },
  {
    "agent_id": "dddc4db7a5753ec2",
    "name": "ai.smithery/kodey-ai-mapwise-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@kodey-ai/mapwise-mcp/mcp",
    "description": "Send friendly, personalized greetings on demand. Generate quick salutations with a simple prompt.‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T21:55:01.522892Z",
    "indexed_at": "2026-02-18T04:07:33.495859",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Send friendly, personalized greetings on demand",
        "Generate quick salutations with a simple prompt"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's greeting generation capability but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "2cbac277fccaaa81",
    "name": "ai.smithery/kodey-ai-salesforce-mcp",
    "source": "mcp",
    "source_url": "https://github.com/kodey-ai/salesforce-mcp",
    "description": "Run SOQL queries to explore and retrieve Salesforce data. Inspect records, fields, and relationshi‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-08T03:10:49.227357Z",
    "indexed_at": "2026-02-18T04:07:35.148936",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# mcp\n\nMCP Server for Interacting with Salesforce Orgs\n\n[![NPM](https://img.shields.io/npm/v/@salesforce/mcp.svg?label=@salesforce/mcp)](https://www.npmjs.com/package/@salesforce/mcp) [![License](https://img.shields.io/badge/License-Apache--2.0-blue.svg)](https://opensource.org/license/apache-2-0)\n[![smithery badge](https://smithery.ai/badge/@kodey-ai/salesforce-mcp-kodey)](https://smithery.ai/server/@kodey-ai/salesforce-mcp-kodey)\n\n## Feedback\n\n Report bugs and issues [here](https://github.com/forcedotcom/mcp/issues).  \nFor feature requests and other related topics, start a Discussion [here](https://github.com/forcedotcom/mcp/discussions).  \n\n## Overview of the Salesforce DX MCP Server (Developer Preview)\n\nThe Salesforce DX MCP Server is a specialized Model Context Protocol (MCP) implementation designed to facilitate seamless interaction between large language models (LLMs) and Salesforce orgs. This MCP server provides a robust set of tools and capabilities that enable LLMs to read, manage, and operate Salesforce resources securely.\n\nKey Features:\n\n- Direct interaction with Salesforce orgs through LLM-driven tools.\n- Secure access using TypeScript libraries (not shelling out to the `sf` Salesforce CLI).\n- Improved security by avoiding the exposure of secrets in plain text.\n- Granular access control with org allowlisting.\n- Modular tool architecture for easy extensibility.\n\n**NOTE**: The Salesforce DX MCP Server is available as a developer preview. The feature isn‚Äôt generally available unless or until Salesforce announces its general availability in documentation or in press releases or public statements. All commands, parameters, and other features are subject to change or deprecation at any time, with or without notice. Don't implement functionality developed with these commands or tools. As we continue to enhance and refine the implementation, the available functionality and tools may evolve. We welcome feedback and contributions to help shape the future of this project.\n\n### Security Features\n\nThe Salesforce DX MCP Server was designed with security as a top priority.\n\n- **Uses TypeScript libraries directly**\n\n  - Greatly decreases the size of the MCP Server.\n  - Significantly reduces the risk of remote code execution (RCE).\n\n- **No secrets needed in configuration**\n\n  - Eliminates the risk of plain text secret exposure.\n  - Accesses pre-existing (encrypted) auth files on the user's machine.\n  - Implements allowlisting for auth info key/values to prevent sensitive data exposure.\n\n- **No secrets exposed via MCP tools**\n\n  - Prevents other tools from accessing unencrypted tokens.\n  - Tools pass usernames around instead of tokens.\n\n- **Granular access control**\n\n  - MCP Server can access auth info for only orgs that have been explicitly allowlisted.\n  - Users specify allowed orgs when starting the server.\n\n### Agentforce for Developers Includes the Salesforce DX MCP Server\n\nAgentforce for Developers is an AI-powered Salesforce developer tool that's available as an easy-to-install Visual Studio Code (VS Code) extension. It includes Dev Agent (developer preview), an intelligent coding partner that provides information and, most importantly, can take action. Through agentic chat powered by MCP, Dev Agent can execute commands and perform complex workflows automatically, right from within VS Code.\n\nThe Salesforce DX MCP server is pre-configured in Agentforce for Developers, so you can start using the DX MCP tools immediately after you install the extension in VS Code. \n\nSee [Set Up Agentforce for Developers](https://developer.salesforce.com/docs/platform/einstein-for-devs/guide/einstein-setup.html) and [Chat with Dev Agent (Dev Preview)](https://developer.salesforce.com/docs/platform/einstein-for-devs/guide/devagent-overview.html) for more information. \n\n## Get Started Using VS Code as the Client\n\nWant to jump in and see what all the fuss is about? Read on!\n\nThis example uses Visual Studio Code (VS Code) as the MCP client. After you configure it with the Salesforce DX MCP Server, you then use GitHub Copilot and natural language to easily execute typical Salesforce DX development tasks, such as listing your authorized orgs, viewing org records, and deploying or retrieving metadata.\n\nBut you're not limited to using only VS Code and Copilot as the MCP client! As already mentioned, you can use [Agentforce for Developers](README.md#agentforce-for-developers-includes-the-salesforce-dx-mcp-server), which is pre-configured with the DX MCP server.  Or you can [configure many other clients](README.md#configure-other-clients-to-use-the-salesforce-dx-mcp-server) to use the Salesforce DX MCP Server, such as Cursor, Cline, Claude Desktop, Zed, Windsurf, and more. \n\n**Before You Begin**\n\nFor the best getting-started experience, make sure that you have a Salesforce DX environment set up on your computer. In particular:\n\n- [Install Salesforce CLI](https://developer.salesforce.com/docs/atlas.en-us.sfdx_setup.meta/sfdx_setup/sfdx_setup_intro.htm) on your computer.\n- [Install VS Code](https://code.visualstudio.com/docs) on your computer.\n- [Create a Salesforce DX project](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_ws_create_new.htm) and open it in VS Code. You can also clone an example repo, such as [dreamhouse-lwc](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_intro_sample_repo.htm), which is a ready-to-use DX project that contains a simple Salesforce application, with metadata and test data.\n- [Authorize at least one Salesforce org](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_web_flow.htm) to use with your DX project. You can also create a scratch org.\n\n**Let's Do It**\n\n1. Create a `.vscode/mcp.json` file at the root of your DX project and add this JSON:\n\n   ```json\n   {\n     \"servers\": {\n       \"Salesforce DX\": {\n         \"type\": \"stdio\",\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"all\"]\n       }\n     }\n   }\n   ```\n\n   You can also configure the MCP server globally by editing the VS Code [settings.json](https://code.visualstudio.com/docs/configure/settings#_settings-file-locations) file and adding a similar JSON snippet but contained in an `mcp:servers` section.\n\n   The `--orgs` flag is required and specifies the authorized orgs you're allowing the MCP server to access. The `--toolsets` and `--tools` flags are used to specify which tools the server will start with. See [Configure the DX MCP Server](README.md#configure-the-dx-mcp-server) for the available values for these flags.\n\n1. Open VS Code, go to **View -> Command Palette** and enter **MCP: List Servers**.\n\n   TIP: You can also get to the command palette by pressing press Ctrl+Shift+P (Windows or Linux) or Command-Shift-P (macOS).\n\n1. Click `Salesforce DX`, then **Start Server**.\n\n   Check the Output tab for the server status.\n\n1. Run **Chat: Open Chat (Agent)** from the command palette to start a new GitHub Copilot chat session.\n\n   Be sure your Copilot chat window is in `Agent` mode; if you're in `Ask` or `Edit` mode, use the [little drop-down](https://github.blog/ai-and-ml/github-copilot/copilot-ask-edit-and-agent-modes-what-they-do-and-when-to-use-them/) to switch.\n\n1. In the GitHub Copilot chat window, use natural language to explain what you want to do. The MCP server determines which configured tool to use, and then shows it to you along with other information. Click **Continue** to run the tool and see the results of your request.\n\n   Try out these examples:\n\n   - List all my orgs.\n   - Which are my active scratch orgs?\n   - Show me all the accounts in the org with alias my-org.\n   - Deploy everything in my project to the org with alias my-org.\n   - Do you see any performance or security issues with the Apex code in the `MyApexClass.cls` file?\n   - I see that my Apex code violates the pmd:ApexCRUDViolation rule; can you give me more information about this rule?\n\n1. To stop, restart, or view the MCP server configuration, run the **MCP: List Servers** command, click `Salesforce DX`, then click the appropriate option.\n\n### Installing via Smithery\n\nTo install salesforce-mcp-kodey automatically via [Smithery](https://smithery.ai/server/@kodey-ai/salesforce-mcp-kodey):\n\n```bash\nnpx -y @smithery/cli install @kodey-ai/salesforce-mcp-kodey\n```\n\n## Configure the DX MCP Server\n\nConfigure the Salesforce DX MCP Server by passing flags to the `args` option. Surround the flag name and its value each in double quotes, and separate all flags and values with commas. Some flags are Boolean and don't take a value. \n\nThis example shows two flags that take a string value (`--orgs` and `--toolsets`) and one Boolean flag (`--allow-non-ga-tools`):\n\n```\n     \"servers\": {\n       \"Salesforce DX\": {\n         \"type\": \"stdio\",\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"all\", \"--allow-non-ga-tools\"]\n       }\n     }\n```\nThe `\"-y\", \"@salesforce/mcp\"` part tells `npx` to automatically install the `@salesforce/mcp` package instead of asking permission. Don't change this. \n\nThese are the available flags that you can pass to the `args` option. \n\n| Flag Name | Description | Required? |Notes |\n| -----------------| -------| ------- | ----- |\n| `--orgs` | One or more orgs that you've locally authorized. | Yes | You must specify at least one org. <br/> <br/>See [Configure Orgs](README.md#configure-orgs) for the values you can pass to this flag. |\n| `--toolsets` | Sets of tools, based on functionality, that you want to enable. | No | Set to \"all\" to enable every tool in every toolset. <br/> <br/>See [Configure Toolsets](README.md#configure-toolsets) for the values you can pass to this flag.|\n| `--tools` | Individual tool names that you want to enable. | No | You can use this flag in combination with the `--toolsets` flag. For example, you can enable all tools in one toolset, and just one tool in a different toolset. |\n| `--no-telemetry` | Boolean flag to disable telemetry, the automatic collection of data for monitoring and analysis. | No | Telemetry is enabled by default, so specify this flag to disable it.  |\n| `--debug` | Boolean flag that requests that the DX MCP Server print debug logs. | No | Debug mode is disabled by default. <br/> <br/>**NOTE:** Not all MCP clients expose MCP logs, so this flag might not work for all IDEs. |\n| `--allow-non-ga-tools` | Boolean flag to allow the DX MCP Server to use both the generally available (GA) and NON-GA tools that are in the toolsets or tools you specify. | No | By default, the DX MCP server uses only the tools marked GA. |\n| `--dynamic-tools` | (experimental) Boolean flag that enables dynamic tool discovery and loading. When specified, the DX MCP server starts with a minimal set of core tools and loads new tools as needed. | No| This flag is useful for reducing the initial context size and improving LLM performance. Dynamic tool discovery is disabled by default.<br/> <br/>**NOTE:** This feature works in VSCode and Cline but may not work in other environments.|\n\n### Configure Orgs\n\nThe Salesforce MCP tools require an org, and so you must include the required `--orgs` flag to specify at least one authorized org when you configure the MCP server. Separate multiple values with commas.\n\nYou must explicitly [authorize the orgs](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_web_flow.htm) on your computer before the MCP server can access them. Use the `org login web` Salesforce CLI command or the VS Code **SFDX: Authorize an Org** command from the command palette.\n\nThese are the available values for the `--orgs` flag:\n\n- `DEFAULT_TARGET_ORG` - Allow access to your default org. If you've set a local default org in your DX project, the MCP server uses it. If not, the server uses a globally-set default org.\n- `DEFAULT_TARGET_DEV_HUB` - Allow access to your default Dev Hub org. If you've set a local default Dev Hub org in your DX project, the MCP server uses it. If not, the server uses a globally-set default Dev Hub org.\n- `ALLOW_ALL_ORGS` - Allow access to all authorized orgs. Use this value with caution.\n- `<username or alias>` - Allow access to a specific org by specifying its username or alias.\n\nThis example shows how to specify that the MCP tools run against your default org when you configure the MCP server for VS Code:\n\n```json\n       \"servers\": {\n         \"Salesforce DX\": {\n           \"type\": \"stdio\",\n           \"command\": \"npx\",\n           \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\"]\n         }\n       }\n```\n\nThis sample snippet shows how to configure access to your default Dev Hub org and an org with username `test-org@example.com`:\n\n```json\n           \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_DEV_HUB,test-org@example.com\"]\n```\n\nThis sample snippet shows how to configure access to two orgs for which you specified aliases when you authorized them:\n\n```json\n           \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"my-scratch-org,my-dev-hub\"]\n```\n\n### Configure Toolsets\n\nThe Salesforce DX MCP Server supports **toolsets** - a way to selectively enable different groups of MCP tools based on your needs. This allows you to run the MCP server with only the tools you require, which in turn reduces the context.\n\nUse the `--toolsets` flag to specify the toolsets when you configure the Salesforce DX MCP Server. Separate multiple toolsets with commas. Set `--toolsets` to `all` to register every available toolset.\n\nThese are the available toolsets.\n\n| Toolset| Description|\n| ----- | ----- |\n| `all` | Enables all available tools from all toolsets. |\n| `orgs` | [Tools to manage your authorized orgs.](README.md#orgs-toolset)|\n| `data` | [Tools to manage the data in your org, such as listing all accounts.](README.md#data-toolset)|\n| `users` | [Tools to manage org users, such as assigning a permission set.](README.md#users-toolset)|\n| `metadata` | [Tools to deploy and retrieve metadata to and from your org and your DX project.](README.md#metadata-toolset)|\n| `testing` | [Tools to test your code and features](README.md#testing-toolset)|\n| `other` | [Other useful tools, such as tools for static analysis of your code using Salesforce Code Analyzer.](README.md#other-toolset)|\n| `mobile` | [Tools for mobile development and capabilities.](README.md#mobile-toolset)|\n| `mobile-core` | [A subset of mobile tools focused on essential mobile capabilities.](README.md#mobile-core-toolset)|\n| `aura-experts` | [Tools which provides Aura component analysis, blueprinting, and migration expertise.](README.md#aura-experts-toolset)|\n| `lwc-experts`  | [Tools to assist with LWC development, testing, optimization, and best practices.](README.md#lwc-experts-toolset)|\n\nThis example shows how to enable the `data`, `orgs`, `metadata`, and `other` toolsets when configuring the MCP server for VS Code:\n\n```json\n       \"servers\": {\n         \"Salesforce DX\": {\n           \"type\": \"stdio\",\n           \"command\": \"npx\",\n           \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"data,orgs,metadata,other\"]\n         }\n       }\n```\n\n### Configure Tools\n\nThe Salesforce DX MCP Server also supports registering individual **tools**. This can be used in combination with **toolsets** to further fine-tune registered tools.\n\nUse the `--tools` flag to enable specific tools when you configure the Salesforce DX MCP Server. Separate multiple tools with commas. The `--tools` flag is optional.\n\nThis example shows how to enable the `orgs` and `metadata` toolsets, as well as the `run_soql_query` and `run_apex_test` tools when configuring the MCP server for VS Code:\n\n```json\n       \"servers\": {\n         \"Salesforce DX\": {\n           \"type\": \"stdio\",\n           \"command\": \"npx\",\n           \"args\": [\n              \"-y\",\n              \"@salesforce/mcp\", \n              \"--orgs\", \n              \"DEFAULT_TARGET_ORG\", \n              \"--toolsets\",\n              \"orgs,metadata\"\n              \"--tools\",\n              \"run_soql_query,run_apex_test\"\n            ]\n         }\n       }\n```\n\n\n#### Core Toolset (always enabled)\n\nIncludes these tools:\n\n- `get_username` - Determines the appropriate username or alias for Salesforce operations, handling both default orgs and Dev Hubs.\n- `resume_tool_operation` - Resumes a long-running operation that wasn't completed by another tool.\n\n#### Orgs Toolset\n\nIncludes these tools:\n\n- `list_all_orgs` - Lists all configured Salesforce orgs, with optional connection status checking.\n- `create_org_snapshot` - (NON-GA) Create a scratch org snapshot. \n- `create_scratch_org` - (NON-GA) Create a scratch org. \n- `delete_org` - (NON-GA) Delete a locally-authorized Salesforce scratch org or sandbox.\n- `org_open` - (NON-GA) Open an org in a browser. \n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Data Toolset\n\nIncludes this tool:\n\n- `run_soql_query` - Runs a SOQL query against a Salesforce org.\n\n#### Users Toolset\n\nIncludes this tool:\n\n- `assign_permission_set` - Assigns a permission set to the user or on behalf of another user.\n\n#### Metadata Toolset\n\nIncludes these tools:\n\n- `deploy_metadata` - Deploys metadata from your DX project to an org.\n- `retrieve_metadata` - Retrieves metadata from your org to your DX project.\n\n#### Testing Toolset\n\nIncludes these tools:\n\n- `run_agent_test` - Executes agent tests in your org.\n- `run_apex_test` - Executes apex tests in your org.\n\n#### Mobile Toolset\n\nIncludes these tools, which aren't yet generally available:\n\n- `create_mobile_lwc_app_review` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC App Review Service, offering expert guidance for implementing app review features in Lightning Web Components.\n- `create_mobile_lwc_ar_space_capture` - (NON-GA) Provides TypeScript API documentation for Salesforce L    WC AR Space Capture, offering expert guidance for implementing AR space capture features in Lightning Web Components.\n- `create_mobile_lwc_barcode_scanner` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Barcode Scanner, offering expert guidance for implementing barcode scanning features in Lightning Web Components.\n- `create_mobile_lwc_biometrics` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Biometrics Service, offering expert guidance for implementing biometric authentication features in Lightning Web Components.\n- `create_mobile_lwc_calendar` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Calendar Service, offering expert guidance for implementing calendar integration features in Lightning Web Components.\n- `create_mobile_lwc_contacts` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Contacts Service, offering expert guidance for implementing contacts management features in Lightning Web Components.\n- `create_mobile_lwc_document_scanner` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Document Scanner, offering expert guidance for implementing document scanning features in Lightning Web Components.\n- `create_mobile_lwc_geofencing` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Geofencing Service, offering expert guidance for implementing geofencing features in Lightning Web Components.\n- `create_mobile_lwc_location` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Location Service, offering expert guidance for implementing location services in Lightning Web Components.\n- `create_mobile_lwc_nfc` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC NFC Service, offering expert guidance for implementing NFC features in Lightning Web Components.\n- `create_mobile_lwc_payments` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Payments Service, offering expert guidance for implementing payment processing features in Lightning Web Components.\n- `get_mobile_lwc_offline_analysis` - (NON-GA) Analyzes Lightning Web Components for mobile-specific issues and provides detailed recommendations for mobile offline compatibility and performance improvements.\n- `get_mobile_lwc_offline_guidance` - (NON-GA) Provides structured review instructions to detect and remediate mobile offline code violations in Lightning Web Components for Salesforce Mobile Apps.\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Mobile-core Toolset\n\nIncludes these essential mobile tools, which aren't yet generally available:\n\n- `create_mobile_lwc_barcode_scanner` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Barcode Scanner, offering expert guidance for implementing barcode scanning features in Lightning Web Components.\n- `create_mobile_lwc_biometrics` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Biometrics Service, offering expert guidance for implementing biometric authentication features in Lightning Web Components.\n- `create_mobile_lwc_location` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Location Service, offering expert guidance for implementing location services in Lightning Web Components.\n- `get_mobile_lwc_offline_analysis` - (NON-GA) Analyzes Lightning Web Components for mobile-specific issues and provides detailed recommendations for mobile offline compatibility and performance improvements.\n- `get_mobile_lwc_offline_guidance` - (NON-GA) Provides structured review instructions to detect and remediate mobile offline code violations in Lightning Web Components for Salesforce Mobile Apps.\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Aura Experts Toolset\n\n - `create_aura_blueprint_draft` - (GA)\nCreates a comprehensive Product Requirements Document (PRD) blueprint for Aura component migration. Analyzes Aura component files and generates framework-agnostic specifications suitable for LWC migration, including business requirements, technical patterns, and migration guidelines.\n\n - `enhance_aura_blueprint_draft` - (GA)\nEnhances an existing draft PRD with expert analysis and unknown resolution. Takes a draft blueprint and applies specialized Aura expert knowledge to resolve dependencies, add technical insights, and improve the migration specifications for better LWC implementation guidance.\n\n - `transition_prd_to_lwc` - (GA)\nProvides migration bridge guidance for creating LWC components from Aura specifications. Takes the enhanced PRD and generates specific implementation guidance, platform service mappings, and step-by-step instructions for building the equivalent LWC component.\n\n - `orchestrate_aura_migration` - (GA)\nOrchestrates the complete Aura to LWC migration workflow. Provides end-to-end guidance for the entire migration process, from initial analysis through final implementation, including best practices, tooling recommendations, and quality assurance steps.\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Lwc Experts Toolset\n\n##### Component Development\n - `create_lwc_component` - (GA) Creates complete LWC components from PRD specifications with proper structure and best practices\n - `create_lwc_jest_tests` - (GA) Generates comprehensive Jest test suites for LWC components with coverage and mocking\n - `review_lwc_jest_tests` - (GA) Reviews and validates Jest test implementations for LWC components\n\n##### Development Guidelines\n - `guide_lwc_accessibility` - (GA) Provides accessibility guidelines and testing instructions for LWC components\n - `guide_lwc_best_practices` - (GA) Offers LWC development best practices and coding standards guidance\n - `guide_lwc_development` - (GA) Comprehensive LWC development workflow and implementation guidelines\n - `guide_lwc_rtl_support` - (GA) Right-to-Left internationalization support and RTL development guidance\n - `guide_uplift_ai_metadata` - (GA) AI metadata enhancement for LWC components\n - `guide_lwc_slds2_uplift_linter_fixes` - (GA) Analyzes the given LWC code along with the slds-linter output to fix issues using the SLDS2 knowledge\n - `guide_lwc_security` - (GA) Comprehensive security analysis in accordance with Product Security Guidelines and Lightning Web Security Guidelines\n\n##### Workflow Tools\n - `orchestrate_lwc_component_creation` - (GA) Step-by-step component creation workflow guidance\n - `orchestrate_lwc_component_optimization` - (GA)  Performance optimization and best practices for LWC components\n - `orchestrate_lwc_component_testing` - (GA) Comprehensive testing workflow and test generation guidance\n - `orchestrate_lwc_slds2_uplift` - (GA) Migration guidance for upgrading to SLDS2 design system\n\n##### LDS (Lightning Design System) Tools\n - `explore_lds_uiapi` - (GA) Explores and documents Lightning Design System UI API capabilities\n - `guide_lds_data_consistency` - (GA) Data consistency patterns and best practices for LDS components\n - `guide_lds_development` - (GA) LDS development guidelines and component integration\n - `guide_lds_referential_integrity` - (GA) Referential integrity patterns for LDS data management\n\n##### Migration & Integration Tools\n - `verify_aura_migration_completeness` - (GA) Aura to LWC migration completeness checklist and validation\n - `guide_figma_to_lwc_conversion` - (GA) Converts Figma designs to LWC component specifications\n - `run_lwc_accessibility_jest_tests` - (GA) Accessibility testing utilities and Jest integration for LWC components\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Code-Analyzer Toolset\n\nIncludes these tools, which aren't yet generally available:\n\n- `run_code_analyzer` - (NON-GA) Performs a static analysis of your code using Salesforce Code Analyzer. Includes validating that the code conforms to best practices, checking for security vulnerabilities, and identifying possible performance issues.\n- `describe_code_analyzer_rule` - (NON-GA) Gets the description of a Salesforce Code Analyzer rule, including the engine it belongs to, its severity, and associated tags.\n\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n## Configure Other Clients to Use the Salesforce DX MCP Server\n\n**Cursor**\n\nTo configure [Cursor](https://www.cursor.com/) to work with Salesforce DX MCP Server, add this snippet to your Cursor `mcp.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"Salesforce DX\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"all\"]\n    }\n  }\n}\n```\n\n**Cline**\n\nTo configure [Cline](https://cline.bot), add this snippet to your Cline `cline_mcp_settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"Salesforce DX\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"all\"]\n    }\n  }\n}\n```\n\n**Other Clients**\n\nFor these other clients, refer to their documentation for adding MCP servers and follow the same pattern as in the preceding VS Code and Cursor JSON snippets:\n\n- [Claude Desktop](https://claude.ai/download)\n- [Zed](https://github.com/zed-industries/zed)\n- [Windsurf](https://www.windsurf.com/)\n- [Trae](https://trae.ai)\n\n### Installing via Smithery\n\nTo install salesforce-mcp-minimal automatically via [Smithery](https://smithery.ai/server/@kodey-ai/salesforce-mcp-minimal):\n\n```bash\nnpx -y @smithery/cli install @kodey-ai/salesforce-mcp-minimal\n```\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Interact directly with Salesforce orgs through LLM-driven tools",
        "Read, manage, and operate Salesforce resources securely",
        "Access Salesforce orgs using TypeScript libraries without shelling out to CLI",
        "Enforce granular access control with org allowlisting",
        "Enable modular tool architecture for extensibility",
        "Execute Salesforce DX development tasks via natural language in VS Code",
        "List authorized orgs and view org records",
        "Deploy and retrieve metadata in Salesforce orgs",
        "Analyze Apex code for performance and security issues"
      ],
      "limitations": [
        "Available only as a developer preview and not generally available",
        "Commands, parameters, and features are subject to change or deprecation without notice",
        "Requires pre-authorized Salesforce orgs on the local machine",
        "Dynamic tool discovery is experimental and may not work in all environments",
        "Does not expose secrets or tokens in plain text or via MCP tools"
      ],
      "requirements": [
        "Salesforce CLI installed on the local machine",
        "Visual Studio Code installed for client usage",
        "At least one authorized Salesforce org configured locally",
        "Salesforce DX project set up and opened in VS Code",
        "Use of MCP-compatible clients such as VS Code with GitHub Copilot or Agentforce for Developers",
        "Configuration of MCP server with required flags including --orgs"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed configuration options, security features, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# mcp\n\nMCP Server for Interacting with Salesforce Orgs\n\n[![NPM](https://img.shields.io/npm/v/@salesforce/mcp.svg?label=@salesforce/mcp)](https://www.npmjs.com/package/@salesforce/mcp) [![License](https://img.shields.io/badge/License-Apache--2.0-blue.svg)](https://opensource.org/license/apache-2-0)\n[![smithery badge](https://smithery.ai/badge/@kodey-ai/salesforce-mcp-kodey)](https://smithery.ai/server/@kodey-ai/salesforce-mcp-kodey)\n\n## Feedback\n\n Report bugs and issues [here](https://github.com/forcedotcom/mcp/issues).  \nFor feature requests and other related topics, start a Discussion [here](https://github.com/forcedotcom/mcp/discussions).  \n\n## Overview of the Salesforce DX MCP Server (Developer Preview)\n\nThe Salesforce DX MCP Server is a specialized Model Context Protocol (MCP) implementation designed to facilitate seamless interaction between large language models (LLMs) and Salesforce orgs. This MCP server provides a robust set of tools and capabilities that enable LLMs to read, manage, and operate Salesforce resources securely.\n\nKey Features:\n\n- Direct interaction with Salesforce orgs through LLM-driven tools.\n- Secure access using TypeScript libraries (not shelling out to the `sf` Salesforce CLI).\n- Improved security by avoiding the exposure of secrets in plain text.\n- Granular access control with org allowlisting.\n- Modular tool architecture for easy extensibility.\n\n**NOTE**: The Salesforce DX MCP Server is available as a developer preview. The feature isn‚Äôt generally available unless or until Salesforce announces its general availability in documentation or in press releases or public statements. All commands, parameters, and other features are subject to change or deprecation at any time, with or without notice. Don't implement functionality developed with these commands or tools. As we continue to enhance and refine the implementation, the available functionality and tools may evolve. We welcome feedback and contributions to help shape the future of this project.",
        "start_pos": 0,
        "end_pos": 2008,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 1,
        "text": "nds or tools. As we continue to enhance and refine the implementation, the available functionality and tools may evolve. We welcome feedback and contributions to help shape the future of this project.\n\n### Security Features\n\nThe Salesforce DX MCP Server was designed with security as a top priority.\n\n- **Uses TypeScript libraries directly**\n\n  - Greatly decreases the size of the MCP Server.\n  - Significantly reduces the risk of remote code execution (RCE).\n\n- **No secrets needed in configuration**\n\n  - Eliminates the risk of plain text secret exposure.\n  - Accesses pre-existing (encrypted) auth files on the user's machine.\n  - Implements allowlisting for auth info key/values to prevent sensitive data exposure.\n\n- **No secrets exposed via MCP tools**\n\n  - Prevents other tools from accessing unencrypted tokens.\n  - Tools pass usernames around instead of tokens.\n\n- **Granular access control**\n\n  - MCP Server can access auth info for only orgs that have been explicitly allowlisted.\n  - Users specify allowed orgs when starting the server.\n\n### Agentforce for Developers Includes the Salesforce DX MCP Server\n\nAgentforce for Developers is an AI-powered Salesforce developer tool that's available as an easy-to-install Visual Studio Code (VS Code) extension. It includes Dev Agent (developer preview), an intelligent coding partner that provides information and, most importantly, can take action. Through agentic chat powered by MCP, Dev Agent can execute commands and perform complex workflows automatically, right from within VS Code.\n\nThe Salesforce DX MCP server is pre-configured in Agentforce for Developers, so you can start using the DX MCP tools immediately after you install the extension in VS Code. \n\nSee [Set Up Agentforce for Developers](https://developer.salesforce.com/docs/platform/einstein-for-devs/guide/einstein-setup.html) and [Chat with Dev Agent (Dev Preview)](https://developer.salesforce.com/docs/platform/einstein-for-devs/guide/devagent-overview.html) for more information.",
        "start_pos": 1808,
        "end_pos": 3817,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 2,
        "text": "instein-for-devs/guide/einstein-setup.html) and [Chat with Dev Agent (Dev Preview)](https://developer.salesforce.com/docs/platform/einstein-for-devs/guide/devagent-overview.html) for more information. \n\n## Get Started Using VS Code as the Client\n\nWant to jump in and see what all the fuss is about? Read on!\n\nThis example uses Visual Studio Code (VS Code) as the MCP client. After you configure it with the Salesforce DX MCP Server, you then use GitHub Copilot and natural language to easily execute typical Salesforce DX development tasks, such as listing your authorized orgs, viewing org records, and deploying or retrieving metadata.\n\nBut you're not limited to using only VS Code and Copilot as the MCP client! As already mentioned, you can use [Agentforce for Developers](README.md#agentforce-for-developers-includes-the-salesforce-dx-mcp-server), which is pre-configured with the DX MCP server.  Or you can [configure many other clients](README.md#configure-other-clients-to-use-the-salesforce-dx-mcp-server) to use the Salesforce DX MCP Server, such as Cursor, Cline, Claude Desktop, Zed, Windsurf, and more. \n\n**Before You Begin**\n\nFor the best getting-started experience, make sure that you have a Salesforce DX environment set up on your computer. In particular:\n\n- [Install Salesforce CLI](https://developer.salesforce.com/docs/atlas.en-us.sfdx_setup.meta/sfdx_setup/sfdx_setup_intro.htm) on your computer.\n- [Install VS Code](https://code.visualstudio.com/docs) on your computer.\n- [Create a Salesforce DX project](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_ws_create_new.htm) and open it in VS Code. You can also clone an example repo, such as [dreamhouse-lwc](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_intro_sample_repo.htm), which is a ready-to-use DX project that contains a simple Salesforce application, with metadata and test data.",
        "start_pos": 3617,
        "end_pos": 5546,
        "token_count_estimate": 482,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 3,
        "text": "r.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_intro_sample_repo.htm), which is a ready-to-use DX project that contains a simple Salesforce application, with metadata and test data.\n- [Authorize at least one Salesforce org](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_web_flow.htm) to use with your DX project. You can also create a scratch org.\n\n**Let's Do It**\n\n1. Create a `.vscode/mcp.json` file at the root of your DX project and add this JSON:\n\n   ```json\n   {\n     \"servers\": {\n       \"Salesforce DX\": {\n         \"type\": \"stdio\",\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"all\"]\n       }\n     }\n   }\n   ```\n\n   You can also configure the MCP server globally by editing the VS Code [settings.json](https://code.visualstudio.com/docs/configure/settings#_settings-file-locations) file and adding a similar JSON snippet but contained in an `mcp:servers` section.\n\n   The `--orgs` flag is required and specifies the authorized orgs you're allowing the MCP server to access. The `--toolsets` and `--tools` flags are used to specify which tools the server will start with. See [Configure the DX MCP Server](README.md#configure-the-dx-mcp-server) for the available values for these flags.\n\n1. Open VS Code, go to **View -> Command Palette** and enter **MCP: List Servers**.\n\n   TIP: You can also get to the command palette by pressing press Ctrl+Shift+P (Windows or Linux) or Command-Shift-P (macOS).\n\n1. Click `Salesforce DX`, then **Start Server**.\n\n   Check the Output tab for the server status.\n\n1. Run **Chat: Open Chat (Agent)** from the command palette to start a new GitHub Copilot chat session.\n\n   Be sure your Copilot chat window is in `Agent` mode; if you're in `Ask` or `Edit` mode, use the [little drop-down](https://github.blog/ai-and-ml/github-copilot/copilot-ask-edit-and-agent-modes-what-they-do-and-when-to-use-them/) to switch.\n\n1.",
        "start_pos": 5346,
        "end_pos": 7332,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 4,
        "text": "nt` mode; if you're in `Ask` or `Edit` mode, use the [little drop-down](https://github.blog/ai-and-ml/github-copilot/copilot-ask-edit-and-agent-modes-what-they-do-and-when-to-use-them/) to switch.\n\n1. In the GitHub Copilot chat window, use natural language to explain what you want to do. The MCP server determines which configured tool to use, and then shows it to you along with other information. Click **Continue** to run the tool and see the results of your request.\n\n   Try out these examples:\n\n   - List all my orgs.\n   - Which are my active scratch orgs?\n   - Show me all the accounts in the org with alias my-org.\n   - Deploy everything in my project to the org with alias my-org.\n   - Do you see any performance or security issues with the Apex code in the `MyApexClass.cls` file?\n   - I see that my Apex code violates the pmd:ApexCRUDViolation rule; can you give me more information about this rule?\n\n1. To stop, restart, or view the MCP server configuration, run the **MCP: List Servers** command, click `Salesforce DX`, then click the appropriate option.\n\n### Installing via Smithery\n\nTo install salesforce-mcp-kodey automatically via [Smithery](https://smithery.ai/server/@kodey-ai/salesforce-mcp-kodey):\n\n```bash\nnpx -y @smithery/cli install @kodey-ai/salesforce-mcp-kodey\n```\n\n## Configure the DX MCP Server\n\nConfigure the Salesforce DX MCP Server by passing flags to the `args` option. Surround the flag name and its value each in double quotes, and separate all flags and values with commas. Some flags are Boolean and don't take a value.",
        "start_pos": 7132,
        "end_pos": 8688,
        "token_count_estimate": 389,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 5,
        "text": "ULT_TARGET_ORG\", \"--toolsets\", \"all\", \"--allow-non-ga-tools\"]\n       }\n     }\n```\nThe `\"-y\", \"@salesforce/mcp\"` part tells `npx` to automatically install the `@salesforce/mcp` package instead of asking permission. Don't change this. \n\nThese are the available flags that you can pass to the `args` option. \n\n| Flag Name | Description | Required? |Notes |\n| -----------------| -------| ------- | ----- |\n| `--orgs` | One or more orgs that you've locally authorized. | Yes | You must specify at least one org. <br/> <br/>See [Configure Orgs](README.md#configure-orgs) for the values you can pass to this flag. |\n| `--toolsets` | Sets of tools, based on functionality, that you want to enable. | No | Set to \"all\" to enable every tool in every toolset. <br/> <br/>See [Configure Toolsets](README.md#configure-toolsets) for the values you can pass to this flag.|\n| `--tools` | Individual tool names that you want to enable. | No | You can use this flag in combination with the `--toolsets` flag. For example, you can enable all tools in one toolset, and just one tool in a different toolset. |\n| `--no-telemetry` | Boolean flag to disable telemetry, the automatic collection of data for monitoring and analysis. | No | Telemetry is enabled by default, so specify this flag to disable it.  |\n| `--debug` | Boolean flag that requests that the DX MCP Server print debug logs. | No | Debug mode is disabled by default. <br/> <br/>**NOTE:** Not all MCP clients expose MCP logs, so this flag might not work for all IDEs. |\n| `--allow-non-ga-tools` | Boolean flag to allow the DX MCP Server to use both the generally available (GA) and NON-GA tools that are in the toolsets or tools you specify. | No | By default, the DX MCP server uses only the tools marked GA. |\n| `--dynamic-tools` | (experimental) Boolean flag that enables dynamic tool discovery and loading. When specified, the DX MCP server starts with a minimal set of core tools and loads new tools as needed.",
        "start_pos": 8980,
        "end_pos": 10937,
        "token_count_estimate": 489,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 6,
        "text": "--dynamic-tools` | (experimental) Boolean flag that enables dynamic tool discovery and loading. When specified, the DX MCP server starts with a minimal set of core tools and loads new tools as needed. | No| This flag is useful for reducing the initial context size and improving LLM performance. Dynamic tool discovery is disabled by default.<br/> <br/>**NOTE:** This feature works in VSCode and Cline but may not work in other environments.|\n\n### Configure Orgs\n\nThe Salesforce MCP tools require an org, and so you must include the required `--orgs` flag to specify at least one authorized org when you configure the MCP server. Separate multiple values with commas.\n\nYou must explicitly [authorize the orgs](https://developer.salesforce.com/docs/atlas.en-us.sfdx_dev.meta/sfdx_dev/sfdx_dev_auth_web_flow.htm) on your computer before the MCP server can access them. Use the `org login web` Salesforce CLI command or the VS Code **SFDX: Authorize an Org** command from the command palette.\n\nThese are the available values for the `--orgs` flag:\n\n- `DEFAULT_TARGET_ORG` - Allow access to your default org. If you've set a local default org in your DX project, the MCP server uses it. If not, the server uses a globally-set default org.\n- `DEFAULT_TARGET_DEV_HUB` - Allow access to your default Dev Hub org. If you've set a local default Dev Hub org in your DX project, the MCP server uses it. If not, the server uses a globally-set default Dev Hub org.\n- `ALLOW_ALL_ORGS` - Allow access to all authorized orgs. Use this value with caution.\n- `<username or alias>` - Allow access to a specific org by specifying its username or alias.",
        "start_pos": 10737,
        "end_pos": 12369,
        "token_count_estimate": 408,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 7,
        "text": "\"command\": \"npx\",\n           \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\"]\n         }\n       }\n```\n\nThis sample snippet shows how to configure access to your default Dev Hub org and an org with username `test-org@example.com`:\n\n```json\n           \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_DEV_HUB,test-org@example.com\"]\n```\n\nThis sample snippet shows how to configure access to two orgs for which you specified aliases when you authorized them:\n\n```json\n           \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"my-scratch-org,my-dev-hub\"]\n```\n\n### Configure Toolsets\n\nThe Salesforce DX MCP Server supports **toolsets** - a way to selectively enable different groups of MCP tools based on your needs. This allows you to run the MCP server with only the tools you require, which in turn reduces the context.\n\nUse the `--toolsets` flag to specify the toolsets when you configure the Salesforce DX MCP Server. Separate multiple toolsets with commas. Set `--toolsets` to `all` to register every available toolset.\n\nThese are the available toolsets.\n\n| Toolset| Description|\n| ----- | ----- |\n| `all` | Enables all available tools from all toolsets.",
        "start_pos": 12585,
        "end_pos": 13776,
        "token_count_estimate": 295,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 8,
        "text": "mobile development and capabilities.](README.md#mobile-toolset)|\n| `mobile-core` | [A subset of mobile tools focused on essential mobile capabilities.](README.md#mobile-core-toolset)|\n| `aura-experts` | [Tools which provides Aura component analysis, blueprinting, and migration expertise.](README.md#aura-experts-toolset)|\n| `lwc-experts`  | [Tools to assist with LWC development, testing, optimization, and best practices.](README.md#lwc-experts-toolset)|\n\nThis example shows how to enable the `data`, `orgs`, `metadata`, and `other` toolsets when configuring the MCP server for VS Code:\n\n```json\n       \"servers\": {\n         \"Salesforce DX\": {\n           \"type\": \"stdio\",\n           \"command\": \"npx\",\n           \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"data,orgs,metadata,other\"]\n         }\n       }\n```\n\n### Configure Tools\n\nThe Salesforce DX MCP Server also supports registering individual **tools**. This can be used in combination with **toolsets** to further fine-tune registered tools.\n\nUse the `--tools` flag to enable specific tools when you configure the Salesforce DX MCP Server. Separate multiple tools with commas. The `--tools` flag is optional.\n\nThis example shows how to enable the `orgs` and `metadata` toolsets, as well as the `run_soql_query` and `run_apex_test` tools when configuring the MCP server for VS Code:\n\n```json\n       \"servers\": {\n         \"Salesforce DX\": {\n           \"type\": \"stdio\",\n           \"command\": \"npx\",\n           \"args\": [\n              \"-y\",\n              \"@salesforce/mcp\", \n              \"--orgs\", \n              \"DEFAULT_TARGET_ORG\", \n              \"--toolsets\",\n              \"orgs,metadata\"\n              \"--tools\",\n              \"run_soql_query,run_apex_test\"\n            ]\n         }\n       }\n```\n\n\n#### Core Toolset (always enabled)\n\nIncludes these tools:\n\n- `get_username` - Determines the appropriate username or alias for Salesforce operations, handling both default orgs and Dev Hubs.",
        "start_pos": 14433,
        "end_pos": 16417,
        "token_count_estimate": 495,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 9,
        "text": "}\n```\n\n\n#### Core Toolset (always enabled)\n\nIncludes these tools:\n\n- `get_username` - Determines the appropriate username or alias for Salesforce operations, handling both default orgs and Dev Hubs.\n- `resume_tool_operation` - Resumes a long-running operation that wasn't completed by another tool.\n\n#### Orgs Toolset\n\nIncludes these tools:\n\n- `list_all_orgs` - Lists all configured Salesforce orgs, with optional connection status checking.\n- `create_org_snapshot` - (NON-GA) Create a scratch org snapshot. \n- `create_scratch_org` - (NON-GA) Create a scratch org. \n- `delete_org` - (NON-GA) Delete a locally-authorized Salesforce scratch org or sandbox.\n- `org_open` - (NON-GA) Open an org in a browser. \n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Data Toolset\n\nIncludes this tool:\n\n- `run_soql_query` - Runs a SOQL query against a Salesforce org.\n\n#### Users Toolset\n\nIncludes this tool:\n\n- `assign_permission_set` - Assigns a permission set to the user or on behalf of another user.\n\n#### Metadata Toolset\n\nIncludes these tools:\n\n- `deploy_metadata` - Deploys metadata from your DX project to an org.\n- `retrieve_metadata` - Retrieves metadata from your org to your DX project.\n\n#### Testing Toolset\n\nIncludes these tools:\n\n- `run_agent_test` - Executes agent tests in your org.\n- `run_apex_test` - Executes apex tests in your org.\n\n#### Mobile Toolset\n\nIncludes these tools, which aren't yet generally available:\n\n- `create_mobile_lwc_app_review` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC App Review Service, offering expert guidance for implementing app review features in Lightning Web Components.\n- `create_mobile_lwc_ar_space_capture` - (NON-GA) Provides TypeScript API documentation for Salesforce L    WC AR Space Capture, offering expert guidance for implementing AR space capture features in Lightning Web Components.",
        "start_pos": 16217,
        "end_pos": 18159,
        "token_count_estimate": 485,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 10,
        "text": "pace_capture` - (NON-GA) Provides TypeScript API documentation for Salesforce L    WC AR Space Capture, offering expert guidance for implementing AR space capture features in Lightning Web Components.\n- `create_mobile_lwc_barcode_scanner` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Barcode Scanner, offering expert guidance for implementing barcode scanning features in Lightning Web Components.\n- `create_mobile_lwc_biometrics` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Biometrics Service, offering expert guidance for implementing biometric authentication features in Lightning Web Components.\n- `create_mobile_lwc_calendar` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Calendar Service, offering expert guidance for implementing calendar integration features in Lightning Web Components.\n- `create_mobile_lwc_contacts` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Contacts Service, offering expert guidance for implementing contacts management features in Lightning Web Components.\n- `create_mobile_lwc_document_scanner` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Document Scanner, offering expert guidance for implementing document scanning features in Lightning Web Components.\n- `create_mobile_lwc_geofencing` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Geofencing Service, offering expert guidance for implementing geofencing features in Lightning Web Components.\n- `create_mobile_lwc_location` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Location Service, offering expert guidance for implementing location services in Lightning Web Components.\n- `create_mobile_lwc_nfc` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC NFC Service, offering expert guidance for implementing NFC features in Lightning Web Components.",
        "start_pos": 17959,
        "end_pos": 19872,
        "token_count_estimate": 478,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 11,
        "text": "mponents.\n- `create_mobile_lwc_nfc` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC NFC Service, offering expert guidance for implementing NFC features in Lightning Web Components.\n- `create_mobile_lwc_payments` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Payments Service, offering expert guidance for implementing payment processing features in Lightning Web Components.\n- `get_mobile_lwc_offline_analysis` - (NON-GA) Analyzes Lightning Web Components for mobile-specific issues and provides detailed recommendations for mobile offline compatibility and performance improvements.\n- `get_mobile_lwc_offline_guidance` - (NON-GA) Provides structured review instructions to detect and remediate mobile offline code violations in Lightning Web Components for Salesforce Mobile Apps.\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Mobile-core Toolset\n\nIncludes these essential mobile tools, which aren't yet generally available:\n\n- `create_mobile_lwc_barcode_scanner` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Barcode Scanner, offering expert guidance for implementing barcode scanning features in Lightning Web Components.\n- `create_mobile_lwc_biometrics` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Biometrics Service, offering expert guidance for implementing biometric authentication features in Lightning Web Components.\n- `create_mobile_lwc_location` - (NON-GA) Provides TypeScript API documentation for Salesforce LWC Location Service, offering expert guidance for implementing location services in Lightning Web Components.\n- `get_mobile_lwc_offline_analysis` - (NON-GA) Analyzes Lightning Web Components for mobile-specific issues and provides detailed recommendations for mobile offline compatibility and performance improvements.",
        "start_pos": 19672,
        "end_pos": 21583,
        "token_count_estimate": 477,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 12,
        "text": "obile_lwc_offline_analysis` - (NON-GA) Analyzes Lightning Web Components for mobile-specific issues and provides detailed recommendations for mobile offline compatibility and performance improvements.\n- `get_mobile_lwc_offline_guidance` - (NON-GA) Provides structured review instructions to detect and remediate mobile offline code violations in Lightning Web Components for Salesforce Mobile Apps.\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Aura Experts Toolset\n\n - `create_aura_blueprint_draft` - (GA)\nCreates a comprehensive Product Requirements Document (PRD) blueprint for Aura component migration. Analyzes Aura component files and generates framework-agnostic specifications suitable for LWC migration, including business requirements, technical patterns, and migration guidelines.\n\n - `enhance_aura_blueprint_draft` - (GA)\nEnhances an existing draft PRD with expert analysis and unknown resolution. Takes a draft blueprint and applies specialized Aura expert knowledge to resolve dependencies, add technical insights, and improve the migration specifications for better LWC implementation guidance.\n\n - `transition_prd_to_lwc` - (GA)\nProvides migration bridge guidance for creating LWC components from Aura specifications. Takes the enhanced PRD and generates specific implementation guidance, platform service mappings, and step-by-step instructions for building the equivalent LWC component.\n\n - `orchestrate_aura_migration` - (GA)\nOrchestrates the complete Aura to LWC migration workflow. Provides end-to-end guidance for the entire migration process, from initial analysis through final implementation, including best practices, tooling recommendations, and quality assurance steps.\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them.",
        "start_pos": 21383,
        "end_pos": 23283,
        "token_count_estimate": 475,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 13,
        "text": "including best practices, tooling recommendations, and quality assurance steps.\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them.",
        "start_pos": 23083,
        "end_pos": 23283,
        "token_count_estimate": 50,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 14,
        "text": "upgrading to SLDS2 design system\n\n##### LDS (Lightning Design System) Tools\n - `explore_lds_uiapi` - (GA) Explores and documents Lightning Design System UI API capabilities\n - `guide_lds_data_consistency` - (GA) Data consistency patterns and best practices for LDS components\n - `guide_lds_development` - (GA) LDS development guidelines and component integration\n - `guide_lds_referential_integrity` - (GA) Referential integrity patterns for LDS data management\n\n##### Migration & Integration Tools\n - `verify_aura_migration_completeness` - (GA) Aura to LWC migration completeness checklist and validation\n - `guide_figma_to_lwc_conversion` - (GA) Converts Figma designs to LWC component specifications\n - `run_lwc_accessibility_jest_tests` - (GA) Accessibility testing utilities and Jest integration for LWC components\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them. \n\n#### Code-Analyzer Toolset\n\nIncludes these tools, which aren't yet generally available:\n\n- `run_code_analyzer` - (NON-GA) Performs a static analysis of your code using Salesforce Code Analyzer. Includes validating that the code conforms to best practices, checking for security vulnerabilities, and identifying possible performance issues.\n- `describe_code_analyzer_rule` - (NON-GA) Gets the description of a Salesforce Code Analyzer rule, including the engine it belongs to, its severity, and associated tags.\n\n\n**NOTE:** The tools marked NON-GA are not yet generally available, specify the `--allow-non-ga-tools` flag to use them.",
        "start_pos": 24931,
        "end_pos": 26506,
        "token_count_estimate": 393,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      },
      {
        "chunk_id": 15,
        "text": "command\": \"npx\",\n      \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"all\"]\n    }\n  }\n}\n```\n\n**Cline**\n\nTo configure [Cline](https://cline.bot), add this snippet to your Cline `cline_mcp_settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"Salesforce DX\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@salesforce/mcp\", \"--orgs\", \"DEFAULT_TARGET_ORG\", \"--toolsets\", \"all\"]\n    }\n  }\n}\n```\n\n**Other Clients**\n\nFor these other clients, refer to their documentation for adding MCP servers and follow the same pattern as in the preceding VS Code and Cursor JSON snippets:\n\n- [Claude Desktop](https://claude.ai/download)\n- [Zed](https://github.com/zed-industries/zed)\n- [Windsurf](https://www.windsurf.com/)\n- [Trae](https://trae.ai)\n\n### Installing via Smithery\n\nTo install salesforce-mcp-minimal automatically via [Smithery](https://smithery.ai/server/@kodey-ai/salesforce-mcp-minimal):\n\n```bash\nnpx -y @smithery/cli install @kodey-ai/salesforce-mcp-minimal\n```",
        "start_pos": 26779,
        "end_pos": 27776,
        "token_count_estimate": 249,
        "source_type": "readme",
        "agent_id": "2cbac277fccaaa81"
      }
    ]
  },
  {
    "agent_id": "45b4f68ca15c9121",
    "name": "ai.smithery/kwp-lab-rss-reader-mcp",
    "source": "mcp",
    "source_url": "https://github.com/kwp-lab/rss-reader-mcp",
    "description": "Track and browse RSS feeds with ease. Fetch the latest entries from any feed URL and extract full‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T17:02:25.896522Z",
    "indexed_at": "2026-02-18T04:07:36.981205",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# RSS Reader MCP\n\nAn MCP (Model Context Protocol) server for RSS feed aggregation and article content extraction. You can use it to subscribe to RSS feeds and get article lists, or extract the full content of an article from a URL and format it as Markdown.\n\nEnglish | [‰∏≠Êñá](./README_zh.md)\n\n[![npm version](https://img.shields.io/npm/v/rss-reader-mcp.svg)](https://www.npmjs.com/package/rss-reader-mcp)\n[![license](https://img.shields.io/github/license/kwp-lab/rss-reader-mcp.svg)](LICENSE)\n[![build status](https://img.shields.io/github/actions/workflow/status/kwp-lab/rss-reader-mcp/publish.yml?branch=main)](https://github.com/kwp-lab/rss-reader-mcp/actions/workflows/publish.yml)\n[![smithery badge](https://smithery.ai/badge/@kwp-lab/rss-reader-mcp)](https://smithery.ai/server/@kwp-lab/rss-reader-mcp)\n\n## üöÄ Quick Start\n\nYou can use this MCP server in MCP-capable clients such as [Claude Desktop](https://claude.ai/download) and [CherryStudio](https://www.cherry-ai.com/).\n\n### Claude Desktop\n\nFor Claude Desktop, add the following configuration under the \"mcpServers\" section in your `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"rss-reader\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"rss-reader-mcp\"\n      ]\n    }\n  }\n}\n```\n\n### Usage Examples\n\n- Basic RSS feed fetching\n\n  > Can you fetch the latest 5 headlines from the BBC News RSS feed?\n  > URL: <https://feeds.bbci.co.uk/news/rss.xml>\n\n- Full article content extraction\n  > Please extract the full content of this article and format it as Markdown:\n  > <https://example.com/news/article-title>\n\n## üîß Tools Reference\n\n### `fetch_feed_entries`\n\nFetch RSS entries from a specified URL\n\n**Parameters:**\n\n- `url` (required string): RSS feed URL\n- `limit` (optional number): Maximum number of entries to return (default 10, max 100)\n\n**Returns:** A JSON object containing feed metadata and a list of entries (including title, link, publication date, and summary)\n\n### `fetch_article_content`\n\nExtract article content from a URL and format it as Markdown\n\n**Parameters:**\n\n- `url` (required string): Article URL\n\n**Returns:** A JSON object containing the title, Markdown content, source URL, and timestamp\n\n## ‚öôÔ∏è Transport & Environment Variables\n\nThis server supports two transport modes:\n\n- stdio (default): Communicates via standard input/output. Suitable for clients that run a local process, such as Claude Desktop.\n- httpStream: Communicates over HTTP streaming. Suitable for clients that support HTTP(S) transport or for containerized deployments.\n\nAvailable environment variables:\n\n- TRANSPORT: Select the transport mode, either `stdio` (default) or `httpStream`.\n- PORT: When `TRANSPORT=httpStream`, the listening port (default `8081`).\n- MCP_SERVER_HOST: When `TRANSPORT=httpStream`, the listening address (default `localhost`). In Docker, set this to `0.0.0.0` to expose the port externally.\n\nHow to switch transport modes:\n\n- Using stdio (no extra setup, default):\n  - Works with Claude Desktop via the `command + args` configuration (see example above).\n- Using httpStream:\n  - Set the environment variable `TRANSPORT=httpStream` and specify `PORT` (defaults to 8081 if not set).\n  - When running in a container, also set `MCP_SERVER_HOST=0.0.0.0` and map the port.\n  - The Dockerfile in this repository already includes related environment variable settings.\n\n## Docker Deployment\n\nYou can also run this MCP server in a Docker container. First, build the image in the project root:\n\n```bash\ndocker build -t rss-reader-mcp .\n```\n\nUsing CherryStudio as an example, the following configuration shows how to run this server over HTTP:\n\n```json\n{\n  \"mcpServers\": {\n    \"rss-reader-mcp\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-p\",\n        \"8081:8081\",\n        \"-e\",\n        \"PORT=8081\",\n        \"rss-reader-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Some RSS Feeds for Testing\n\n- **BBC News:** `https://feeds.bbci.co.uk/news/rss.xml`\n- **TechCrunch:** `https://techcrunch.com/feed/`\n- **Hacker News:** `https://hnrss.org/frontpage`\n- **MIT Technology Review:** `https://www.technologyreview.com/feed/`\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Fetch RSS feed entries from a specified URL with a configurable limit",
        "Extract full article content from a URL and format it as Markdown",
        "Operate using stdio or HTTP streaming transport modes",
        "Run as a local process or inside a Docker container",
        "Provide feed metadata and article summaries along with entries",
        "Support integration with MCP-capable clients like Claude Desktop and CherryStudio"
      ],
      "limitations": [
        "Maximum of 100 entries can be fetched per RSS feed request",
        "HTTP streaming mode requires environment variable configuration and port mapping",
        "No mention of authentication or handling private/protected RSS feeds",
        "No explicit rate limiting or concurrency details provided"
      ],
      "requirements": [
        "Node.js environment to run via npx",
        "For HTTP streaming mode, environment variables TRANSPORT, PORT, and MCP_SERVER_HOST must be set appropriately",
        "Docker installed if running via Docker container",
        "MCP-capable client to interact with the server (e.g., Claude Desktop, CherryStudio)"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation includes installation instructions, usage examples, detailed tool descriptions, environment variable configurations, Docker deployment guidance, and known limitations, providing comprehensive coverage.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# RSS Reader MCP\n\nAn MCP (Model Context Protocol) server for RSS feed aggregation and article content extraction. You can use it to subscribe to RSS feeds and get article lists, or extract the full content of an article from a URL and format it as Markdown.\n\nEnglish | [‰∏≠Êñá](./README_zh.md)\n\n[![npm version](https://img.shields.io/npm/v/rss-reader-mcp.svg)](https://www.npmjs.com/package/rss-reader-mcp)\n[![license](https://img.shields.io/github/license/kwp-lab/rss-reader-mcp.svg)](LICENSE)\n[![build status](https://img.shields.io/github/actions/workflow/status/kwp-lab/rss-reader-mcp/publish.yml?branch=main)](https://github.com/kwp-lab/rss-reader-mcp/actions/workflows/publish.yml)\n[![smithery badge](https://smithery.ai/badge/@kwp-lab/rss-reader-mcp)](https://smithery.ai/server/@kwp-lab/rss-reader-mcp)\n\n## üöÄ Quick Start\n\nYou can use this MCP server in MCP-capable clients such as [Claude Desktop](https://claude.ai/download) and [CherryStudio](https://www.cherry-ai.com/).\n\n### Claude Desktop\n\nFor Claude Desktop, add the following configuration under the \"mcpServers\" section in your `claude_desktop_config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"rss-reader\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"rss-reader-mcp\"\n      ]\n    }\n  }\n}\n```\n\n### Usage Examples\n\n- Basic RSS feed fetching\n\n  > Can you fetch the latest 5 headlines from the BBC News RSS feed?",
        "start_pos": 0,
        "end_pos": 1393,
        "token_count_estimate": 348,
        "source_type": "readme",
        "agent_id": "45b4f68ca15c9121"
      },
      {
        "chunk_id": 1,
        "text": "A JSON object containing feed metadata and a list of entries (including title, link, publication date, and summary)\n\n### `fetch_article_content`\n\nExtract article content from a URL and format it as Markdown\n\n**Parameters:**\n\n- `url` (required string): Article URL\n\n**Returns:** A JSON object containing the title, Markdown content, source URL, and timestamp\n\n## ‚öôÔ∏è Transport & Environment Variables\n\nThis server supports two transport modes:\n\n- stdio (default): Communicates via standard input/output. Suitable for clients that run a local process, such as Claude Desktop.\n- httpStream: Communicates over HTTP streaming. Suitable for clients that support HTTP(S) transport or for containerized deployments.\n\nAvailable environment variables:\n\n- TRANSPORT: Select the transport mode, either `stdio` (default) or `httpStream`.\n- PORT: When `TRANSPORT=httpStream`, the listening port (default `8081`).\n- MCP_SERVER_HOST: When `TRANSPORT=httpStream`, the listening address (default `localhost`). In Docker, set this to `0.0.0.0` to expose the port externally.\n\nHow to switch transport modes:\n\n- Using stdio (no extra setup, default):\n  - Works with Claude Desktop via the `command + args` configuration (see example above).\n- Using httpStream:\n  - Set the environment variable `TRANSPORT=httpStream` and specify `PORT` (defaults to 8081 if not set).\n  - When running in a container, also set `MCP_SERVER_HOST=0.0.0.0` and map the port.\n  - The Dockerfile in this repository already includes related environment variable settings.\n\n## Docker Deployment\n\nYou can also run this MCP server in a Docker container. First, build the image in the project root:\n\n```bash\ndocker build -t rss-reader-mcp .",
        "start_pos": 1848,
        "end_pos": 3538,
        "token_count_estimate": 422,
        "source_type": "readme",
        "agent_id": "45b4f68ca15c9121"
      },
      {
        "chunk_id": 2,
        "text": "{\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-p\",\n        \"8081:8081\",\n        \"-e\",\n        \"PORT=8081\",\n        \"rss-reader-mcp\"\n      ]\n    }\n  }\n}\n```\n\n## Some RSS Feeds for Testing\n\n- **BBC News:** `https://feeds.bbci.co.uk/news/rss.xml`\n- **TechCrunch:** `https://techcrunch.com/feed/`\n- **Hacker News:** `https://hnrss.org/frontpage`\n- **MIT Technology Review:** `https://www.technologyreview.com/feed/`",
        "start_pos": 3696,
        "end_pos": 4148,
        "token_count_estimate": 112,
        "source_type": "readme",
        "agent_id": "45b4f68ca15c9121"
      }
    ]
  },
  {
    "agent_id": "e8fa38a72eb5b5c2",
    "name": "ai.smithery/leandrogavidia-vechain-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/leandrogavidia/vechain-mcp-server",
    "description": "Search VeChain documentation, query on-chain data, and fetch fee suggestions with direct links to‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T18:35:25.040994Z",
    "indexed_at": "2026-02-18T04:07:38.426531",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<div align=\"center\">\n\n<img src=\"./docs/images/isotipo-bg.png\" alt=\"VeChain logo\" width=\"140\">\n\n<p></p>\n\n<h1>VeChain MCP Server</h1>\n\n<p>VeChain MCP Server is an MCP server specifically designed for the VeChain network. It provides advanced functionalities such as querying the official VeChain documentation, executing HTTP requests to the Thor REST API in both Mainnet and Testnet environments, and managing cryptographic signatures through an integrated wallet.</p>\n\n</div>\n\n## Integration\n\nRegister the server in your MCP-aware host configuration.\n\n```json\n{\n  \"mcpServers\": {\n    \"vechainMcp\": {\n      \"url\": \"https://server.smithery.ai/@leandrogavidia/vechain-mcp-server/mcp\",\n      \"type\": \"streamable-http\"\n    }\n  }\n}\n```\n\n## Tools\n\n- ### Vechain Docs \n\n    - **Docs**\n        \n        - `search_documentation`: Search VeChain Documentation.\n\n- ### Thorest API\n\n    - **Accounts**\n        \n        - `get_account`: Retrieve account details.\n\n    - **Transactions**\n        \n        - `get_transaction`: Retrieve a transaction by ID.\n        \n    - **Blocks**\n        \n        - `get_block`: Get a VeChain block.\n    \n    - **Fees**\n        \n        - `get_priority_fee`: Suggest a priority fee.\n\n- ### Wallet & signatures\n\n    - **Wallet**\n       \n        - `create_wallet`: Create a VeChain wallet (mnemonic + keys).\n    \n    - **Signatures**\n       \n        - `sign_certificate`: Create and sign a canonical certificate.\n       \n        - `sign_raw_transaction`: Sign raw transaction.\n\n- ### Goat SDK (VeChain Tools)\n\n    - `get_address`: Get the address of the wallet\n    \n    - `get_chain`: Get the chain of the wallet\n    \n    - `sign_message`: Sign a message with the wallet\n    \n    - `get_balance`: Get the balance of the wallet for native currency or a specific ERC20 token.\n    \n    - `get_token_info_by_ticker`: Get information about a configured token (like contract address and decimals) by its ticker symbol.\n    \n    - `convert_to_base_units`: Convert a token amount from human-readable units to its smallest unit (e.g., wei).\n    \n    - `convert_from_base_units`: Convert a token amount from its smallest unit (e.g., wei) to human-readable units.\n    \n    - `sign_typed_data_evm`: Sign an EIP-712 typed data structure (EVM)\n    \n    - `get_token_allowance_evm`: Get the allowance of an ERC20 token for a spender (returns amount in base units)\n    \n    - `send_token`: Send native currency or an ERC20 token to a recipient, in base units.\n    \n    - `approve_token_evm`: Approve an amount (specified in base units) of an ERC20 token for a spender\n\n    - `revoke_token_approval_evm`: Revoke approval for an ERC20 token from a spender (sets allowance to 0)\n\n\n---\n\n## .env Config\n\n- `AGENT_SECRET_KEY`: CSecret key in string format that allows your MCP server to use the signatures tools.\n\n- `ENVIRONMENT`: Working environment, either mainnet or test.\n\n- `USE_STREAMABLE_HTTP`: Specifies whether your MCP server will run on stdio or streamable-http.\n\n- `PORT`: Port where your MCP server will run when using streamable-http.\n\n- `HOST`: Host where your MCP server will run when using streamable-http.\n\n## Run the project locally\n\nIn one terminal window, run the following command: `pnpx @modelcontextprotocol/inspector pnpx tsx ./src/index.ts` in `stdio` mode.\n\n## Build and run\n\nRun the command: `pnpm run build` and then: `pnpm run start`\n\n## Deployment\n\nTo deploy this MCP server, fork this project into your GitHub account, log in to [smithery.ai](https://smithery.ai/), and click Publish server. Complete the steps, and once it is deployed, add the required environment variables in settings.\n\n---\n\n## License\n\nMIT"
    },
    "llm_extracted": {
      "capabilities": [
        "Query the official VeChain documentation",
        "Execute HTTP requests to the Thor REST API on Mainnet and Testnet",
        "Retrieve VeChain account details",
        "Retrieve VeChain transactions by ID",
        "Retrieve VeChain blocks",
        "Suggest priority fees for transactions",
        "Create VeChain wallets with mnemonic and keys",
        "Sign canonical certificates",
        "Sign raw transactions",
        "Get wallet address and chain information",
        "Sign messages and EIP-712 typed data structures",
        "Get wallet balance for native currency and ERC20 tokens",
        "Retrieve token information by ticker symbol",
        "Convert token amounts between human-readable and base units",
        "Get ERC20 token allowance for spenders",
        "Send native currency or ERC20 tokens",
        "Approve and revoke ERC20 token allowances"
      ],
      "limitations": [],
      "requirements": [
        "AGENT_SECRET_KEY environment variable for signature tools access",
        "ENVIRONMENT environment variable set to mainnet or test",
        "USE_STREAMABLE_HTTP environment variable to specify server mode",
        "PORT and HOST environment variables when using streamable-http mode",
        "MCP-aware host configuration to register the server"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, environment configuration, detailed tool descriptions, usage examples, and deployment guidance, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<div align=\"center\">\n\n<img src=\"./docs/images/isotipo-bg.png\" alt=\"VeChain logo\" width=\"140\">\n\n<p></p>\n\n<h1>VeChain MCP Server</h1>\n\n<p>VeChain MCP Server is an MCP server specifically designed for the VeChain network. It provides advanced functionalities such as querying the official VeChain documentation, executing HTTP requests to the Thor REST API in both Mainnet and Testnet environments, and managing cryptographic signatures through an integrated wallet.</p>\n\n</div>\n\n## Integration\n\nRegister the server in your MCP-aware host configuration.\n\n```json\n{\n  \"mcpServers\": {\n    \"vechainMcp\": {\n      \"url\": \"https://server.smithery.ai/@leandrogavidia/vechain-mcp-server/mcp\",\n      \"type\": \"streamable-http\"\n    }\n  }\n}\n```\n\n## Tools\n\n- ### Vechain Docs \n\n    - **Docs**\n        \n        - `search_documentation`: Search VeChain Documentation.\n\n- ### Thorest API\n\n    - **Accounts**\n        \n        - `get_account`: Retrieve account details.\n\n    - **Transactions**\n        \n        - `get_transaction`: Retrieve a transaction by ID.\n        \n    - **Blocks**\n        \n        - `get_block`: Get a VeChain block.\n    \n    - **Fees**\n        \n        - `get_priority_fee`: Suggest a priority fee.\n\n- ### Wallet & signatures\n\n    - **Wallet**\n       \n        - `create_wallet`: Create a VeChain wallet (mnemonic + keys).\n    \n    - **Signatures**\n       \n        - `sign_certificate`: Create and sign a canonical certificate.\n       \n        - `sign_raw_transaction`: Sign raw transaction.\n\n- ### Goat SDK (VeChain Tools)\n\n    - `get_address`: Get the address of the wallet\n    \n    - `get_chain`: Get the chain of the wallet\n    \n    - `sign_message`: Sign a message with the wallet\n    \n    - `get_balance`: Get the balance of the wallet for native currency or a specific ERC20 token.\n    \n    - `get_token_info_by_ticker`: Get information about a configured token (like contract address and decimals) by its ticker symbol.",
        "start_pos": 0,
        "end_pos": 1930,
        "token_count_estimate": 482,
        "source_type": "readme",
        "agent_id": "e8fa38a72eb5b5c2"
      },
      {
        "chunk_id": 1,
        "text": "of the wallet for native currency or a specific ERC20 token.\n    \n    - `get_token_info_by_ticker`: Get information about a configured token (like contract address and decimals) by its ticker symbol.\n    \n    - `convert_to_base_units`: Convert a token amount from human-readable units to its smallest unit (e.g., wei).\n    \n    - `convert_from_base_units`: Convert a token amount from its smallest unit (e.g., wei) to human-readable units.\n    \n    - `sign_typed_data_evm`: Sign an EIP-712 typed data structure (EVM)\n    \n    - `get_token_allowance_evm`: Get the allowance of an ERC20 token for a spender (returns amount in base units)\n    \n    - `send_token`: Send native currency or an ERC20 token to a recipient, in base units.\n    \n    - `approve_token_evm`: Approve an amount (specified in base units) of an ERC20 token for a spender\n\n    - `revoke_token_approval_evm`: Revoke approval for an ERC20 token from a spender (sets allowance to 0)\n\n\n---\n\n## .env Config\n\n- `AGENT_SECRET_KEY`: CSecret key in string format that allows your MCP server to use the signatures tools.\n\n- `ENVIRONMENT`: Working environment, either mainnet or test.\n\n- `USE_STREAMABLE_HTTP`: Specifies whether your MCP server will run on stdio or streamable-http.\n\n- `PORT`: Port where your MCP server will run when using streamable-http.\n\n- `HOST`: Host where your MCP server will run when using streamable-http.\n\n## Run the project locally\n\nIn one terminal window, run the following command: `pnpx @modelcontextprotocol/inspector pnpx tsx ./src/index.ts` in `stdio` mode.\n\n## Build and run\n\nRun the command: `pnpm run build` and then: `pnpm run start`\n\n## Deployment\n\nTo deploy this MCP server, fork this project into your GitHub account, log in to [smithery.ai](https://smithery.ai/), and click Publish server. Complete the steps, and once it is deployed, add the required environment variables in settings.\n\n---\n\n## License\n\nMIT",
        "start_pos": 1730,
        "end_pos": 3638,
        "token_count_estimate": 476,
        "source_type": "readme",
        "agent_id": "e8fa38a72eb5b5c2"
      },
      {
        "chunk_id": 2,
        "text": "account, log in to [smithery.ai](https://smithery.ai/), and click Publish server. Complete the steps, and once it is deployed, add the required environment variables in settings.\n\n---\n\n## License\n\nMIT",
        "start_pos": 3438,
        "end_pos": 3638,
        "token_count_estimate": 50,
        "source_type": "readme",
        "agent_id": "e8fa38a72eb5b5c2"
      }
    ]
  },
  {
    "agent_id": "9e779978318b5a93",
    "name": "ai.smithery/lineex-pubmed-mcp-smithery",
    "source": "mcp",
    "source_url": "https://github.com/lineex/pubmed-mcp-smithery",
    "description": "Search PubMed with precision using keyword and journal filters and smart sorting. Uncover MeSH ter‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-16T01:10:37.458959Z",
    "indexed_at": "2026-02-18T04:07:40.313523",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# PubMed Enhanced Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@leescot/pubmed-mcp-smithery)](https://smithery.ai/server/@leescot/pubmed-mcp-smithery)\n\nA Model Content Protocol server that provides enhanced tools to search and retrieve academic papers from PubMed database, with additional features such as MeSH term lookup, publication count statistics, and PICO-based evidence search.\n\n## Features\n\n- Search PubMed by keywords with optional journal filter\n- Support for sorting results by relevance or date (newest/oldest first)\n- Get MeSH (Medical Subject Headings) terms related to a search word\n- Get publication counts for multiple search terms (useful for comparing prevalence)\n- Retrieve detailed paper information including abstract, DOI, authors, and keywords\n- Perform structured PICO-based searches with support for synonyms and combination queries\n\n## Installing\n\n### Prerequisites\n\n- Python 3.6+\n- pip\n\n### Installation\n\n1. Clone this repository:\n\n   ```\n   git clone https://github.com/leescot/pubmed-mcp-smithery\n   cd pubmed-mcp-smithery\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests\n   ```\n\n## Usage\n\n### Running locally\n\nStart the server:\n\n```\npython pubmed_enhanced_mcp_server.py\n```\n\nFor development mode with auto-reloading:\n\n```\nmcp dev pubmed_enhanced_mcp_server.py\n```\n\n### Adding to Claude Desktop\n\nEdit your Claude Desktop configuration file (_CLAUDE_DIRECTORY/claude_desktop_config.json_) to add the server:\n\n```json\n\"pubmed-enhanced\": {\n    \"command\": \"python\",\n    \"args\": [\n        \"/path/pubmed-mcp-smithery/pubmed_enhanced_mcp_server.py\"\n    ]\n}\n```\n\n## MCP Functions\n\nThe server provides these main functions:\n\n1. `search_pubmed` - Search PubMed for articles matching keywords with optional journal filtering\n\n   ```python\n   # Example\n   results = await search_pubmed(\n       keywords=[\"diabetes\", \"insulin resistance\"],\n       journal=\"Nature Medicine\",\n       num_results=5,\n       sort_by=\"date_desc\"\n   )\n   ```\n\n2. `get_mesh_terms` - Look up MeSH terms related to a medical concept\n\n   ```python\n   # Example\n   mesh_terms = await get_mesh_terms(\"diabetes\")\n   ```\n\n3. `get_pubmed_count` - Get the count of publications for multiple search terms\n\n   ```python\n   # Example\n   counts = await get_pubmed_count([\"diabetes\", \"obesity\", \"hypertension\"])\n   ```\n\n4. `format_paper_details` - Get detailed information about specific papers by PMID\n\n   ```python\n   # Example\n   paper_details = await format_paper_details([\"12345678\", \"87654321\"])\n   ```\n\n5. `pico_search` - Perform structured PICO (Population, Intervention, Comparison, Outcome) searches with synonyms\n   ```python\n   # Example\n   pico_results = await pico_search(\n       p_terms=[\"diabetes\", \"type 2 diabetes\", \"T2DM\"],\n       i_terms=[\"metformin\", \"glucophage\"],\n       c_terms=[\"sulfonylurea\", \"glipizide\"],\n       o_terms=[\"HbA1c reduction\", \"glycemic control\"]\n   )\n   ```\n\n## PICO Search Functionality\n\nThe PICO search tool helps researchers conduct evidence-based literature searches by:\n\n1. Allowing multiple synonym terms for each PICO element\n2. Combining terms within each element using OR operators\n3. Performing AND combinations between elements (P AND I, P AND I AND C, etc.)\n4. Returning both search queries and publication counts for each combination\n\nThis approach helps refine research questions and identify the most relevant literature.\n\n## Rate Limiting\n\nThe server implements automatic retry mechanism with backoff delays to handle potential rate limiting by NCBI's E-utilities service.\n\n## License\n\nThis project is licensed under the BSD 3-Clause License - see the LICENSE file for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search PubMed for articles by keywords with optional journal filtering",
        "Sort PubMed search results by relevance or date (newest/oldest first)",
        "Retrieve MeSH (Medical Subject Headings) terms related to a medical concept",
        "Get publication counts for multiple search terms to compare prevalence",
        "Retrieve detailed paper information including abstract, DOI, authors, and keywords by PMID",
        "Perform structured PICO-based searches with support for synonyms and combination queries",
        "Combine PICO terms using OR within elements and AND between elements for refined searches",
        "Handle rate limiting from NCBI's E-utilities with automatic retry and backoff"
      ],
      "limitations": [
        "Dependent on NCBI's E-utilities service which may impose rate limits",
        "Requires Python 3.6+ environment to run",
        "No mention of support for full-text retrieval beyond abstracts",
        "No explicit support for authentication or API keys mentioned"
      ],
      "requirements": [
        "Python version 3.6 or higher",
        "pip package manager to install dependencies",
        "Installation of 'fastmcp' and 'requests' Python packages",
        "Access to NCBI's PubMed database via E-utilities (no API key required)"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for all main functions, clear descriptions of capabilities including PICO search, and notes on rate limiting, fulfilling all key documentation aspects.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# PubMed Enhanced Search MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@leescot/pubmed-mcp-smithery)](https://smithery.ai/server/@leescot/pubmed-mcp-smithery)\n\nA Model Content Protocol server that provides enhanced tools to search and retrieve academic papers from PubMed database, with additional features such as MeSH term lookup, publication count statistics, and PICO-based evidence search.\n\n## Features\n\n- Search PubMed by keywords with optional journal filter\n- Support for sorting results by relevance or date (newest/oldest first)\n- Get MeSH (Medical Subject Headings) terms related to a search word\n- Get publication counts for multiple search terms (useful for comparing prevalence)\n- Retrieve detailed paper information including abstract, DOI, authors, and keywords\n- Perform structured PICO-based searches with support for synonyms and combination queries\n\n## Installing\n\n### Prerequisites\n\n- Python 3.6+\n- pip\n\n### Installation\n\n1. Clone this repository:\n\n   ```\n   git clone https://github.com/leescot/pubmed-mcp-smithery\n   cd pubmed-mcp-smithery\n   ```\n\n2. Install dependencies:\n   ```\n   pip install fastmcp requests\n   ```\n\n## Usage\n\n### Running locally\n\nStart the server:\n\n```\npython pubmed_enhanced_mcp_server.py\n```\n\nFor development mode with auto-reloading:\n\n```\nmcp dev pubmed_enhanced_mcp_server.py\n```\n\n### Adding to Claude Desktop\n\nEdit your Claude Desktop configuration file (_CLAUDE_DIRECTORY/claude_desktop_config.json_) to add the server:\n\n```json\n\"pubmed-enhanced\": {\n    \"command\": \"python\",\n    \"args\": [\n        \"/path/pubmed-mcp-smithery/pubmed_enhanced_mcp_server.py\"\n    ]\n}\n```\n\n## MCP Functions\n\nThe server provides these main functions:\n\n1. `search_pubmed` - Search PubMed for articles matching keywords with optional journal filtering\n\n   ```python\n   # Example\n   results = await search_pubmed(\n       keywords=[\"diabetes\", \"insulin resistance\"],\n       journal=\"Nature Medicine\",\n       num_results=5,\n       sort_by=\"date_desc\"\n   )\n   ```\n\n2.",
        "start_pos": 0,
        "end_pos": 1999,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "9e779978318b5a93"
      },
      {
        "chunk_id": 1,
        "text": "on\n   # Example\n   results = await search_pubmed(\n       keywords=[\"diabetes\", \"insulin resistance\"],\n       journal=\"Nature Medicine\",\n       num_results=5,\n       sort_by=\"date_desc\"\n   )\n   ```\n\n2. `get_mesh_terms` - Look up MeSH terms related to a medical concept\n\n   ```python\n   # Example\n   mesh_terms = await get_mesh_terms(\"diabetes\")\n   ```\n\n3. `get_pubmed_count` - Get the count of publications for multiple search terms\n\n   ```python\n   # Example\n   counts = await get_pubmed_count([\"diabetes\", \"obesity\", \"hypertension\"])\n   ```\n\n4. `format_paper_details` - Get detailed information about specific papers by PMID\n\n   ```python\n   # Example\n   paper_details = await format_paper_details([\"12345678\", \"87654321\"])\n   ```\n\n5. `pico_search` - Perform structured PICO (Population, Intervention, Comparison, Outcome) searches with synonyms\n   ```python\n   # Example\n   pico_results = await pico_search(\n       p_terms=[\"diabetes\", \"type 2 diabetes\", \"T2DM\"],\n       i_terms=[\"metformin\", \"glucophage\"],\n       c_terms=[\"sulfonylurea\", \"glipizide\"],\n       o_terms=[\"HbA1c reduction\", \"glycemic control\"]\n   )\n   ```\n\n## PICO Search Functionality\n\nThe PICO search tool helps researchers conduct evidence-based literature searches by:\n\n1. Allowing multiple synonym terms for each PICO element\n2. Combining terms within each element using OR operators\n3. Performing AND combinations between elements (P AND I, P AND I AND C, etc.)\n4. Returning both search queries and publication counts for each combination\n\nThis approach helps refine research questions and identify the most relevant literature.\n\n## Rate Limiting\n\nThe server implements automatic retry mechanism with backoff delays to handle potential rate limiting by NCBI's E-utilities service.\n\n## License\n\nThis project is licensed under the BSD 3-Clause License - see the LICENSE file for details.",
        "start_pos": 1799,
        "end_pos": 3658,
        "token_count_estimate": 464,
        "source_type": "readme",
        "agent_id": "9e779978318b5a93"
      },
      {
        "chunk_id": 2,
        "text": "mechanism with backoff delays to handle potential rate limiting by NCBI's E-utilities service.\n\n## License\n\nThis project is licensed under the BSD 3-Clause License - see the LICENSE file for details.",
        "start_pos": 3458,
        "end_pos": 3658,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "9e779978318b5a93"
      }
    ]
  },
  {
    "agent_id": "85a0bc150e64558f",
    "name": "ai.smithery/lukaskostka99-marketing-miner-mcp",
    "source": "mcp",
    "source_url": "https://github.com/lukaskostka99/marketing-miner-mcp",
    "description": "Discover high-impact keyword ideas across Central and Eastern European and English markets. Analyz‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-16T19:53:18.718565Z",
    "indexed_at": "2026-02-18T04:07:41.657905",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Marketing Miner MCP\n\n> P≈ôipojte Marketing Miner k Cursor, Claude, Windsurf a dal≈°√≠m AI asistent≈Øm.\n\nMCP server pro anal√Ωzu kl√≠ƒçov√Ωch slov a hledanost pomoc√≠ Marketing Miner API.\n\n## Pou≈æit√≠ p≈ôes Smithery (Doporuƒçeno)\n\nNejjednodu≈°≈°√≠ zp≈Øsob pou≈æit√≠ je p≈ôes Smithery hosted URL:\n\n1. Jdƒõte na [smithery.ai](https://smithery.ai) \n2. Vyhledejte \"marketing-miner-mcp\"\n3. **Zadejte sv√© API kl√≠ƒçe** v Smithery Connect formul√°≈ôi\n4. Z√≠skejte remote hosted URL\n5. P≈ôidejte do sv√©ho MCP klienta\n\n### SHTTP (pro klienty s podporou):\n```json\n{\n  \"mcpServers\": {\n    \"marketing-miner\": {\n      \"url\": \"your-smithery-url.com\"\n    }\n  }\n}\n```\n\n### Pro klienty bez SHTTP podpory:\n```json\n{\n  \"mcpServers\": {\n    \"marketing-miner\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"your-smithery-url.com\"]\n    }\n  }\n}\n```\n\n## Lok√°ln√≠ spu≈°tƒõn√≠\n\n```bash\nnpm install\nnpm run build\nexport MARKETING_MINER_API_TOKEN=YOUR_TOKEN\nnpm start\n```\n\n## API Token\n\nZ√≠skejte sv≈Øj API token na [Marketing Miner API](https://www.marketingminer.com/cs/features/api)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Analyze keywords using Marketing Miner API",
        "Retrieve keyword search volume data",
        "Integrate Marketing Miner with AI assistants like Cursor, Claude, and Windsurf",
        "Provide MCP server interface for keyword analysis",
        "Support remote usage via Smithery hosted URL",
        "Allow local server deployment for direct API access"
      ],
      "limitations": [],
      "requirements": [
        "Marketing Miner API token",
        "Node.js environment for local deployment",
        "NPM for package installation and build",
        "Smithery account for hosted usage and API key input"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "Documentation includes installation instructions, usage examples for both hosted and local setups, and requirement details, but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Marketing Miner MCP\n\n> P≈ôipojte Marketing Miner k Cursor, Claude, Windsurf a dal≈°√≠m AI asistent≈Øm.\n\nMCP server pro anal√Ωzu kl√≠ƒçov√Ωch slov a hledanost pomoc√≠ Marketing Miner API.\n\n## Pou≈æit√≠ p≈ôes Smithery (Doporuƒçeno)\n\nNejjednodu≈°≈°√≠ zp≈Øsob pou≈æit√≠ je p≈ôes Smithery hosted URL:\n\n1. Jdƒõte na [smithery.ai](https://smithery.ai) \n2. Vyhledejte \"marketing-miner-mcp\"\n3. **Zadejte sv√© API kl√≠ƒçe** v Smithery Connect formul√°≈ôi\n4. Z√≠skejte remote hosted URL\n5. P≈ôidejte do sv√©ho MCP klienta\n\n### SHTTP (pro klienty s podporou):\n```json\n{\n  \"mcpServers\": {\n    \"marketing-miner\": {\n      \"url\": \"your-smithery-url.com\"\n    }\n  }\n}\n```\n\n### Pro klienty bez SHTTP podpory:\n```json\n{\n  \"mcpServers\": {\n    \"marketing-miner\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"your-smithery-url.com\"]\n    }\n  }\n}\n```\n\n## Lok√°ln√≠ spu≈°tƒõn√≠\n\n```bash\nnpm install\nnpm run build\nexport MARKETING_MINER_API_TOKEN=YOUR_TOKEN\nnpm start\n```\n\n## API Token\n\nZ√≠skejte sv≈Øj API token na [Marketing Miner API](https://www.marketingminer.com/cs/features/api)",
        "start_pos": 0,
        "end_pos": 1036,
        "token_count_estimate": 258,
        "source_type": "readme",
        "agent_id": "85a0bc150e64558f"
      }
    ]
  },
  {
    "agent_id": "b4b30dcb6154f168",
    "name": "ai.smithery/luminati-io-brightdata-mcp",
    "source": "mcp",
    "source_url": "https://github.com/brightdata/brightdata-mcp-sse",
    "description": "One MCP for the Web. Easily search, crawl, navigate, and extract websites without getting blocked.‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-06T11:04:51.228096Z",
    "indexed_at": "2026-02-18T04:07:43.327425",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search websites",
        "Crawl websites",
        "Navigate websites",
        "Extract data from websites",
        "Avoid getting blocked during web interactions"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's web interaction capabilities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "f06ca90f932d59db",
    "name": "ai.smithery/magenie33-quality-dimension-generator",
    "source": "mcp",
    "source_url": "https://github.com/magenie33/quality-dimension-generator",
    "description": "Generate tailored quality criteria and scoring guides from your task descriptions. Refine objectiv‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T15:11:34.739058Z",
    "indexed_at": "2026-02-18T04:07:47.286232",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Quality Dimension Generator\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.8+-blue.svg)](https://www.typescriptlang.org/)\n[![MCP](https://img.shields.io/badge/MCP-1.6+-purple.svg)](https://modelcontextprotocol.io/)\n\nAn **MCP server** that generates quality evaluation standards for any task. Transform vague requirements into precise, measurable quality criteria with AI-powered analysis, ultimately improving your final work quality.\n\n## üéØ What It Does\n\n- **üìä Analyzes your tasks** - Understand what needs to be accomplished\n- **üéØ Creates evaluation standards** - Generate specific quality dimensions with scoring criteria\n- **üìà Sets target scores** - Define expected quality levels (e.g., 8/10)\n- **‚úÖ Guides execution** - Help you complete tasks with clear quality standards\n\n## üöÄ Quick Start\n\n### Installation\n\nInstall from the **Smithery AI Model Context Protocol Registry**:\n\nüîó **[Get Quality Dimension Generator on Smithery](https://smithery.ai/server/@magenie33/quality-dimension-generator)**\n\n### Basic Usage\n\n**Step 1:** Generate task analysis\n```javascript\ngenerate_task_analysis_prompt({\n  userMessage: \"Write a 1000-word article about AI\"\n})\n```\n\n**Step 2:** Generate quality standards\n```javascript\ngenerate_quality_dimensions_prompt({\n  taskAnalysisJson: \"...\" // JSON from step 1\n})\n```\n\n**Result:** Get comprehensive quality evaluation criteria with target scores, then complete your task following those standards.\n\n## üìã Example Output\n\nFor the task \"Write a technical blog post\":\n\n```json\n{\n  \"expectedScore\": 8,\n  \"scoreCalculation\": \"Average of all 5 dimension scores\",\n  \"dimensions\": [\n    {\n      \"name\": \"Technical Accuracy\",\n      \"description\": \"Correctness and depth of technical content\",\n      \"importance\": \"Ensures readers get reliable information\",\n      \"scoring\": {\n        \"10\": \"All technical details verified and comprehensive\",\n        \"8\": \"Mostly accurate with minor gaps\",\n        \"6\": \"Generally correct but lacks depth\"\n      }\n    }\n    // ... 4 more dimensions\n  ]\n}\n```\n\n## üí° Use Cases\n\n- **Software Development** - Code quality, testing, documentation standards\n- **Content Creation** - Writing quality, SEO, engagement metrics\n- **Project Management** - Deliverable criteria, timeline adherence\n- **Research** - Methodology, accuracy, presentation standards\n\n## ü§ù Contributing\n\nContributions welcome! This project is open source under the MIT License.\n\n## üîó Resources\n\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io/docs)\n- [Smithery MCP Registry](https://smithery.ai/)\n\n---\n\n**Transform your work quality today!** üöÄ\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Analyze tasks to understand objectives",
        "Generate specific quality evaluation standards for tasks",
        "Create measurable quality dimensions with detailed scoring criteria",
        "Set target quality scores for task completion",
        "Guide task execution using clear quality standards"
      ],
      "limitations": [],
      "requirements": [
        "Access to Smithery AI Model Context Protocol Registry to install the server",
        "TypeScript 5.8+ environment recommended",
        "MCP protocol version 1.6+ compatibility"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, usage examples with code snippets, detailed capability descriptions, example outputs, use cases, and references, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Quality Dimension Generator\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![TypeScript](https://img.shields.io/badge/TypeScript-5.8+-blue.svg)](https://www.typescriptlang.org/)\n[![MCP](https://img.shields.io/badge/MCP-1.6+-purple.svg)](https://modelcontextprotocol.io/)\n\nAn **MCP server** that generates quality evaluation standards for any task. Transform vague requirements into precise, measurable quality criteria with AI-powered analysis, ultimately improving your final work quality.\n\n## üéØ What It Does\n\n- **üìä Analyzes your tasks** - Understand what needs to be accomplished\n- **üéØ Creates evaluation standards** - Generate specific quality dimensions with scoring criteria\n- **üìà Sets target scores** - Define expected quality levels (e.g., 8/10)\n- **‚úÖ Guides execution** - Help you complete tasks with clear quality standards\n\n## üöÄ Quick Start\n\n### Installation\n\nInstall from the **Smithery AI Model Context Protocol Registry**:\n\nüîó **[Get Quality Dimension Generator on Smithery](https://smithery.ai/server/@magenie33/quality-dimension-generator)**\n\n### Basic Usage\n\n**Step 1:** Generate task analysis\n```javascript\ngenerate_task_analysis_prompt({\n  userMessage: \"Write a 1000-word article about AI\"\n})\n```\n\n**Step 2:** Generate quality standards\n```javascript\ngenerate_quality_dimensions_prompt({\n  taskAnalysisJson: \"...\" // JSON from step 1\n})\n```\n\n**Result:** Get comprehensive quality evaluation criteria with target scores, then complete your task following those standards.",
        "start_pos": 0,
        "end_pos": 1552,
        "token_count_estimate": 388,
        "source_type": "readme",
        "agent_id": "f06ca90f932d59db"
      },
      {
        "chunk_id": 1,
        "text": "ortance\": \"Ensures readers get reliable information\",\n      \"scoring\": {\n        \"10\": \"All technical details verified and comprehensive\",\n        \"8\": \"Mostly accurate with minor gaps\",\n        \"6\": \"Generally correct but lacks depth\"\n      }\n    }\n    // ... 4 more dimensions\n  ]\n}\n```\n\n## üí° Use Cases\n\n- **Software Development** - Code quality, testing, documentation standards\n- **Content Creation** - Writing quality, SEO, engagement metrics\n- **Project Management** - Deliverable criteria, timeline adherence\n- **Research** - Methodology, accuracy, presentation standards\n\n## ü§ù Contributing\n\nContributions welcome! This project is open source under the MIT License.\n\n## üîó Resources\n\n- [Model Context Protocol Documentation](https://modelcontextprotocol.io/docs)\n- [Smithery MCP Registry](https://smithery.ai/)\n\n---\n\n**Transform your work quality today!** üöÄ",
        "start_pos": 1848,
        "end_pos": 2712,
        "token_count_estimate": 215,
        "source_type": "readme",
        "agent_id": "f06ca90f932d59db"
      }
    ]
  },
  {
    "agent_id": "15d915445d89a063",
    "name": "ai.smithery/mayla-debug-mcp-google-calendar2",
    "source": "mcp",
    "source_url": "https://github.com/mayla-debug/mcp-google-calendar2",
    "description": "Schedule and manage Google Calendar events directly from your workspace. Check availability, view‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-07T11:08:53.670394Z",
    "indexed_at": "2026-02-18T04:07:49.003973",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Schedule Google Calendar events",
        "Manage Google Calendar events",
        "Check availability on Google Calendar",
        "View Google Calendar events"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of scheduling and managing Google Calendar events but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "8758ba4f44818986",
    "name": "ai.smithery/mfukushim-map-traveler-mcp",
    "source": "mcp",
    "source_url": "https://github.com/mfukushim/map-traveler-mcp",
    "description": "Create immersive travel experiences by instructing an avatar to navigate Google Maps. Report on th‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-29T15:14:24.853367Z",
    "indexed_at": "2026-02-18T04:07:52.875623",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Virtual Traveling bot environment for MCP\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/073d88cc-277d-40b6-8c20-bcabf6c275e9)\n[![smithery badge](https://smithery.ai/badge/@mfukushim/map-traveler-mcp)](https://smithery.ai/server/@mfukushim/map-traveler-mcp)\n\nEnglish / [Japanese](./README_jp.md)\n\nThis is an MCP server that creates an environment for an avatar to virtually travel on Google Maps.\n\nFrom an MCP client such as Claude Desktop, you can give instructions to the avatar and report on the progress of its journey with photos.\n\n<img alt=\"img_5.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_5.png\" width=\"400\"/>\n\n> Preparing for MCP Registry Support https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/  \n\n> Added gemini-2.5-flash-image-preview (nano-banana) to travel image generation  \n\nSupport for nano-banana has been added. Nano-banana's semantic mask allows you to generate composite travel images in a short time without setting remBg.  \nAlthough conventional image synthesis is still possible, we recommend using Gemini nano-banana.  \n\n> Supports both Streamable-HTTP and stdio (compliant with Smithery.ai's config interface)  \n\nIt can be used as a stdio-type MCP as before, or as Streamable-HTTP.  \nAlthough it supports multiple users, the database API must be specified per session using the Smithery.ai config interface.  \nSince it supports both Streamable-HTTP and stdio, it is expected to work as is with the previous MCP client, but if you use the previous stdio version, please use v0.0.x (v0.0.81).  \n``` npx -y @mfukushim/map-traveler-mcp@0.0.81 ```  \n\n> Now supports librechat https://www.librechat.ai/.\n\n> Now supports Smithery https://smithery.ai/server/@mfukushim/map-traveler-mcp (images are excluded because they are heavy to run).\n\n> Now verified MseeP https://mseep.ai/app/mfukushim-map-traveler-mcp \n\n## Functions\n\n#### MCP server tools function\n\nThe following functions can be used as an MCP server. The available functions vary depending on the settings and execution state.\n\nYou can specify the function name directly, but Claude LLM will automatically recognize it, so you can specify the operation in general terms.\n\nExample:\n\"Where are you now?\" \"Let's leave for Tokyo Station.\"\n\n- get_traveler_view_info(includePhoto:boolean,includeNearbyFacilities:boolean)  \n  Gets information about the current travel avatar's location.  \n  - includePhoto: Gets nearby Google Street View photos. If you have set up an image generation AI, it will synthesize the avatar.\n  - includeNearbyFacilities: Gets information about nearby facilities.\n- get_traveler_location()  \n  Gets information about the current travel avatar's address and nearby facilities.\n- reach_a_percentage_of_destination()\n  Reach a specified percentage of the destination (moveMode=skip only)\n  timeElapsedPercentage: Percent progress towards destination(0~100)\n- set_traveler_location(address: string)  \n  Sets the current travel avatar's location.\n  - address: Address information (exact address, or general name that Google Maps or Claude can recognize, etc.)\n- get_traveler_destination_address  \n  Get the destination of the travel avatar you set\n- set_traveler_destination_address(address: string)  \n  Set the destination of the travel avatar\n   - address: Address information (exact address, or general name that Google Maps or Claude can recognize, etc.)\n- start_traveler_journey  \n  Start the journey at the destination.(moveMode=realtime only)\n- stop_traveler_journey  \n  Stop the journey.(moveMode=realtime only)\n- set_traveler_info(settings:string)  \n  Set the traveler's attributes. Set the traveler's personality that you want to change dynamically, such as name and personality. However, if you use a role script, the script is more stable.\n  - settings: Setting information such as name and personality.\n- get_traveler_info  \n  Get the traveler's attributes. Get the traveler's personality.\n- set_avatar_prompt(prompt:string)  \n  Set the prompt when generating the travel avatar image. The default is an anime-style woman. The anime style is enforced to prevent fake images.\n  - prompt\n- reset_avatar_prompt  \n  Reset avatar generation prompts to default.\n- get_sns_feeds  \n  Gets Bluesky SNS articles for the specified custom feed (feeds containing a specific tag).\n- get_sns_mentions  \n  Gets recent mentions (likes, replies) to Bluesky SNS posts that you made yourself.\n- post_sns_writer(message:string)  \n  Posts an article to Bluesky SNS with the specified custom feed. Set a specific tag so that it can be determined that the post was generated by the travel bot.\n  - message: article\n- reply_sns_writer(message:string,id:string)  \n  Reply to the article with the specified id. Set a specific tag so that it can be determined that the post was generated by the travel bot.\n  - message: reply\n  - id: The ID of the post to reply to\n- add_like(id:string)  \n  Add a like to the specified post.\n  - id: The ID of the post to like\n- tips  \n  Guides you on how to set up features that have not yet been set.\n- get_setting  \n  Get environment and image settings.\n\n#### MCP resources\n\nHas five custom prompt samples.\nWhen you import a prompt with Claude Desktop, Claude will act as a traveler.\nThe SNS-compatible version controls SNS input and output while having a travel conversation.\n\n- role.txt  \n  Claude will act as a traveler.\n\n- roleWithSns.txt  \n  Claude will act as a traveler. It also controls reading and posting to SNS.\n- carBattle.txt  \n  This is a small novel game about a story of transporting secret documents from Yokohama to Tokyo. Scenes are automatically generated. Set moveMode=skip to play.\n- japanMapChallenge.txt,japanMapChallenge2.txt  \n  Two AIs communicate with each other via SNS and play a challenge game using landscape images.  \n  To play, you need two Bluesky accounts and two Claude Desktops. Also set moveMode=skip. (However, the operation is somewhat unstable.)  \n  japanMapChallenge2 has a challenge reflection rule.\n\n## Setting\n\nYou will need to obtain and set access keys for multiple APIs, such as for accessing multiple Google maps and generating images.\nUse of the API may incur charges.\n\n#### Settings for using with Claude Desktop \n\n- claude_desktop_config.json (stdio type)\n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mfukushim/map-traveler-mcp\"],\n      \"env\":{\n        \"MT_GOOGLE_MAP_KEY\":\"(Google Map API key)\",\n        \"MT_GEMINI_IMAGE_KEY\": \"(Gemini Image Api key)\",\n        \"MT_MAX_RETRY_GEMINI\": \"(Number of retries when generating Gemini images Default: 0)\",\n        \"MT_AVATAR_IMAGE_URI\": \"(Character reference image uri (file:// or https://) when generating Gemini image. Multiple settings can be made by separating them with the '|'. When multiple settings are made, they will be selected randomly.)\",\n        \"MT_MAP_API_URL\": \"(Optional: Map API custom endpoint. Example: direction=https://xxxx,places=https://yyyy )\",\n        \"MT_TIME_SCALE\": \"(Optional:Scale of travel time on real roads duration. default 4)\",\n        \"MT_SQLITE_PATH\":\"(db save path: e.g. %USERPROFILE%/Desktop/traveler.sqlite ,$HOME/traveler.sqlite )\",\n        \"MT_TURSO_URL\":\"(Turso sqlite API URL)\",\n        \"MT_TURSO_TOKEN\":\"(Turso sqlite API access token)\",\n        \"MT_REMBG_PATH\": \"(absolute path of the installed rembg cli)\",\n        \"MT_REMBG_URL\": \"(rembg API URL)\",\n        \"MT_REMBG_WO_KEY\": \"(withoutbg.com rembg API key)\",\n        \"MT_PIXAI_KEY\":\"(pixAi API key)\",\n        \"MT_SD_KEY\":\"(or Stability.ai image generation API key\",\n        \"MT_PIXAI_MODEL_ID\": \"(Optional: pixAi ModelId, if not set use default model 1648918127446573124 \",\n        \"MT_COMFY_URL\": \"(Option: Generate image using ComfyUI API at specified URL. Example: http://192.168.1.100:8188)\",\n        \"MT_COMFY_WORKFLOW_T2I\": \"(Optional: Path to API workflow file when using text to image with ComfyUI. If not specified: assets/comfy/t2i_sample.json)\",\n        \"MT_COMFY_WORKFLOW_I2I\": \"(Optional: Path of API workflow file when image to image in ComfyUI. If not specified: assets/comfy/i2i_sample.json)\",\n        \"MT_COMFY_PARAMS\": \"(Optional: Variable values to send to the workflow via comfyUI API)\",\n        \"MT_FIXED_MODEL_PROMPT\": \"(Optional: Fixed avatar generation prompt. You will no longer be able to change your avatar during conversations.)\",\n        \"MT_BODY_AREA_RATIO\": \"(Optional: Acceptable avatar image area ratio. default 0.042)\",\n        \"MT_BODY_HW_RATIO\": \"(Optional: Acceptable avatar image aspect ratios. default 1.5~2.3)\",\n        \"MT_BODY_WINDOW_RATIO_W\": \"(Optional: Avatar composite window horizontal ratio. default 0.5)\",\n        \"MT_BODY_WINDOW_RATIO_H\": \"(Optional: Avatar composite window aspect ratio. default 0.75)\",\n        \"MT_BS_ID\":\"(Bluesky sns registration address)\",\n        \"MT_BS_PASS\":\"(bluesky sns password)\",\n        \"MT_BS_HANDLE\":\"(bluesky sns handle name: e.g. xxxxxxxx.bsky.social )\",\n        \"MT_FILTER_TOOLS\": \"(Optional: Directly filter the tools to be used. All are available if not specified. e.g. tips,set_traveler_location)\",\n        \"MT_MOVE_MODE\": \"(Option: Specify whether the movement mode is realtime or skip. default realtime)\",\n        \"MT_IMAGE_WIDTH\": \"(Option: Output image width (pixels) Default is 512)\",\n        \"MT_NO_IMAGE\": \"(Options: true = do not output image, not specified = output image if possible, default is not specified)\",\n        \"MT_NO_AVATAR\": \"(Option: true = Output StreetView image as is without avatar superimposition. Not specified = Superimpose avatar image. Default is not specified.)\",\n        \"MT_FEED_TAG\": \"(Optional: Specify the feed tag when posting to SNS (#required, 15 characters or more) Default is #geo_less_traveler)\",\n        \"MT_MAX_SESSIONS\": \"(Maximum number of sessions when using Streamable-http)\",\n        \"MT_SESSION_TTL_MS\": \"(Session TTL when using Streamable-http)\",\n        \"MT_SERVICE_TTL_MS\": \"(Service TTL when using Streamable-http)\"\n      }\n    }\n  }\n}\n```  \n\n- claude_desktop_config.json (streamable-http type)  \nThe above MT_ environment variables should be set as environment variables for the server that runs the map-traveler-mcp web service.  \n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://(mcp server address)/mcp?config=(base64 config json)\"\n    }\n  }\n}\n```  \n\nbase64 config json (Smithery.ai Expansion)  \nBy concatenating the json in the following format into a single line of string, converting it to base64, and setting it as (base64 setting json), you can overwrite different APIs and settings for each user session.  \nIf the database is not set base64 config json, it will be shared across the entire service (the location of the traveler will be shared across the database and counted for one person).  \nWe plan to reconsider the operation of assigning an individual UserId for each session once the MCP authentication mechanism has become a little clearer.  \n```json\n{\n  \"MT_GOOGLE_MAP_KEY\": \"xxxyyyzzz\",\n  \"MT_GEMINI_IMAGE_KEY\": \"xxyyzz\",\n  \"MT_MAX_RETRY_GEMINI\": \"1\",\n  \"MT_AVATAR_IMAGE_URI\": \"file:///C:/Users/xxxx/Desktop/avatar.png\",\n  \"MT_TURSO_URL\": \"libsql://xxxyyyzzz\",\n  \"MT_TURSO_TOKEN\": \"abcdabcd\",\n  \"MT_BS_ID\": \"xyxyxyxyx\",\n  \"MT_BS_PASS\": \"1234xyz\",\n  \"MT_BS_HANDLE\": \"aabbccdd\",\n  \"MT_FILTER_TOOLS\": \"tips,set_traveler_location\",\n  \"MT_MOVE_MODE\": \"direct\",\n  \"MT_FEED_TAG\": \"#abcdefgabcdefgabcdefg\"\n}\n```  \n(All json values can be omitted)  \n‚Üì (json text concatenation)  \n```text\n{\"MT_GOOGLE_MAP_KEY\": \"xxxyyyzzz\", \"MT_GEMINI_IMAGE_KEY\": \"xxyyzz\", \"MT_MAX_RETRY_GEMINI\": \"1\", \"MT_TURSO_URL\": \"libsql://xxxyyyzzz\", \"MT_TURSO_TOKEN\": \"abcdabcd\", \"MT_BS_ID\": \"xyxyxyxyx\", \"MT_BS_PASS\": \"1234xyz\", \"MT_BS_HANDLE\": \"aabbccdd\", \"MT_FILTER_TOOLS\": \"tips,set_traveler_location\", \"MT_MOVE_MODE\": \"direct\", \"MT_FEED_TAG\": \"#abcdefgabcdefgabcdefg\"}\n```\n‚Üì (Set the base64 version to config=)  \n```text\neyJNVF9HT09HTEVfTUFQX0tFWSI6ICJ4eHh5eXl6enoiLCAiTVRfR0VNSU5JX0lNQUdFX0tFWSI6ICJ4eHl5enoiLCAiTVRfTUFYX1JFVFJZX0dFTUlOSSI6ICIxIiwgIk1UX1RVUlNPX1VSTCI6ICJsaWJzcWw6Ly94eHh5eXl6enoiLCAiTVRfVFVSU09fVE9LRU4iOiAiYWJjZGFiY2QiLCAiTVRfQlNfSUQiOiAieHl4eXh5eHl4IiwgIk1UX0JTX1BBU1MiOiAiMTIzNHh5eiIsICJNVF9CU19IQU5ETEUiOiAiYWFiYmNjZGQiLCAiTVRfRklMVEVSX1RPT0xTIjogInRpcHMsc2V0X3RyYXZlbGVyX2xvY2F0aW9uIiwgIk1UX01PVkVfTU9ERSI6ICJkaXJlY3QiLCAiTVRfRkVFRF9UQUciOiAiI2FiY2RlZmdhYmNkZWZnYWJjZGVmZyJ9\n```\n\n\n> NOTE: The environment variables have been renamed to standard snake case. The MT_ prefix is added because they may be used in conjunction with other environment variables, such as in librechat. The old names can still be used for backward compatibility.  \n\nPlease set the following three Credentials for Google Map API.  \n- Street View Static API\n- Places API (New)\n- Time Zone API\n- Directions API\n\nhttps://developers.google.com/maps/documentation/streetview/get-api-key\n\nIf you want to use the image generation AI, set either pixAi_key or sd_key. You also need to have python3.7~3.11 installed on your PC and rembg cli installed (virtual environment recommended).\n\nhttps://platform.pixai.art/docs  \nhttps://platform.stability.ai/docs/api-reference#tag/SDXL-1.0-and-SD1.6/operation/textToImage\n\nThe bluesky SNS address/password are optional. It is recommended that you create a dedicated account as it will post automatically.\n\nhttps://bsky.app/\n\nYou can also run it in practice mode, which does not require an API key for verification.\n\n#### Practice mode settings  \nclaude_desktop_config.json\n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mfukushim/map-traveler-mcp\"]\n    }\n  }\n}\n```\n\n## How to use\n\n#### Use the practice mode\n\n1. Install nodejs 22.\n\n2. Set up Claude Desktop for use.\n\n3. Reflect one of the above settings in claude_desktop_config.json.\n\n4. Restart Claude Desktop. It may take some time to set up (if an error occurs, try restarting Claude Desktop again. If it doesn't work, see the notes below). Make sure the following mark appears in the bottom right of the screen.\n\n  <img alt=\"img_1.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_1.png\" width=\"150\"/>\n\n5. Ask \"Where are you now?\" and \"Go on a journey.\" A conversation will begin. When using the API, a confirmation screen will appear, so select Allow.\n\n<img alt=\"img_4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_4.png\" width=\"200\"/>\n\n6. Select Attach from MCP and select role.txt.\n\n<img alt=\"img_2.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_2.png\" width=\"200\"/>\n\n<img alt=\"img_3.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_3.png\" width=\"200\"/>\n\n7. A travel prompt has been built in, so feel free to talk to it.\n\n#### Use the full feature\n\n1. Get a Google Map API access key and set the permissions for Street View Static API, Places API (New), Time Zone API, and Directions API. Set this in the env of claude_desktop_config.json and restart.\n   At this point, the travel log will be based on the real map. Travel images will also be output if they are not superimposed.\n2. Decide on a path that will not interfere with the disk and set it in the sqlite_path of the env of claude_desktop_config.json. (Example: %USERPROFILE%/Desktop/traveler.sqlite $HOME/Documents/traveler.sqlite, etc.)\n   At this point, your travel log will be saved and you can continue your journey even if you close Claude Desktop.\n3. Install python 3.7 to 3.11 and install rembg with cli. We recommend using a virtual environment such as venv.\n  ```bash\n  python3 -m venv venv\n  . venv/bin/activate or .\\venv\\Scripts\\activate\n  pip install \"rembg[cpu,cli]\"\n  ```\n  Check if rembg cli works properly using a sample image file. Input an image with a person in it, and if the person is cut out in the output file, it's OK.  \n  ```bash\n  rembg i source_image_file dest_image_file\n  ```\n4. rembg cli will be installed in the python exe location, so get the path. The file location varies depending on the OS and python installation status, but in the case of venv, it is (virtual environment name)\\Scripts\\rembg.exe or (virtual environment name)/bin/rembg above the directory you set. If you can't find it, search for the path with a file search software. Set that path to rembg_path of env in claude_desktop_config.json. (Example: \"rembg_path\": \"C:\\\\Users\\\\xxxx\\\\Documents\\\\rembg_venv\\\\venv\\\\Scripts\\\\rembg.exe\")\n5. Get an image generation API key from the pixAI or Stability.ai site. Set the key to pixAi_key or sd_key in env of claude_desktop_config.json.\n   The avatar will now be overlaid on the travel image.\n6. Get the bluesky SNS address/password and handle name. Set these in bs_id, bs_pass, and bs_handle in env of claude_desktop_config.json, respectively.\n   Import the travel knowledge prompt roleWithSns.txt to report travel actions to SNS (it will automatically post as a bot, so we recommend allocating a dedicated account)\n\nInstead of preparing rembg with the cli, we have added a setting that allows you to handle rembg as a service API.  \nIf you configure the following rembg service, you can use rembg by setting the URL in remBgUrl.  \n\nhttps://github.com/danielgatis/rembg?tab=readme-ov-file#rembg-s  \n\nSetup is simple if you use the Docker version to launch a container and access it.  \n\nhttps://github.com/danielgatis/rembg?tab=readme-ov-file#usage-as-a-docker  \n\n#### Use Turso libsql API for configuration database\n\nIf you want to use the cloud API Turso libsql (https://turso.tech/libsql) without having a local sqlite file, sign up for Turso and allocate a sqlite database (paid, free tier available).   \nThis add-in will automatically configure (migrate) the database.  \nMT_TURSO_URL = turso db URL  \nMT_TURSO_TOKEN = turso db access token  \n\n\n#### Use Cloud API for rembg\n\nLocal settings around rembg are complicated no matter what method you use, but we have added settings for the paid cloud rembg (https://withoutbg.com/).  \n> Note: There is a small free trial available, but please be aware that this is a commercial API and is quite expensive (about 0.1 euros per image).\n\nMT_REMBG_WO_KEY = withoutbg access token\n\n\n#### When using external ComfyUI (for more advanced users)\n\nYou can also use a local ComfyUI as an image generation server. You can configure the image generation characteristics yourself in detail to reduce API costs.\n\nHowever, the configuration will be quite complicated and image generation may take longer.\n\n1. Configure ComfyUI to run in API mode.\n2. Set the server URL to comfy_url in env.\n3. Set detailed configuration values such as the model to be used in env in the form of a json string.\nexample.\n```json\n{\n  \"env\": {\n    \"comfy_url\": \"http://192.168.1.100:8188\",\n    \"comfy_workflow_t2i\": \"C:\\\\Documents\\\\t2itest.json\",\n    \"comfy_workflow_i2i\":\"C:\\\\Documents\\\\i2itest.json\",\n    \"comfy_params\":\"ckpt_name='animagineXL40_v40.safetensors',denoise=0.65\"\n  }\n}\n```\n4. The default workflow can use assets/comfy/t2i_sample.json and assets/comfy/i2i_sample.json in the package. You can specify variables using % and specify the variables in comfy_params.\n\n## Using libreChat\n\nIt has been adapted to work with libreChat. This makes it easier to use, but some additional settings are required.  \nAlso, it seems that it will not be stable unless the PC you use has a decent level of performance, such as one that can stably run Docker.\n\n#### Install libreChat  \n\nPlease make sure it works as described on the official website.  \nIn this case, we recommend using Docker configuration due to additional settings.\n\nhttps://www.librechat.ai/docs/local/docker  \n\nConfigure librechat.yaml using the official procedure.  \nI think you will need to add a local or API LLM service.  \n\nhttps://www.librechat.ai/docs/configuration/librechat_yaml  \n\nAdd a user for login.  \n\nhttps://www.librechat.ai/docs/configuration/authentication#create-user-script  \n\nPlease set it so that you can have general chat conversations.  \n\n#### Add a rembg container with additional settings  \n\nTo use rembg with Docker, add pulling and running the rembg Docker container.  \n\ndocker-compose.override.yml\n```yml\n services:\n   api:\n     volumes:\n       - type: bind\n         source: ./librechat.yaml\n         target: /app/librechat.yaml\n\n   rembg:\n     image: danielgatis/rembg:latest\n     restart: always\n     command: \"s --host 0.0.0.0 --port 7000 --log_level info\"\n\n```\n\n#### Add map-traveler-mcp to the MCP service  \n\nAdd librechat.yaml\n```yaml\nmcpServers:\n  traveler:\n    type: stdio\n    command: npx\n    args:\n      - -y\n      - \"@mfukushim/map-traveler-mcp\"\n```\n\nAdd .env (Same as env in claude_desktop_config.json)\n\n```env\n# map-traveler-mcp\nGoogleMapApi_key=(Google Map API key)\nsqlite_path=/home/run_test.sqlite (e.g. librechat in an unobtrusive location inside the container, or in an external directory that you don't want to mount.)\nremBgUrl=http://rembg:7000 (rembg Service API URL, container URL)\n(Other settings such as image generation AI settings, PixAI key, stability.ai API key, ComfyUI settings, etc.)\n\n```\n\nAfter setting, restart the container.  \nOn slow PCs, mcp initialization may fail. Multiple restarts may work, but this may be difficult to run...\n\n#### llibreChat settings\n\nTo use the MCP function in libreChat, use the Agents function.  \n\n1. On the conversation screen, select Agents.  \n   <img alt=\"libre1.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre1.png\" width=\"200\"/>\n2. Select Agent Builder from the panel on the right side of the screen and configure your agent.  \n   <img alt=\"libre2.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre2.png\" width=\"200\"/>\n3. Select Add Tools to use map-traveler.  \n   <img alt=\"libre3.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre3.png\" width=\"200\"/>\n4. The agent tools screen will appear, so select and add all the map-traveler-mcp tools (if the map-traveler-mcp tools are not listed, MCP initialization has failed, so please restart the container or review the settings by checking the logs, etc.)  \n   <img alt=\"libre4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre4.png\" width=\"200\"/>  \n   <img alt=\"libre5.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre5.png\" width=\"200\"/>  \n5. Enter additional script in the instruction area.  \n   Since libreChat does not have the MCP resource function, enter the content text of the following URL into the instruction area instead.   \n   https://github.com/mfukushim/map-traveler-mcp/blob/main/assets/scenario/role.txt  \n   <img alt=\"libre7.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre7.png\" width=\"200\"/>  \n6. Click the Create button to save the agent.  \n   <img alt=\"libre6.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre6.png\" width=\"200\"/>\n7. Start a new chat.\n\n## Smithery\n\nPlease refer to https://smithery.ai/server/@mfukushim/map-traveler-mcp.  \nRemote MCP (Streamable-http mode) is supported. Image generation is only available on nano-banana.  \nDatabase settings can now be recorded with Turso sqlite, so if you configure Turso, your travel progress will also be saved.  \n<img alt=\"smithery.png\" src=\"tools/smithery.png\" width=\"400\"/>\n\n\n\n## Install guide (Japanese, but lots of photos)\n\n1. introduction and Practice mode  \n   https://note.com/marble_walkers/n/n7a8f79e4fb30\n2. DB, Google Map API, Image gen API  \n   https://note.com/marble_walkers/n/n765257c27f3b\n3. Avatar prompt  \n   https://note.com/marble_walkers/n/nc7273724faea\n4. SNS integration  \n   https://note.com/marble_walkers/n/na7c956befe7b\n5. Application 1  \n   https://note.com/marble_walkers/n/n3c86edd8e817\n6. ComfyUI API  \n   https://note.com/marble_walkers/n/ncefc7c05d102  \n7. Application 2  \n   https://note.com/marble_walkers/n/ne7584ed231c8\n8. LibreChat setting  \n   https://note.com/marble_walkers/n/n339bf7905324\n9. AI Agent SNS Battle Map Challenge  \n   https://note.com/marble_walkers/n/n6db937573eaa\n10. Support Smithery, Turso libSQL, and rembg API   \n   https://note.com/marble_walkers/n/ne3b3c0f99707\n11. Streamable-HTTP support  \n    https://note.com/marble_walkers/n/n030063f22dc0\n12. Nano-Banana support  \n    https://note.com/marble_walkers/n/n5d49514dddec  \n\n\n#### Additional about the source code\n\nI use Effect.ts to simplify error management & for my own learning.  \nWe also use the Effect Service, but due to the way MCP calls work, we believe that consolidating it using the Service was not optimal.  \nI think it would be simpler to handle the MCP calls directly in the Effect.  \nAddendum: I'm aware that I will be able to reconsider how to use the Effect Service and rewrite it neatly, but I'm still considering whether to rewrite it.  \n\n#### Notes on the latest updates\n\n- Added image_width to env. The default is 512. Setting it smaller may reduce the cost of LLM API.  \n- Added an env setting that does not output images for MCP clients that do not have image input/output.  \n\"MT_NO_IMAGE\": \"true\" will not generate or output any images. Other image-related settings can be omitted.  \n```\n{\n  \n  \"env\": {\n    \"MT_NO_IMAGE\": \"true\"\n  }\n  \n}\nor\n{\n  \n  \"env\": {\n    \"GoogleMapApi_key\": \"xxxx\",\n    \"MT_NO_IMAGE\": \"true\"\n  }\n  \n}\n\n```  \n- You can now specify the tag name to be added when posting to SNS (Bluesky). #Required and must be at least 15 characters. If not specified, it will become \"#geo_less_traveler\".  \n- The information obtained from SNS has been slightly changed. The information posted to SNS has been slightly changed.  \n- A script has been added that allows multiple travel bots to converse and play via SNS.  \n\n- Supports remote use from Smithery.  \n  If you do not want to configure detailed settings, start the app in practice mode.\n  You can also run the app at full speed by configuring each cloud API, but please be aware of charges as it uses many paid APIs such as rembg API.\n  If you do not want to synthesize avatars, you can run the app with the minimum settings of Google Map API and Turso sqlite API.\n\n- Added the MT_NO_AVATAR option.  \n  If set, an avatar image will not be composited onto the landscape image. Since there will be no retry processing for avatar composition, the time it takes to obtain a response will be significantly shorter.  \n  Set this option if image composition is slow or fails unavoidably.\n\n- Partially applied MCP version 2025-06-18.  \n  I added title to the schema. I plan to apply outputSchema and structured response in the future, but I haven't implemented them this time. Since the output of Travel Bot is simple text, I don't think structuring is necessary yet.  \n  https://modelcontextprotocol.io/specification/2025-06-18/server/tools  \n- Fixed an issue where some functions, such as SNS functions, could not be called regardless of the env settings due to an initialization error.  \n\n- Added support for Streamable-http. This was done in a hurry, so if you experience any issues, please consider using version 0.0.81 or similar.  \n\n- Support for nano-banana (gemini-2.5-flash-image-preview) image generation has been added. When using nano-banana, no rembg settings are required. The characteristics of the avatar prompt have changed, so image generation may fail with the previous avatar prompt. In this case, you will need to adjust the avatar appearance prompt to one that is acceptable for nano-banana.\n\n- When generating images for nano-banana, you can now reference the original character image with MT_AVATAR_IMAGE_URI. Please use it in a way that does not infringe on copyrights.\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/mfukushim-map-traveler-mcp-badge.png)](https://mseep.ai/app/mfukushim-map-traveler-mcp)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Create a virtual traveling environment on Google Maps for an avatar",
        "Retrieve current avatar location information including photos and nearby facilities",
        "Set and get the travel avatar's location and destination addresses",
        "Control the travel avatar's journey start, stop, and progress percentage",
        "Set and get traveler's attributes such as name and personality",
        "Generate and customize travel avatar images with prompts and composite techniques",
        "Interact with Bluesky SNS by posting, replying, liking, and fetching feeds and mentions",
        "Support multiple transport modes including realtime and skip movement",
        "Operate as both stdio-type and Streamable-HTTP MCP server",
        "Provide custom prompt samples for various travel and game scenarios"
      ],
      "limitations": [
        "Requires multiple API keys for Google Maps, image generation, and SNS access",
        "Image generation and avatar synthesis depend on external AI services and may incur charges",
        "Operation with multiple users requires specifying database API per session",
        "Some features like multi-AI SNS challenge require multiple Bluesky accounts and may be unstable",
        "Previous stdio version compatibility limited to v0.0.x (v0.0.81)",
        "Heavy image processing features may be excluded in some client integrations due to resource constraints"
      ],
      "requirements": [
        "Google Maps API key",
        "Gemini Image API key or alternative image generation API keys (pixAi, Stability.ai, ComfyUI)",
        "Bluesky SNS account credentials (ID, password, handle) for SNS features",
        "SQLite database path or Turso SQLite API URL and token for session data storage",
        "Optional rembg CLI or API key for background removal in image synthesis",
        "Node.js environment to run the MCP server via npx",
        "Claude Desktop or compatible MCP client for interaction",
        "Configuration of environment variables or base64 encoded JSON config for session-specific settings"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation and configuration instructions, detailed descriptions of all server functions with usage examples, supported features, limitations, and environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Virtual Traveling bot environment for MCP\n\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/073d88cc-277d-40b6-8c20-bcabf6c275e9)\n[![smithery badge](https://smithery.ai/badge/@mfukushim/map-traveler-mcp)](https://smithery.ai/server/@mfukushim/map-traveler-mcp)\n\nEnglish / [Japanese](./README_jp.md)\n\nThis is an MCP server that creates an environment for an avatar to virtually travel on Google Maps.\n\nFrom an MCP client such as Claude Desktop, you can give instructions to the avatar and report on the progress of its journey with photos.\n\n<img alt=\"img_5.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_5.png\" width=\"400\"/>\n\n> Preparing for MCP Registry Support https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/  \n\n> Added gemini-2.5-flash-image-preview (nano-banana) to travel image generation  \n\nSupport for nano-banana has been added. Nano-banana's semantic mask allows you to generate composite travel images in a short time without setting remBg.  \nAlthough conventional image synthesis is still possible, we recommend using Gemini nano-banana.  \n\n> Supports both Streamable-HTTP and stdio (compliant with Smithery.ai's config interface)  \n\nIt can be used as a stdio-type MCP as before, or as Streamable-HTTP.  \nAlthough it supports multiple users, the database API must be specified per session using the Smithery.ai config interface.  \nSince it supports both Streamable-HTTP and stdio, it is expected to work as is with the previous MCP client, but if you use the previous stdio version, please use v0.0.x (v0.0.81).  \n``` npx -y @mfukushim/map-traveler-mcp@0.0.81 ```  \n\n> Now supports librechat https://www.librechat.ai/.\n\n> Now supports Smithery https://smithery.ai/server/@mfukushim/map-traveler-mcp (images are excluded because they are heavy to run).\n\n> Now verified MseeP https://mseep.ai/app/mfukushim-map-traveler-mcp \n\n## Functions\n\n#### MCP server tools function\n\nThe following functions can be used as an MCP server.",
        "start_pos": 0,
        "end_pos": 2030,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 1,
        "text": "ause they are heavy to run).\n\n> Now verified MseeP https://mseep.ai/app/mfukushim-map-traveler-mcp \n\n## Functions\n\n#### MCP server tools function\n\nThe following functions can be used as an MCP server. The available functions vary depending on the settings and execution state.\n\nYou can specify the function name directly, but Claude LLM will automatically recognize it, so you can specify the operation in general terms.\n\nExample:\n\"Where are you now?\" \"Let's leave for Tokyo Station.\"\n\n- get_traveler_view_info(includePhoto:boolean,includeNearbyFacilities:boolean)  \n  Gets information about the current travel avatar's location.  \n  - includePhoto: Gets nearby Google Street View photos. If you have set up an image generation AI, it will synthesize the avatar.\n  - includeNearbyFacilities: Gets information about nearby facilities.\n- get_traveler_location()  \n  Gets information about the current travel avatar's address and nearby facilities.\n- reach_a_percentage_of_destination()\n  Reach a specified percentage of the destination (moveMode=skip only)\n  timeElapsedPercentage: Percent progress towards destination(0~100)\n- set_traveler_location(address: string)  \n  Sets the current travel avatar's location.\n  - address: Address information (exact address, or general name that Google Maps or Claude can recognize, etc.)\n- get_traveler_destination_address  \n  Get the destination of the travel avatar you set\n- set_traveler_destination_address(address: string)  \n  Set the destination of the travel avatar\n   - address: Address information (exact address, or general name that Google Maps or Claude can recognize, etc.)\n- start_traveler_journey  \n  Start the journey at the destination.(moveMode=realtime only)\n- stop_traveler_journey  \n  Stop the journey.(moveMode=realtime only)\n- set_traveler_info(settings:string)  \n  Set the traveler's attributes. Set the traveler's personality that you want to change dynamically, such as name and personality. However, if you use a role script, the script is more stable.",
        "start_pos": 1830,
        "end_pos": 3846,
        "token_count_estimate": 504,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 2,
        "text": "ring)  \n  Set the traveler's attributes. Set the traveler's personality that you want to change dynamically, such as name and personality. However, if you use a role script, the script is more stable.\n  - settings: Setting information such as name and personality.\n- get_traveler_info  \n  Get the traveler's attributes. Get the traveler's personality.\n- set_avatar_prompt(prompt:string)  \n  Set the prompt when generating the travel avatar image. The default is an anime-style woman. The anime style is enforced to prevent fake images.\n  - prompt\n- reset_avatar_prompt  \n  Reset avatar generation prompts to default.\n- get_sns_feeds  \n  Gets Bluesky SNS articles for the specified custom feed (feeds containing a specific tag).\n- get_sns_mentions  \n  Gets recent mentions (likes, replies) to Bluesky SNS posts that you made yourself.\n- post_sns_writer(message:string)  \n  Posts an article to Bluesky SNS with the specified custom feed. Set a specific tag so that it can be determined that the post was generated by the travel bot.\n  - message: article\n- reply_sns_writer(message:string,id:string)  \n  Reply to the article with the specified id. Set a specific tag so that it can be determined that the post was generated by the travel bot.\n  - message: reply\n  - id: The ID of the post to reply to\n- add_like(id:string)  \n  Add a like to the specified post.\n  - id: The ID of the post to like\n- tips  \n  Guides you on how to set up features that have not yet been set.\n- get_setting  \n  Get environment and image settings.\n\n#### MCP resources\n\nHas five custom prompt samples.\nWhen you import a prompt with Claude Desktop, Claude will act as a traveler.\nThe SNS-compatible version controls SNS input and output while having a travel conversation.\n\n- role.txt  \n  Claude will act as a traveler.\n\n- roleWithSns.txt  \n  Claude will act as a traveler. It also controls reading and posting to SNS.\n- carBattle.txt  \n  This is a small novel game about a story of transporting secret documents from Yokohama to Tokyo. Scenes are automatically generated.",
        "start_pos": 3646,
        "end_pos": 5691,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 3,
        "text": ". It also controls reading and posting to SNS.\n- carBattle.txt  \n  This is a small novel game about a story of transporting secret documents from Yokohama to Tokyo. Scenes are automatically generated. Set moveMode=skip to play.\n- japanMapChallenge.txt,japanMapChallenge2.txt  \n  Two AIs communicate with each other via SNS and play a challenge game using landscape images.  \n  To play, you need two Bluesky accounts and two Claude Desktops. Also set moveMode=skip. (However, the operation is somewhat unstable.)  \n  japanMapChallenge2 has a challenge reflection rule.\n\n## Setting\n\nYou will need to obtain and set access keys for multiple APIs, such as for accessing multiple Google maps and generating images.\nUse of the API may incur charges.\n\n#### Settings for using with Claude Desktop \n\n- claude_desktop_config.json (stdio type)\n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mfukushim/map-traveler-mcp\"],\n      \"env\":{\n        \"MT_GOOGLE_MAP_KEY\":\"(Google Map API key)\",\n        \"MT_GEMINI_IMAGE_KEY\": \"(Gemini Image Api key)\",\n        \"MT_MAX_RETRY_GEMINI\": \"(Number of retries when generating Gemini images Default: 0)\",\n        \"MT_AVATAR_IMAGE_URI\": \"(Character reference image uri (file:// or https://) when generating Gemini image. Multiple settings can be made by separating them with the '|'. When multiple settings are made, they will be selected randomly.)\",\n        \"MT_MAP_API_URL\": \"(Optional: Map API custom endpoint. Example: direction=https://xxxx,places=https://yyyy )\",\n        \"MT_TIME_SCALE\": \"(Optional:Scale of travel time on real roads duration. default 4)\",\n        \"MT_SQLITE_PATH\":\"(db save path: e.g.",
        "start_pos": 5491,
        "end_pos": 7169,
        "token_count_estimate": 419,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 4,
        "text": "n)\",\n        \"MT_REMBG_PATH\": \"(absolute path of the installed rembg cli)\",\n        \"MT_REMBG_URL\": \"(rembg API URL)\",\n        \"MT_REMBG_WO_KEY\": \"(withoutbg.com rembg API key)\",\n        \"MT_PIXAI_KEY\":\"(pixAi API key)\",\n        \"MT_SD_KEY\":\"(or Stability.ai image generation API key\",\n        \"MT_PIXAI_MODEL_ID\": \"(Optional: pixAi ModelId, if not set use default model 1648918127446573124 \",\n        \"MT_COMFY_URL\": \"(Option: Generate image using ComfyUI API at specified URL. Example: http://192.168.1.100:8188)\",\n        \"MT_COMFY_WORKFLOW_T2I\": \"(Optional: Path to API workflow file when using text to image with ComfyUI. If not specified: assets/comfy/t2i_sample.json)\",\n        \"MT_COMFY_WORKFLOW_I2I\": \"(Optional: Path of API workflow file when image to image in ComfyUI. If not specified: assets/comfy/i2i_sample.json)\",\n        \"MT_COMFY_PARAMS\": \"(Optional: Variable values to send to the workflow via comfyUI API)\",\n        \"MT_FIXED_MODEL_PROMPT\": \"(Optional: Fixed avatar generation prompt. You will no longer be able to change your avatar during conversations.)\",\n        \"MT_BODY_AREA_RATIO\": \"(Optional: Acceptable avatar image area ratio. default 0.042)\",\n        \"MT_BODY_HW_RATIO\": \"(Optional: Acceptable avatar image aspect ratios. default 1.5~2.3)\",\n        \"MT_BODY_WINDOW_RATIO_W\": \"(Optional: Avatar composite window horizontal ratio. default 0.5)\",\n        \"MT_BODY_WINDOW_RATIO_H\": \"(Optional: Avatar composite window aspect ratio. default 0.75)\",\n        \"MT_BS_ID\":\"(Bluesky sns registration address)\",\n        \"MT_BS_PASS\":\"(bluesky sns password)\",\n        \"MT_BS_HANDLE\":\"(bluesky sns handle name: e.g. xxxxxxxx.bsky.social )\",\n        \"MT_FILTER_TOOLS\": \"(Optional: Directly filter the tools to be used. All are available if not specified. e.g. tips,set_traveler_location)\",\n        \"MT_MOVE_MODE\": \"(Option: Specify whether the movement mode is realtime or skip.",
        "start_pos": 7339,
        "end_pos": 9234,
        "token_count_estimate": 473,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 5,
        "text": "l: Directly filter the tools to be used. All are available if not specified. e.g. tips,set_traveler_location)\",\n        \"MT_MOVE_MODE\": \"(Option: Specify whether the movement mode is realtime or skip. default realtime)\",\n        \"MT_IMAGE_WIDTH\": \"(Option: Output image width (pixels) Default is 512)\",\n        \"MT_NO_IMAGE\": \"(Options: true = do not output image, not specified = output image if possible, default is not specified)\",\n        \"MT_NO_AVATAR\": \"(Option: true = Output StreetView image as is without avatar superimposition. Not specified = Superimpose avatar image. Default is not specified.)\",\n        \"MT_FEED_TAG\": \"(Optional: Specify the feed tag when posting to SNS (#required, 15 characters or more) Default is #geo_less_traveler)\",\n        \"MT_MAX_SESSIONS\": \"(Maximum number of sessions when using Streamable-http)\",\n        \"MT_SESSION_TTL_MS\": \"(Session TTL when using Streamable-http)\",\n        \"MT_SERVICE_TTL_MS\": \"(Service TTL when using Streamable-http)\"\n      }\n    }\n  }\n}\n```  \n\n- claude_desktop_config.json (streamable-http type)  \nThe above MT_ environment variables should be set as environment variables for the server that runs the map-traveler-mcp web service.  \n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://(mcp server address)/mcp?config=(base64 config json)\"\n    }\n  }\n}\n```  \n\nbase64 config json (Smithery.ai Expansion)  \nBy concatenating the json in the following format into a single line of string, converting it to base64, and setting it as (base64 setting json), you can overwrite different APIs and settings for each user session.  \nIf the database is not set base64 config json, it will be shared across the entire service (the location of the traveler will be shared across the database and counted for one person).  \nWe plan to reconsider the operation of assigning an individual UserId for each session once the MCP authentication mechanism has become a little clearer.",
        "start_pos": 9034,
        "end_pos": 11011,
        "token_count_estimate": 494,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 6,
        "text": "ss the database and counted for one person).  \nWe plan to reconsider the operation of assigning an individual UserId for each session once the MCP authentication mechanism has become a little clearer.  \n```json\n{\n  \"MT_GOOGLE_MAP_KEY\": \"xxxyyyzzz\",\n  \"MT_GEMINI_IMAGE_KEY\": \"xxyyzz\",\n  \"MT_MAX_RETRY_GEMINI\": \"1\",\n  \"MT_AVATAR_IMAGE_URI\": \"file:///C:/Users/xxxx/Desktop/avatar.png\",\n  \"MT_TURSO_URL\": \"libsql://xxxyyyzzz\",\n  \"MT_TURSO_TOKEN\": \"abcdabcd\",\n  \"MT_BS_ID\": \"xyxyxyxyx\",\n  \"MT_BS_PASS\": \"1234xyz\",\n  \"MT_BS_HANDLE\": \"aabbccdd\",\n  \"MT_FILTER_TOOLS\": \"tips,set_traveler_location\",\n  \"MT_MOVE_MODE\": \"direct\",\n  \"MT_FEED_TAG\": \"#abcdefgabcdefgabcdefg\"\n}\n```  \n(All json values can be omitted)  \n‚Üì (json text concatenation)  \n```text\n{\"MT_GOOGLE_MAP_KEY\": \"xxxyyyzzz\", \"MT_GEMINI_IMAGE_KEY\": \"xxyyzz\", \"MT_MAX_RETRY_GEMINI\": \"1\", \"MT_TURSO_URL\": \"libsql://xxxyyyzzz\", \"MT_TURSO_TOKEN\": \"abcdabcd\", \"MT_BS_ID\": \"xyxyxyxyx\", \"MT_BS_PASS\": \"1234xyz\", \"MT_BS_HANDLE\": \"aabbccdd\", \"MT_FILTER_TOOLS\": \"tips,set_traveler_location\", \"MT_MOVE_MODE\": \"direct\", \"MT_FEED_TAG\": \"#abcdefgabcdefgabcdefg\"}\n```\n‚Üì (Set the base64 version to config=)  \n```text\neyJNVF9HT09HTEVfTUFQX0tFWSI6ICJ4eHh5eXl6enoiLCAiTVRfR0VNSU5JX0lNQUdFX0tFWSI6ICJ4eHl5enoiLCAiTVRfTUFYX1JFVFJZX0dFTUlOSSI6ICIxIiwgIk1UX1RVUlNPX1VSTCI6ICJsaWJzcWw6Ly94eHh5eXl6enoiLCAiTVRfVFVSU09fVE9LRU4iOiAiYWJjZGFiY2QiLCAiTVRfQlNfSUQiOiAieHl4eXh5eHl4IiwgIk1UX0JTX1BBU1MiOiAiMTIzNHh5eiIsICJNVF9CU19IQU5ETEUiOiAiYWFiYmNjZGQiLCAiTVRfRklMVEVSX1RPT0xTIjogInRpcHMsc2V0X3RyYXZlbGVyX2xvY2F0aW9uIiwgIk1UX01PVkVfTU9ERSI6ICJkaXJlY3QiLCAiTVRfRkVFRF9UQUciOiAiI2FiY2RlZmdhYmNkZWZnYWJjZGVmZyJ9\n```\n\n\n> NOTE: The environment variables have been renamed to standard snake case. The MT_ prefix is added because they may be used in conjunction with other environment variables, such as in librechat. The old names can still be used for backward compatibility.  \n\nPlease set the following three Credentials for Google Map API.",
        "start_pos": 10811,
        "end_pos": 12766,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 7,
        "text": "sed in conjunction with other environment variables, such as in librechat. The old names can still be used for backward compatibility.  \n\nPlease set the following three Credentials for Google Map API.  \n- Street View Static API\n- Places API (New)\n- Time Zone API\n- Directions API\n\nhttps://developers.google.com/maps/documentation/streetview/get-api-key\n\nIf you want to use the image generation AI, set either pixAi_key or sd_key. You also need to have python3.7~3.11 installed on your PC and rembg cli installed (virtual environment recommended).\n\nhttps://platform.pixai.art/docs  \nhttps://platform.stability.ai/docs/api-reference#tag/SDXL-1.0-and-SD1.6/operation/textToImage\n\nThe bluesky SNS address/password are optional. It is recommended that you create a dedicated account as it will post automatically.\n\nhttps://bsky.app/\n\nYou can also run it in practice mode, which does not require an API key for verification.\n\n#### Practice mode settings  \nclaude_desktop_config.json\n```json\n{\n  \"mcpServers\": {\n    \"traveler\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@mfukushim/map-traveler-mcp\"]\n    }\n  }\n}\n```\n\n## How to use\n\n#### Use the practice mode\n\n1. Install nodejs 22.\n\n2. Set up Claude Desktop for use.\n\n3. Reflect one of the above settings in claude_desktop_config.json.\n\n4. Restart Claude Desktop. It may take some time to set up (if an error occurs, try restarting Claude Desktop again. If it doesn't work, see the notes below). Make sure the following mark appears in the bottom right of the screen.\n\n  <img alt=\"img_1.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_1.png\" width=\"150\"/>\n\n5. Ask \"Where are you now?\" and \"Go on a journey.\" A conversation will begin. When using the API, a confirmation screen will appear, so select Allow.\n\n<img alt=\"img_4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_4.png\" width=\"200\"/>\n\n6. Select Attach from MCP and select role.txt.",
        "start_pos": 12566,
        "end_pos": 14541,
        "token_count_estimate": 493,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 8,
        "text": "ppear, so select Allow.\n\n<img alt=\"img_4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_4.png\" width=\"200\"/>\n\n6. Select Attach from MCP and select role.txt.\n\n<img alt=\"img_2.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_2.png\" width=\"200\"/>\n\n<img alt=\"img_3.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/img_3.png\" width=\"200\"/>\n\n7. A travel prompt has been built in, so feel free to talk to it.\n\n#### Use the full feature\n\n1. Get a Google Map API access key and set the permissions for Street View Static API, Places API (New), Time Zone API, and Directions API. Set this in the env of claude_desktop_config.json and restart.\n   At this point, the travel log will be based on the real map. Travel images will also be output if they are not superimposed.\n2. Decide on a path that will not interfere with the disk and set it in the sqlite_path of the env of claude_desktop_config.json. (Example: %USERPROFILE%/Desktop/traveler.sqlite $HOME/Documents/traveler.sqlite, etc.)\n   At this point, your travel log will be saved and you can continue your journey even if you close Claude Desktop.\n3. Install python 3.7 to 3.11 and install rembg with cli. We recommend using a virtual environment such as venv.\n  ```bash\n  python3 -m venv venv\n  . venv/bin/activate or .\\venv\\Scripts\\activate\n  pip install \"rembg[cpu,cli]\"\n  ```\n  Check if rembg cli works properly using a sample image file. Input an image with a person in it, and if the person is cut out in the output file, it's OK.  \n  ```bash\n  rembg i source_image_file dest_image_file\n  ```\n4. rembg cli will be installed in the python exe location, so get the path. The file location varies depending on the OS and python installation status, but in the case of venv, it is (virtual environment name)\\Scripts\\rembg.exe or (virtual environment name)/bin/rembg above the directory you set. If you can't find it, search for the path with a file search software.",
        "start_pos": 14341,
        "end_pos": 16378,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 9,
        "text": "of venv, it is (virtual environment name)\\Scripts\\rembg.exe or (virtual environment name)/bin/rembg above the directory you set. If you can't find it, search for the path with a file search software. Set that path to rembg_path of env in claude_desktop_config.json. (Example: \"rembg_path\": \"C:\\\\Users\\\\xxxx\\\\Documents\\\\rembg_venv\\\\venv\\\\Scripts\\\\rembg.exe\")\n5. Get an image generation API key from the pixAI or Stability.ai site. Set the key to pixAi_key or sd_key in env of claude_desktop_config.json.\n   The avatar will now be overlaid on the travel image.\n6. Get the bluesky SNS address/password and handle name. Set these in bs_id, bs_pass, and bs_handle in env of claude_desktop_config.json, respectively.\n   Import the travel knowledge prompt roleWithSns.txt to report travel actions to SNS (it will automatically post as a bot, so we recommend allocating a dedicated account)\n\nInstead of preparing rembg with the cli, we have added a setting that allows you to handle rembg as a service API.  \nIf you configure the following rembg service, you can use rembg by setting the URL in remBgUrl.  \n\nhttps://github.com/danielgatis/rembg?tab=readme-ov-file#rembg-s  \n\nSetup is simple if you use the Docker version to launch a container and access it.  \n\nhttps://github.com/danielgatis/rembg?tab=readme-ov-file#usage-as-a-docker  \n\n#### Use Turso libsql API for configuration database\n\nIf you want to use the cloud API Turso libsql (https://turso.tech/libsql) without having a local sqlite file, sign up for Turso and allocate a sqlite database (paid, free tier available).   \nThis add-in will automatically configure (migrate) the database.  \nMT_TURSO_URL = turso db URL  \nMT_TURSO_TOKEN = turso db access token  \n\n\n#### Use Cloud API for rembg\n\nLocal settings around rembg are complicated no matter what method you use, but we have added settings for the paid cloud rembg (https://withoutbg.com/).",
        "start_pos": 16178,
        "end_pos": 18076,
        "token_count_estimate": 474,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 10,
        "text": "access token  \n\n\n#### Use Cloud API for rembg\n\nLocal settings around rembg are complicated no matter what method you use, but we have added settings for the paid cloud rembg (https://withoutbg.com/).  \n> Note: There is a small free trial available, but please be aware that this is a commercial API and is quite expensive (about 0.1 euros per image).\n\nMT_REMBG_WO_KEY = withoutbg access token\n\n\n#### When using external ComfyUI (for more advanced users)\n\nYou can also use a local ComfyUI as an image generation server. You can configure the image generation characteristics yourself in detail to reduce API costs.\n\nHowever, the configuration will be quite complicated and image generation may take longer.\n\n1. Configure ComfyUI to run in API mode.\n2. Set the server URL to comfy_url in env.\n3. Set detailed configuration values such as the model to be used in env in the form of a json string.\nexample.\n```json\n{\n  \"env\": {\n    \"comfy_url\": \"http://192.168.1.100:8188\",\n    \"comfy_workflow_t2i\": \"C:\\\\Documents\\\\t2itest.json\",\n    \"comfy_workflow_i2i\":\"C:\\\\Documents\\\\i2itest.json\",\n    \"comfy_params\":\"ckpt_name='animagineXL40_v40.safetensors',denoise=0.65\"\n  }\n}\n```\n4. The default workflow can use assets/comfy/t2i_sample.json and assets/comfy/i2i_sample.json in the package. You can specify variables using % and specify the variables in comfy_params.\n\n## Using libreChat\n\nIt has been adapted to work with libreChat. This makes it easier to use, but some additional settings are required.  \nAlso, it seems that it will not be stable unless the PC you use has a decent level of performance, such as one that can stably run Docker.\n\n#### Install libreChat  \n\nPlease make sure it works as described on the official website.  \nIn this case, we recommend using Docker configuration due to additional settings.\n\nhttps://www.librechat.ai/docs/local/docker  \n\nConfigure librechat.yaml using the official procedure.  \nI think you will need to add a local or API LLM service.",
        "start_pos": 17876,
        "end_pos": 19846,
        "token_count_estimate": 492,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 11,
        "text": "nfiguration due to additional settings.\n\nhttps://www.librechat.ai/docs/local/docker  \n\nConfigure librechat.yaml using the official procedure.  \nI think you will need to add a local or API LLM service.  \n\nhttps://www.librechat.ai/docs/configuration/librechat_yaml  \n\nAdd a user for login.  \n\nhttps://www.librechat.ai/docs/configuration/authentication#create-user-script  \n\nPlease set it so that you can have general chat conversations.  \n\n#### Add a rembg container with additional settings  \n\nTo use rembg with Docker, add pulling and running the rembg Docker container.  \n\ndocker-compose.override.yml\n```yml\n services:\n   api:\n     volumes:\n       - type: bind\n         source: ./librechat.yaml\n         target: /app/librechat.yaml\n\n   rembg:\n     image: danielgatis/rembg:latest\n     restart: always\n     command: \"s --host 0.0.0.0 --port 7000 --log_level info\"\n\n```\n\n#### Add map-traveler-mcp to the MCP service  \n\nAdd librechat.yaml\n```yaml\nmcpServers:\n  traveler:\n    type: stdio\n    command: npx\n    args:\n      - -y\n      - \"@mfukushim/map-traveler-mcp\"\n```\n\nAdd .env (Same as env in claude_desktop_config.json)\n\n```env\n# map-traveler-mcp\nGoogleMapApi_key=(Google Map API key)\nsqlite_path=/home/run_test.sqlite (e.g. librechat in an unobtrusive location inside the container, or in an external directory that you don't want to mount.)\nremBgUrl=http://rembg:7000 (rembg Service API URL, container URL)\n(Other settings such as image generation AI settings, PixAI key, stability.ai API key, ComfyUI settings, etc.)\n\n```\n\nAfter setting, restart the container.  \nOn slow PCs, mcp initialization may fail. Multiple restarts may work, but this may be difficult to run...\n\n#### llibreChat settings\n\nTo use the MCP function in libreChat, use the Agents function.  \n\n1. On the conversation screen, select Agents.  \n   <img alt=\"libre1.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre1.png\" width=\"200\"/>\n2. Select Agent Builder from the panel on the right side of the screen and configure your agent.",
        "start_pos": 19646,
        "end_pos": 21687,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 12,
        "text": "\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre1.png\" width=\"200\"/>\n2. Select Agent Builder from the panel on the right side of the screen and configure your agent.  \n   <img alt=\"libre2.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre2.png\" width=\"200\"/>\n3. Select Add Tools to use map-traveler.  \n   <img alt=\"libre3.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre3.png\" width=\"200\"/>\n4. The agent tools screen will appear, so select and add all the map-traveler-mcp tools (if the map-traveler-mcp tools are not listed, MCP initialization has failed, so please restart the container or review the settings by checking the logs, etc.)  \n   <img alt=\"libre4.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre4.png\" width=\"200\"/>  \n   <img alt=\"libre5.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre5.png\" width=\"200\"/>  \n5. Enter additional script in the instruction area.  \n   Since libreChat does not have the MCP resource function, enter the content text of the following URL into the instruction area instead.   \n   https://github.com/mfukushim/map-traveler-mcp/blob/main/assets/scenario/role.txt  \n   <img alt=\"libre7.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre7.png\" width=\"200\"/>  \n6. Click the Create button to save the agent.  \n   <img alt=\"libre6.png\" src=\"https://raw.githubusercontent.com/mfukushim/map-traveler-mcp/for_image/tools/libre6.png\" width=\"200\"/>\n7. Start a new chat.\n\n## Smithery\n\nPlease refer to https://smithery.ai/server/@mfukushim/map-traveler-mcp.  \nRemote MCP (Streamable-http mode) is supported. Image generation is only available on nano-banana.  \nDatabase settings can now be recorded with Turso sqlite, so if you configure Turso, your travel progress will also be saved.",
        "start_pos": 21487,
        "end_pos": 23452,
        "token_count_estimate": 491,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 13,
        "text": "tp mode) is supported. Image generation is only available on nano-banana.  \nDatabase settings can now be recorded with Turso sqlite, so if you configure Turso, your travel progress will also be saved.  \n<img alt=\"smithery.png\" src=\"tools/smithery.png\" width=\"400\"/>\n\n\n\n## Install guide (Japanese, but lots of photos)\n\n1. introduction and Practice mode  \n   https://note.com/marble_walkers/n/n7a8f79e4fb30\n2. DB, Google Map API, Image gen API  \n   https://note.com/marble_walkers/n/n765257c27f3b\n3. Avatar prompt  \n   https://note.com/marble_walkers/n/nc7273724faea\n4. SNS integration  \n   https://note.com/marble_walkers/n/na7c956befe7b\n5. Application 1  \n   https://note.com/marble_walkers/n/n3c86edd8e817\n6. ComfyUI API  \n   https://note.com/marble_walkers/n/ncefc7c05d102  \n7. Application 2  \n   https://note.com/marble_walkers/n/ne7584ed231c8\n8. LibreChat setting  \n   https://note.com/marble_walkers/n/n339bf7905324\n9. AI Agent SNS Battle Map Challenge  \n   https://note.com/marble_walkers/n/n6db937573eaa\n10. Support Smithery, Turso libSQL, and rembg API   \n   https://note.com/marble_walkers/n/ne3b3c0f99707\n11. Streamable-HTTP support  \n    https://note.com/marble_walkers/n/n030063f22dc0\n12. Nano-Banana support  \n    https://note.com/marble_walkers/n/n5d49514dddec  \n\n\n#### Additional about the source code\n\nI use Effect.ts to simplify error management & for my own learning.  \nWe also use the Effect Service, but due to the way MCP calls work, we believe that consolidating it using the Service was not optimal.  \nI think it would be simpler to handle the MCP calls directly in the Effect.  \nAddendum: I'm aware that I will be able to reconsider how to use the Effect Service and rewrite it neatly, but I'm still considering whether to rewrite it.  \n\n#### Notes on the latest updates\n\n- Added image_width to env. The default is 512. Setting it smaller may reduce the cost of LLM API.  \n- Added an env setting that does not output images for MCP clients that do not have image input/output.",
        "start_pos": 23252,
        "end_pos": 25252,
        "token_count_estimate": 500,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 14,
        "text": "ded image_width to env. The default is 512. Setting it smaller may reduce the cost of LLM API.  \n- Added an env setting that does not output images for MCP clients that do not have image input/output.  \n\"MT_NO_IMAGE\": \"true\" will not generate or output any images. Other image-related settings can be omitted.  \n```\n{\n  \n  \"env\": {\n    \"MT_NO_IMAGE\": \"true\"\n  }\n  \n}\nor\n{\n  \n  \"env\": {\n    \"GoogleMapApi_key\": \"xxxx\",\n    \"MT_NO_IMAGE\": \"true\"\n  }\n  \n}\n\n```  \n- You can now specify the tag name to be added when posting to SNS (Bluesky). #Required and must be at least 15 characters. If not specified, it will become \"#geo_less_traveler\".  \n- The information obtained from SNS has been slightly changed. The information posted to SNS has been slightly changed.  \n- A script has been added that allows multiple travel bots to converse and play via SNS.  \n\n- Supports remote use from Smithery.  \n  If you do not want to configure detailed settings, start the app in practice mode.\n  You can also run the app at full speed by configuring each cloud API, but please be aware of charges as it uses many paid APIs such as rembg API.\n  If you do not want to synthesize avatars, you can run the app with the minimum settings of Google Map API and Turso sqlite API.\n\n- Added the MT_NO_AVATAR option.  \n  If set, an avatar image will not be composited onto the landscape image. Since there will be no retry processing for avatar composition, the time it takes to obtain a response will be significantly shorter.  \n  Set this option if image composition is slow or fails unavoidably.\n\n- Partially applied MCP version 2025-06-18.  \n  I added title to the schema. I plan to apply outputSchema and structured response in the future, but I haven't implemented them this time. Since the output of Travel Bot is simple text, I don't think structuring is necessary yet.",
        "start_pos": 25052,
        "end_pos": 26903,
        "token_count_estimate": 462,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      },
      {
        "chunk_id": 15,
        "text": "I plan to apply outputSchema and structured response in the future, but I haven't implemented them this time. Since the output of Travel Bot is simple text, I don't think structuring is necessary yet.  \n  https://modelcontextprotocol.io/specification/2025-06-18/server/tools  \n- Fixed an issue where some functions, such as SNS functions, could not be called regardless of the env settings due to an initialization error.  \n\n- Added support for Streamable-http. This was done in a hurry, so if you experience any issues, please consider using version 0.0.81 or similar.  \n\n- Support for nano-banana (gemini-2.5-flash-image-preview) image generation has been added. When using nano-banana, no rembg settings are required. The characteristics of the avatar prompt have changed, so image generation may fail with the previous avatar prompt. In this case, you will need to adjust the avatar appearance prompt to one that is acceptable for nano-banana.\n\n- When generating images for nano-banana, you can now reference the original character image with MT_AVATAR_IMAGE_URI. Please use it in a way that does not infringe on copyrights.\n\n[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/mfukushim-map-traveler-mcp-badge.png)](https://mseep.ai/app/mfukushim-map-traveler-mcp)",
        "start_pos": 26703,
        "end_pos": 27981,
        "token_count_estimate": 319,
        "source_type": "readme",
        "agent_id": "8758ba4f44818986"
      }
    ]
  },
  {
    "agent_id": "ca1b8080c15cbb56",
    "name": "ai.smithery/miguelgarzons-mcp-cun",
    "source": "mcp",
    "source_url": "https://github.com/miguelgarzons/mcp-cun",
    "description": "Greet people by name with friendly, personalized messages. Add a warm touch to onboarding, demos,‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T03:30:01.02979Z",
    "indexed_at": "2026-02-18T04:07:54.971085",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Character Counter MCP Server - Python Quickstart\n\nA simple example of creating an MCP server using FastMCP and Python, designed to work with Smithery.\n\n## What This Does\n\nThis server provides a character counter tool called `count_character` that counts how many times a specific character appears in a given text. You'll test it using the Smithery Playground for interactive development.\n\n## Prerequisites\n\n- Python 3.12 or higher\n- A Python package manager ([uv](https://docs.astral.sh/uv/) recommended, but pip, poetry, etc. also work)\n- Node.js and npx (optional, for Smithery Playground)\n\n## Quick Start\n\n1. **Clone the repository:**\n   ```bash\n   git clone https://github.com/smithery-ai/smithery-cookbook.git\n   cd smithery-cookbook/servers/python/quickstart\n   ```\n\n2. **Install dependencies:**\n   \n   **With uv (recommended):**\n   ```bash\n   uv sync\n   ```\n   \n   **With poetry:**\n   ```bash\n   poetry install\n   ```\n   \n   **With pip:**\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Run the server:**\n   \n   You have two options:\n   \n   **Option A: Just run the server**\n   ```bash\n   # With uv\n   uv run smithery dev\n   # or use the shorter script alias:\n   uv run dev\n   \n   # With poetry\n   poetry run smithery dev\n   # or use the shorter script alias:\n   poetry run dev\n   \n   # With pip (after installing dependencies)\n   smithery dev\n   ```\n   This starts the MCP server on `http://localhost:8081` and keeps it running.\n   \n   **Option B: Run server + open playground (recommended for testing)**\n   ```bash\n   # With uv\n   uv run smithery playground\n   # or use the shorter script alias:\n   uv run playground\n   \n   # With poetry\n   poetry run smithery playground\n   # or use the shorter script alias:\n   poetry run playground\n   \n   # With pip (after installing dependencies)\n   smithery playground\n   ```\n   This starts the MCP server AND automatically opens the Smithery Playground in your browser where you can:\n   - Interact with your MCP server in real-time\n   - Test the `count_character` tool with different text and characters\n   - See the complete request/response flow\n   - Debug and iterate on your MCP tools quickly\n\n## Testing the Character Counter\n\nTry asking: **\"How many r's are in strawberry?\"**\n\n4. **Deploy to Smithery:**\n   To deploy your MCP server:\n   - Push your code to GitHub (make sure to include the `smithery.yaml`)\n   - Connect your repository at [https://smithery.ai/new](https://smithery.ai/new)\n\nYour server will be available over HTTP and ready to use with any MCP-compatible client!\n\n## Stopping the Server\n\nPress `Ctrl+C` in the terminal to stop the server.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Count how many times a specific character appears in a given text",
        "Run an MCP server locally on http://localhost:8081",
        "Provide a character counting tool named count_character",
        "Allow interactive testing and debugging via Smithery Playground",
        "Deploy the MCP server to Smithery platform for public HTTP access"
      ],
      "limitations": [
        "Only counts occurrences of single characters in text",
        "No mention of handling multi-character substrings or complex text analysis",
        "Requires local environment setup with Python 3.12+ and optionally Node.js for playground",
        "No stated support for authentication or rate limiting"
      ],
      "requirements": [
        "Python 3.12 or higher",
        "Python package manager such as uv, poetry, or pip",
        "Node.js and npx (optional, for Smithery Playground)",
        "Git for cloning the repository",
        "Smithery account and repository connection for deployment"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, usage examples, tool description, deployment steps, and environment prerequisites, making it comprehensive and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Character Counter MCP Server - Python Quickstart\n\nA simple example of creating an MCP server using FastMCP and Python, designed to work with Smithery.\n\n## What This Does\n\nThis server provides a character counter tool called `count_character` that counts how many times a specific character appears in a given text. You'll test it using the Smithery Playground for interactive development.\n\n## Prerequisites\n\n- Python 3.12 or higher\n- A Python package manager ([uv](https://docs.astral.sh/uv/) recommended, but pip, poetry, etc. also work)\n- Node.js and npx (optional, for Smithery Playground)\n\n## Quick Start\n\n1. **Clone the repository:**\n   ```bash\n   git clone https://github.com/smithery-ai/smithery-cookbook.git\n   cd smithery-cookbook/servers/python/quickstart\n   ```\n\n2. **Install dependencies:**\n   \n   **With uv (recommended):**\n   ```bash\n   uv sync\n   ```\n   \n   **With poetry:**\n   ```bash\n   poetry install\n   ```\n   \n   **With pip:**\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. **Run the server:**\n   \n   You have two options:\n   \n   **Option A: Just run the server**\n   ```bash\n   # With uv\n   uv run smithery dev\n   # or use the shorter script alias:\n   uv run dev\n   \n   # With poetry\n   poetry run smithery dev\n   # or use the shorter script alias:\n   poetry run dev\n   \n   # With pip (after installing dependencies)\n   smithery dev\n   ```\n   This starts the MCP server on `http://localhost:8081` and keeps it running.",
        "start_pos": 0,
        "end_pos": 1450,
        "token_count_estimate": 362,
        "source_type": "readme",
        "agent_id": "ca1b8080c15cbb56"
      },
      {
        "chunk_id": 1,
        "text": "arts the MCP server AND automatically opens the Smithery Playground in your browser where you can:\n   - Interact with your MCP server in real-time\n   - Test the `count_character` tool with different text and characters\n   - See the complete request/response flow\n   - Debug and iterate on your MCP tools quickly\n\n## Testing the Character Counter\n\nTry asking: **\"How many r's are in strawberry?\"**\n\n4. **Deploy to Smithery:**\n   To deploy your MCP server:\n   - Push your code to GitHub (make sure to include the `smithery.yaml`)\n   - Connect your repository at [https://smithery.ai/new](https://smithery.ai/new)\n\nYour server will be available over HTTP and ready to use with any MCP-compatible client!\n\n## Stopping the Server\n\nPress `Ctrl+C` in the terminal to stop the server.",
        "start_pos": 1848,
        "end_pos": 2625,
        "token_count_estimate": 194,
        "source_type": "readme",
        "agent_id": "ca1b8080c15cbb56"
      }
    ]
  },
  {
    "agent_id": "46728c6749e901c3",
    "name": "ai.smithery/minionszyw-bazi",
    "source": "mcp",
    "source_url": "https://github.com/minionszyw/bazi",
    "description": "Generate BaZi charts from birth details. Explore Four Pillars, solar terms, and Luck Pillars for d‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-18T08:13:10.601278Z",
    "indexed_at": "2026-02-18T04:07:56.633598",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# ÂÖ´Â≠óÊéíÁõòÂ∑•ÂÖ∑\n\nËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éPythonÁöÑÂÖ´Â≠óËæÖÂä©Â∑•ÂÖ∑ÔºåÂèØ‰ª•ËÆ°ÁÆóÂÖ´Â≠ó„ÄÅËäÇÊ∞î‰ø°ÊÅØÂíåÂ§ßËøê‰ø°ÊÅØ„ÄÇ\n\n## ÂäüËÉΩÁâπÊÄß\n\n- ÂÖ´Â≠óËÆ°ÁÆóÔºàÂπ¥Êü±„ÄÅÊúàÊü±„ÄÅÊó•Êü±„ÄÅÊó∂Êü±Ôºâ\n- ÁúüÂ§™Èò≥Êó∂Ê†°Ê≠£\n- ËäÇÊ∞î‰ø°ÊÅØËÆ°ÁÆó\n- Â§ßËøê‰ø°ÊÅØËÆ°ÁÆó\n- ÂÜúÂéÜËΩ¨Êç¢\n- ÂÜúÂéÜÂá∫ÁîüÊó∂Èó¥ÊòæÁ§∫‰∏∫ÁúüÂ§™Èò≥Êó∂Ê†°Ê≠£ÂêéÁöÑÊó∂Èó¥\n\n## ÂÆâË£Ö‰æùËµñ\n```bash\npip install -r requirements.txt\n```\n\n### Áõ¥Êé•ËøêË°å\n```bash\npython mcp_server.py\n```\n\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÊúçÂä°Â∞ÜÂú® `http://localhost:8001/mcp` ‰∏äËøêË°å„ÄÇ\n\n## ÂêØÂä®MCPÊúçÂä°\n\n### ‰ΩøÁî®DockerÔºàÊé®ËçêÔºâ\n```bash\ndocker-compose up --build\n```\n\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÊúçÂä°Â∞ÜÂú® `http://localhost:8001/mcp` ‰∏äËøêË°å„ÄÇ\n\nÂ¶ÇÊûú‰ªéÂÆø‰∏ªÊú∫ËÆøÈóÆDockerÂÆπÂô®‰∏≠ÁöÑÊúçÂä°Ôºå‰ΩøÁî®‰ª•‰∏ãÂú∞ÂùÄÔºö\n- Windows/Linux/macOS: `http://host.docker.internal:8001/mcp`\n\n### MCPÂÆ¢Êà∑Á´ØË∞ÉÁî®\n\nMCPÊúçÂä°Êèê‰æõ‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ `bazi` ÁöÑÂ∑•ÂÖ∑ÔºåÂèØ‰ª•ËÆ°ÁÆóÂÖ´Â≠ó‰ø°ÊÅØ„ÄÇËØ•Â∑•ÂÖ∑Êé•Âèó‰ª•‰∏ãÂèÇÊï∞Ôºö\n\n- `name` (str): ÂßìÂêç\n- `gender` (str): ÊÄßÂà´ ('Áî∑' Êàñ 'Â•≥')\n- `calendar` (str): Êó•ÂéÜÁ±ªÂûã ('ÂÖ¨ÂéÜ' Êàñ 'ÂÜúÂéÜ')\n- `year` (int): Âá∫ÁîüÂπ¥‰ªΩ\n- `month` (int): Âá∫ÁîüÊúà‰ªΩ\n- `day` (int): Âá∫ÁîüÊó•Êúü\n- `hour` (int): Âá∫ÁîüÂ∞èÊó∂ (0-23)\n- `minute` (int): Âá∫ÁîüÂàÜÈíü (0-59)\n- `birth_city` (str): Âá∫ÁîüÂüéÂ∏Ç\n- `current_city` (str, ÂèØÈÄâ): ÂΩìÂâçÂ±Ö‰ΩèÂüéÂ∏Ç\n\n## È°πÁõÆÁªìÊûÑ\n\n```\nbazi/ \n‚îú‚îÄ‚îÄ mcp_server.py         # MCPÊúçÂä°Á´Ø\n‚îú‚îÄ‚îÄ bazi_tool.py          # ÂÖ´Â≠óËÆ°ÁÆóÂ∑•ÂÖ∑\n‚îú‚îÄ‚îÄ query_longitude.py    # Êü•ËØ¢ÁªèÂ∫¶\n‚îú‚îÄ‚îÄ region.json           # ÂüéÂ∏ÇÁªèÁ∫¨Â∫¶Êï∞ÊçÆ   \n‚îú‚îÄ‚îÄ README.md             # È°πÁõÆËØ¥Êòé\n\n```\n\n## Á§∫‰æãËæìÂá∫\n\n```\n=== Âº†‰∏âÁöÑÂÖ´Â≠ó‰ø°ÊÅØ ===\nÂßìÂêçÔºöÂº†‰∏â\nÊÄßÂà´ÔºöÁî∑ (‰πæÈÄ†)\nÂá∫ÁîüÊó∂Èó¥Ôºö\n  ÂÖ¨ÂéÜÔºö[Âπ¥‰ªΩ]Âπ¥[Êúà‰ªΩ]Êúà[Êó•Êúü]Êó•[Â∞èÊó∂]:[ÂàÜÈíü]\n  ÂÜúÂéÜÔºö[Âπ¥‰ªΩ]Âπ¥[Êúà‰ªΩ]Êúà[Êó•Êúü]Êó•[Â∞èÊó∂]:[ÂàÜÈíü] (ÈùûÈó∞Êúà)\nÂá∫ÁîüÂüéÂ∏ÇÔºö[ÂüéÂ∏ÇÂêç]([ÁªèÂ∫¶]¬∞E)\nÁé∞Â±ÖÂüéÂ∏ÇÔºö[ÂüéÂ∏ÇÂêç]\nÂÖ´Â≠ó‰ø°ÊÅØÔºö[Âπ¥Êü±] [ÊúàÊü±] [Êó•Êü±] [Êó∂Êü±]\nËäÇÊ∞î‰ø°ÊÅØÔºöÁîü‰∫é[ËäÇÊ∞î1]ËäÇÊ∞îÂêé[Â§©Êï∞]Â§©Ôºå[ËäÇÊ∞î2]ËäÇÊ∞îÂâç[Â§©Êï∞]Â§©\nÂ§ßËøê‰ø°ÊÅØÔºö[Èò¥Èò≥]Áî∑Ôºå[È°∫Êéí/ÈÄÜÊéí]ÔºåËµ∑ËøêÊó∂Èó¥[Âπ¥Êï∞]Âπ¥[ÊúàÊï∞]ÊúàÔºå[Ëµ∑ËøêÂπ¥ÈæÑ]Â≤ÅËµ∑Ëøê\n```"
    },
    "llm_extracted": {
      "capabilities": [
        "Calculate BaZi (Four Pillars) including year, month, day, and hour pillars",
        "Perform true solar time correction",
        "Compute solar term (Jieqi) information",
        "Calculate DaYun (major luck cycle) information",
        "Convert Gregorian calendar dates to Lunar calendar",
        "Display birth time corrected by true solar time",
        "Provide MCP server interface for BaZi calculations",
        "Accept detailed birth data including name, gender, calendar type, birth date and time, birth city, and optionally current city"
      ],
      "limitations": [],
      "requirements": [
        "Python environment with dependencies installed via requirements.txt",
        "Optional Docker environment for containerized deployment",
        "Access to city longitude data from region.json for accurate calculations"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, usage examples, detailed tool descriptions, project structure, and parameter explanations, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# ÂÖ´Â≠óÊéíÁõòÂ∑•ÂÖ∑\n\nËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éPythonÁöÑÂÖ´Â≠óËæÖÂä©Â∑•ÂÖ∑ÔºåÂèØ‰ª•ËÆ°ÁÆóÂÖ´Â≠ó„ÄÅËäÇÊ∞î‰ø°ÊÅØÂíåÂ§ßËøê‰ø°ÊÅØ„ÄÇ\n\n## ÂäüËÉΩÁâπÊÄß\n\n- ÂÖ´Â≠óËÆ°ÁÆóÔºàÂπ¥Êü±„ÄÅÊúàÊü±„ÄÅÊó•Êü±„ÄÅÊó∂Êü±Ôºâ\n- ÁúüÂ§™Èò≥Êó∂Ê†°Ê≠£\n- ËäÇÊ∞î‰ø°ÊÅØËÆ°ÁÆó\n- Â§ßËøê‰ø°ÊÅØËÆ°ÁÆó\n- ÂÜúÂéÜËΩ¨Êç¢\n- ÂÜúÂéÜÂá∫ÁîüÊó∂Èó¥ÊòæÁ§∫‰∏∫ÁúüÂ§™Èò≥Êó∂Ê†°Ê≠£ÂêéÁöÑÊó∂Èó¥\n\n## ÂÆâË£Ö‰æùËµñ\n```bash\npip install -r requirements.txt\n```\n\n### Áõ¥Êé•ËøêË°å\n```bash\npython mcp_server.py\n```\n\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÊúçÂä°Â∞ÜÂú® `http://localhost:8001/mcp` ‰∏äËøêË°å„ÄÇ\n\n## ÂêØÂä®MCPÊúçÂä°\n\n### ‰ΩøÁî®DockerÔºàÊé®ËçêÔºâ\n```bash\ndocker-compose up --build\n```\n\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåÊúçÂä°Â∞ÜÂú® `http://localhost:8001/mcp` ‰∏äËøêË°å„ÄÇ\n\nÂ¶ÇÊûú‰ªéÂÆø‰∏ªÊú∫ËÆøÈóÆDockerÂÆπÂô®‰∏≠ÁöÑÊúçÂä°Ôºå‰ΩøÁî®‰ª•‰∏ãÂú∞ÂùÄÔºö\n- Windows/Linux/macOS: `http://host.docker.internal:8001/mcp`\n\n### MCPÂÆ¢Êà∑Á´ØË∞ÉÁî®\n\nMCPÊúçÂä°Êèê‰æõ‰∫Ü‰∏Ä‰∏™Âêç‰∏∫ `bazi` ÁöÑÂ∑•ÂÖ∑ÔºåÂèØ‰ª•ËÆ°ÁÆóÂÖ´Â≠ó‰ø°ÊÅØ„ÄÇËØ•Â∑•ÂÖ∑Êé•Âèó‰ª•‰∏ãÂèÇÊï∞Ôºö\n\n- `name` (str): ÂßìÂêç\n- `gender` (str): ÊÄßÂà´ ('Áî∑' Êàñ 'Â•≥')\n- `calendar` (str): Êó•ÂéÜÁ±ªÂûã ('ÂÖ¨ÂéÜ' Êàñ 'ÂÜúÂéÜ')\n- `year` (int): Âá∫ÁîüÂπ¥‰ªΩ\n- `month` (int): Âá∫ÁîüÊúà‰ªΩ\n- `day` (int): Âá∫ÁîüÊó•Êúü\n- `hour` (int): Âá∫ÁîüÂ∞èÊó∂ (0-23)\n- `minute` (int): Âá∫ÁîüÂàÜÈíü (0-59)\n- `birth_city` (str): Âá∫ÁîüÂüéÂ∏Ç\n- `current_city` (str, ÂèØÈÄâ): ÂΩìÂâçÂ±Ö‰ΩèÂüéÂ∏Ç\n\n## È°πÁõÆÁªìÊûÑ\n\n```\nbazi/ \n‚îú‚îÄ‚îÄ mcp_server.py         # MCPÊúçÂä°Á´Ø\n‚îú‚îÄ‚îÄ bazi_tool.py          # ÂÖ´Â≠óËÆ°ÁÆóÂ∑•ÂÖ∑\n‚îú‚îÄ‚îÄ query_longitude.py    # Êü•ËØ¢ÁªèÂ∫¶\n‚îú‚îÄ‚îÄ region.json           # ÂüéÂ∏ÇÁªèÁ∫¨Â∫¶Êï∞ÊçÆ   \n‚îú‚îÄ‚îÄ README.md             # È°πÁõÆËØ¥Êòé\n\n```\n\n## Á§∫‰æãËæìÂá∫\n\n```\n=== Âº†‰∏âÁöÑÂÖ´Â≠ó‰ø°ÊÅØ ===\nÂßìÂêçÔºöÂº†‰∏â\nÊÄßÂà´ÔºöÁî∑ (‰πæÈÄ†)\nÂá∫ÁîüÊó∂Èó¥Ôºö\n  ÂÖ¨ÂéÜÔºö[Âπ¥‰ªΩ]Âπ¥[Êúà‰ªΩ]Êúà[Êó•Êúü]Êó•[Â∞èÊó∂]:[ÂàÜÈíü]\n  ÂÜúÂéÜÔºö[Âπ¥‰ªΩ]Âπ¥[Êúà‰ªΩ]Êúà[Êó•Êúü]Êó•[Â∞èÊó∂]:[ÂàÜÈíü] (ÈùûÈó∞Êúà)\nÂá∫ÁîüÂüéÂ∏ÇÔºö[ÂüéÂ∏ÇÂêç]([ÁªèÂ∫¶]¬∞E)\nÁé∞Â±ÖÂüéÂ∏ÇÔºö[ÂüéÂ∏ÇÂêç]\nÂÖ´Â≠ó‰ø°ÊÅØÔºö[Âπ¥Êü±] [ÊúàÊü±] [Êó•Êü±] [Êó∂Êü±]\nËäÇÊ∞î‰ø°ÊÅØÔºöÁîü‰∫é[ËäÇÊ∞î1]ËäÇÊ∞îÂêé[Â§©Êï∞]Â§©Ôºå[ËäÇÊ∞î2]ËäÇÊ∞îÂâç[Â§©Êï∞]Â§©\nÂ§ßËøê‰ø°ÊÅØÔºö[Èò¥Èò≥]Áî∑Ôºå[È°∫Êéí/ÈÄÜÊéí]ÔºåËµ∑ËøêÊó∂Èó¥[Âπ¥Êï∞]Âπ¥[ÊúàÊï∞]ÊúàÔºå[Ëµ∑ËøêÂπ¥ÈæÑ]Â≤ÅËµ∑Ëøê\n```",
        "start_pos": 0,
        "end_pos": 1270,
        "token_count_estimate": 317,
        "source_type": "readme",
        "agent_id": "46728c6749e901c3"
      }
    ]
  },
  {
    "agent_id": "8332f637baf87f2b",
    "name": "ai.smithery/mjucius-cozi_mcp",
    "source": "mcp",
    "source_url": "https://github.com/mjucius/cozi_mcp",
    "description": "Manage your family's calendars and lists in Cozi. View, create, and update appointments; organize‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage",
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-13T23:46:02.266315Z",
    "indexed_at": "2026-02-18T04:07:59.924432",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Cozi MCP Server\n\nAn unofficial Model Context Protocol (MCP) server that provides AI assistants like Claude Desktop with access to [Cozi Family Organizer](https://www.cozi.com/) functionality. This server exposes Cozi's lists, calendar, and family management features through a standardized MCP interface so you can ask your AI to manage events and lists for you.\n\nüöÄ **Now deployable on [Smithery.ai](https://smithery.ai)** - Deploy this MCP server to the cloud with secure credential management!\n\n## Features\n\n### Family Management\n- Get family members and their information\n\n### List Management  \n- View all lists (shopping and todo lists)\n- Filter lists by type\n- Create and delete lists\n\n### Item Management\n- Add items to lists\n- Update item text\n- Mark items as complete/incomplete\n- Remove items from lists\n\n### Calendar Management\n- View appointments for any month\n- Create new appointments\n- Update existing appointments\n- Delete appointments\n\n## Installation\n\n### Using Smithery.ai (Recommended)\n\nThe easiest way to use this MCP server is through Smithery.ai:\n\n**üöÄ [Deploy on Smithery.ai](https://smithery.ai/server/@mjucius/cozi_mcp)**\n\nVisit the server page for complete installation instructions and one-click deployment to your AI assistant.\n\n### Local Development\n\nFor developers who want to modify or contribute to the project:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/mjucius/cozi-mcp.git\ncd cozi-mcp\n```\n\n2. Install dependencies:\n```bash\nuv sync\n```\n\n3. Start the development playground:\n```bash\nuv run playground\n```\n\n## Usage\n\n### Cloud Deployment (Smithery.ai)\n\nOnce deployed on Smithery.ai, your MCP server runs in the cloud and can be accessed by any MCP-compatible AI assistant using the provided endpoint URL.\n\n### Local Development & Testing\n\nTest the server locally with the interactive playground:\n```bash\n# Start the interactive playground\nuv run playground\n\n# Or start development server\nuv run dev\n```\n\nThe playground provides a web interface to test all MCP tools with real-time responses and debugging information.\n\n### Integration with AI Assistants\n\nThe easiest way to integrate this MCP server is through the [Smithery.ai server page](https://smithery.ai/server/@mjucius/cozi_mcp), which provides step-by-step instructions for your specific AI assistant.\n\nFor advanced users doing local development, the server can be run locally using the stdio interface.\n\n## Development\n\n### Requirements\n- Python 3.10+\n- Cozi Family Organizer account\n- uv (recommended) or pip\n\n### Dependencies\n- `mcp>=1.0.0` - Model Context Protocol framework\n- `py-cozi-client>=1.3.0` - Cozi API client library\n- `smithery` - Smithery.ai deployment framework\n\n### Development Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/cozi-mcp.git\ncd cozi-mcp\n```\n\n2. Install dependencies:\n```bash\n# With uv (recommended)\nuv sync\n\n# Or with pip\npip install -e .\n```\n\n3. Start the development playground:\n```bash\nuv run playground\n```\n\n### Project Structure\n\n```\ncozi-mcp/\n‚îú‚îÄ‚îÄ smithery.yaml              # Smithery.ai deployment config\n‚îú‚îÄ‚îÄ pyproject.toml             # Project dependencies and metadata  \n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ cozi_mcp/\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py       # Package exports\n‚îÇ       ‚îî‚îÄ‚îÄ server.py         # MCP server implementation\n‚îî‚îÄ‚îÄ [other files...]\n```\n\n## Available MCP Tools\n\nThe server exposes these tools for AI assistants:\n\n### Family Management\n- `get_family_members` - Get all family members in the account\n\n### List Management  \n- `get_lists` - Get all lists (shopping and todo)\n- `get_lists_by_type` - Filter lists by type (shopping/todo)  \n- `create_list` - Create new lists\n- `delete_list` - Delete existing lists\n\n### Item Management\n- `add_item` - Add items to lists\n- `update_item_text` - Update item text\n- `mark_item` - Mark items complete/incomplete\n- `remove_items` - Remove items from lists\n\n### Calendar Management\n- `get_calendar` - Get appointments for a specific month\n- `create_appointment` - Create new calendar appointments\n- `update_appointment` - Update existing appointments  \n- `delete_appointment` - Delete appointments\n\n## Architecture\n\nThis MCP server is built using:\n- **FastMCP** - Simplified MCP server framework  \n- **Smithery.ai** - Cloud deployment and credential management\n- **py-cozi-client** - Python client library for Cozi's API\n- **Pydantic models** - All API responses use structured data models\n\nThe server maintains a single authenticated session with Cozi and exposes all functionality through the MCP protocol. When deployed on Smithery.ai, credentials are securely managed through the platform's configuration system.\n\n## License\n\nMIT License - see LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request."
    },
    "llm_extracted": {
      "capabilities": [
        "Get family members and their information",
        "View all lists including shopping and todo lists",
        "Filter lists by type such as shopping or todo",
        "Create new lists",
        "Delete existing lists",
        "Add items to lists",
        "Update item text in lists",
        "Mark items as complete or incomplete",
        "Remove items from lists",
        "View calendar appointments for any month",
        "Create new calendar appointments",
        "Update existing calendar appointments",
        "Delete calendar appointments"
      ],
      "limitations": [],
      "requirements": [
        "Python 3.10 or higher",
        "Cozi Family Organizer account",
        "MCP-compatible AI assistant for integration",
        "Dependencies: mcp>=1.0.0, py-cozi-client>=1.3.0, smithery",
        "Optional: uv tool for development and deployment"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear descriptions of all available tools, architecture overview, and explicit requirements, making it excellent for users and developers.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Cozi MCP Server\n\nAn unofficial Model Context Protocol (MCP) server that provides AI assistants like Claude Desktop with access to [Cozi Family Organizer](https://www.cozi.com/) functionality. This server exposes Cozi's lists, calendar, and family management features through a standardized MCP interface so you can ask your AI to manage events and lists for you.\n\nüöÄ **Now deployable on [Smithery.ai](https://smithery.ai)** - Deploy this MCP server to the cloud with secure credential management!\n\n## Features\n\n### Family Management\n- Get family members and their information\n\n### List Management  \n- View all lists (shopping and todo lists)\n- Filter lists by type\n- Create and delete lists\n\n### Item Management\n- Add items to lists\n- Update item text\n- Mark items as complete/incomplete\n- Remove items from lists\n\n### Calendar Management\n- View appointments for any month\n- Create new appointments\n- Update existing appointments\n- Delete appointments\n\n## Installation\n\n### Using Smithery.ai (Recommended)\n\nThe easiest way to use this MCP server is through Smithery.ai:\n\n**üöÄ [Deploy on Smithery.ai](https://smithery.ai/server/@mjucius/cozi_mcp)**\n\nVisit the server page for complete installation instructions and one-click deployment to your AI assistant.\n\n### Local Development\n\nFor developers who want to modify or contribute to the project:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/mjucius/cozi-mcp.git\ncd cozi-mcp\n```\n\n2. Install dependencies:\n```bash\nuv sync\n```\n\n3. Start the development playground:\n```bash\nuv run playground\n```\n\n## Usage\n\n### Cloud Deployment (Smithery.ai)\n\nOnce deployed on Smithery.ai, your MCP server runs in the cloud and can be accessed by any MCP-compatible AI assistant using the provided endpoint URL.",
        "start_pos": 0,
        "end_pos": 1757,
        "token_count_estimate": 439,
        "source_type": "readme",
        "agent_id": "8332f637baf87f2b"
      },
      {
        "chunk_id": 1,
        "text": "```bash\n# Start the interactive playground\nuv run playground\n\n# Or start development server\nuv run dev\n```\n\nThe playground provides a web interface to test all MCP tools with real-time responses and debugging information.\n\n### Integration with AI Assistants\n\nThe easiest way to integrate this MCP server is through the [Smithery.ai server page](https://smithery.ai/server/@mjucius/cozi_mcp), which provides step-by-step instructions for your specific AI assistant.\n\nFor advanced users doing local development, the server can be run locally using the stdio interface.\n\n## Development\n\n### Requirements\n- Python 3.10+\n- Cozi Family Organizer account\n- uv (recommended) or pip\n\n### Dependencies\n- `mcp>=1.0.0` - Model Context Protocol framework\n- `py-cozi-client>=1.3.0` - Cozi API client library\n- `smithery` - Smithery.ai deployment framework\n\n### Development Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/yourusername/cozi-mcp.git\ncd cozi-mcp\n```\n\n2. Install dependencies:\n```bash\n# With uv (recommended)\nuv sync\n\n# Or with pip\npip install -e .\n```\n\n3.",
        "start_pos": 1848,
        "end_pos": 2924,
        "token_count_estimate": 268,
        "source_type": "readme",
        "agent_id": "8332f637baf87f2b"
      },
      {
        "chunk_id": 2,
        "text": "em Management\n- `add_item` - Add items to lists\n- `update_item_text` - Update item text\n- `mark_item` - Mark items complete/incomplete\n- `remove_items` - Remove items from lists\n\n### Calendar Management\n- `get_calendar` - Get appointments for a specific month\n- `create_appointment` - Create new calendar appointments\n- `update_appointment` - Update existing appointments  \n- `delete_appointment` - Delete appointments\n\n## Architecture\n\nThis MCP server is built using:\n- **FastMCP** - Simplified MCP server framework  \n- **Smithery.ai** - Cloud deployment and credential management\n- **py-cozi-client** - Python client library for Cozi's API\n- **Pydantic models** - All API responses use structured data models\n\nThe server maintains a single authenticated session with Cozi and exposes all functionality through the MCP protocol. When deployed on Smithery.ai, credentials are securely managed through the platform's configuration system.\n\n## License\n\nMIT License - see LICENSE file for details.\n\n## Contributing\n\nContributions are welcome! Please feel free to submit a Pull Request.",
        "start_pos": 3696,
        "end_pos": 4778,
        "token_count_estimate": 270,
        "source_type": "readme",
        "agent_id": "8332f637baf87f2b"
      }
    ]
  },
  {
    "agent_id": "4be0a8df5827abf6",
    "name": "ai.smithery/morosss-sdfsdf",
    "source": "mcp",
    "source_url": "https://github.com/morosss/sdfsdf",
    "description": "Find academic papers across major sources like arXiv, PubMed, bioRxiv, and more. Download PDFs whe‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-13T17:26:11.859Z",
    "indexed_at": "2026-02-18T04:08:01.282429",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Paper Search MCP\n\nA Model Context Protocol (MCP) server for searching and downloading academic papers from multiple sources, including arXiv, PubMed, bioRxiv, and Sci-Hub (optional). Designed for seamless integration with large language models like Claude Desktop.\n\n![PyPI](https://img.shields.io/pypi/v/paper-search-mcp.svg) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![smithery badge](https://smithery.ai/badge/@openags/paper-search-mcp)](https://smithery.ai/server/@openags/paper-search-mcp)\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n    - [Install Package](#install-package)\n    - [Configure Claude Desktop](#configure-claude-desktop)\n  - [For Development](#for-development)\n    - [Setup Environment](#setup-environment)\n    - [Install Dependencies](#install-dependencies)\n- [Contributing](#contributing)\n- [Demo](#demo)\n- [License](#license)\n- [TODO](#todo)\n\n---\n\n## Overview\n\n`paper-search-mcp` is a Python-based MCP server that enables users to search and download academic papers from various platforms. It provides tools for searching papers (e.g., `search_arxiv`) and downloading PDFs (e.g., `download_arxiv`), making it ideal for researchers and AI-driven workflows. Built with the MCP Python SDK, it integrates seamlessly with LLM clients like Claude Desktop.\n\n---\n\n## Features\n\n- **Multi-Source Support**: Search and download papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, Semantic Scholar.\n- **Standardized Output**: Papers are returned in a consistent dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.\n\n---\n\n## Installation\n\n`paper-search-mcp` can be installed using `uv` or `pip`. Below are two approaches: a quick start for immediate use and a detailed setup for development.\n\n### Installing via Smithery\n\nTo install paper-search-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openags/paper-search-mcp):\n\n```bash\nnpx -y @smithery/cli install @openags/paper-search-mcp --client claude\n```\n\n### Quick Start\n\nFor users who want to quickly run the server:\n\n1. **Install Package**:\n\n   ```bash\n   uv add paper-search-mcp\n   ```\n\n2. **Configure Claude Desktop**:\n   Add this configuration to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n   ```json\n   {\n     \"mcpServers\": {\n       \"paper_search_server\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--directory\",\n           \"/path/to/your/paper-search-mcp\",\n           \"-m\",\n           \"paper_search_mcp.server\"\n         ],\n         \"env\": {\n           \"SEMANTIC_SCHOLAR_API_KEY\": \"\" // Optional: For enhanced Semantic Scholar features\n         }\n       }\n     }\n   }\n   ```\n   > Note: Replace `/path/to/your/paper-search-mcp` with your actual installation path.\n\n### For Development\n\nFor developers who want to modify the code or contribute:\n\n1. **Setup Environment**:\n\n   ```bash\n   # Install uv if not installed\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Clone repository\n   git clone https://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .\n\n   # Add development dependencies (optional)\n   uv add pytest flake8\n   ```\n\n---\n\n## Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. **Fork the Repository**:\n   Click \"Fork\" on GitHub.\n\n2. **Clone and Set Up**:\n\n   ```bash\n   git clone https://github.com/yourusername/paper-search-mcp.git\n   cd paper-search-mcp\n   pip install -e \".[dev]\"  # Install dev dependencies (if added to pyproject.toml)\n   ```\n\n3. **Make Changes**:\n\n   - Add new platforms in `academic_platforms/`.\n   - Update tests in `tests/`.\n\n4. **Submit a Pull Request**:\n   Push changes and create a PR on GitHub.\n\n---\n\n## Demo\n\n<img src=\"docs\\images\\demo.png\" alt=\"Demo\" width=\"800\">\n\n## TODO\n\n### Planned Academic Platforms\n\n- [‚àö] arXiv\n- [‚àö] PubMed\n- [‚àö] bioRxiv\n- [‚àö] medRxiv\n- [‚àö] Google Scholar\n- [‚àö] IACR ePrint Archive\n- [‚àö] Semantic Scholar\n- [ ] PubMed Central (PMC)\n- [ ] Science Direct\n- [ ] Springer Link\n- [ ] IEEE Xplore\n- [ ] ACM Digital Library\n- [ ] Web of Science\n- [ ] Scopus\n- [ ] JSTOR\n- [ ] ResearchGate\n- [ ] CORE\n- [ ] Microsoft Academic\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n---\n\nHappy researching with `paper-search-mcp`! If you encounter issues, open a GitHub issue.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search academic papers from multiple sources including arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, and Semantic Scholar",
        "Download academic paper PDFs from supported platforms",
        "Return paper metadata in a standardized dictionary format using the Paper class",
        "Handle asynchronous network requests efficiently using httpx",
        "Integrate seamlessly with MCP clients such as Claude Desktop for enhanced LLM context",
        "Allow extensibility by adding new academic platforms through the academic_platforms module"
      ],
      "limitations": [
        "Does not currently support some academic platforms like PubMed Central, Science Direct, Springer Link, IEEE Xplore, ACM Digital Library, Web of Science, Scopus, JSTOR, ResearchGate, CORE, and Microsoft Academic",
        "Semantic Scholar enhanced features require an optional API key",
        "No explicit mention of rate limits or usage quotas in the documentation"
      ],
      "requirements": [
        "Python 3.10 or higher",
        "Optional Semantic Scholar API key for enhanced features",
        "Installation of uv or pip for package management",
        "For development: git, virtual environment setup, and optional development dependencies like pytest and flake8",
        "Configuration of MCP server in Claude Desktop or other MCP clients"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed feature descriptions, contribution guidelines, and notes on limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Paper Search MCP\n\nA Model Context Protocol (MCP) server for searching and downloading academic papers from multiple sources, including arXiv, PubMed, bioRxiv, and Sci-Hub (optional). Designed for seamless integration with large language models like Claude Desktop.\n\n![PyPI](https://img.shields.io/pypi/v/paper-search-mcp.svg) ![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/python-3.10+-blue.svg)\n[![smithery badge](https://smithery.ai/badge/@openags/paper-search-mcp)](https://smithery.ai/server/@openags/paper-search-mcp)\n\n---\n\n## Table of Contents\n\n- [Overview](#overview)\n- [Features](#features)\n- [Installation](#installation)\n  - [Quick Start](#quick-start)\n    - [Install Package](#install-package)\n    - [Configure Claude Desktop](#configure-claude-desktop)\n  - [For Development](#for-development)\n    - [Setup Environment](#setup-environment)\n    - [Install Dependencies](#install-dependencies)\n- [Contributing](#contributing)\n- [Demo](#demo)\n- [License](#license)\n- [TODO](#todo)\n\n---\n\n## Overview\n\n`paper-search-mcp` is a Python-based MCP server that enables users to search and download academic papers from various platforms. It provides tools for searching papers (e.g., `search_arxiv`) and downloading PDFs (e.g., `download_arxiv`), making it ideal for researchers and AI-driven workflows. Built with the MCP Python SDK, it integrates seamlessly with LLM clients like Claude Desktop.\n\n---\n\n## Features\n\n- **Multi-Source Support**: Search and download papers from arXiv, PubMed, bioRxiv, medRxiv, Google Scholar, IACR ePrint Archive, Semantic Scholar.\n- **Standardized Output**: Papers are returned in a consistent dictionary format via the `Paper` class.\n- **Asynchronous Tools**: Efficiently handles network requests using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.",
        "start_pos": 0,
        "end_pos": 1987,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "4be0a8df5827abf6"
      },
      {
        "chunk_id": 1,
        "text": "using `httpx`.\n- **MCP Integration**: Compatible with MCP clients for LLM context enhancement.\n- **Extensible Design**: Easily add new academic platforms by extending the `academic_platforms` module.\n\n---\n\n## Installation\n\n`paper-search-mcp` can be installed using `uv` or `pip`. Below are two approaches: a quick start for immediate use and a detailed setup for development.\n\n### Installing via Smithery\n\nTo install paper-search-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@openags/paper-search-mcp):\n\n```bash\nnpx -y @smithery/cli install @openags/paper-search-mcp --client claude\n```\n\n### Quick Start\n\nFor users who want to quickly run the server:\n\n1. **Install Package**:\n\n   ```bash\n   uv add paper-search-mcp\n   ```\n\n2. **Configure Claude Desktop**:\n   Add this configuration to `~/Library/Application Support/Claude/claude_desktop_config.json` (Mac) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n   ```json\n   {\n     \"mcpServers\": {\n       \"paper_search_server\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"run\",\n           \"--directory\",\n           \"/path/to/your/paper-search-mcp\",\n           \"-m\",\n           \"paper_search_mcp.server\"\n         ],\n         \"env\": {\n           \"SEMANTIC_SCHOLAR_API_KEY\": \"\" // Optional: For enhanced Semantic Scholar features\n         }\n       }\n     }\n   }\n   ```\n   > Note: Replace `/path/to/your/paper-search-mcp` with your actual installation path.\n\n### For Development\n\nFor developers who want to modify the code or contribute:\n\n1. **Setup Environment**:\n\n   ```bash\n   # Install uv if not installed\n   curl -LsSf https://astral.sh/uv/install.sh | sh\n\n   # Clone repository\n   git clone https://github.com/openags/paper-search-mcp.git\n   cd paper-search-mcp\n\n   # Create and activate virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .",
        "start_pos": 1787,
        "end_pos": 3782,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "4be0a8df5827abf6"
      },
      {
        "chunk_id": 2,
        "text": "virtual environment\n   uv venv\n   source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n   ```\n\n2. **Install Dependencies**:\n\n   ```bash\n   # Install project in editable mode\n   uv add -e .\n\n   # Add development dependencies (optional)\n   uv add pytest flake8\n   ```\n\n---\n\n## Contributing\n\nWe welcome contributions! Here's how to get started:\n\n1. **Fork the Repository**:\n   Click \"Fork\" on GitHub.\n\n2. **Clone and Set Up**:\n\n   ```bash\n   git clone https://github.com/yourusername/paper-search-mcp.git\n   cd paper-search-mcp\n   pip install -e \".[dev]\"  # Install dev dependencies (if added to pyproject.toml)\n   ```\n\n3. **Make Changes**:\n\n   - Add new platforms in `academic_platforms/`.\n   - Update tests in `tests/`.\n\n4. **Submit a Pull Request**:\n   Push changes and create a PR on GitHub.\n\n---\n\n## Demo\n\n<img src=\"docs\\images\\demo.png\" alt=\"Demo\" width=\"800\">\n\n## TODO\n\n### Planned Academic Platforms\n\n- [‚àö] arXiv\n- [‚àö] PubMed\n- [‚àö] bioRxiv\n- [‚àö] medRxiv\n- [‚àö] Google Scholar\n- [‚àö] IACR ePrint Archive\n- [‚àö] Semantic Scholar\n- [ ] PubMed Central (PMC)\n- [ ] Science Direct\n- [ ] Springer Link\n- [ ] IEEE Xplore\n- [ ] ACM Digital Library\n- [ ] Web of Science\n- [ ] Scopus\n- [ ] JSTOR\n- [ ] ResearchGate\n- [ ] CORE\n- [ ] Microsoft Academic\n\n---\n\n## License\n\nThis project is licensed under the MIT License. See the LICENSE file for details.\n\n---\n\nHappy researching with `paper-search-mcp`! If you encounter issues, open a GitHub issue.",
        "start_pos": 3582,
        "end_pos": 5031,
        "token_count_estimate": 361,
        "source_type": "readme",
        "agent_id": "4be0a8df5827abf6"
      }
    ]
  },
  {
    "agent_id": "c8e744dad25381fe",
    "name": "ai.smithery/motorboy1-my-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/motorboy1/my-mcp-server",
    "description": "Send friendly greetings by name. Discover the origin story of 'Hello, World' for quick context.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-05T06:36:24.851605Z",
    "indexed_at": "2026-02-18T04:08:02.647493",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# my-mcp-server"
    },
    "llm_extracted": {
      "capabilities": [
        "Send friendly greetings by name",
        "Provide the origin story of 'Hello, World' for quick context"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of two capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# my-mcp-server",
        "start_pos": 0,
        "end_pos": 15,
        "token_count_estimate": 3,
        "source_type": "readme",
        "agent_id": "c8e744dad25381fe"
      }
    ]
  },
  {
    "agent_id": "7f3759b75126c84f",
    "name": "ai.smithery/mrugankpednekar-bill_splitter_mcp",
    "source": "mcp",
    "source_url": "https://github.com/mrugankpednekar/bill_splitter_mcp",
    "description": "Track and split shared expenses across trips, events, and groups. Create groups, add expenses, and‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T06:59:08.923806Z",
    "indexed_at": "2026-02-18T04:08:05.995509",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Track shared expenses across trips, events, and groups",
        "Split shared expenses among group members",
        "Create groups for expense tracking",
        "Add expenses to groups"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of core functionalities but lacks detailed examples, structure, or information on limitations and requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b3ecadbb9bd70f87",
    "name": "ai.smithery/mrugankpednekar-mcp-optimizer",
    "source": "mcp",
    "source_url": "https://github.com/mrugankpednekar/mcp-optimizer",
    "description": "Optimize crew and workforce schedules, resource allocation, and routing with linear and mixed-inte‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T21:15:31.815401Z",
    "indexed_at": "2026-02-18T04:08:08.383847",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Crew Optimizer\n\nCrew Optimizer rebuilds the original optimisation project around the [CrewAI](https://github.com/joaomdmoura/crewai) ecosystem. It provides reusable CrewAI tools and agents capable of solving linear programs via SciPy's HiGHS backend, exploring mixed-integer models with a lightweight branch-and-bound search (or OR-Tools fallback), translating natural language prompts into LP JSON, and diagnosing infeasibility. You can embed the tools inside your own crews or call them programmatically through the `OptimizerCrew` convenience wrapper, or serve them over the MCP protocol for clients such as Smithery.\n\n## Installation\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -e .[mip]\n```\n\nThis installs Crew Optimizer together with optional OR-Tools support for MILP solving. Add `pytest`, `ruff`, or other dev tools as needed (`pip install pytest`).\n\n## Quick Usage\n\n```python\nfrom crew_optimizer import OptimizerCrew\n\ncrew = OptimizerCrew(verbose=False)\n\nlp_model = {\n    \"name\": \"diet-toy\",\n    \"sense\": \"min\",\n    \"objective\": {\n        \"terms\": [\n            {\"var\": \"x\", \"coef\": 3},\n            {\"var\": \"y\", \"coef\": 2},\n        ],\n        \"constant\": 0,\n    },\n    \"variables\": [\n        {\"name\": \"x\", \"lb\": 0},\n        {\"name\": \"y\", \"lb\": 0},\n    ],\n    \"constraints\": [\n        {\n            \"name\": \"c1\",\n            \"lhs\": {\n                \"terms\": [\n                    {\"var\": \"x\", \"coef\": 1},\n                    {\"var\": \"y\", \"coef\": 2},\n                ],\n                \"constant\": 0,\n            },\n            \"cmp\": \">=\",\n            \"rhs\": 8,\n        },\n        {\n            \"name\": \"c2\",\n            \"lhs\": {\n                \"terms\": [\n                    {\"var\": \"x\", \"coef\": 3},\n                    {\"var\": \"y\", \"coef\": 1},\n                ],\n                \"constant\": 0,\n            },\n            \"cmp\": \">=\",\n            \"rhs\": 6,\n        },\n    ],\n}\n\nsolution = crew.solve_lp(lp_model)\nprint(solution)\n```\n\nTo integrate with a wider multi-agent workflow, call `crew.build_crew()` to obtain a `Crew` populated with the LP, MILP, and parser agents. Provide model inputs through CrewAI‚Äôs shared context as usual.\n\n## MCP / Smithery Hosting\n\nCrew Optimizer ships an MCP server (`python -m crew_optimizer.server`) that wraps the same solvers. The repository already contains a Smithery manifest (`smithery.json`) and build config (`smithery.yaml`).\n\n1. Push the repository to GitHub.\n2. In Smithery, choose **Publish an MCP Server**, connect GitHub, and select the repo.\n3. Smithery installs the package (`pip install .`) and launches `mcp http src/crew_optimizer/server.py --port 3333` using the bundled startup script.\n4. The server exposes the following tools:\n   - `solve_linear_program`\n   - `solve_mixed_integer_program`\n   - `parse_natural_language`\n   - `diagnose_infeasibility`\n   - `solve_word_problem_with_data` - Solve optimization problems using data from files\n\nFor local testing:\n\n```bash\nmcp http src/crew_optimizer/server.py --port 3333 --cors \"*\"\n```\n\n## Testing\n\nInstall test dependencies (`pip install pytest`) and run:\n\n```bash\npython -m pytest\n```\n\nThe suite covers the LP solver, MILP branch-and-bound, and the NL parser.\n\n## Solving Word Problems with Data Files\n\nThe MCP server includes a `solve_word_problem_with_data` tool that can parse data files (CSV, JSON, Excel) and use them to solve optimization word problems. This is particularly useful when you have data in files and want to formulate and solve optimization problems based on that data.\n\n### Example Usage\n\n```python\n# Example: Solve a production planning problem with data from a CSV file\ncsv_data = \"\"\"product,cost,capacity,demand\nWidget,10,100,50\nGadget,15,80,60\nThing,12,120,40\"\"\"\n\nproblem = \"\"\"\nMinimize total cost subject to:\n- Production of each product cannot exceed capacity\n- Production must meet demand\n- All production quantities are non-negative\n\"\"\"\n\n# The tool will parse the CSV, extract the cost, capacity, and demand values,\n# and formulate the optimization problem automatically.\n```\n\nThe tool supports:\n- **CSV/TSV files**: Automatically detects and parses comma or tab-separated values\n- **JSON files**: Parses JSON arrays or objects\n- **Excel files**: Requires `pandas` and `openpyxl` (install with `pip install crew-optimizer[excel]`)\n- **Auto-detection**: Automatically detects file format if not specified\n\nThe parsed data is incorporated into the problem description, allowing the natural language parser to extract values and formulate constraints and objective functions based on the actual data.\n\n## Licence\n\nDistributed under the MIT Licence. See `LICENSE` for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Solve linear programming problems using SciPy's HiGHS backend",
        "Solve mixed-integer programming problems with a lightweight branch-and-bound search or OR-Tools fallback",
        "Translate natural language prompts into linear programming JSON models",
        "Diagnose infeasibility in optimization problems",
        "Solve optimization word problems using data from CSV, JSON, or Excel files",
        "Embed optimization tools inside custom CrewAI crews or call them programmatically",
        "Serve optimization tools over the MCP protocol for integration with clients like Smithery"
      ],
      "limitations": [
        "Requires optional OR-Tools for MILP solving fallback",
        "Excel file support requires additional dependencies (pandas and openpyxl)",
        "No explicit mention of handling very large-scale optimization problems or performance constraints",
        "No stated support for non-linear or non-convex optimization problems"
      ],
      "requirements": [
        "Python environment with virtual environment recommended",
        "Installation of crew-optimizer package with optional [mip] extras for MILP support",
        "For Excel file support, installation of pandas and openpyxl via crew-optimizer[excel]",
        "Smithery account and GitHub repository for MCP server publishing",
        "Optional installation of pytest and ruff for development and testing"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, MCP server deployment steps, testing guidance, and explicit requirements and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Crew Optimizer\n\nCrew Optimizer rebuilds the original optimisation project around the [CrewAI](https://github.com/joaomdmoura/crewai) ecosystem. It provides reusable CrewAI tools and agents capable of solving linear programs via SciPy's HiGHS backend, exploring mixed-integer models with a lightweight branch-and-bound search (or OR-Tools fallback), translating natural language prompts into LP JSON, and diagnosing infeasibility. You can embed the tools inside your own crews or call them programmatically through the `OptimizerCrew` convenience wrapper, or serve them over the MCP protocol for clients such as Smithery.\n\n## Installation\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\npip install -e .[mip]\n```\n\nThis installs Crew Optimizer together with optional OR-Tools support for MILP solving. Add `pytest`, `ruff`, or other dev tools as needed (`pip install pytest`).",
        "start_pos": 0,
        "end_pos": 884,
        "token_count_estimate": 221,
        "source_type": "readme",
        "agent_id": "b3ecadbb9bd70f87"
      },
      {
        "chunk_id": 1,
        "text": "\"cmp\": \">=\",\n            \"rhs\": 6,\n        },\n    ],\n}\n\nsolution = crew.solve_lp(lp_model)\nprint(solution)\n```\n\nTo integrate with a wider multi-agent workflow, call `crew.build_crew()` to obtain a `Crew` populated with the LP, MILP, and parser agents. Provide model inputs through CrewAI‚Äôs shared context as usual.\n\n## MCP / Smithery Hosting\n\nCrew Optimizer ships an MCP server (`python -m crew_optimizer.server`) that wraps the same solvers. The repository already contains a Smithery manifest (`smithery.json`) and build config (`smithery.yaml`).\n\n1. Push the repository to GitHub.\n2. In Smithery, choose **Publish an MCP Server**, connect GitHub, and select the repo.\n3. Smithery installs the package (`pip install .`) and launches `mcp http src/crew_optimizer/server.py --port 3333` using the bundled startup script.\n4. The server exposes the following tools:\n   - `solve_linear_program`\n   - `solve_mixed_integer_program`\n   - `parse_natural_language`\n   - `diagnose_infeasibility`\n   - `solve_word_problem_with_data` - Solve optimization problems using data from files\n\nFor local testing:\n\n```bash\nmcp http src/crew_optimizer/server.py --port 3333 --cors \"*\"\n```\n\n## Testing\n\nInstall test dependencies (`pip install pytest`) and run:\n\n```bash\npython -m pytest\n```\n\nThe suite covers the LP solver, MILP branch-and-bound, and the NL parser.\n\n## Solving Word Problems with Data Files\n\nThe MCP server includes a `solve_word_problem_with_data` tool that can parse data files (CSV, JSON, Excel) and use them to solve optimization word problems. This is particularly useful when you have data in files and want to formulate and solve optimization problems based on that data.",
        "start_pos": 1848,
        "end_pos": 3534,
        "token_count_estimate": 418,
        "source_type": "readme",
        "agent_id": "b3ecadbb9bd70f87"
      },
      {
        "chunk_id": 2,
        "text": "0\nGadget,15,80,60\nThing,12,120,40\"\"\"\n\nproblem = \"\"\"\nMinimize total cost subject to:\n- Production of each product cannot exceed capacity\n- Production must meet demand\n- All production quantities are non-negative\n\"\"\"\n\n# The tool will parse the CSV, extract the cost, capacity, and demand values,\n# and formulate the optimization problem automatically.\n```\n\nThe tool supports:\n- **CSV/TSV files**: Automatically detects and parses comma or tab-separated values\n- **JSON files**: Parses JSON arrays or objects\n- **Excel files**: Requires `pandas` and `openpyxl` (install with `pip install crew-optimizer[excel]`)\n- **Auto-detection**: Automatically detects file format if not specified\n\nThe parsed data is incorporated into the problem description, allowing the natural language parser to extract values and formulate constraints and objective functions based on the actual data.\n\n## Licence\n\nDistributed under the MIT Licence. See `LICENSE` for details.",
        "start_pos": 3696,
        "end_pos": 4647,
        "token_count_estimate": 237,
        "source_type": "readme",
        "agent_id": "b3ecadbb9bd70f87"
      }
    ]
  },
  {
    "agent_id": "29d8cdce5c1ad0e9",
    "name": "ai.smithery/neverinfamous-memory-journal-mcp",
    "source": "mcp",
    "source_url": "https://github.com/neverinfamous/memory-journal-mcp",
    "description": "A MCP server built for developers enabling Git based project management with project and personal‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-15T03:39:21.90803Z",
    "indexed_at": "2026-02-18T04:08:10.051619",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Memory Journal MCP Server\n\n**Last Updated February 13, 2026**\n\n<!-- mcp-name: io.github.neverinfamous/memory-journal-mcp -->\n\n[![GitHub](https://img.shields.io/badge/GitHub-neverinfamous/memory--journal--mcp-blue?logo=github)](https://github.com/neverinfamous/memory-journal-mcp)\n[![npm](https://img.shields.io/npm/v/memory-journal-mcp)](https://www.npmjs.com/package/memory-journal-mcp)\n[![Docker Pulls](https://img.shields.io/docker/pulls/writenotenow/memory-journal-mcp)](https://hub.docker.com/r/writenotenow/memory-journal-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n![Status](https://img.shields.io/badge/status-Production%2FStable-brightgreen)\n[![MCP Registry](https://img.shields.io/badge/MCP_Registry-Published-green)](https://registry.modelcontextprotocol.io/v0/servers?search=io.github.neverinfamous/memory-journal-mcp)\n[![Security](https://img.shields.io/badge/Security-Enhanced-green.svg)](SECURITY.md)\n[![TypeScript](https://img.shields.io/badge/TypeScript-Strict-blue.svg)](https://github.com/neverinfamous/memory-journal-mcp)\n\nüéØ **AI Context + Project Intelligence:** Bridge disconnected AI sessions with persistent project memory, while integrating your complete GitHub workflow ‚Äî Issues, PRs, Actions, Kanban boards, and knowledge graphs ‚Äî into every conversation.\n\n**[GitHub](https://github.com/neverinfamous/memory-journal-mcp)** ‚Ä¢ **[Wiki](https://github.com/neverinfamous/memory-journal-mcp/wiki)** ‚Ä¢ **[Changelog](https://github.com/neverinfamous/memory-journal-mcp/wiki/CHANGELOG)** ‚Ä¢ **[Release Article](https://adamic.tech/articles/memory-journal-mcp-server)**\n\n**üöÄ Quick Deploy:**\n\n- **[npm Package](https://www.npmjs.com/package/memory-journal-mcp)** - `npm install -g memory-journal-mcp`\n- **[Docker Hub](https://hub.docker.com/r/writenotenow/memory-journal-mcp)** - Alpine-based with full semantic search\n\n## üéØ What This Does\n\n### Key Benefits\n\n- üß† **Dynamic Context Management** - AI agents automatically query your project history and create entries at the right moments\n- üìù **Auto-capture Git/GitHub context** (commits, branches, issues, PRs, projects)\n- üîó **Build knowledge graphs** linking specs ‚Üí implementations ‚Üí tests ‚Üí PRs\n- üîç **Triple search** (full-text, semantic, date range)\n- üìä **Generate reports** (standups, retrospectives, PR summaries, status)\n- üóÑÔ∏è **Backup & restore** your journal data with one command\n\n```mermaid\nflowchart TB\n    subgraph Session[\"ü§ñ AI Session Start\"]\n        Briefing[\"üìã Read Briefing<br/>(memory://briefing)\"]\n    end\n\n    subgraph Core[\"üìù Journal Operations\"]\n        Create[\"Create Entry\"]\n        Retrieve[\"Retrieve & Search\"]\n        Link[\"Link Entries\"]\n    end\n\n    subgraph Search[\"üîç Triple Search\"]\n        FTS[\"Full-Text (FTS5)\"]\n        Semantic[\"Semantic (Vector)\"]\n        DateRange[\"Date Range\"]\n    end\n\n    subgraph GitHub[\"üêô GitHub Integration\"]\n        Issues[\"Issues & Milestones\"]\n        PRs[\"Pull Requests\"]\n        Actions[\"GitHub Actions\"]\n        Kanban[\"Kanban Boards\"]\n    end\n\n    subgraph Outputs[\"üìä Outputs\"]\n        Reports[\"Standups & Retrospectives\"]\n        Graphs[\"Knowledge Graphs\"]\n        Timeline[\"Project Timelines\"]\n    end\n\n    Session --> Core\n    Core --> Search\n    Core <--> GitHub\n    Search --> Outputs\n    GitHub --> Outputs\n```\n\n### üìà **Current Capabilities**\n\n- **33 MCP tools** - Complete development workflow + backup/restore + Kanban + issue management\n- **15 workflow prompts** - Standups, retrospectives, PR workflows, CI/CD failure analysis, session acknowledgment\n- **18 MCP resources** - 12 static + 6 template (require parameters)\n- **GitHub Integration** - Projects, Issues, Pull Requests, Actions, **Kanban boards**\n- **8 tool groups** - `core`, `search`, `analytics`, `relationships`, `export`, `admin`, `github`, `backup`\n- **Knowledge graphs** - 8 relationship types, Mermaid visualization\n- **Semantic search** - AI-powered conceptual search via `@xenova/transformers`\n\n---\n\n## üéØ Why Memory Journal?\n\n### **The Fragmented AI Context Problem**\n\nWhen managing large projects with AI assistance, you face a critical challenge:\n\n- **Thread Amnesia** - Each new AI conversation starts from zero, unaware of previous work\n- **Lost Context** - Decisions, implementations, and learnings scattered across disconnected threads\n- **Repeated Work** - AI suggests solutions you've already tried or abandoned\n- **Context Overload** - Manually copying project history into every new conversation\n\n### **The Solution: Persistent Project Memory**\n\nMemory Journal acts as your project's **long-term memory**, bridging the gap between fragmented AI threads:\n\n**For Developers:**\n\n- üìù **Automatic Context Capture** - Git commits, branches, GitHub issues, PRs, and project state captured with every entry\n- üîó **Knowledge Graph** - Link related work (specs ‚Üí implementations ‚Üí tests ‚Üí PRs) to build a connected history\n- üîç **Intelligent Search** - Find past decisions, solutions, and context across your entire project timeline\n- üìä **Project Analytics** - Track progress from issues through PRs, generate reports for standups/retrospectives\n\n**For AI-Assisted Work:**\n\n- üß† **Dynamic Context Management** - Built-in guidance teaches AI agents when to query your project history and when to create entries\n- üí° AI can query your **complete project history** in any conversation\n- üîç **Semantic search** finds conceptually related work, even without exact keywords\n- üìñ **Context bundles** provide AI with comprehensive project state instantly\n- üîó **Relationship visualization** shows how different pieces of work connect\n\n---\n\n## üöÄ Quick Start\n\n### Option 1: npm (Recommended)\n\n**Step 1: Install the package**\n\n```bash\nnpm install -g memory-journal-mcp\n```\n\n**Step 2: Add to ~/.cursor/mcp.json**\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"memory-journal-mcp\"\n    }\n  }\n}\n```\n\n**Step 3: Restart Cursor**\n\nRestart Cursor or your MCP client, then start journaling!\n\n### Option 2: npx (No Installation)\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"memory-journal-mcp\"]\n    }\n  }\n}\n```\n\n### Option 3: From Source\n\n```bash\ngit clone https://github.com/neverinfamous/memory-journal-mcp.git\ncd memory-journal-mcp\nnpm install\nnpm run build\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"dist/cli.js\", \"--default-project\", \"1\"]\n    }\n  }\n}\n```\n\n### Option 4: HTTP/SSE Transport (Remote Access)\n\nFor remote access or web-based clients, run the server in HTTP mode:\n\n```bash\nmemory-journal-mcp --transport http --port 3000\n```\n\nTo bind to all interfaces (required for containers):\n\n```bash\nmemory-journal-mcp --transport http --port 3000 --server-host 0.0.0.0\n```\n\n**Endpoints:**\n\n- `POST /mcp` ‚Äî JSON-RPC requests (initialize, tools/call, etc.)\n- `GET /mcp` ‚Äî SSE stream for server-to-client notifications\n- `DELETE /mcp` ‚Äî Session termination\n\n**Session Management:** The server uses stateful sessions by default. Include the `mcp-session-id` header (returned from initialization) in subsequent requests.\n\n**Example with curl:**\n\n```bash\n# Initialize session\ncurl -X POST http://localhost:3000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"initialize\",\"params\":{\"protocolVersion\":\"2025-03-26\",\"capabilities\":{},\"clientInfo\":{\"name\":\"test\",\"version\":\"1.0\"}}}'\n# Returns mcp-session-id header\n\n# List tools (with session)\ncurl -X POST http://localhost:3000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"mcp-session-id: YOUR_SESSION_ID\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":2,\"method\":\"tools/list\",\"params\":{}}'\n```\n\n#### Stateless Mode (Serverless)\n\nFor serverless deployments (Lambda, Workers, Vercel), use stateless mode:\n\n```bash\nmemory-journal-mcp --transport http --port 3000 --stateless\n```\n\n| Mode                      | Progress Notifications | SSE Streaming | Serverless |\n| ------------------------- | ---------------------- | ------------- | ---------- |\n| Stateful (default)        | ‚úÖ Yes                 | ‚úÖ Yes        | ‚ö†Ô∏è Complex |\n| Stateless (`--stateless`) | ‚ùå No                  | ‚ùå No         | ‚úÖ Native  |\n\n### GitHub Integration Configuration\n\nThe GitHub tools (`get_github_issues`, `get_github_prs`, etc.) can auto-detect the repository from your git context. However, MCP clients may run the server from a different directory than your project.\n\n**To enable GitHub auto-detection**, add `GITHUB_REPO_PATH` to your config:\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"memory-journal-mcp\",\n      \"env\": {\n        \"GITHUB_TOKEN\": \"ghp_your_token_here\",\n        \"GITHUB_REPO_PATH\": \"/path/to/your/git/repo\"\n      }\n    }\n  }\n}\n```\n\n| Environment Variable     | Description                                                            |\n| ------------------------ | ---------------------------------------------------------------------- |\n| `GITHUB_TOKEN`           | GitHub personal access token for API access                            |\n| `GITHUB_REPO_PATH`       | Path to the git repository for auto-detecting owner/repo               |\n| `DEFAULT_PROJECT_NUMBER` | Default GitHub Project number for auto-assignment when creating issues |\n| `AUTO_REBUILD_INDEX`     | Set to `true` to rebuild vector index on server startup                |\n| `MCP_HOST`               | Server bind host (`0.0.0.0` for containers, default: `localhost`)      |\n\n**Without `GITHUB_REPO_PATH`**: You'll need to explicitly provide `owner` and `repo` parameters when calling GitHub tools.\n\n#### Fallback Behavior\n\nWhen GitHub tools cannot auto-detect repository information:\n\n1. **With `GITHUB_REPO_PATH` set**: Tools auto-detect `owner` and `repo` from git remote URL\n2. **Without `GITHUB_REPO_PATH`**: Tools return structured response with `requiresUserInput: true` and instructions to provide `owner` and `repo` parameters\n3. **With explicit parameters**: Always preferred - specify `owner` and `repo` directly in tool calls\n\n**Example response when auto-detection fails:**\n\n```json\n{\n  \"error\": \"Could not auto-detect repository\",\n  \"requiresUserInput\": true,\n  \"instruction\": \"Please provide owner and repo parameters\"\n}\n```\n\n### Client-Specific Notes\n\n**Cursor IDE:**\n\n- **Listing MCP Resources**: If the agent has trouble listing resources, instruct it to call `ListMcpResources()` without specifying a server parameter, or with `server: \"user-memory-journal-mcp\"` (Cursor prefixes server names with `user-`).\n\n**Google AntiGravity IDE:**\n\n- **AntiGravity Users:** Server instructions are automatically sent to MCP clients during initialization. However, AntiGravity does not currently support MCP server instructions. For optimal usage in AntiGravity, manually provide the contents of [`src/constants/ServerInstructions.ts`](src/constants/ServerInstructions.ts) to the agent in your prompt or user rules.\n\n- **Session start**: Add to your user rules: \"At session start, read `memory://briefing` from memory-journal-mcp.\"\n\n- **Full guidance**: If behaviors missing, read `memory://instructions` for complete Dynamic Context Management patterns.\n\n- **Prompts not available**: AntiGravity does not currently support MCP prompts. The 15 workflow prompts are not accessible.\n\n---\n\n## üìã Core Capabilities\n\n### üõ†Ô∏è **33 MCP Tools** (8 Groups)\n\n| Group           | Tools | Description                                             |\n| --------------- | ----- | ------------------------------------------------------- |\n| `core`          | 6     | Entry CRUD, tags, test                                  |\n| `search`        | 4     | Text search, date range, semantic, vector stats         |\n| `analytics`     | 2     | Statistics, cross-project insights                      |\n| `relationships` | 2     | Link entries, visualize graphs                          |\n| `export`        | 1     | JSON/Markdown export                                    |\n| `admin`         | 5     | Update, delete, rebuild/add to vector index, merge tags |\n| `github`        | 9     | Issues, PRs, context, Kanban, **issue lifecycle**       |\n| `backup`        | 4     | Backup, list, restore, cleanup                          |\n\n**[Complete tools reference ‚Üí](https://github.com/neverinfamous/memory-journal-mcp/wiki/Tools)**\n\n### üéØ **15 Workflow Prompts**\n\n- `find-related` - Discover connected entries via semantic similarity\n- `prepare-standup` - Daily standup summaries\n- `prepare-retro` - Sprint retrospectives\n- `weekly-digest` - Day-by-day weekly summaries\n- `analyze-period` - Deep period analysis with insights\n- `goal-tracker` - Milestone and achievement tracking\n- `get-context-bundle` - Project context with Git/GitHub/Kanban\n- `pr-summary` - Pull request journal activity summary\n- `code-review-prep` - Comprehensive PR review preparation\n- `pr-retrospective` - Completed PR analysis with learnings\n- `actions-failure-digest` - CI/CD failure analysis\n- `confirm-briefing` - **NEW** Acknowledge session context to user\n\n**[Complete prompts guide ‚Üí](https://github.com/neverinfamous/memory-journal-mcp/wiki/Prompts)**\n\n### üì° **18 Resources** (12 Static + 6 Template)\n\n**Static Resources** (appear in resource lists):\n\n- `memory://briefing` - **Session initialization**: compact context for AI agents (~300 tokens)\n- `memory://instructions` - **Behavioral guidance**: complete server instructions (supports `?level=essential|standard|full`)\n- `memory://recent` - 10 most recent entries\n- `memory://significant` - Significant milestones and breakthroughs\n- `memory://graph/recent` - Live Mermaid diagram of recent relationships\n- `memory://team/recent` - Recent team-shared entries\n- `memory://health` - Server health & diagnostics\n- `memory://graph/actions` - CI/CD narrative graph\n- `memory://actions/recent` - Recent workflow runs\n- `memory://tags` - All tags with usage counts\n- `memory://statistics` - Journal statistics\n- `memory://github/status` - GitHub repository status overview\n\n**Template Resources** (require parameters, fetch directly by URI):\n\n- `memory://projects/{number}/timeline` - Project activity timeline\n- `memory://issues/{issue_number}/entries` - Entries linked to issue\n- `memory://prs/{pr_number}/entries` - Entries linked to PR\n- `memory://prs/{pr_number}/timeline` - Combined PR + journal timeline\n- `memory://kanban/{project_number}` - GitHub Project Kanban board\n- `memory://kanban/{project_number}/diagram` - Kanban Mermaid visualization\n\n---\n\n## üîß Configuration\n\n### GitHub Integration (Optional)\n\n```bash\nexport GITHUB_TOKEN=\"your_token\"              # For Projects/Issues/PRs\nexport GITHUB_ORG_TOKEN=\"your_org_token\"      # Optional: org projects\nexport DEFAULT_ORG=\"your-org-name\"            # Optional: default org\n```\n\n**Scopes:** `repo`, `project`, `read:org` (org only)\n\n### GitHub Management Capabilities\n\nMemory Journal provides a **hybrid approach** to GitHub management:\n\n| Capability Source  | Purpose                                                                        |\n| ------------------ | ------------------------------------------------------------------------------ |\n| **MCP Server**     | Specialized features: Kanban visualization, journal linking, project timelines |\n| **Agent (gh CLI)** | Full GitHub mutations: create/close issues, create/merge PRs, manage releases  |\n\n**MCP Server Tools (Read + Kanban + Issue Lifecycle):**\n\n- `get_github_issues` / `get_github_issue` - Query issues\n- `get_github_prs` / `get_github_pr` - Query pull requests\n- `get_github_context` - Full repository context\n- `get_kanban_board` / `move_kanban_item` - **Kanban management**\n- `create_github_issue_with_entry` / `close_github_issue_with_entry` - **Issue lifecycle with journal linking**\n\n**Agent Operations (via gh CLI):**\n\n```bash\n# Issues\ngh issue create --title \"Bug fix\" --body \"Description\"\ngh issue close 42\n\n# Pull Requests\ngh pr create --fill\ngh pr merge 123\n\n# Projects\ngh project item-add 5 --owner neverinfamous --url \"issue-url\"\n\n# Releases\ngh release create v1.0.0 --generate-notes\n```\n\n> **Why this design?** The MCP server focuses on value-added features that integrate journal entries with GitHub (Kanban views, timeline resources, context linking). Standard GitHub operations are already excellently handled by `gh` CLI, which agents can invoke directly.\n\n**[Complete GitHub integration guide ‚Üí](https://github.com/neverinfamous/memory-journal-mcp/wiki/Git-Integration)**\n\n### Tool Filtering (Optional)\n\nControl which tools are exposed using `MEMORY_JOURNAL_MCP_TOOL_FILTER`:\n\n```bash\nexport MEMORY_JOURNAL_MCP_TOOL_FILTER=\"-analytics,-github\"\n```\n\n**Filter Syntax:**\n\n- `-group` - Disable all tools in a group\n- `-tool` - Disable a specific tool\n- `+tool` - Re-enable after group disable\n- Meta-groups: `starter`, `essential`, `full`, `readonly`\n\n**Example Configurations:**\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"memory-journal-mcp\",\n      \"env\": {\n        \"MEMORY_JOURNAL_MCP_TOOL_FILTER\": \"starter\",\n        \"GITHUB_TOKEN\": \"your_token\"\n      }\n    }\n  }\n}\n```\n\n| Configuration  | Filter String | Tools |\n| -------------- | ------------- | ----- |\n| Starter        | `starter`     | ~10   |\n| Essential      | `essential`   | ~6    |\n| Full (default) | `full`        | 33    |\n| Read-only      | `readonly`    | ~20   |\n\n**[Complete tool filtering guide ‚Üí](https://github.com/neverinfamous/memory-journal-mcp/wiki/Tool-Filtering)**\n\n---\n\n## üèóÔ∏è Architecture\n\n### Data Flow\n\n```mermaid\nflowchart TB\n    AI[\"ü§ñ AI Agent<br/>(Cursor, Windsurf, Claude)\"]\n\n    subgraph MCP[\"Memory Journal MCP Server\"]\n        Tools[\"üõ†Ô∏è 33 Tools\"]\n        Resources[\"üì° 18 Resources\"]\n        Prompts[\"üí¨ 15 Prompts\"]\n    end\n\n    subgraph Storage[\"Persistence Layer\"]\n        SQLite[(\"üíæ SQLite<br/>Entries, Tags, Relationships\")]\n        Vector[(\"üîç Vector Index<br/>Semantic Embeddings\")]\n        Backups[\"üì¶ Backups\"]\n    end\n\n    subgraph External[\"External Integrations\"]\n        GitHub[\"üêô GitHub API<br/>Issues, PRs, Actions\"]\n        Kanban[\"üìã Projects v2<br/>Kanban Boards\"]\n    end\n\n    AI <-->|\"MCP Protocol\"| MCP\n    Tools --> Storage\n    Tools --> External\n    Resources --> Storage\n    Resources --> External\n```\n\n### Stack\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ MCP Server Layer (TypeScript)                               ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ Tools (33)      ‚îÇ  ‚îÇ Resources (18)  ‚îÇ  ‚îÇ Prompts (15)‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ with Annotations‚îÇ  ‚îÇ with Annotations‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Pure JS Stack (No Native Dependencies)                      ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ sql.js          ‚îÇ  ‚îÇ vectra          ‚îÇ  ‚îÇ transformers‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ (SQLite)        ‚îÇ  ‚îÇ (Vector Index)  ‚îÇ  ‚îÇ (Embeddings)‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ SQLite Database with Hybrid Search                          ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n‚îÇ  ‚îÇ entries + tags + relationships + embeddings + backups   ‚îÇ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n## üîß Technical Highlights\n\n### Performance & Portability\n\n- **TypeScript + Pure JS Stack** - No native compilation, works everywhere\n- **sql.js** - SQLite in pure JavaScript with disk sync\n- **vectra** - Vector similarity search without native dependencies\n- **@xenova/transformers** - ML embeddings in JavaScript\n- **Lazy loading** - ML models load on first use, not startup\n\n### Security\n\n- **Local-first** - All data stored locally, no external API calls (except optional GitHub)\n- **Input validation** - Zod schemas, content size limits, SQL injection prevention\n- **Path traversal protection** - Backup filenames validated\n- **MCP 2025-11-25 annotations** - Behavioral hints (`readOnlyHint`, `destructiveHint`, etc.)\n\n### Data & Privacy\n\n- **Single SQLite file** - You own your data\n- **Portable** - Move your `.db` file anywhere\n- **Soft delete** - Entries can be recovered\n- **Auto-backup on restore** - Never lose data accidentally\n\n---\n\n## üìö Documentation & Resources\n\n- **[GitHub Wiki](https://github.com/neverinfamous/memory-journal-mcp/wiki)** - Complete documentation\n- **[Docker Hub](https://hub.docker.com/r/writenotenow/memory-journal-mcp)** - Container images\n- **[npm Package](https://www.npmjs.com/package/memory-journal-mcp)** - Node.js distribution\n- **[Issues](https://github.com/neverinfamous/memory-journal-mcp/issues)** - Bug reports & feature requests\n\n---\n\n## üìÑ License\n\nMIT License - See [LICENSE](LICENSE) file for details.\n\n## ü§ù Contributing\n\nBuilt by developers, for developers. PRs welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n---\n\n_Migrating from v2.x?_ Your existing database is fully compatible. The TypeScript version uses the same schema and data format.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Manage dynamic AI context with automatic project history querying and entry creation",
        "Auto-capture Git and GitHub context including commits, branches, issues, pull requests, and projects",
        "Build and visualize knowledge graphs linking specifications, implementations, tests, and pull requests",
        "Perform triple search: full-text, semantic (AI-powered), and date range searches",
        "Generate project reports such as standups, retrospectives, pull request summaries, and status updates",
        "Backup and restore journal data with a single command",
        "Integrate deeply with GitHub workflow including issues, pull requests, actions, and Kanban boards",
        "Support stateful and stateless HTTP/SSE transport modes for flexible deployment",
        "Provide 33 MCP tools organized in 8 groups covering entry CRUD, search, analytics, relationships, export, admin, GitHub, and backup",
        "Offer 15 workflow prompts for common development workflows and session management"
      ],
      "limitations": [
        "AntiGravity IDE does not support MCP prompts or server instructions natively, requiring manual workarounds",
        "Stateless mode disables progress notifications and SSE streaming, limiting real-time interaction",
        "GitHub auto-detection requires setting GITHUB_REPO_PATH or explicit owner/repo parameters; otherwise tools return errors requiring user input",
        "Serverless deployments are complex with stateful mode and require stateless mode for native support"
      ],
      "requirements": [
        "Node.js environment for npm installation or Docker for containerized deployment",
        "GitHub personal access token (GITHUB_TOKEN) for API access to GitHub features",
        "GITHUB_REPO_PATH environment variable or explicit repository parameters for GitHub auto-detection",
        "Configuration of MCP client to include memory-journal-mcp server command and environment variables",
        "Port and host configuration for HTTP transport mode if remote access is needed"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed capability descriptions, usage examples, environment requirements, deployment modes, integration details, and known limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Memory Journal MCP Server\n\n**Last Updated February 13, 2026**\n\n<!-- mcp-name: io.github.neverinfamous/memory-journal-mcp -->\n\n[![GitHub](https://img.shields.io/badge/GitHub-neverinfamous/memory--journal--mcp-blue?logo=github)](https://github.com/neverinfamous/memory-journal-mcp)\n[![npm](https://img.shields.io/npm/v/memory-journal-mcp)](https://www.npmjs.com/package/memory-journal-mcp)\n[![Docker Pulls](https://img.shields.io/docker/pulls/writenotenow/memory-journal-mcp)](https://hub.docker.com/r/writenotenow/memory-journal-mcp)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n![Status](https://img.shields.io/badge/status-Production%2FStable-brightgreen)\n[![MCP Registry](https://img.shields.io/badge/MCP_Registry-Published-green)](https://registry.modelcontextprotocol.io/v0/servers?search=io.github.neverinfamous/memory-journal-mcp)\n[![Security](https://img.shields.io/badge/Security-Enhanced-green.svg)](SECURITY.md)\n[![TypeScript](https://img.shields.io/badge/TypeScript-Strict-blue.svg)](https://github.com/neverinfamous/memory-journal-mcp)\n\nüéØ **AI Context + Project Intelligence:** Bridge disconnected AI sessions with persistent project memory, while integrating your complete GitHub workflow ‚Äî Issues, PRs, Actions, Kanban boards, and knowledge graphs ‚Äî into every conversation.",
        "start_pos": 0,
        "end_pos": 1351,
        "token_count_estimate": 337,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 1,
        "text": "journal-mcp)** - Alpine-based with full semantic search\n\n## üéØ What This Does\n\n### Key Benefits\n\n- üß† **Dynamic Context Management** - AI agents automatically query your project history and create entries at the right moments\n- üìù **Auto-capture Git/GitHub context** (commits, branches, issues, PRs, projects)\n- üîó **Build knowledge graphs** linking specs ‚Üí implementations ‚Üí tests ‚Üí PRs\n- üîç **Triple search** (full-text, semantic, date range)\n- üìä **Generate reports** (standups, retrospectives, PR summaries, status)\n- üóÑÔ∏è **Backup & restore** your journal data with one command\n\n```mermaid\nflowchart TB\n    subgraph Session[\"ü§ñ AI Session Start\"]\n        Briefing[\"üìã Read Briefing<br/>(memory://briefing)\"]\n    end\n\n    subgraph Core[\"üìù Journal Operations\"]\n        Create[\"Create Entry\"]\n        Retrieve[\"Retrieve & Search\"]\n        Link[\"Link Entries\"]\n    end\n\n    subgraph Search[\"üîç Triple Search\"]\n        FTS[\"Full-Text (FTS5)\"]\n        Semantic[\"Semantic (Vector)\"]\n        DateRange[\"Date Range\"]\n    end\n\n    subgraph GitHub[\"üêô GitHub Integration\"]\n        Issues[\"Issues & Milestones\"]\n        PRs[\"Pull Requests\"]\n        Actions[\"GitHub Actions\"]\n        Kanban[\"Kanban Boards\"]\n    end\n\n    subgraph Outputs[\"üìä Outputs\"]\n        Reports[\"Standups & Retrospectives\"]\n        Graphs[\"Knowledge Graphs\"]\n        Timeline[\"Project Timelines\"]\n    end\n\n    Session --> Core\n    Core --> Search\n    Core <--> GitHub\n    Search --> Outputs\n    GitHub --> Outputs\n```\n\n### üìà **Current Capabilities**\n\n- **33 MCP tools** - Complete development workflow + backup/restore + Kanban + issue management\n- **15 workflow prompts** - Standups, retrospectives, PR workflows, CI/CD failure analysis, session acknowledgment\n- **18 MCP resources** - 12 static + 6 template (require parameters)\n- **GitHub Integration** - Projects, Issues, Pull Requests, Actions, **Kanban boards**\n- **8 tool groups** - `core`, `search`, `analytics`, `relationships`, `export`, `admin`, `github`, `backup`\n- **Knowledge graphs** - 8 relationship types, Mermaid visualization\n-",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 2,
        "text": "ns, **Kanban boards**\n- **8 tool groups** - `core`, `search`, `analytics`, `relationships`, `export`, `admin`, `github`, `backup`\n- **Knowledge graphs** - 8 relationship types, Mermaid visualization\n- **Semantic search** - AI-powered conceptual search via `@xenova/transformers`\n\n---\n\n## üéØ Why Memory Journal?",
        "start_pos": 3696,
        "end_pos": 4005,
        "token_count_estimate": 77,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 3,
        "text": "ualization** shows how different pieces of work connect\n\n---\n\n## üöÄ Quick Start\n\n### Option 1: npm (Recommended)\n\n**Step 1: Install the package**\n\n```bash\nnpm install -g memory-journal-mcp\n```\n\n**Step 2: Add to ~/.cursor/mcp.json**\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"memory-journal-mcp\"\n    }\n  }\n}\n```\n\n**Step 3: Restart Cursor**\n\nRestart Cursor or your MCP client, then start journaling!\n\n### Option 2: npx (No Installation)\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"memory-journal-mcp\"]\n    }\n  }\n}\n```\n\n### Option 3: From Source\n\n```bash\ngit clone https://github.com/neverinfamous/memory-journal-mcp.git\ncd memory-journal-mcp\nnpm install\nnpm run build\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"dist/cli.js\", \"--default-project\", \"1\"]\n    }\n  }\n}\n```\n\n### Option 4: HTTP/SSE Transport (Remote Access)\n\nFor remote access or web-based clients, run the server in HTTP mode:\n\n```bash\nmemory-journal-mcp --transport http --port 3000\n```\n\nTo bind to all interfaces (required for containers):\n\n```bash\nmemory-journal-mcp --transport http --port 3000 --server-host 0.0.0.0\n```\n\n**Endpoints:**\n\n- `POST /mcp` ‚Äî JSON-RPC requests (initialize, tools/call, etc.)\n- `GET /mcp` ‚Äî SSE stream for server-to-client notifications\n- `DELETE /mcp` ‚Äî Session termination\n\n**Session Management:** The server uses stateful sessions by default. Include the `mcp-session-id` header (returned from initialization) in subsequent requests.",
        "start_pos": 5544,
        "end_pos": 7118,
        "token_count_estimate": 393,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 4,
        "text": "-03-26\",\"capabilities\":{},\"clientInfo\":{\"name\":\"test\",\"version\":\"1.0\"}}}'\n# Returns mcp-session-id header\n\n# List tools (with session)\ncurl -X POST http://localhost:3000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"mcp-session-id: YOUR_SESSION_ID\" \\\n  -d '{\"jsonrpc\":\"2.0\",\"id\":2,\"method\":\"tools/list\",\"params\":{}}'\n```\n\n#### Stateless Mode (Serverless)\n\nFor serverless deployments (Lambda, Workers, Vercel), use stateless mode:\n\n```bash\nmemory-journal-mcp --transport http --port 3000 --stateless\n```\n\n| Mode                      | Progress Notifications | SSE Streaming | Serverless |\n| ------------------------- | ---------------------- | ------------- | ---------- |\n| Stateful (default)        | ‚úÖ Yes                 | ‚úÖ Yes        | ‚ö†Ô∏è Complex |\n| Stateless (`--stateless`) | ‚ùå No                  | ‚ùå No         | ‚úÖ Native  |\n\n### GitHub Integration Configuration\n\nThe GitHub tools (`get_github_issues`, `get_github_prs`, etc.) can auto-detect the repository from your git context. However, MCP clients may run the server from a different directory than your project.",
        "start_pos": 7392,
        "end_pos": 8527,
        "token_count_estimate": 283,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 5,
        "text": "|\n| `DEFAULT_PROJECT_NUMBER` | Default GitHub Project number for auto-assignment when creating issues |\n| `AUTO_REBUILD_INDEX`     | Set to `true` to rebuild vector index on server startup                |\n| `MCP_HOST`               | Server bind host (`0.0.0.0` for containers, default: `localhost`)      |\n\n**Without `GITHUB_REPO_PATH`**: You'll need to explicitly provide `owner` and `repo` parameters when calling GitHub tools.\n\n#### Fallback Behavior\n\nWhen GitHub tools cannot auto-detect repository information:\n\n1. **With `GITHUB_REPO_PATH` set**: Tools auto-detect `owner` and `repo` from git remote URL\n2. **Without `GITHUB_REPO_PATH`**: Tools return structured response with `requiresUserInput: true` and instructions to provide `owner` and `repo` parameters\n3. **With explicit parameters**: Always preferred - specify `owner` and `repo` directly in tool calls\n\n**Example response when auto-detection fails:**\n\n```json\n{\n  \"error\": \"Could not auto-detect repository\",\n  \"requiresUserInput\": true,\n  \"instruction\": \"Please provide owner and repo parameters\"\n}\n```\n\n### Client-Specific Notes\n\n**Cursor IDE:**\n\n- **Listing MCP Resources**: If the agent has trouble listing resources, instruct it to call `ListMcpResources()` without specifying a server parameter, or with `server: \"user-memory-journal-mcp\"` (Cursor prefixes server names with `user-`).\n\n**Google AntiGravity IDE:**\n\n- **AntiGravity Users:** Server instructions are automatically sent to MCP clients during initialization. However, AntiGravity does not currently support MCP server instructions. For optimal usage in AntiGravity, manually provide the contents of [`src/constants/ServerInstructions.ts`](src/constants/ServerInstructions.ts) to the agent in your prompt or user rules.\n\n- **Session start**: Add to your user rules: \"At session start, read `memory://briefing` from memory-journal-mcp.\"\n\n- **Full guidance**: If behaviors missing, read `memory://instructions` for complete Dynamic Context Management patterns.",
        "start_pos": 9240,
        "end_pos": 11241,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 6,
        "text": "rules: \"At session start, read `memory://briefing` from memory-journal-mcp.\"\n\n- **Full guidance**: If behaviors missing, read `memory://instructions` for complete Dynamic Context Management patterns.\n\n- **Prompts not available**: AntiGravity does not currently support MCP prompts. The 15 workflow prompts are not accessible.",
        "start_pos": 11041,
        "end_pos": 11367,
        "token_count_estimate": 81,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 7,
        "text": "ensive PR review preparation\n- `pr-retrospective` - Completed PR analysis with learnings\n- `actions-failure-digest` - CI/CD failure analysis\n- `confirm-briefing` - **NEW** Acknowledge session context to user\n\n**[Complete prompts guide ‚Üí](https://github.com/neverinfamous/memory-journal-mcp/wiki/Prompts)**\n\n### üì° **18 Resources** (12 Static + 6 Template)\n\n**Static Resources** (appear in resource lists):\n\n- `memory://briefing` - **Session initialization**: compact context for AI agents (~300 tokens)\n- `memory://instructions` - **Behavioral guidance**: complete server instructions (supports `?level=essential|standard|full`)\n- `memory://recent` - 10 most recent entries\n- `memory://significant` - Significant milestones and breakthroughs\n- `memory://graph/recent` - Live Mermaid diagram of recent relationships\n- `memory://team/recent` - Recent team-shared entries\n- `memory://health` - Server health & diagnostics\n- `memory://graph/actions` - CI/CD narrative graph\n- `memory://actions/recent` - Recent workflow runs\n- `memory://tags` - All tags with usage counts\n- `memory://statistics` - Journal statistics\n- `memory://github/status` - GitHub repository status overview\n\n**Template Resources** (require parameters, fetch directly by URI):\n\n- `memory://projects/{number}/timeline` - Project activity timeline\n- `memory://issues/{issue_number}/entries` - Entries linked to issue\n- `memory://prs/{pr_number}/entries` - Entries linked to PR\n- `memory://prs/{pr_number}/timeline` - Combined PR + journal timeline\n- `memory://kanban/{project_number}` - GitHub Project Kanban board\n- `memory://kanban/{project_number}/diagram` - Kanban Mermaid visualization\n\n---\n\n## üîß Configuration\n\n### GitHub Integration (Optional)\n\n```bash\nexport GITHUB_TOKEN=\"your_token\"              # For Projects/Issues/PRs\nexport GITHUB_ORG_TOKEN=\"your_org_token\"      # Optional: org projects\nexport DEFAULT_ORG=\"your-org-name\"            # Optional: default org\n```\n\n**Scopes:** `repo`, `project`, `read:org` (org only)\n\n### GitHub Management Capabilities\n\nMemory Journal",
        "start_pos": 12889,
        "end_pos": 14937,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 8,
        "text": "ional: org projects\nexport DEFAULT_ORG=\"your-org-name\"            # Optional: default org\n```\n\n**Scopes:** `repo`, `project`, `read:org` (org only)\n\n### GitHub Management Capabilities\n\nMemory Journal provides a **hybrid approach** to GitHub management:\n\n| Capability Source  | Purpose                                                                        |\n| ------------------ | ------------------------------------------------------------------------------ |\n| **MCP Server**     | Specialized features: Kanban visualization, journal linking, project timelines |\n| **Agent (gh CLI)** | Full GitHub mutations: create/close issues, create/merge PRs, manage releases  |\n\n**MCP Server Tools (Read + Kanban + Issue Lifecycle):**\n\n- `get_github_issues` / `get_github_issue` - Query issues\n- `get_github_prs` / `get_github_pr` - Query pull requests\n- `get_github_context` - Full repository context\n- `get_kanban_board` / `move_kanban_item` - **Kanban management**\n- `create_github_issue_with_entry` / `close_github_issue_with_entry` - **Issue lifecycle with journal linking**\n\n**Agent Operations (via gh CLI):**\n\n```bash\n# Issues\ngh issue create --title \"Bug fix\" --body \"Description\"\ngh issue close 42\n\n# Pull Requests\ngh pr create --fill\ngh pr merge 123\n\n# Projects\ngh project item-add 5 --owner neverinfamous --url \"issue-url\"\n\n# Releases\ngh release create v1.0.0 --generate-notes\n```\n\n> **Why this design?** The MCP server focuses on value-added features that integrate journal entries with GitHub (Kanban views, timeline resources, context linking). Standard GitHub operations are already excellently handled by `gh` CLI, which agents can invoke directly.",
        "start_pos": 14737,
        "end_pos": 16393,
        "token_count_estimate": 414,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 9,
        "text": "ORY_JOURNAL_MCP_TOOL_FILTER`:\n\n```bash\nexport MEMORY_JOURNAL_MCP_TOOL_FILTER=\"-analytics,-github\"\n```\n\n**Filter Syntax:**\n\n- `-group` - Disable all tools in a group\n- `-tool` - Disable a specific tool\n- `+tool` - Re-enable after group disable\n- Meta-groups: `starter`, `essential`, `full`, `readonly`\n\n**Example Configurations:**\n\n```json\n{\n  \"mcpServers\": {\n    \"memory-journal-mcp\": {\n      \"command\": \"memory-journal-mcp\",\n      \"env\": {\n        \"MEMORY_JOURNAL_MCP_TOOL_FILTER\": \"starter\",\n        \"GITHUB_TOKEN\": \"your_token\"\n      }\n    }\n  }\n}\n```\n\n| Configuration  | Filter String | Tools |\n| -------------- | ------------- | ----- |\n| Starter        | `starter`     | ~10   |\n| Essential      | `essential`   | ~6    |\n| Full (default) | `full`        | 33    |\n| Read-only      | `readonly`    | ~20   |\n\n**[Complete tool filtering guide ‚Üí](https://github.com/neverinfamous/memory-journal-mcp/wiki/Tool-Filtering)**\n\n---\n\n## üèóÔ∏è Architecture\n\n### Data Flow\n\n```mermaid\nflowchart TB\n    AI[\"ü§ñ AI Agent<br/>(Cursor, Windsurf, Claude)\"]\n\n    subgraph MCP[\"Memory Journal MCP Server\"]\n        Tools[\"üõ†Ô∏è 33 Tools\"]\n        Resources[\"üì° 18 Resources\"]\n        Prompts[\"üí¨ 15 Prompts\"]\n    end\n\n    subgraph Storage[\"Persistence Layer\"]\n        SQLite[(\"üíæ SQLite<br/>Entries, Tags, Relationships\")]\n        Vector[(\"üîç Vector Index<br/>Semantic Embeddings\")]\n        Backups[\"üì¶ Backups\"]\n    end\n\n    subgraph External[\"External Integrations\"]\n        GitHub[\"üêô GitHub API<br/>Issues, PRs, Actions\"]\n        Kanban[\"üìã Projects v2<br/>Kanban Boards\"]\n    end\n\n    AI <-->|\"MCP Protocol\"| MCP\n    Tools --> Storage\n    Tools --> External\n    Resources --> Storage\n    Resources --> External\n```\n\n### Stack\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ MCP Server Layer (TypeScript)                               ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ Tools (33)      ‚îÇ  ‚îÇ Resources (18)  ‚îÇ  ‚îÇ Prompts (15)‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ with Annotations‚îÇ  ‚îÇ with Annotations‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
        "start_pos": 16585,
        "end_pos": 18633,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 10,
        "text": "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ Tools (33)      ‚îÇ  ‚îÇ Resources (18)  ‚îÇ  ‚îÇ Prompts (15)‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ with Annotations‚îÇ  ‚îÇ with Annotations‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Pure JS Stack (No Native Dependencies)                      ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ sql.js          ‚îÇ  ‚îÇ vectra          ‚îÇ  ‚îÇ transformers‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ (SQLite)        ‚îÇ  ‚îÇ (Vector Index)  ‚îÇ  ‚îÇ (Embeddings)‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ SQLite Database with Hybrid Search                          ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n‚îÇ  ‚îÇ entries + tags + relationships + embeddings + backups   ‚îÇ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n## üîß Technical Highlights\n\n### Performance & Portability\n\n- **TypeScript + Pure JS Stack** - No native compilation, works everywhere\n- **sql.js** - SQLite in pure JavaScript with disk sync\n- **vectra** - Vector similarity search without native dependencies\n- **@xenova/transformers** - ML embeddings in JavaScript\n- **Lazy loading** - ML models load on first use, not startup\n\n### Security\n\n- **Local-first** - All data stored locally, no external API calls (except optional GitHub)\n- **Input validation** - Zod schemas, content size limits, SQL injection prevention\n- **Path traversal protection** - Backup filenames validated\n- **MCP 2025-11-25 annotations** - Behavioral hints (`readOnlyHint`, `destructiveHint`, etc.)\n\n### Data & Privacy\n\n- **Single SQLite file** - You own your data\n- **Portable** - Move your `.db` file anywhere\n- **Soft delete** - Entries can be recovered\n- **Auto-backup on restore** - Never lose data accidentally\n\n---\n\n## üìö Documentation & Resources\n\n- **[GitHub Wiki](https://github.com/neverinfam",
        "start_pos": 18433,
        "end_pos": 20481,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      },
      {
        "chunk_id": 11,
        "text": "anywhere\n- **Soft delete** - Entries can be recovered\n- **Auto-backup on restore** - Never lose data accidentally\n\n---\n\n## üìö Documentation & Resources\n\n- **[GitHub Wiki](https://github.com/neverinfamous/memory-journal-mcp/wiki)** - Complete documentation\n- **[Docker Hub](https://hub.docker.com/r/writenotenow/memory-journal-mcp)** - Container images\n- **[npm Package](https://www.npmjs.com/package/memory-journal-mcp)** - Node.js distribution\n- **[Issues](https://github.com/neverinfamous/memory-journal-mcp/issues)** - Bug reports & feature requests\n\n---\n\n## üìÑ License\n\nMIT License - See [LICENSE](LICENSE) file for details.\n\n## ü§ù Contributing\n\nBuilt by developers, for developers. PRs welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\n\n---\n\n_Migrating from v2.x?_ Your existing database is fully compatible. The TypeScript version uses the same schema and data format.",
        "start_pos": 20281,
        "end_pos": 21168,
        "token_count_estimate": 221,
        "source_type": "readme",
        "agent_id": "29d8cdce5c1ad0e9"
      }
    ]
  },
  {
    "agent_id": "9c6e1b1ad80635bf",
    "name": "ai.smithery/nickthelegend-test-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@nickthelegend/test-mcp/mcp",
    "description": "Create friendly, personalized greetings in seconds. Toggle Pirate Mode to speak like a pirate for‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-02T14:15:26.542177Z",
    "indexed_at": "2026-02-18T04:08:11.792456",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create friendly, personalized greetings",
        "Toggle Pirate Mode to generate pirate-style speech"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of functionality but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "27754a9b792e70ef",
    "name": "ai.smithery/oxylabs-oxylabs-mcp",
    "source": "mcp",
    "source_url": "https://github.com/oxylabs/oxylabs-mcp",
    "description": "Fetch and process content from specified URLs using the Oxylabs Web Scraper API.",
    "tools": [],
    "detected_capabilities": [
      "process"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T14:22:20.353742Z",
    "indexed_at": "2026-02-18T04:08:13.941484",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<p align=\"center\">\n  <img src=\"https://storage.googleapis.com/oxylabs-public-assets/oxylabs_mcp.svg\" alt=\"Oxylabs + MCP\">\n</p>\n<h1 align=\"center\" style=\"border-bottom: none;\">\n  Oxylabs MCP Server\n</h1>\n\n<p align=\"center\">\n  <em>The missing link between AI models and the real‚Äëworld web: one API that delivers clean, structured data from any site.</em>\n</p>\n\n<div align=\"center\">\n\n[![smithery badge](https://smithery.ai/badge/@oxylabs/oxylabs-mcp)](https://smithery.ai/server/@oxylabs/oxylabs-mcp)\n[![pypi package](https://img.shields.io/pypi/v/oxylabs-mcp?color=%2334D058&label=pypi%20package)](https://pypi.org/project/oxylabs-mcp/)\n[![](https://dcbadge.vercel.app/api/server/eWsVUJrnG5?style=flat)](https://discord.gg/Pds3gBmKMH)\n[![Licence](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/f6a9c0bc-83a6-4f78-89d9-f2cec4ece98d)\n![Coverage badge](https://raw.githubusercontent.com/oxylabs/oxylabs-mcp/coverage/coverage-badge.svg)\n\n<br/>\n<a href=\"https://glama.ai/mcp/servers/@oxylabs/oxylabs-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@oxylabs/oxylabs-mcp/badge\" alt=\"Oxylabs Server MCP server\" />\n</a>\n\n</div>\n\n---\n\n## üìñ Overview\n\nThe Oxylabs MCP server provides a bridge between AI models and the web. It enables them to scrape any URL, render JavaScript-heavy pages, extract and format content for AI use, bypass anti-scraping measures, and access geo-restricted web data from 195+ countries.\n\n\n## üõ†Ô∏è MCP Tools\n\nOxylabs MCP provides two sets of tools that can be used together or independently:\n\n### Oxylabs Web Scraper API Tools\n1. **universal_scraper**: Uses Oxylabs Web Scraper API for general website scraping;\n2. **google_search_scraper**: Uses Oxylabs Web Scraper API to extract results from Google Search;\n3. **amazon_search_scraper**: Uses Oxylabs Web Scraper API to scrape Amazon search result pages;\n4. **amazon_product_scraper**: Uses Oxylabs Web Scraper API to extract data from individual Amazon product pages.\n\n### Oxylabs AI Studio Tools\n\n5. **ai_scraper**: Scrape content from any URL in JSON or Markdown format with AI-powered data extraction;\n6. **ai_crawler**: Based on a prompt, crawls a website and collects data in Markdown or JSON format across multiple pages;\n7. **ai_browser_agent**: Based on prompt, controls a browser and returns data in Markdown, JSON, HTML, or screenshot formats;\n8. **ai_search**: Search the web for URLs and their contents with AI-powered content extraction.\n\n\n## ‚úÖ Prerequisites\n\nBefore you begin, make sure you have **at least one** of the following:\n\n- **Oxylabs Web Scraper API Account**: Obtain your username and password from [Oxylabs](https://dashboard.oxylabs.io/) (1-week free trial available);\n- **Oxylabs AI Studio API Key**: Obtain your API key from [Oxylabs AI Studio](https://aistudio.oxylabs.io/settings/api-key). (1000 credits free).\n\n## üì¶ Configuration\n\n### Environment variables\n\nOxylabs MCP server supports the following environment variables:\n| Name                       | Description                                   | Default |\n|----------------------------|-----------------------------------------------|---------|\n| `OXYLABS_USERNAME`         | Your Oxylabs Web Scraper API username         |         |\n| `OXYLABS_PASSWORD`         | Your Oxylabs Web Scraper API password         |         |\n| `OXYLABS_AI_STUDIO_API_KEY`| Your Oxylabs AI Studio API key                |         |\n| `LOG_LEVEL`                | Log level for the logs returned to the client | `INFO`  |\n\nBased on provided credentials, the server will automatically expose the corresponding tools:\n- If only `OXYLABS_USERNAME` and `OXYLABS_PASSWORD` are provided, the server will expose the Web Scraper API tools;\n- If only `OXYLABS_AI_STUDIO_API_KEY` is provided, the server will expose the AI Studio tools;\n- If both `OXYLABS_USERNAME` and `OXYLABS_PASSWORD` and `OXYLABS_AI_STUDIO_API_KEY` are provided, the server will expose all tools.\n\n‚ùó‚ùó‚ùó **Important note: if you don't have Web Scraper API or Oxylabs AI studio credentials, delete the corresponding environment variables placeholders.\nLeaving placeholder values will result in exposed tools that do not work.**\n\n\n\n### Configure with uvx\n\n- Install the uvx package manager:\n  ```bash\n  # macOS and Linux\n  curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n  OR:\n  ```bash\n  # Windows\n  powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n- Use the following config:\n  ```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"command\": \"uvx\",\n        \"args\": [\"oxylabs-mcp\"],\n        \"env\": {\n          \"OXYLABS_USERNAME\": \"OXYLABS_USERNAME\",\n          \"OXYLABS_PASSWORD\": \"OXYLABS_PASSWORD\",\n          \"OXYLABS_AI_STUDIO_API_KEY\": \"OXYLABS_AI_STUDIO_API_KEY\"\n        }\n      }\n    }\n  }\n  ```\n\n### Configure with uv\n\n- Install the uv package manager:\n  ```bash\n  # macOS and Linux\n  curl -LsSf https://astral.sh/uv/install.sh | sh\n  ```\n  OR:\n  ```bash\n  # Windows\n  powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n  ```\n\n- Use the following config:\n  ```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"command\": \"uv\",\n        \"args\": [\n          \"--directory\",\n          \"/<Absolute-path-to-folder>/oxylabs-mcp\",\n          \"run\",\n          \"oxylabs-mcp\"\n        ],\n        \"env\": {\n          \"OXYLABS_USERNAME\": \"OXYLABS_USERNAME\",\n          \"OXYLABS_PASSWORD\": \"OXYLABS_PASSWORD\",\n          \"OXYLABS_AI_STUDIO_API_KEY\": \"OXYLABS_AI_STUDIO_API_KEY\"\n        }\n      }\n    }\n  }\n  ```\n\n### Configure with Smithery Oauth2\n\n- Go to https://smithery.ai/server/@oxylabs/oxylabs-mcp;\n- Click _Auto_ to install the Oxylabs MCP configuration for the respective client;\n- OR use the following config:\n```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"url\": \"https://server.smithery.ai/@oxylabs/oxylabs-mcp/mcp\"\n      }\n    }\n  }\n```\n- Follow the instructions to authenticate Oxylabs MCP with Oauth2 flow\n\n### Configure with Smithery query parameters\n\nIn case your client does not support the Oauth2 authentication, you can pass the Oxylabs authentication parameters directly in url\n```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"url\": \"https://server.smithery.ai/@oxylabs/oxylabs-mcp/mcp?oxylabsUsername=OXYLABS_USERNAME&oxylabsPassword=OXYLABS_PASSWORD&oxylabsAiStudioApiKey=OXYLABS_AI_STUDIO_API_KEY\"\n      }\n    }\n  }\n```\n\n### Manual Setup with Claude Desktop\n\nNavigate to **Claude ‚Üí Settings ‚Üí Developer ‚Üí Edit Config** and add one of the configurations above to the `claude_desktop_config.json` file.\n\n### Manual Setup with Cursor AI\n\nNavigate to **Cursor ‚Üí Settings ‚Üí Cursor Settings ‚Üí MCP**. Click **Add new global MCP server** and add one of the configurations above.\n\n\n\n## üìù Logging\n\nServer provides additional information about the tool calls in `notification/message` events\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"info\",\n    \"data\": \"Create job with params: {\\\"url\\\": \\\"https://ip.oxylabs.io\\\"}\"\n  }\n}\n```\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"info\",\n    \"data\": \"Job info: job_id=7333113830223918081 job_status=done\"\n  }\n}\n```\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"error\",\n    \"data\": \"Error: request to Oxylabs API failed\"\n  }\n}\n```\n\n---\n\n## üõ°Ô∏è License\n\nDistributed under the MIT License ‚Äì see [LICENSE](LICENSE) for details.\n\n---\n\n## About Oxylabs\n\nEstablished in 2015, Oxylabs is a market-leading web intelligence collection\nplatform, driven by the highest business, ethics, and compliance standards,\nenabling companies worldwide to unlock data-driven insights.\n\n[![image](https://oxylabs.io/images/og-image.png)](https://oxylabs.io/)\n\n<div align=\"center\">\n<sub>\n  Made with ‚òï by <a href=\"https://oxylabs.io\">Oxylabs</a>.  Feel free to give us a ‚≠ê if MCP saved you a weekend.\n</sub>\n</div>\n\n\n## ‚ú® Key Features\n\n<details>\n<summary><strong> Scrape content from any site</strong></summary>\n<br>\n\n- Extract data from any URL, including complex single-page applications\n- Fully render dynamic websites using headless browser support\n- Choose full JavaScript rendering, HTML-only, or none\n- Emulate Mobile and Desktop viewports for realistic rendering\n\n</details>\n\n<details>\n<summary><strong> Automatically get AI-ready data</strong></summary>\n<br>\n\n- Automatically clean and convert HTML to Markdown for improved readability\n- Use automated parsers for popular targets like Google, Amazon, and more\n\n</details>\n\n<details>\n<summary><strong> Bypass blocks & geo-restrictions</strong></summary>\n<br>\n\n- Bypass sophisticated bot protection systems with high success rate\n- Reliably scrape even the most complex websites\n- Get automatically rotating IPs from a proxy pool covering 195+ countries\n\n</details>\n\n<details>\n<summary><strong> Flexible setup & cross-platform support</strong></summary>\n<br>\n\n- Set rendering and parsing options if needed\n- Feed data directly into AI models or analytics tools\n- Works on macOS, Windows, and Linux\n\n</details>\n\n<details>\n<summary><strong> Built-in error handling and request management</strong></summary>\n<br>\n\n- Comprehensive error handling and reporting\n- Smart rate limiting and request management\n\n</details>\n\n---\n\n\n## Why Oxylabs MCP? &nbsp;üï∏Ô∏è ‚ûú üì¶ ‚ûú ü§ñ\n\nImagine telling your LLM *\"Summarise the latest Hacker News discussion about GPT‚Äë5\"* ‚Äì and it simply answers.  \nMCP (Multi‚ÄëClient Proxy) makes that happen by doing the boring parts for you:\n\n| What Oxylabs MCP does                                             | Why it matters to you                    |\n|-------------------------------------------------------------------|------------------------------------------|\n| **Bypasses anti‚Äëbot walls** with the Oxylabs global proxy network | Keeps you unblocked and anonymous        |\n| **Renders JavaScript** in headless Chrome                         | Single‚Äëpage apps, sorted                 |\n| **Cleans HTML ‚Üí JSON**                                            | Drop straight into vector DBs or prompts |\n| **Optional structured parsers** (Google, Amazon, etc.)            | One‚Äëline access to popular targets       |\n\nmcp-name: io.oxylabs/oxylabs-mcp\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Scrape content from any URL including JavaScript-heavy and single-page applications",
        "Render dynamic websites using headless Chrome with options for full JavaScript rendering, HTML-only, or no rendering",
        "Extract and format web content into AI-ready JSON or Markdown formats",
        "Bypass anti-scraping measures and sophisticated bot protection systems using a global proxy network",
        "Access geo-restricted web data from over 195 countries with automatically rotating IPs",
        "Use specialized parsers for popular targets such as Google Search and Amazon product and search pages",
        "Control a browser via AI prompts to return data in Markdown, JSON, HTML, or screenshot formats",
        "Crawl websites based on prompts to collect data across multiple pages",
        "Provide comprehensive error handling, logging, and smart rate limiting for request management"
      ],
      "limitations": [
        "Requires valid Oxylabs Web Scraper API credentials or Oxylabs AI Studio API key to function",
        "Tools will not work if placeholder environment variables for credentials are left without real values",
        "No explicit mention of support for non-web data sources or APIs outside Oxylabs ecosystem",
        "No stated support for real-time streaming data or continuous monitoring",
        "No detailed rate limit numbers or quotas disclosed in the documentation"
      ],
      "requirements": [
        "Oxylabs Web Scraper API account with username and password for Web Scraper API tools",
        "Oxylabs AI Studio API key for AI Studio tools",
        "Environment variables set for credentials: OXYLABS_USERNAME, OXYLABS_PASSWORD, and/or OXYLABS_AI_STUDIO_API_KEY",
        "Supported operating systems: macOS, Windows, Linux",
        "Optional: uv or uvx package managers for server configuration",
        "Optional: Smithery platform for OAuth2 authentication and configuration"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples of multiple tools, clear prerequisites and environment setup, descriptions of capabilities and limitations, and guidance for various configuration methods including OAuth2 and manual setups.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<p align=\"center\">\n  <img src=\"https://storage.googleapis.com/oxylabs-public-assets/oxylabs_mcp.svg\" alt=\"Oxylabs + MCP\">\n</p>\n<h1 align=\"center\" style=\"border-bottom: none;\">\n  Oxylabs MCP Server\n</h1>\n\n<p align=\"center\">\n  <em>The missing link between AI models and the real‚Äëworld web: one API that delivers clean, structured data from any site.</em>\n</p>\n\n<div align=\"center\">\n\n[![smithery badge](https://smithery.ai/badge/@oxylabs/oxylabs-mcp)](https://smithery.ai/server/@oxylabs/oxylabs-mcp)\n[![pypi package](https://img.shields.io/pypi/v/oxylabs-mcp?color=%2334D058&label=pypi%20package)](https://pypi.org/project/oxylabs-mcp/)\n[![](https://dcbadge.vercel.app/api/server/eWsVUJrnG5?style=flat)](https://discord.gg/Pds3gBmKMH)\n[![Licence](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/f6a9c0bc-83a6-4f78-89d9-f2cec4ece98d)\n![Coverage badge](https://raw.githubusercontent.com/oxylabs/oxylabs-mcp/coverage/coverage-badge.svg)\n\n<br/>\n<a href=\"https://glama.ai/mcp/servers/@oxylabs/oxylabs-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@oxylabs/oxylabs-mcp/badge\" alt=\"Oxylabs Server MCP server\" />\n</a>\n\n</div>\n\n---\n\n## üìñ Overview\n\nThe Oxylabs MCP server provides a bridge between AI models and the web. It enables them to scrape any URL, render JavaScript-heavy pages, extract and format content for AI use, bypass anti-scraping measures, and access geo-restricted web data from 195+ countries.\n\n\n## üõ†Ô∏è MCP Tools\n\nOxylabs MCP provides two sets of tools that can be used together or independently:\n\n### Oxylabs Web Scraper API Tools\n1. **universal_scraper**: Uses Oxylabs Web Scraper API for general website scraping;\n2. **google_search_scraper**: Uses Oxylabs Web Scraper API to extract results from Google Search;\n3. **amazon_search_scraper**: Uses Oxylabs Web Scraper API to scrape Amazon search result pages;\n4. **amazon_product_scraper**: Uses Oxylabs Web Scraper API to extract data from individual Amazon product pages.",
        "start_pos": 0,
        "end_pos": 2042,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "27754a9b792e70ef"
      },
      {
        "chunk_id": 1,
        "text": "n_search_scraper**: Uses Oxylabs Web Scraper API to scrape Amazon search result pages;\n4. **amazon_product_scraper**: Uses Oxylabs Web Scraper API to extract data from individual Amazon product pages.\n\n### Oxylabs AI Studio Tools\n\n5. **ai_scraper**: Scrape content from any URL in JSON or Markdown format with AI-powered data extraction;\n6. **ai_crawler**: Based on a prompt, crawls a website and collects data in Markdown or JSON format across multiple pages;\n7. **ai_browser_agent**: Based on prompt, controls a browser and returns data in Markdown, JSON, HTML, or screenshot formats;\n8. **ai_search**: Search the web for URLs and their contents with AI-powered content extraction.\n\n\n## ‚úÖ Prerequisites\n\nBefore you begin, make sure you have **at least one** of the following:\n\n- **Oxylabs Web Scraper API Account**: Obtain your username and password from [Oxylabs](https://dashboard.oxylabs.io/) (1-week free trial available);\n- **Oxylabs AI Studio API Key**: Obtain your API key from [Oxylabs AI Studio](https://aistudio.oxylabs.io/settings/api-key). (1000 credits free).",
        "start_pos": 1842,
        "end_pos": 2916,
        "token_count_estimate": 268,
        "source_type": "readme",
        "agent_id": "27754a9b792e70ef"
      },
      {
        "chunk_id": 2,
        "text": "`OXYLABS_PASSWORD` are provided, the server will expose the Web Scraper API tools;\n- If only `OXYLABS_AI_STUDIO_API_KEY` is provided, the server will expose the AI Studio tools;\n- If both `OXYLABS_USERNAME` and `OXYLABS_PASSWORD` and `OXYLABS_AI_STUDIO_API_KEY` are provided, the server will expose all tools.\n\n‚ùó‚ùó‚ùó **Important note: if you don't have Web Scraper API or Oxylabs AI studio credentials, delete the corresponding environment variables placeholders.",
        "start_pos": 3690,
        "end_pos": 4152,
        "token_count_estimate": 115,
        "source_type": "readme",
        "agent_id": "27754a9b792e70ef"
      },
      {
        "chunk_id": 3,
        "text": "_STUDIO_API_KEY\"\n        }\n      }\n    }\n  }\n  ```\n\n### Configure with Smithery Oauth2\n\n- Go to https://smithery.ai/server/@oxylabs/oxylabs-mcp;\n- Click _Auto_ to install the Oxylabs MCP configuration for the respective client;\n- OR use the following config:\n```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"url\": \"https://server.smithery.ai/@oxylabs/oxylabs-mcp/mcp\"\n      }\n    }\n  }\n```\n- Follow the instructions to authenticate Oxylabs MCP with Oauth2 flow\n\n### Configure with Smithery query parameters\n\nIn case your client does not support the Oauth2 authentication, you can pass the Oxylabs authentication parameters directly in url\n```json\n  {\n    \"mcpServers\": {\n      \"oxylabs\": {\n        \"url\": \"https://server.smithery.ai/@oxylabs/oxylabs-mcp/mcp?oxylabsUsername=OXYLABS_USERNAME&oxylabsPassword=OXYLABS_PASSWORD&oxylabsAiStudioApiKey=OXYLABS_AI_STUDIO_API_KEY\"\n      }\n    }\n  }\n```\n\n### Manual Setup with Claude Desktop\n\nNavigate to **Claude ‚Üí Settings ‚Üí Developer ‚Üí Edit Config** and add one of the configurations above to the `claude_desktop_config.json` file.\n\n### Manual Setup with Cursor AI\n\nNavigate to **Cursor ‚Üí Settings ‚Üí Cursor Settings ‚Üí MCP**. Click **Add new global MCP server** and add one of the configurations above.\n\n\n\n## üìù Logging\n\nServer provides additional information about the tool calls in `notification/message` events\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"info\",\n    \"data\": \"Create job with params: {\\\"url\\\": \\\"https://ip.oxylabs.io\\\"}\"\n  }\n}\n```\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"info\",\n    \"data\": \"Job info: job_id=7333113830223918081 job_status=done\"\n  }\n}\n```\n\n```json\n{\n  \"method\": \"notifications/message\",\n  \"params\": {\n    \"level\": \"error\",\n    \"data\": \"Error: request to Oxylabs API failed\"\n  }\n}\n```\n\n---\n\n## üõ°Ô∏è License\n\nDistributed under the MIT License ‚Äì see [LICENSE](LICENSE) for details.",
        "start_pos": 5538,
        "end_pos": 7470,
        "token_count_estimate": 483,
        "source_type": "readme",
        "agent_id": "27754a9b792e70ef"
      },
      {
        "chunk_id": 4,
        "text": "/message\",\n  \"params\": {\n    \"level\": \"error\",\n    \"data\": \"Error: request to Oxylabs API failed\"\n  }\n}\n```\n\n---\n\n## üõ°Ô∏è License\n\nDistributed under the MIT License ‚Äì see [LICENSE](LICENSE) for details.\n\n---\n\n## About Oxylabs\n\nEstablished in 2015, Oxylabs is a market-leading web intelligence collection\nplatform, driven by the highest business, ethics, and compliance standards,\nenabling companies worldwide to unlock data-driven insights.\n\n[![image](https://oxylabs.io/images/og-image.png)](https://oxylabs.io/)\n\n<div align=\"center\">\n<sub>\n  Made with ‚òï by <a href=\"https://oxylabs.io\">Oxylabs</a>.  Feel free to give us a ‚≠ê if MCP saved you a weekend.",
        "start_pos": 7270,
        "end_pos": 7922,
        "token_count_estimate": 163,
        "source_type": "readme",
        "agent_id": "27754a9b792e70ef"
      },
      {
        "chunk_id": 5,
        "text": "Built-in error handling and request management</strong></summary>\n<br>\n\n- Comprehensive error handling and reporting\n- Smart rate limiting and request management\n\n</details>\n\n---\n\n\n## Why Oxylabs MCP? &nbsp;üï∏Ô∏è ‚ûú üì¶ ‚ûú ü§ñ\n\nImagine telling your LLM *\"Summarise the latest Hacker News discussion about GPT‚Äë5\"* ‚Äì and it simply answers.  \nMCP (Multi‚ÄëClient Proxy) makes that happen by doing the boring parts for you:\n\n| What Oxylabs MCP does                                             | Why it matters to you                    |\n|-------------------------------------------------------------------|------------------------------------------|\n| **Bypasses anti‚Äëbot walls** with the Oxylabs global proxy network | Keeps you unblocked and anonymous        |\n| **Renders JavaScript** in headless Chrome                         | Single‚Äëpage apps, sorted                 |\n| **Cleans HTML ‚Üí JSON**                                            | Drop straight into vector DBs or prompts |\n| **Optional structured parsers** (Google, Amazon, etc.)            | One‚Äëline access to popular targets       |\n\nmcp-name: io.oxylabs/oxylabs-mcp",
        "start_pos": 9118,
        "end_pos": 10240,
        "token_count_estimate": 280,
        "source_type": "readme",
        "agent_id": "27754a9b792e70ef"
      }
    ]
  },
  {
    "agent_id": "cb501b94163fd1fe",
    "name": "ai.smithery/pinion05-supabase-mcp-lite",
    "source": "mcp",
    "source_url": "https://github.com/pinion05/supabase-mcp-lite",
    "description": "Same functionality, consuming only 1/20 of the context window tokens.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-17T13:24:18.881488Z",
    "indexed_at": "2026-02-18T04:08:15.828096",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Supabase MCP Lite\n<img width=\"518\" height=\"141\" alt=\"image\" src=\"https://github.com/user-attachments/assets/04193768-632c-4e32-93d2-291158c804b5\" />\n\nMinimal Supabase MCP server - 70% less context usage than standard implementations.\n\n## Why Lite?\n\n- **4 tools instead of 50+** - Only essential operations  \n- **Minimal descriptions** - No verbose explanations\n- **Simple parameters** - No complex nested schemas\n- **Auto-truncated results** - Max 100 rows per query\n\n## üîë Personal Access Token Required\n\nThis MCP uses your **Supabase Personal Access Token** (starts with `sbp_`) to automatically fetch service role keys for any project you own.\n\n### How to get your Personal Access Token:\n1. Go to https://supabase.com/dashboard/account/tokens\n2. Click \"Generate New Token\"\n3. Give it a name (e.g., \"MCP Access\")\n4. Copy the token (starts with `sbp_`)\n5. **Save it securely** - you won't be able to see it again!\n\n## Setup\n\n1. Add to your MCP client configuration:\n\n```json\n{\n  \"supabase-lite\": {\n    \"command\": \"npx\",\n    \"args\": [\"@smithery/cli\", \"connect\", \"@pinion05/supabase-mcp-lite\"],\n    \"config\": {\n      \"accessToken\": \"sbp_xxxxxxxxxxxx\"  // Your Personal Access Token\n    }\n  }\n}\n```\n\n**Note**: Project URL is required for each tool call. The service role key will be fetched automatically using your access token.\n\n## Tools (4)\n\nAll tools require `projectUrl` as the first parameter.\n\n| Tool | Purpose | Parameters |\n|------|---------|------------|\n| `select` | Get data | projectUrl, table, where?, limit? |\n| `mutate` | Change data | projectUrl, action, table, data?, where? |\n| `storage` | Files | projectUrl, action, bucket, path?, data? |\n| `auth` | Users | projectUrl, action, email?, password?, id? |\n\n## Examples\n\n```javascript\n// Select tool  \nselect(\n  projectUrl: \"https://your-project.supabase.co\",\n  table: \"posts\", \n  where: {status: \"published\"}, \n  limit: 10\n)\n\n// Mutate tool\nmutate(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"insert\", \n  table: \"todos\", \n  data: {title: \"New task\"}\n)\nmutate(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"update\", \n  table: \"todos\", \n  data: {done: true}, \n  where: {id: 1}\n)\nmutate(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"delete\", \n  table: \"todos\", \n  where: {id: 1}\n)\n\n// Storage tool\nstorage(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"upload\", \n  bucket: \"images\", \n  path: \"avatar.jpg\", \n  data: \"base64...\"\n)\nstorage(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"list\", \n  bucket: \"images\"\n)\n\n// Auth tool\nauth(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"list\"\n)\nauth(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"create\", \n  email: \"user@example.com\", \n  password: \"secure123\"\n)\n```\n\n## How it Works\n\n1. You provide your Personal Access Token (`sbp_xxx`)\n2. When you call a tool with a project URL, the MCP:\n   - Extracts the project ID from the URL\n   - Uses your access token to fetch the service role key via Supabase Management API\n   - Caches the key for future requests to the same project\n   - Creates a client with full admin access\n\n## Security Notes\n\n- Personal Access Token gives access to ALL your Supabase projects\n- Service role keys are fetched automatically and cached in memory\n- Service role key bypasses Row Level Security (RLS)\n- Keep your access token secure - never expose it client-side\n- This tool is intended for server-side/admin use only\n\n## Features\n\n- ‚úÖ Works with any Supabase project you own\n- ‚úÖ Automatic service role key retrieval\n- ‚úÖ Key caching to minimize API calls\n- ‚úÖ Full database access (bypasses RLS)\n- ‚úÖ Support for multiple projects in one session\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Fetch service role keys automatically for owned Supabase projects using a personal access token",
        "Select data from Supabase tables with optional filtering and row limits",
        "Mutate data in Supabase tables including insert, update, and delete operations",
        "Manage files in Supabase storage buckets including upload and list actions",
        "Manage Supabase users including listing and creating users"
      ],
      "limitations": [
        "Maximum of 100 rows returned per query due to auto-truncation",
        "Requires a Supabase Personal Access Token with access to projects",
        "Service role key caching is in-memory only and scoped per session",
        "Intended for server-side/admin use only; not safe for client-side exposure",
        "Limited to 4 essential tools instead of full Supabase API capabilities"
      ],
      "requirements": [
        "Supabase Personal Access Token starting with 'sbp_'",
        "Supabase project URL for each tool call",
        "Node.js environment to run the MCP client with npx and @smithery/cli",
        "Access to Supabase Management API to fetch service role keys"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for all tools, clear explanation of requirements, and explicit limitations and security notes.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Supabase MCP Lite\n<img width=\"518\" height=\"141\" alt=\"image\" src=\"https://github.com/user-attachments/assets/04193768-632c-4e32-93d2-291158c804b5\" />\n\nMinimal Supabase MCP server - 70% less context usage than standard implementations.\n\n## Why Lite?\n\n- **4 tools instead of 50+** - Only essential operations  \n- **Minimal descriptions** - No verbose explanations\n- **Simple parameters** - No complex nested schemas\n- **Auto-truncated results** - Max 100 rows per query\n\n## üîë Personal Access Token Required\n\nThis MCP uses your **Supabase Personal Access Token** (starts with `sbp_`) to automatically fetch service role keys for any project you own.\n\n### How to get your Personal Access Token:\n1. Go to https://supabase.com/dashboard/account/tokens\n2. Click \"Generate New Token\"\n3. Give it a name (e.g., \"MCP Access\")\n4. Copy the token (starts with `sbp_`)\n5. **Save it securely** - you won't be able to see it again!\n\n## Setup\n\n1. Add to your MCP client configuration:\n\n```json\n{\n  \"supabase-lite\": {\n    \"command\": \"npx\",\n    \"args\": [\"@smithery/cli\", \"connect\", \"@pinion05/supabase-mcp-lite\"],\n    \"config\": {\n      \"accessToken\": \"sbp_xxxxxxxxxxxx\"  // Your Personal Access Token\n    }\n  }\n}\n```\n\n**Note**: Project URL is required for each tool call. The service role key will be fetched automatically using your access token.\n\n## Tools (4)\n\nAll tools require `projectUrl` as the first parameter.",
        "start_pos": 0,
        "end_pos": 1398,
        "token_count_estimate": 349,
        "source_type": "readme",
        "agent_id": "cb501b94163fd1fe"
      },
      {
        "chunk_id": 1,
        "text": "here: {status: \"published\"}, \n  limit: 10\n)\n\n// Mutate tool\nmutate(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"insert\", \n  table: \"todos\", \n  data: {title: \"New task\"}\n)\nmutate(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"update\", \n  table: \"todos\", \n  data: {done: true}, \n  where: {id: 1}\n)\nmutate(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"delete\", \n  table: \"todos\", \n  where: {id: 1}\n)\n\n// Storage tool\nstorage(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"upload\", \n  bucket: \"images\", \n  path: \"avatar.jpg\", \n  data: \"base64...\"\n)\nstorage(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"list\", \n  bucket: \"images\"\n)\n\n// Auth tool\nauth(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"list\"\n)\nauth(\n  projectUrl: \"https://your-project.supabase.co\",\n  action: \"create\", \n  email: \"user@example.com\", \n  password: \"secure123\"\n)\n```\n\n## How it Works\n\n1. You provide your Personal Access Token (`sbp_xxx`)\n2. When you call a tool with a project URL, the MCP:\n   - Extracts the project ID from the URL\n   - Uses your access token to fetch the service role key via Supabase Management API\n   - Caches the key for future requests to the same project\n   - Creates a client with full admin access\n\n## Security Notes\n\n- Personal Access Token gives access to ALL your Supabase projects\n- Service role keys are fetched automatically and cached in memory\n- Service role key bypasses Row Level Security (RLS)\n- Keep your access token secure - never expose it client-side\n- This tool is intended for server-side/admin use only\n\n## Features\n\n- ‚úÖ Works with any Supabase project you own\n- ‚úÖ Automatic service role key retrieval\n- ‚úÖ Key caching to minimize API calls\n- ‚úÖ Full database access (bypasses RLS)\n- ‚úÖ Support for multiple projects in one session\n\n## License\n\nMIT",
        "start_pos": 1848,
        "end_pos": 3706,
        "token_count_estimate": 464,
        "source_type": "readme",
        "agent_id": "cb501b94163fd1fe"
      },
      {
        "chunk_id": 2,
        "text": "roject you own\n- ‚úÖ Automatic service role key retrieval\n- ‚úÖ Key caching to minimize API calls\n- ‚úÖ Full database access (bypasses RLS)\n- ‚úÖ Support for multiple projects in one session\n\n## License\n\nMIT",
        "start_pos": 3506,
        "end_pos": 3706,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "cb501b94163fd1fe"
      }
    ]
  },
  {
    "agent_id": "1e68479e6d2f9490",
    "name": "ai.smithery/pinkpixel-dev-web-scout-mcp",
    "source": "mcp",
    "source_url": "https://github.com/pinkpixel-dev/web-scout-mcp",
    "description": "Search the web and extract clean, readable text from webpages. Process multiple URLs at once to sp‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search",
      "process"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T03:20:23.189856Z",
    "indexed_at": "2026-02-18T04:08:17.780228",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/pinkpixel-dev-web-scout-mcp-badge.png)](https://mseep.ai/app/pinkpixel-dev-web-scout-mcp)\n\n<p align=\"center\">\n  <img src=\"assets/logo.png\" alt=\"Web Scout MCP Logo\" width=\"300\"/>\n</p>\n\n<h1 align=\"center\">Web Scout MCP Server</h1>\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/@pinkpixel/web-scout-mcp\"><img src=\"https://img.shields.io/npm/v/@pinkpixel/web-scout-mcp.svg\" alt=\"npm version\"></a>\n  <a href=\"https://github.com/pinkpixel-dev/web-scout-mcp/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"License\"></a>\n  <a href=\"https://nodejs.org/en/\"><img src=\"https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg\" alt=\"Node.js Version\"></a>\n  <a href=\"https://smithery.ai/badge/@pinkpixel-dev/web-scout-mcp\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@pinkpixel-dev/web-scout-mcp\"></a>\n  <a href=\"https://mseep.ai/app/f19a6453-c635-4bc8-b26a-3e9e36428a98\"><img src=\"https://mseep.ai/badge.svg\" alt=\"Verified on MseeP\"></a>\n</p>\n\n<p align=\"center\">\n  An MCP server for web search using DuckDuckGo and content extraction, with support for multiple URLs and memory optimizations.\n</p>\n\n## ‚ú® Features\n\n- üîç **DuckDuckGo Search**: Fast and privacy-focused web search capability\n- üìÑ **Content Extraction**: Clean, readable text extraction from web pages\n- üöÄ **Parallel Processing**: Support for extracting content from multiple URLs simultaneously\n- üíæ **Memory Optimization**: Smart memory management to prevent application crashes\n- ‚è±Ô∏è **Rate Limiting**: Intelligent request throttling to avoid API blocks\n- üõ°Ô∏è **Error Handling**: Robust error handling for reliable operation\n\n## üì¶ Installation\n\n### Installing via Smithery\n\nTo install Web Scout for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@pinkpixel-dev/web-scout-mcp):\n\n```bash\nnpx -y @smithery/cli install @pinkpixel-dev/web-scout-mcp --client claude\n```\n\n### Global Installation\n\n```bash\nnpm install -g @pinkpixel/web-scout-mcp\n```\n\n### Local Installation\n\n```bash\nnpm install @pinkpixel/web-scout-mcp\n```\n\n## üöÄ Usage\n\n### Command Line\n\nAfter installing globally, run:\n\n```bash\nweb-scout-mcp\n```\n\n### With MCP Clients\n\nAdd this to your MCP client's `config.json` (Claude Desktop, Cursor, etc.):\n\n```json\n{\n  \"mcpServers\": {\n    \"web-scout\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@pinkpixel/web-scout-mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n### Environment Variables\n\nSet the `WEB_SCOUT_DISABLE_AUTOSTART=1` environment variable when embedding the package and calling `createServer()` yourself. By default running the published entrypoint (for example `node dist/index.js` or `npx @pinkpixel/web-scout-mcp`) automatically bootstraps the stdio transport.\n\n## üß∞ Tools\n\nThe server provides the following MCP tools:\n\n### üîç DuckDuckGoWebSearch\n\nInitiates a web search query using the DuckDuckGo search engine and returns a well-structured list of findings.\n\n**Input:**\n- `query` (string): The search query string\n- `maxResults` (number, optional): Maximum number of results to return (default: 10)\n\n**Example:**\n```json\n{\n  \"query\": \"latest advancements in AI\",\n  \"maxResults\": 5\n}\n```\n\n**Output:**\nA formatted list of search results with titles, URLs, and snippets.\n\n### üìÑ UrlContentExtractor\n\nFetches and extracts clean, readable content from web pages by removing unnecessary elements like scripts, styles, and navigation.\n\n**Input:**\n- `url`: Either a single URL string or an array of URL strings\n\n**Example (single URL):**\n```json\n{\n  \"url\": \"https://example.com/article\"\n}\n```\n\n**Example (multiple URLs):**\n```json\n{\n  \"url\": [\n    \"https://example.com/article1\",\n    \"https://example.com/article2\"\n  ]\n}\n```\n\n**Output:**\nExtracted text content from the specified URL(s).\n\n## üõ†Ô∏è Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/pinkpixel-dev/web-scout-mcp.git\ncd web-scout-mcp\n\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run\nnpm start\n```\n\n## üìö Documentation\n\nFor more detailed information about the project, check out these resources:\n\n- [OVERVIEW.md](OVERVIEW.md) - Technical overview and architecture\n- [CONTRIBUTING.md](CONTRIBUTING.md) - Guidelines for contributors\n- [CHANGELOG.md](CHANGELOG.md) - Version history and changes\n\n## üìã Requirements\n\n- Node.js >= 18.0.0\n- npm or yarn\n\n## üìÑ License\n\nThis project is licensed under the [Apache 2.0 License](LICENSE).\n\n<p align=\"center\">\n  <sub>Made with ‚ù§Ô∏è by <a href=\"https://pinkpixel.dev\">Pink Pixel</a></sub>\n  <br>\n  <sub>‚ú® Dream it, Pixel it ‚ú®</sub>\n</p>\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Perform web searches using the DuckDuckGo search engine",
        "Extract clean, readable text content from single or multiple web pages",
        "Process multiple URLs in parallel for content extraction",
        "Manage memory efficiently to prevent application crashes",
        "Throttle requests intelligently to avoid API rate limiting",
        "Handle errors robustly for reliable server operation"
      ],
      "limitations": [
        "Does not support search engines other than DuckDuckGo",
        "Requires Node.js version 18.0.0 or higher",
        "No mention of support for authenticated or paywalled content extraction",
        "Rate limiting is implemented but exact limits or quotas are not specified"
      ],
      "requirements": [
        "Node.js version 18.0.0 or higher",
        "npm or yarn package manager",
        "No API keys or authentication tokens required"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, environment requirements, and notes on limitations, making it excellent in quality.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/pinkpixel-dev-web-scout-mcp-badge.png)](https://mseep.ai/app/pinkpixel-dev-web-scout-mcp)\n\n<p align=\"center\">\n  <img src=\"assets/logo.png\" alt=\"Web Scout MCP Logo\" width=\"300\"/>\n</p>\n\n<h1 align=\"center\">Web Scout MCP Server</h1>\n<p align=\"center\">\n  <a href=\"https://www.npmjs.com/package/@pinkpixel/web-scout-mcp\"><img src=\"https://img.shields.io/npm/v/@pinkpixel/web-scout-mcp.svg\" alt=\"npm version\"></a>\n  <a href=\"https://github.com/pinkpixel-dev/web-scout-mcp/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"License\"></a>\n  <a href=\"https://nodejs.org/en/\"><img src=\"https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen.svg\" alt=\"Node.js Version\"></a>\n  <a href=\"https://smithery.ai/badge/@pinkpixel-dev/web-scout-mcp\"><img alt=\"Smithery Badge\" src=\"https://smithery.ai/badge/@pinkpixel-dev/web-scout-mcp\"></a>\n  <a href=\"https://mseep.ai/app/f19a6453-c635-4bc8-b26a-3e9e36428a98\"><img src=\"https://mseep.ai/badge.svg\" alt=\"Verified on MseeP\"></a>\n</p>\n\n<p align=\"center\">\n  An MCP server for web search using DuckDuckGo and content extraction, with support for multiple URLs and memory optimizations.",
        "start_pos": 0,
        "end_pos": 1209,
        "token_count_estimate": 302,
        "source_type": "readme",
        "agent_id": "1e68479e6d2f9490"
      },
      {
        "chunk_id": 1,
        "text": "@pinkpixel-dev/web-scout-mcp):\n\n```bash\nnpx -y @smithery/cli install @pinkpixel-dev/web-scout-mcp --client claude\n```\n\n### Global Installation\n\n```bash\nnpm install -g @pinkpixel/web-scout-mcp\n```\n\n### Local Installation\n\n```bash\nnpm install @pinkpixel/web-scout-mcp\n```\n\n## üöÄ Usage\n\n### Command Line\n\nAfter installing globally, run:\n\n```bash\nweb-scout-mcp\n```\n\n### With MCP Clients\n\nAdd this to your MCP client's `config.json` (Claude Desktop, Cursor, etc.):\n\n```json\n{\n  \"mcpServers\": {\n    \"web-scout\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@pinkpixel/web-scout-mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n### Environment Variables\n\nSet the `WEB_SCOUT_DISABLE_AUTOSTART=1` environment variable when embedding the package and calling `createServer()` yourself. By default running the published entrypoint (for example `node dist/index.js` or `npx @pinkpixel/web-scout-mcp`) automatically bootstraps the stdio transport.\n\n## üß∞ Tools\n\nThe server provides the following MCP tools:\n\n### üîç DuckDuckGoWebSearch\n\nInitiates a web search query using the DuckDuckGo search engine and returns a well-structured list of findings.\n\n**Input:**\n- `query` (string): The search query string\n- `maxResults` (number, optional): Maximum number of results to return (default: 10)\n\n**Example:**\n```json\n{\n  \"query\": \"latest advancements in AI\",\n  \"maxResults\": 5\n}\n```\n\n**Output:**\nA formatted list of search results with titles, URLs, and snippets.\n\n### üìÑ UrlContentExtractor\n\nFetches and extracts clean, readable content from web pages by removing unnecessary elements like scripts, styles, and navigation.\n\n**Input:**\n- `url`: Either a single URL string or an array of URL strings\n\n**Example (single URL):**\n```json\n{\n  \"url\": \"https://example.com/article\"\n}\n```\n\n**Example (multiple URLs):**\n```json\n{\n  \"url\": [\n    \"https://example.com/article1\",\n    \"https://example.com/article2\"\n  ]\n}\n```\n\n**Output:**\nExtracted text content from the specified URL(s).",
        "start_pos": 1848,
        "end_pos": 3807,
        "token_count_estimate": 489,
        "source_type": "readme",
        "agent_id": "1e68479e6d2f9490"
      },
      {
        "chunk_id": 2,
        "text": "}\n```\n\n**Example (multiple URLs):**\n```json\n{\n  \"url\": [\n    \"https://example.com/article1\",\n    \"https://example.com/article2\"\n  ]\n}\n```\n\n**Output:**\nExtracted text content from the specified URL(s).\n\n## üõ†Ô∏è Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/pinkpixel-dev/web-scout-mcp.git\ncd web-scout-mcp\n\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run\nnpm start\n```\n\n## üìö Documentation\n\nFor more detailed information about the project, check out these resources:\n\n- [OVERVIEW.md](OVERVIEW.md) - Technical overview and architecture\n- [CONTRIBUTING.md](CONTRIBUTING.md) - Guidelines for contributors\n- [CHANGELOG.md](CHANGELOG.md) - Version history and changes\n\n## üìã Requirements\n\n- Node.js >= 18.0.0\n- npm or yarn\n\n## üìÑ License\n\nThis project is licensed under the [Apache 2.0 License](LICENSE).\n\n<p align=\"center\">\n  <sub>Made with ‚ù§Ô∏è by <a href=\"https://pinkpixel.dev\">Pink Pixel</a></sub>\n  <br>\n  <sub>‚ú® Dream it, Pixel it ‚ú®</sub>\n</p>",
        "start_pos": 3607,
        "end_pos": 4589,
        "token_count_estimate": 245,
        "source_type": "readme",
        "agent_id": "1e68479e6d2f9490"
      }
    ]
  },
  {
    "agent_id": "018ca03023c3e2a5",
    "name": "ai.smithery/plainyogurt21-clintrials-mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@plainyogurt21/clintrials-mcp/mcp",
    "description": "Provide structured access to ClinicalTrials.gov data for searching, retrieving, and analyzing clin‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-05T12:56:11.696599Z",
    "indexed_at": "2026-02-18T04:08:19.694804",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide structured access to ClinicalTrials.gov data",
        "Search ClinicalTrials.gov data",
        "Retrieve ClinicalTrials.gov data",
        "Analyze ClinicalTrials.gov data"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functions but lacks detail, examples, or information on limitations and requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "eb6bdb066604149a",
    "name": "ai.smithery/plainyogurt21-sec-edgar-mcp",
    "source": "mcp",
    "source_url": "https://github.com/plainyogurt21/sec-edgar-mcp",
    "description": "Provide AI assistants with real-time access to official SEC EDGAR filings and financial data. Enab‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-13T21:20:44.610658Z",
    "indexed_at": "2026-02-18T04:08:21.764715",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<div align=\"center\">\n\n# SEC EDGAR MCP\n\n</div>\n\n<p align=\"center\">\n  <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" />\n  <img alt=\"Python: 3.9+\" src=\"https://img.shields.io/badge/python-3.9+-brightgreen.svg\" />\n  <img alt=\"Platform: Windows | Mac | Linux\" src=\"https://img.shields.io/badge/platform-Windows%20%7C%20Mac%20%7C%20Linux-lightgrey.svg\" />\n  <img alt=\"Build Status\" src=\"https://img.shields.io/badge/build-passing-brightgreen.svg\" />\n  <a href=\"https://pypi.org/project/sec-edgar-mcp/\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/sec-edgar-mcp.svg\" /></a>\n  <a href=\"https://mseep.ai/app/0132880c-5e83-410b-a1d5-d3df08ed7b5c\"><img alt=\"Verified on MseeP\" src=\"https://mseep.ai/badge.svg\" /></a>\n</p>\n\nhttps://github.com/user-attachments/assets/d310eb42-b3ca-467d-92f7-7d132e6274fe\n\n> [!IMPORTANT]\n> EDGAR¬Æ and SEC¬Æ are trademarks of the U.S. Securities and Exchange Commission. This open-source project is not affiliated with or approved by the U.S. Securities and Exchange Commission.\n\n## Introduction üì£\n\nSEC EDGAR MCP is an open-source MCP server that connects AI models to the rich dataset of [SEC EDGAR filings](https://www.sec.gov/edgar). EDGAR (Electronic Data Gathering, Analysis, and Retrieval) is the U.S. SEC's primary system for companies to submit official filings. It contains millions of filings and \"increases the efficiency, transparency, and fairness of the securities markets\" by providing free public access to corporate financial information. This project makes that trove of public company data accessible to AI assistants (LLMs) for financial research, investment insights, and corporate transparency use cases.\n\nUsing the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) ‚Äì an open standard that \"enables seamless integration between LLM applications and external data sources and tools\" ‚Äì the SEC EDGAR MCP server exposes a comprehensive set of tools for accessing SEC filing data. Under the hood, it leverages the [EdgarTools Python library](https://github.com/dgunning/edgartools) to fetch data from official SEC sources and performs direct XBRL parsing for exact financial precision. This means an AI agent can ask questions like \"What's the latest 10-K filing for Apple?\" or \"Show me Tesla's exact revenue from their latest 10-K\" and the MCP server will retrieve the answer directly from EDGAR's official data with complete accuracy and filing references.\n\n> [!TIP]\n> If you use this software, please cite it following [CITATION.cff](CITATION.cff), or the following APA entry:\n\n`Amorelli, Stefano (2025). SEC EDGAR MCP (Model Context Protocol) Server [Computer software]. GitHub. https://github.com/stefanoamorelli/sec-edgar-mcp`\n\n## Usage üöÄ\n\nOnce the SEC EDGAR MCP server is running, you can connect to it with any MCP-compatible client (such as an AI assistant or the MCP CLI tool). The client will discover the available EDGAR tools and can invoke them to get real-time data from SEC filings. For example, an AI assistant could use this server to fetch a company's recent filings or query specific financial metrics without manual web searching.\n\nFor comprehensive guides, examples, and tool documentation, visit the [SEC EDGAR MCP Documentation](https://sec-edgar-mcp.amorelli.tech/).\n\n**Demo**: Here's a demonstration of an AI assistant using SEC EDGAR MCP to retrieve Apple's latest filings and financial facts (click to view the video):\n\n<div align=\"center\">\n    <a href=\"https://www.loom.com/share/17fcd7d891fe496f9a6b8fb85ede66bb\">\n      <img style=\"max-width:300px;\" src=\"https://cdn.loom.com/sessions/thumbnails/17fcd7d891fe496f9a6b8fb85ede66bb-7f8590d1d4bcc2fb-full-play.gif\">\n    </a>\n    <a href=\"https://www.loom.com/share/17fcd7d891fe496f9a6b8fb85ede66bb\">\n      <p>SEC EDGAR MCP - Demo - Watch Video</p>\n    </a>\n</div>\n\nIn the demo above, the assistant uses SEC EDGAR MCP tools to retrieve Apple's filings and financial data, showcasing how EDGAR information is fetched and presented in real-time with exact precision and filing references. üìä\n\n## Documentation üìö\n\nFor installation and setup instructions, visit the [SEC EDGAR MCP Quickstart Guide](https://sec-edgar-mcp.amorelli.tech/setup/quickstart). For complete tool documentation, usage examples, and configuration guides, visit the [SEC EDGAR MCP Documentation](https://sec-edgar-mcp.amorelli.tech/).\n\n## Architecture üèóÔ∏è\n\nThe SEC EDGAR MCP server acts as a middleman between an AI (MCP client) and the SEC's EDGAR backend:\n\n- üî∏ **MCP Client**: Could be an AI assistant (like [Claude](https://claude.ai/) or other MCP-compatible tools) or any app that speaks the MCP protocol. The client sends JSON-RPC requests to invoke tools and receives JSON results.\n\n- üî∏ **MCP Server (SEC EDGAR MCP)**: This server defines comprehensive EDGAR tools and handles incoming requests. It features:\n  - **Company Tools**: CIK lookup, company information, and company facts\n  - **Filing Tools**: Recent filings, filing content, 8-K analysis, and section extraction\n  - **Financial Tools**: Financial statements with direct XBRL parsing for exact precision\n  - **Insider Trading Tools**: Form 3/4/5 analysis with detailed transaction data\n\n- üî∏ **EDGAR Data Sources**: The server uses the [edgartools Python library](https://github.com/dgunning/edgartools) to access:\n  - **SEC EDGAR REST API**: Official SEC endpoint for company data and filing metadata\n  - **Direct XBRL Parsing**: Extracts financial data directly from SEC filings using regex patterns for exact numeric precision\n  - **Filing Content**: Downloads and parses complete SEC filing documents (.txt format)\n\n**Key Features**:\n- **Deterministic Responses**: All tools include strict instructions to prevent AI hallucination and ensure responses are based only on SEC filing data\n- **Exact Precision**: Financial data maintains exact numeric precision (no rounding) as filed with the SEC\n- **Filing References**: Every response includes clickable SEC URLs for independent verification\n- **Flexible XBRL Extraction**: Uses pattern matching to find financial concepts without hardcoded mappings\n\n**How it works**: The MCP client discovers available tools (company lookup, financial statements, insider transactions, etc.). When invoked, each tool fetches data from SEC sources, applies deterministic processing rules, and returns structured JSON with filing references. This ensures AI responses are accurate, verifiable, and based solely on official SEC data.\n\n## References üìö\n\n- **[SEC EDGAR](https://www.sec.gov/edgar)** ‚Äì About EDGAR, SEC.gov (2024). EDGAR is the SEC's database for electronic company filings.\n\n- **[Model Context Protocol (MCP)](https://modelcontextprotocol.io/)** ‚Äì Official documentation and SDKs. ModelContextProtocol.io ‚Äì An open standard for connecting LLMs to tools.\n\n- **[EdgarTools](https://github.com/dgunning/edgartools)** ‚Äì A modern Python library for accessing SEC EDGAR data with powerful filing analysis capabilities. [GitHub repo](https://github.com/dgunning/edgartools), [Documentation](https://dgunning.github.io/edgartools/).\n\n\n## License ‚öñÔ∏è\n\nThis project is licensed under the [MIT License](LICENSE). You are free to use, modify, and distribute it. See the LICENSE file for details.\n\n---\n\n¬© 2025 [Stefano Amorelli](https://amorelli.tech) ‚Äì Released under the [MIT license](LICENSE).  Enjoy! üéâ\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect AI models to SEC EDGAR filings dataset",
        "Fetch company information and perform CIK lookup",
        "Retrieve recent SEC filings and filing content",
        "Analyze 8-K filings and extract specific sections",
        "Parse financial statements with exact numeric precision using direct XBRL parsing",
        "Analyze insider trading forms (Form 3/4/5) with detailed transaction data",
        "Provide deterministic, hallucination-free responses based solely on SEC filing data",
        "Include clickable SEC URLs for independent verification in all responses",
        "Support MCP-compatible clients for seamless integration with AI assistants"
      ],
      "limitations": [
        "Does not generate data beyond official SEC filings",
        "Responses are strictly limited to SEC filing data to prevent hallucinations",
        "Requires MCP-compatible clients to interact with the server",
        "No mention of support for filings outside the SEC EDGAR system"
      ],
      "requirements": [
        "Python 3.9 or higher",
        "MCP-compatible client to connect and invoke tools",
        "Internet access to fetch data from SEC EDGAR REST API",
        "No API keys or authentication tokens required as SEC data is public"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation prerequisites, detailed architecture and tool descriptions, usage examples including a demo video, explicit limitations, and external references, making it excellent in quality.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<div align=\"center\">\n\n# SEC EDGAR MCP\n\n</div>\n\n<p align=\"center\">\n  <img alt=\"License: MIT\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" />\n  <img alt=\"Python: 3.9+\" src=\"https://img.shields.io/badge/python-3.9+-brightgreen.svg\" />\n  <img alt=\"Platform: Windows | Mac | Linux\" src=\"https://img.shields.io/badge/platform-Windows%20%7C%20Mac%20%7C%20Linux-lightgrey.svg\" />\n  <img alt=\"Build Status\" src=\"https://img.shields.io/badge/build-passing-brightgreen.svg\" />\n  <a href=\"https://pypi.org/project/sec-edgar-mcp/\"><img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/sec-edgar-mcp.svg\" /></a>\n  <a href=\"https://mseep.ai/app/0132880c-5e83-410b-a1d5-d3df08ed7b5c\"><img alt=\"Verified on MseeP\" src=\"https://mseep.ai/badge.svg\" /></a>\n</p>\n\nhttps://github.com/user-attachments/assets/d310eb42-b3ca-467d-92f7-7d132e6274fe\n\n> [!IMPORTANT]\n> EDGAR¬Æ and SEC¬Æ are trademarks of the U.S. Securities and Exchange Commission. This open-source project is not affiliated with or approved by the U.S. Securities and Exchange Commission.\n\n## Introduction üì£\n\nSEC EDGAR MCP is an open-source MCP server that connects AI models to the rich dataset of [SEC EDGAR filings](https://www.sec.gov/edgar). EDGAR (Electronic Data Gathering, Analysis, and Retrieval) is the U.S. SEC's primary system for companies to submit official filings. It contains millions of filings and \"increases the efficiency, transparency, and fairness of the securities markets\" by providing free public access to corporate financial information. This project makes that trove of public company data accessible to AI assistants (LLMs) for financial research, investment insights, and corporate transparency use cases.\n\nUsing the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) ‚Äì an open standard that \"enables seamless integration between LLM applications and external data sources and tools\" ‚Äì the SEC EDGAR MCP server exposes a comprehensive set of tools for accessing SEC filing data.",
        "start_pos": 0,
        "end_pos": 1970,
        "token_count_estimate": 492,
        "source_type": "readme",
        "agent_id": "eb6bdb066604149a"
      },
      {
        "chunk_id": 1,
        "text": "standard that \"enables seamless integration between LLM applications and external data sources and tools\" ‚Äì the SEC EDGAR MCP server exposes a comprehensive set of tools for accessing SEC filing data. Under the hood, it leverages the [EdgarTools Python library](https://github.com/dgunning/edgartools) to fetch data from official SEC sources and performs direct XBRL parsing for exact financial precision. This means an AI agent can ask questions like \"What's the latest 10-K filing for Apple?\" or \"Show me Tesla's exact revenue from their latest 10-K\" and the MCP server will retrieve the answer directly from EDGAR's official data with complete accuracy and filing references.\n\n> [!TIP]\n> If you use this software, please cite it following [CITATION.cff](CITATION.cff), or the following APA entry:\n\n`Amorelli, Stefano (2025). SEC EDGAR MCP (Model Context Protocol) Server [Computer software]. GitHub. https://github.com/stefanoamorelli/sec-edgar-mcp`\n\n## Usage üöÄ\n\nOnce the SEC EDGAR MCP server is running, you can connect to it with any MCP-compatible client (such as an AI assistant or the MCP CLI tool). The client will discover the available EDGAR tools and can invoke them to get real-time data from SEC filings. For example, an AI assistant could use this server to fetch a company's recent filings or query specific financial metrics without manual web searching.\n\nFor comprehensive guides, examples, and tool documentation, visit the [SEC EDGAR MCP Documentation](https://sec-edgar-mcp.amorelli.tech/).",
        "start_pos": 1770,
        "end_pos": 3281,
        "token_count_estimate": 377,
        "source_type": "readme",
        "agent_id": "eb6bdb066604149a"
      },
      {
        "chunk_id": 2,
        "text": "fcd7d891fe496f9a6b8fb85ede66bb-7f8590d1d4bcc2fb-full-play.gif\">\n    </a>\n    <a href=\"https://www.loom.com/share/17fcd7d891fe496f9a6b8fb85ede66bb\">\n      <p>SEC EDGAR MCP - Demo - Watch Video</p>\n    </a>\n</div>\n\nIn the demo above, the assistant uses SEC EDGAR MCP tools to retrieve Apple's filings and financial data, showcasing how EDGAR information is fetched and presented in real-time with exact precision and filing references. üìä\n\n## Documentation üìö\n\nFor installation and setup instructions, visit the [SEC EDGAR MCP Quickstart Guide](https://sec-edgar-mcp.amorelli.tech/setup/quickstart). For complete tool documentation, usage examples, and configuration guides, visit the [SEC EDGAR MCP Documentation](https://sec-edgar-mcp.amorelli.tech/).\n\n## Architecture üèóÔ∏è\n\nThe SEC EDGAR MCP server acts as a middleman between an AI (MCP client) and the SEC's EDGAR backend:\n\n- üî∏ **MCP Client**: Could be an AI assistant (like [Claude](https://claude.ai/) or other MCP-compatible tools) or any app that speaks the MCP protocol. The client sends JSON-RPC requests to invoke tools and receives JSON results.\n\n- üî∏ **MCP Server (SEC EDGAR MCP)**: This server defines comprehensive EDGAR tools and handles incoming requests.",
        "start_pos": 3618,
        "end_pos": 4834,
        "token_count_estimate": 304,
        "source_type": "readme",
        "agent_id": "eb6bdb066604149a"
      },
      {
        "chunk_id": 3,
        "text": "m SEC filings using regex patterns for exact numeric precision\n  - **Filing Content**: Downloads and parses complete SEC filing documents (.txt format)\n\n**Key Features**:\n- **Deterministic Responses**: All tools include strict instructions to prevent AI hallucination and ensure responses are based only on SEC filing data\n- **Exact Precision**: Financial data maintains exact numeric precision (no rounding) as filed with the SEC\n- **Filing References**: Every response includes clickable SEC URLs for independent verification\n- **Flexible XBRL Extraction**: Uses pattern matching to find financial concepts without hardcoded mappings\n\n**How it works**: The MCP client discovers available tools (company lookup, financial statements, insider transactions, etc.). When invoked, each tool fetches data from SEC sources, applies deterministic processing rules, and returns structured JSON with filing references. This ensures AI responses are accurate, verifiable, and based solely on official SEC data.\n\n## References üìö\n\n- **[SEC EDGAR](https://www.sec.gov/edgar)** ‚Äì About EDGAR, SEC.gov (2024). EDGAR is the SEC's database for electronic company filings.\n\n- **[Model Context Protocol (MCP)](https://modelcontextprotocol.io/)** ‚Äì Official documentation and SDKs. ModelContextProtocol.io ‚Äì An open standard for connecting LLMs to tools.\n\n- **[EdgarTools](https://github.com/dgunning/edgartools)** ‚Äì A modern Python library for accessing SEC EDGAR data with powerful filing analysis capabilities. [GitHub repo](https://github.com/dgunning/edgartools), [Documentation](https://dgunning.github.io/edgartools/).\n\n\n## License ‚öñÔ∏è\n\nThis project is licensed under the [MIT License](LICENSE). You are free to use, modify, and distribute it. See the LICENSE file for details.\n\n---\n\n¬© 2025 [Stefano Amorelli](https://amorelli.tech) ‚Äì Released under the [MIT license](LICENSE).  Enjoy! üéâ",
        "start_pos": 5466,
        "end_pos": 7342,
        "token_count_estimate": 468,
        "source_type": "readme",
        "agent_id": "eb6bdb066604149a"
      },
      {
        "chunk_id": 4,
        "text": "ENSE). You are free to use, modify, and distribute it. See the LICENSE file for details.\n\n---\n\n¬© 2025 [Stefano Amorelli](https://amorelli.tech) ‚Äì Released under the [MIT license](LICENSE).  Enjoy! üéâ",
        "start_pos": 7142,
        "end_pos": 7342,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "eb6bdb066604149a"
      }
    ]
  },
  {
    "agent_id": "854cc832bc999452",
    "name": "ai.smithery/proflulab-documentassistant",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@proflulab/documentassistant/mcp",
    "description": "Convert files between formats without quality loss. Speed up your workflow with fast, reliable con‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-06T07:24:02.799169Z",
    "indexed_at": "2026-02-18T04:08:23.114614",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Convert files between formats without quality loss",
        "Speed up workflow with fast and reliable conversion"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of file conversion capabilities and performance but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "ad36854ced1ca70e",
    "name": "ai.smithery/pythondev-pro-egw_writings_mcp_server",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@pythondev-pro/egw_writings_mcp_server/mcp",
    "description": "Search Ellen G. White‚Äôs writings by topic or phrase. Retrieve exact references and passages instan‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-19T16:18:36.558204Z",
    "indexed_at": "2026-02-18T04:08:24.966761",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search Ellen G. White‚Äôs writings by topic or phrase",
        "Retrieve exact references from Ellen G. White‚Äôs writings",
        "Retrieve passages from Ellen G. White‚Äôs writings"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of search and retrieval capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "ad36854ced1ca70e",
    "name": "ai.smithery/pythondev-pro-egw_writings_mcp_server",
    "source": "mcp",
    "source_url": "https://github.com/pythondev-pro/egw_writings_mcp_server",
    "description": "Search Ellen G. White‚Äôs writings by keyword to surface relevant quotations. Retrieve exact passage‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-11T12:48:16.278544Z",
    "indexed_at": "2026-02-18T04:08:27.039194",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search Ellen G. White‚Äôs writings by keyword",
        "Retrieve exact passages from Ellen G. White‚Äôs writings"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of search and retrieval capabilities but lacks detail, examples, or additional context.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "5c4a037e0c928d4f",
    "name": "ai.smithery/rainbowgore-stealthee-mcp-tools",
    "source": "mcp",
    "source_url": "https://github.com/rainbowgore/stealthee-MCP-tools",
    "description": "Spot pre-launch products before they trend. Search the web and tech sites, extract and parse pages‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T08:35:04.917713Z",
    "indexed_at": "2026-02-18T04:08:30.986841",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Stealthee MCP - Tools for being early\n\n[![Python](https://img.shields.io/badge/python-3.12%2B-blue)](https://www.python.org/)\n[![FastAPI](https://img.shields.io/badge/Framework-FastAPI-009688)](https://fastapi.tiangolo.com/)\n[![MCP](https://img.shields.io/badge/MCP-Server-4b8bbe)](https://github.com/nimbleai/mcp)\n[![OpenAI API](https://img.shields.io/badge/OpenAI-Integrated-orange)](https://platform.openai.com/)\n[![Tavily](https://img.shields.io/badge/Tavily-Search-green)](https://docs.tavily.com/)\n[![Nimble](https://img.shields.io/badge/Nimble-AI%20Parsing-purple)](https://docs.nimbleai.dev/)\n[![Slack Alerts](https://img.shields.io/badge/Slack-Alerts%20Enabled-4A154B?logo=slack)](https://slack.com/)\n[![Smithery](https://img.shields.io/badge/Smithery-Compatible-%23007acc)](https://smithery.tools/)\n\n![Stealthee Logo](./mcp-stealthee.png)\n\nStealthee is a dev-first system for surfacing pre-public product signals - before they trend. Built for CTOs and tech leaders who need competitive intelligence and early threat detection.\nIt combines search, extraction, scoring, and alerting into a plug-and-play pipeline you can integrate into Claude, LangGraph, Smithery, or your own AI stack via MCP.\n\nPerfect for competitive intelligence, technology trend monitoring, and strategic planning.\n\nUse it if you're:\n\n- A **CTO** or tech leader needing competitive intelligence, early threat detection, and innovation scouting to inform strategic decisions\n- An **investor** hunting for pre-traction signals\n- A **founder** scanning for competitors before launch\n- A **researcher** tracking emerging markets\n- A **developer** building agents, dashboards, or alerting tools that need fresh product intel.\n\n## What's cookin'?\n\n### MCP Tools\n\n| Tool                  | Description                                  |\n| --------------------- | -------------------------------------------- |\n| `web_search`          | Search the web for stealth launches (Tavily) |\n| `url_extract`         | Extract content from URLs (BeautifulSoup)    |\n| `score_signal`        | AI-powered signal scoring (OpenAI)           |\n| `batch_score_signals` | Batch process multiple signals               |\n| `search_tech_sites`   | Search tech news sites only                  |\n| `parse_fields`        | Extract structured fields from HTML          |\n| `run_pipeline`        | End-to-end detection pipeline                |\n\n## Installation & Setup\n\n### Prerequisites\n\n- API keys for external services (see Environment Variables)\n\n### Quick Start\n\n1. **Clone and Setup**\n\n   ```bash\n   git clone https://github.com/rainbowgore/Stealthee-MCP-tools\n   cd stealthee-MCP-tools\n   python3 -m venv .venv\n   source .venv/bin/activate\n   pip install -r requirements.txt\n   ```\n\n2. **Configure Environment**\n\n   Fill the `.env` file with your API keys:\n\n   ```bash\n   # Required\n   TAVILY_API_KEY=your_tavily_key_here\n   OPENAI_API_KEY=your_openai_key_here\n   NIMBLE_API_KEY=your_nimble_key_here\n\n   # Optional\n   SLACK_WEBHOOK_URL=your_slack_webhook_here\n   ```\n\n3. **Start Servers**\n\n   ```bash\n   # MCP Server (for Claude Desktop)\n   python mcp_server_stdio.py\n\n   # FastMCP Server (for Smithery)\n   smithery dev\n\n   # FastAPI Server (Optional - Legacy)\n   python start_fastapi.py\n   ```\n\n## Smithery & Claude Desktop Integration\n\nAll MCP tools listed above are available out-of-the-box in [Smithery](https://smithery.ai/server/rainbowgore/Product-Stealth-Launch-Radar). Smithery is a visual agent and workflow builder for AI tools, letting you chain, test, and orchestrate these tools with no code.\n\n### Available Tools\n\n- **web_search**: Search the web for stealth launches using Tavily.\n- **url_extract**: Extract and clean content from any URL.\n- **score_signal**: Use OpenAI to score a single signal for stealthiness.\n- **batch_score_signals**: Score multiple signals in one go.\n- **search_tech_sites**: Search only trusted tech news sources.\n- **parse_fields**: Extract structured fields (like pricing, changelog) from HTML.\n- **run_pipeline**: End-to-end pipeline: search, extract, parse, score, and store.\n\n### How to Use in Smithery\n\n1. **Open the [Stealthee MCP Tools page on Smithery](https://smithery.ai/server/rainbowgore/Product-Stealth-Launch-Radar).**\n2. Click \"Try in Playground\" to test any tool interactively.\n3. Use the visual workflow builder to chain tools together (e.g., search ‚Üí extract ‚Üí score).\n4. Integrate with Claude Desktop or your own agents by copying the workflow or using the API endpoints provided by Smithery.\n\n### Cursor (Stealth Radar MCP)\n\nTo use **Stealth Radar MCP** in Cursor via the hosted URL (Streamable HTTP):\n\n1. Open **Cursor Settings** ‚Üí **MCP** (or search for \"MCP\" in settings).\n2. Under **Install MCP Server**, fill in:\n   - **Name:** `Stealth Radar` (or any name you like).\n   - **Type:** `streamableHttp`.\n   - **URL:** Use either:\n     - **Smithery:** The connection URL from your server's Smithery **Connect** page (e.g. `https://smithery.ai/server/rainbowgore/Product-Stealth-Launch-Radar`), or\n     - **Direct:** Your server's MCP endpoint, e.g. `https://your-ngrok-url.ngrok-free.app/mcp` (must end with `/mcp`).\n3. Click **Install**. Cursor will connect to the server; once added, it loads automatically when you use Cursor.\n\nIf you run the server locally, use **stdio** instead: set **Type** to `stdio`, **Command** to your Python path, and **Args** to `mcp_server_stdio.py` with **cwd** pointing at the repo.\n\n### Claude Desktop Integration\n\nAdd to your Claude Desktop `config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"stealth-mcp\": {\n      \"command\": \"/path/to/stealthee-MCP-tools/.venv/bin/python\",\n      \"args\": [\"/path/to/stealthee-MCP-tools/mcp_server_stdio.py\"],\n      \"cwd\": \"/path/to/stealthee-MCP-tools\",\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your_tavily_key\",\n        \"OPENAI_API_KEY\": \"your_openai_key\"\n      }\n    }\n  }\n}\n```\n\n## Tool Use Cases\n\n**For Analysts & Builders:**\n\n- `web_search`: Find stealth product mentions across the web\n- `url_extract`: Pull and clean raw text from landing pages\n- `score_signal`: Judge how likely a change log implies launch\n- `batch_score_signals`: Quickly triage dozens of scraped URLs\n- `search_tech_sites`: Limit queries to trusted domains only\n- `parse_fields`: Extract pricing/release info from messy HTML\n- `run_pipeline`: Full pipeline ‚Äî search ‚Üí extract ‚Üí parse ‚Üí score\n\n## Signal Intelligence Workflow\n\n1. **Search Phase**: Use `web_search` or `search_tech_sites` to find relevant URLs\n2. **Extraction Phase**: Use `url_extract` to get clean content from URLs\n3. **Parsing Phase**: Use `parse_fields` to extract structured data (pricing, changelog, etc.)\n4. **Analysis Phase**: Use `score_signal` or `batch_score_signals` for AI-powered analysis\n5. **Storage Phase**: All signals are stored in SQLite database\n6. **Alert Phase**: High-confidence signals trigger Slack notifications\n\n## FastAPI Server\n\nYou can also run this project as a FastAPI server for REST-style access to all MCP tools.\n\n### Base Endpoints\n\n- **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs)\n- **Health Check**: [http://localhost:8000/health](http://localhost:8000/health)\n- **Tool Manifest**: [http://localhost:8000/tools](http://localhost:8000/tools)\n\n---\n\n### Example Usage\n\n**Search for stealth launches:**\n\n```bash\ncurl -X POST \"http://localhost:8000/tools/web_search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"stealth startup AI\", \"num_results\": 5}'\n```\n\n**Run full detection pipeline:**\n\n```bash\ncurl -X POST \"http://localhost:8000/tools/run_pipeline\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"new AI product launch\", \"num_results\": 3}'\n```\n\n### Pipeline Parameters\n\n- `query` (required): Search phrase (e.g. \"AI roadmap\")\n- `num_results` (optional, default: 5): Number of search results to analyze\n- `target_fields` (optional, default: [\"pricing\", \"changelog\"]): Fields to extract from HTML\n\n---\n\n### What run_pipeline Does\n\n1. Searches tech and stealth-friendly sources using Tavily\n2. Extracts raw content from each result\n3. Parses structured signals (pricing, changelog, etc.)\n4. Scores each result with OpenAI to estimate stealthiness\n5. Stores results in local SQLite\n6. Notifies via Slack if confidence is high\n\n### AI Scoring Logic\n\nThe score_signal and batch_score_signals tools use GPT-3.5 to evaluate:\n\n- Stealth indicators (e.g. private changelogs, missing press, beta flags)\n- Confidence level (Low / Medium / High)\n- Textual reasoning (used in UI or alerting)\n\n### Database Schema (data/signals.db)\n\n| Field          | Type    | Description                     |\n| -------------- | ------- | ------------------------------- |\n| `id`           | INTEGER | Primary key                     |\n| `url`          | TEXT    | Source URL                      |\n| `title`        | TEXT    | Signal title                    |\n| `html_excerpt` | TEXT    | First 500 characters of content |\n| `changelog`    | TEXT    | Parsed changelog (optional)     |\n| `pricing`      | TEXT    | Parsed pricing info (optional)  |\n| `score`        | REAL    | Stealth likelihood (0‚Äì1)        |\n| `confidence`   | TEXT    | Confidence level                |\n| `reasoning`    | TEXT    | AI rationale for the score      |\n| `created_at`   | TEXT    | ISO timestamp                   |\n\n### Dev Quickstart (FastAPI)\n\n```bash\npython start_fastapi.py\n```\n\nThen visit: [http://localhost:8000/docs](http://localhost:8000/docs)\n\n---\n\nBuilt with üíú for those who spot what others miss.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search the web for stealth product launches using Tavily",
        "Extract and clean content from URLs using BeautifulSoup",
        "Score individual signals for stealthiness using OpenAI GPT-3.5",
        "Batch process multiple signals for scoring",
        "Search only trusted tech news sites for relevant information",
        "Parse structured fields such as pricing and changelog from HTML content",
        "Run an end-to-end detection pipeline combining search, extraction, parsing, scoring, storage, and alerting",
        "Store analyzed signals in a local SQLite database",
        "Send Slack notifications for high-confidence stealth signals",
        "Integrate with Smithery, Claude Desktop, and Cursor for AI workflow orchestration"
      ],
      "limitations": [
        "Requires valid API keys for Tavily, OpenAI, and Nimble services",
        "Slack alerting is optional and requires a Slack webhook URL",
        "The server must be run locally or hosted with proper MCP endpoint URLs",
        "Limited to stealth launch and product signal detection use cases",
        "Relies on external services for search and AI scoring, subject to their rate limits and availability"
      ],
      "requirements": [
        "Python 3.12 or higher",
        "API keys for Tavily, OpenAI, and Nimble services set in environment variables",
        "Optional Slack webhook URL for alert notifications",
        "Virtual environment setup with dependencies installed from requirements.txt",
        "Access to MCP-compatible clients like Smithery, Claude Desktop, or Cursor for integration"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions, usage examples, integration guides, limitations, and environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Stealthee MCP - Tools for being early\n\n[![Python](https://img.shields.io/badge/python-3.12%2B-blue)](https://www.python.org/)\n[![FastAPI](https://img.shields.io/badge/Framework-FastAPI-009688)](https://fastapi.tiangolo.com/)\n[![MCP](https://img.shields.io/badge/MCP-Server-4b8bbe)](https://github.com/nimbleai/mcp)\n[![OpenAI API](https://img.shields.io/badge/OpenAI-Integrated-orange)](https://platform.openai.com/)\n[![Tavily](https://img.shields.io/badge/Tavily-Search-green)](https://docs.tavily.com/)\n[![Nimble](https://img.shields.io/badge/Nimble-AI%20Parsing-purple)](https://docs.nimbleai.dev/)\n[![Slack Alerts](https://img.shields.io/badge/Slack-Alerts%20Enabled-4A154B?logo=slack)](https://slack.com/)\n[![Smithery](https://img.shields.io/badge/Smithery-Compatible-%23007acc)](https://smithery.tools/)\n\n![Stealthee Logo](./mcp-stealthee.png)\n\nStealthee is a dev-first system for surfacing pre-public product signals - before they trend. Built for CTOs and tech leaders who need competitive intelligence and early threat detection.\nIt combines search, extraction, scoring, and alerting into a plug-and-play pipeline you can integrate into Claude, LangGraph, Smithery, or your own AI stack via MCP.\n\nPerfect for competitive intelligence, technology trend monitoring, and strategic planning.\n\nUse it if you're:\n\n- A **CTO** or tech leader needing competitive intelligence, early threat detection, and innovation scouting to inform strategic decisions\n- An **investor** hunting for pre-traction signals\n- A **founder** scanning for competitors before launch\n- A **researcher** tracking emerging markets\n- A **developer** building agents, dashboards, or alerting tools that need fresh product intel.\n\n## What's cookin'?",
        "start_pos": 0,
        "end_pos": 1723,
        "token_count_estimate": 430,
        "source_type": "readme",
        "agent_id": "5c4a037e0c928d4f"
      },
      {
        "chunk_id": 1,
        "text": "----------------------------------- |\n| `web_search`          | Search the web for stealth launches (Tavily) |\n| `url_extract`         | Extract content from URLs (BeautifulSoup)    |\n| `score_signal`        | AI-powered signal scoring (OpenAI)           |\n| `batch_score_signals` | Batch process multiple signals               |\n| `search_tech_sites`   | Search tech news sites only                  |\n| `parse_fields`        | Extract structured fields from HTML          |\n| `run_pipeline`        | End-to-end detection pipeline                |\n\n## Installation & Setup\n\n### Prerequisites\n\n- API keys for external services (see Environment Variables)\n\n### Quick Start\n\n1. **Clone and Setup**\n\n   ```bash\n   git clone https://github.com/rainbowgore/Stealthee-MCP-tools\n   cd stealthee-MCP-tools\n   python3 -m venv .venv\n   source .venv/bin/activate\n   pip install -r requirements.txt\n   ```\n\n2. **Configure Environment**\n\n   Fill the `.env` file with your API keys:\n\n   ```bash\n   # Required\n   TAVILY_API_KEY=your_tavily_key_here\n   OPENAI_API_KEY=your_openai_key_here\n   NIMBLE_API_KEY=your_nimble_key_here\n\n   # Optional\n   SLACK_WEBHOOK_URL=your_slack_webhook_here\n   ```\n\n3. **Start Servers**\n\n   ```bash\n   # MCP Server (for Claude Desktop)\n   python mcp_server_stdio.py\n\n   # FastMCP Server (for Smithery)\n   smithery dev\n\n   # FastAPI Server (Optional - Legacy)\n   python start_fastapi.py\n   ```\n\n## Smithery & Claude Desktop Integration\n\nAll MCP tools listed above are available out-of-the-box in [Smithery](https://smithery.ai/server/rainbowgore/Product-Stealth-Launch-Radar). Smithery is a visual agent and workflow builder for AI tools, letting you chain, test, and orchestrate these tools with no code.\n\n### Available Tools\n\n- **web_search**: Search the web for stealth launches using Tavily.\n- **url_extract**: Extract and clean content from any URL.\n- **score_signal**: Use OpenAI to score a single signal for stealthiness.\n- **batch_score_signals**: Score multiple signals in one go.",
        "start_pos": 1848,
        "end_pos": 3850,
        "token_count_estimate": 500,
        "source_type": "readme",
        "agent_id": "5c4a037e0c928d4f"
      },
      {
        "chunk_id": 2,
        "text": "avily.\n- **url_extract**: Extract and clean content from any URL.\n- **score_signal**: Use OpenAI to score a single signal for stealthiness.\n- **batch_score_signals**: Score multiple signals in one go.\n- **search_tech_sites**: Search only trusted tech news sources.\n- **parse_fields**: Extract structured fields (like pricing, changelog) from HTML.\n- **run_pipeline**: End-to-end pipeline: search, extract, parse, score, and store.\n\n### How to Use in Smithery\n\n1. **Open the [Stealthee MCP Tools page on Smithery](https://smithery.ai/server/rainbowgore/Product-Stealth-Launch-Radar).**\n2. Click \"Try in Playground\" to test any tool interactively.\n3. Use the visual workflow builder to chain tools together (e.g., search ‚Üí extract ‚Üí score).\n4. Integrate with Claude Desktop or your own agents by copying the workflow or using the API endpoints provided by Smithery.\n\n### Cursor (Stealth Radar MCP)\n\nTo use **Stealth Radar MCP** in Cursor via the hosted URL (Streamable HTTP):\n\n1. Open **Cursor Settings** ‚Üí **MCP** (or search for \"MCP\" in settings).\n2. Under **Install MCP Server**, fill in:\n   - **Name:** `Stealth Radar` (or any name you like).\n   - **Type:** `streamableHttp`.\n   - **URL:** Use either:\n     - **Smithery:** The connection URL from your server's Smithery **Connect** page (e.g. `https://smithery.ai/server/rainbowgore/Product-Stealth-Launch-Radar`), or\n     - **Direct:** Your server's MCP endpoint, e.g. `https://your-ngrok-url.ngrok-free.app/mcp` (must end with `/mcp`).\n3. Click **Install**. Cursor will connect to the server; once added, it loads automatically when you use Cursor.\n\nIf you run the server locally, use **stdio** instead: set **Type** to `stdio`, **Command** to your Python path, and **Args** to `mcp_server_stdio.py` with **cwd** pointing at the repo.",
        "start_pos": 3650,
        "end_pos": 5438,
        "token_count_estimate": 447,
        "source_type": "readme",
        "agent_id": "5c4a037e0c928d4f"
      },
      {
        "chunk_id": 3,
        "text": "`config.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"stealth-mcp\": {\n      \"command\": \"/path/to/stealthee-MCP-tools/.venv/bin/python\",\n      \"args\": [\"/path/to/stealthee-MCP-tools/mcp_server_stdio.py\"],\n      \"cwd\": \"/path/to/stealthee-MCP-tools\",\n      \"env\": {\n        \"TAVILY_API_KEY\": \"your_tavily_key\",\n        \"OPENAI_API_KEY\": \"your_openai_key\"\n      }\n    }\n  }\n}\n```\n\n## Tool Use Cases\n\n**For Analysts & Builders:**\n\n- `web_search`: Find stealth product mentions across the web\n- `url_extract`: Pull and clean raw text from landing pages\n- `score_signal`: Judge how likely a change log implies launch\n- `batch_score_signals`: Quickly triage dozens of scraped URLs\n- `search_tech_sites`: Limit queries to trusted domains only\n- `parse_fields`: Extract pricing/release info from messy HTML\n- `run_pipeline`: Full pipeline ‚Äî search ‚Üí extract ‚Üí parse ‚Üí score\n\n## Signal Intelligence Workflow\n\n1. **Search Phase**: Use `web_search` or `search_tech_sites` to find relevant URLs\n2. **Extraction Phase**: Use `url_extract` to get clean content from URLs\n3. **Parsing Phase**: Use `parse_fields` to extract structured data (pricing, changelog, etc.)\n4. **Analysis Phase**: Use `score_signal` or `batch_score_signals` for AI-powered analysis\n5. **Storage Phase**: All signals are stored in SQLite database\n6. **Alert Phase**: High-confidence signals trigger Slack notifications\n\n## FastAPI Server\n\nYou can also run this project as a FastAPI server for REST-style access to all MCP tools.",
        "start_pos": 5498,
        "end_pos": 6986,
        "token_count_estimate": 371,
        "source_type": "readme",
        "agent_id": "5c4a037e0c928d4f"
      },
      {
        "chunk_id": 4,
        "text": "/tools/web_search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"stealth startup AI\", \"num_results\": 5}'\n```\n\n**Run full detection pipeline:**\n\n```bash\ncurl -X POST \"http://localhost:8000/tools/run_pipeline\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"new AI product launch\", \"num_results\": 3}'\n```\n\n### Pipeline Parameters\n\n- `query` (required): Search phrase (e.g. \"AI roadmap\")\n- `num_results` (optional, default: 5): Number of search results to analyze\n- `target_fields` (optional, default: [\"pricing\", \"changelog\"]): Fields to extract from HTML\n\n---\n\n### What run_pipeline Does\n\n1. Searches tech and stealth-friendly sources using Tavily\n2. Extracts raw content from each result\n3. Parses structured signals (pricing, changelog, etc.)\n4. Scores each result with OpenAI to estimate stealthiness\n5. Stores results in local SQLite\n6. Notifies via Slack if confidence is high\n\n### AI Scoring Logic\n\nThe score_signal and batch_score_signals tools use GPT-3.5 to evaluate:\n\n- Stealth indicators (e.g.",
        "start_pos": 7346,
        "end_pos": 8373,
        "token_count_estimate": 256,
        "source_type": "readme",
        "agent_id": "5c4a037e0c928d4f"
      },
      {
        "chunk_id": 5,
        "text": "| TEXT    | AI rationale for the score      |\n| `created_at`   | TEXT    | ISO timestamp                   |\n\n### Dev Quickstart (FastAPI)\n\n```bash\npython start_fastapi.py\n```\n\nThen visit: [http://localhost:8000/docs](http://localhost:8000/docs)\n\n---\n\nBuilt with üíú for those who spot what others miss.",
        "start_pos": 9194,
        "end_pos": 9496,
        "token_count_estimate": 75,
        "source_type": "readme",
        "agent_id": "5c4a037e0c928d4f"
      }
    ]
  },
  {
    "agent_id": "a843568637a8a842",
    "name": "ai.smithery/ramadasmr-networkcalc-mcp",
    "source": "mcp",
    "source_url": "https://github.com/ramadasmr/networkcalc-mcp",
    "description": "Look up DNS information for any domain to troubleshoot issues and gather insights. Get fast, relia‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-20T10:10:47.321873Z",
    "indexed_at": "2026-02-18T04:08:32.527241",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# üåê networkcalc-mcp\n[![smithery badge](https://smithery.ai/badge/@ramadasmr/networkcalc-mcp)](https://smithery.ai/server/@ramadasmr/networkcalc-mcp)\n\n**networkcalc-mcp** is a server component (MCP Server) that powers utility services from [networkcalc.com](https://networkcalc.com). It provides a set of networking tools accessible via API or integration with your own frontend.\n\n## üîß Features\n\nThis server includes support for the following tools:\n\n- üîç **DNS Lookup** ‚Äì Resolve domain names to IP addresses and view DNS records.\n- üóÇÔ∏è **WHOIS Lookup** ‚Äì Retrieve registration and ownership details for domains and IPs.\n- üìú **SPF Lookup** ‚Äì Inspect Sender Policy Framework (SPF) records for domains.\n- üîê **Certificate Lookup** ‚Äì Check SSL/TLS certificates for domains.\n- üßÆ **Subnet Lookup** ‚Äì Analyze IP subnets and CIDR ranges.\n\n## üìñ How to Use\n\nRun below command to add it to claude \n```bash\nclaude mcp add --transport http ramadasmr-networkcalc-mcp \"https://server.smithery.ai/@ramadasmr/networkcalc-mcp/mcp\"\n```\n\n## Installing via Smithery\n\nTo install NetworkCalc automatically via [Smithery](https://smithery.ai/server/@ramadasmr/networkcalc-mcp):\n\n```bash\nnpx -y @smithery/cli install @ramadasmr/networkcalc-mcp\n```\n\nTo see full usage instructions and available endpoints, please refer to the hosted service:\nüëâ [https://smithery.ai/server/@ramadasmr/networkcalc-mcp](https://smithery.ai/server/@ramadasmr/networkcalc-mcp)\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Resolve domain names to IP addresses and view DNS records",
        "Retrieve registration and ownership details for domains and IPs via WHOIS lookup",
        "Inspect Sender Policy Framework (SPF) records for domains",
        "Check SSL/TLS certificates for domains",
        "Analyze IP subnets and CIDR ranges"
      ],
      "limitations": [],
      "requirements": [
        "Access to the MCP server endpoint URL",
        "Use of the Smithery CLI or compatible client to add and interact with the MCP server"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "The documentation provides a clear feature list, installation instructions, and usage example, but lacks detailed usage examples and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# üåê networkcalc-mcp\n[![smithery badge](https://smithery.ai/badge/@ramadasmr/networkcalc-mcp)](https://smithery.ai/server/@ramadasmr/networkcalc-mcp)\n\n**networkcalc-mcp** is a server component (MCP Server) that powers utility services from [networkcalc.com](https://networkcalc.com). It provides a set of networking tools accessible via API or integration with your own frontend.\n\n## üîß Features\n\nThis server includes support for the following tools:\n\n- üîç **DNS Lookup** ‚Äì Resolve domain names to IP addresses and view DNS records.\n- üóÇÔ∏è **WHOIS Lookup** ‚Äì Retrieve registration and ownership details for domains and IPs.\n- üìú **SPF Lookup** ‚Äì Inspect Sender Policy Framework (SPF) records for domains.\n- üîê **Certificate Lookup** ‚Äì Check SSL/TLS certificates for domains.\n- üßÆ **Subnet Lookup** ‚Äì Analyze IP subnets and CIDR ranges.\n\n## üìñ How to Use\n\nRun below command to add it to claude \n```bash\nclaude mcp add --transport http ramadasmr-networkcalc-mcp \"https://server.smithery.ai/@ramadasmr/networkcalc-mcp/mcp\"\n```\n\n## Installing via Smithery\n\nTo install NetworkCalc automatically via [Smithery](https://smithery.ai/server/@ramadasmr/networkcalc-mcp):\n\n```bash\nnpx -y @smithery/cli install @ramadasmr/networkcalc-mcp\n```\n\nTo see full usage instructions and available endpoints, please refer to the hosted service:\nüëâ [https://smithery.ai/server/@ramadasmr/networkcalc-mcp](https://smithery.ai/server/@ramadasmr/networkcalc-mcp)",
        "start_pos": 0,
        "end_pos": 1428,
        "token_count_estimate": 356,
        "source_type": "readme",
        "agent_id": "a843568637a8a842"
      }
    ]
  },
  {
    "agent_id": "aed3bbc66176a8a3",
    "name": "ai.smithery/ref-tools-ref-tools-mcp",
    "source": "mcp",
    "source_url": "https://github.com/ref-tools/ref-tools-mcp",
    "description": "Provide your AI coding tools with token-efficient access to up-to-date technical documentation for‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-28T04:05:07.66948Z",
    "indexed_at": "2026-02-18T04:08:34.059186",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![Documentation for your agent](header.png)](https://ref.tools)\n[![smithery badge](https://smithery.ai/badge/@ref-tools/ref-tools-mcp)](https://smithery.ai/server/@ref-tools/ref-tools-mcp)\n[![Website](https://img.shields.io/badge/Website-ref.tools-blue)](https://ref.tools)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![npm version](https://img.shields.io/npm/v/ref-tools-mcp)](https://www.npmjs.com/package/ref-tools-mcp)\n\n# Ref MCP\n\nA [ModelContextProtocol](https://modelcontextprotocol.io) server that gives your AI coding tool or agent access to documentation for APIs, services, libraries etc. It's your one-stop-shop to keep your agent up-to-date on documentation in a fast and token-efficient way.\n\nFor more see info [ref.tools](https://ref.tools)\n\n## Agentic search for exactly the right context\n\nRef's tools are design to match how models search while using as little context as possible to reduce [context rot](https://research.trychroma.com/context-rot). The goal is to find exactly the context your coding agent needs to be successful while using minimum tokens.\n\nDepending on the complexity of the prompt, LLM coding agents like Claude Code will typically do one or more searches and then choose a few resources to read in more depth.\n\nFor a simple query about Figma's Comment REST API it will make a couple calls to get exactly what it needs:\n```\nSEARCH 'Figma API post comment endpoint documentation' (54 tokens)\nREAD https://www.figma.com/developers/api#post-comments-endpoint (385 tokens)\n```\n\nFor more complex situations, the LLM will try to refine it's prompt as it reads results. For example:\n```\nSEARCH 'n8n merge node vs Code node multiple inputs best practices' (126)\nREAD https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.merge/#merge (4961)\nREAD https://docs.n8n.io/flow-logic/merging/#merge-data-from-multiple-node-executions (138)\nSEARCH 'n8n Code node multiple inputs best practices when to use' (107)\nREAD https://docs.n8n.io/code/code-node/#usage (80)\nSEARCH 'n8n Code node access multiple inputs from different nodes' (370)\nSEARCH 'n8n Code node $input access multiple node inputs' (372)\nREAD https://docs.n8n.io/code/builtin/output-other-nodes/#output-of-other-nodes (2310)\n  ```\n\nRef takes advantage of MCP sessions to track search trajectory and minimize context usage. There's a lot more ideas cooking but here's what we've implemented so far.\n\n### 1. Filtering search results\nFor repeated similar searches in a session, Ref will never return repeated results. Traditionally, you dig farther in to search results by paging to the next result but this approach allows the agent to page AND adjust the prompt at the same time.\n\n### 2. Fetching the part of the page that matters\nWhen reading a page of documentation, Ref will use the agent's session search history to dropout less relevant sections and return the most relevant 5k tokens. This helps Ref avoid a big problem with standard `fetch()` web scraping which is when it hits a large documentation page you can easily end up pull in 20k+ tokens into context, most of which are irrelevant. \n\n## Why does minimizing tokens from documentation context matter?\n\n### 1. More context makes models dumber\n\nIt's well documented that as of July 2025 that models get dumber as you put in more tokens. You might have heard about how models are great with long context now and that's kind of true but not the whole picture. For a quick primer on some research, [checkout this video from the team at Chroma](https://www.youtube.com/watch?v=TUjQuC4ugak).\n\n### 2. Tokens cost $$$\n\nImagine you are using Claude Opus as a background agent and you start by having the agent pull in documentation context and suppose it pulls in 10000 tokens of context with 4000 being relevant and 6000 being extra noise. At API pricing, that 6k tokens cost about $0.09 PER STEP. If one prompt ends up taking 11 steps with Opus, you've spent $1 for no reason. \n\n\n## Setup\n\nThere are two options for setting up Ref as an MCP server, either via the streamable-http server (recommended) or local stdio server (legacy). \n\nThis repo contains the legacy stdio server. \n\n### Streamable HTTP (recommended)\n\n[![Install Ref MCP in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=Ref&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIm1jcC1yZW1vdGVAMC4xLjAtMCIsImh0dHBzOi8vYXBpLnJlZi50b29scy9tY3AiLCItLWhlYWRlcj14LXJlZi1hcGkta2V5OjxzaWduIHVwIHRvIGdldCBhbiBhcGkga2V5PiJdfQ==)\n\n```\n\"Ref\": {\n  \"type\": \"http\",\n  \"url\": \"https://api.ref.tools/mcp?apiKey=YOUR_API_KEY\"\n}\n```\n\n### stdio \n\n[![Install Ref MCP in Cursor (stdio)](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=Ref&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyJyZWYtdG9vbHMtbWNwIl0sImVudiI6eyJSRUZfQVBJX0tFWSI6IjxzaWduIHVwIHRvIGdldCBhbiBhcGkga2V5PiJ9fQ==)\n\n```\n\"Ref\": {\n  \"command\": \"npx\",\n  \"args\": [\"ref-tools-mcp@latest\"],\n  \"env\": {\n    \"REF_API_KEY\": <sign up to get an api key>\n  }\n}\n```\n\n## Tools\n\nRef MCP server provides all the documentation related tools for your agent needs.\n\n### ref_search_documentation\n\nA powerful search tool to check technical documentation. Great for finding facts or code snippets. Can be used to search for public documentation on the web or github as well from private resources like repos and pdfs.\n\n**Parameters:**\n- `query` (required): Query to search for relevant documentation. This should be a full sentence or question.\n\n### ref_read_url\n\nA tool that fetches content from a URL and converts it to markdown for easy reading with Ref. This is powerful when used in conjunction with the ref_search_documentation tool that returns urls of relevant content.\n\n**Parameters:**\n- `url` (required): The URL of the webpage to read.\n\n\n## OpenAI deep research support\n\nRef can be used as a source for deep research. OpenAI requires specific tool definitions so when used with an OpenAI client, Ref will provide the same tools with slightly different naming.\n\n```\nref_search_documentation(query) -> search(query)\nref_read_url(url) -> fetch(id)\n```\n\n## Development\n\n```\nnpm install\nnpm run dev\n```\n\n### Running with Inspector\n\nFor development and debugging purposes, you can use the MCP Inspector tool. The Inspector provides a visual interface for testing and monitoring MCP server interactions.\n\nVisit the [Inspector documentation](https://modelcontextprotocol.io/docs/tools/inspector) for detailed setup instructions.\n\nTo test locally with Inspector:\n```\nnpm run inspect\n```\n\nOr run both the watcher and inspector:\n```\nnpm run dev\n```\n\n### Local Development\n\n1. Clone the repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Build the project:\n```bash\nnpm run build\n```\n4. For development with auto-rebuilding:\n```bash\nnpm run watch\n```\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide access to up-to-date documentation for APIs, services, and libraries",
        "Perform agentic search to find exactly relevant documentation context",
        "Filter search results to avoid repeated results in a session",
        "Fetch and return the most relevant sections of documentation pages up to 5k tokens",
        "Support both streamable HTTP and local stdio server setups",
        "Offer tools to search documentation and read URLs with markdown conversion",
        "Integrate with OpenAI clients using adapted tool naming conventions",
        "Track search trajectory within MCP sessions to minimize token usage"
      ],
      "limitations": [
        "Legacy stdio server is less recommended compared to streamable HTTP server",
        "Does not fetch entire large documentation pages to avoid excessive token usage",
        "Requires API key for streamable HTTP server usage",
        "Limited to documentation and code-related context retrieval; not a general search engine"
      ],
      "requirements": [
        "API key for streamable HTTP server access",
        "Node.js environment for local stdio server setup",
        "npm for installing dependencies and running development scripts",
        "Permissions to run npm commands and network access for HTTP server"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, limitations, and environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![Documentation for your agent](header.png)](https://ref.tools)\n[![smithery badge](https://smithery.ai/badge/@ref-tools/ref-tools-mcp)](https://smithery.ai/server/@ref-tools/ref-tools-mcp)\n[![Website](https://img.shields.io/badge/Website-ref.tools-blue)](https://ref.tools)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)\n[![npm version](https://img.shields.io/npm/v/ref-tools-mcp)](https://www.npmjs.com/package/ref-tools-mcp)\n\n# Ref MCP\n\nA [ModelContextProtocol](https://modelcontextprotocol.io) server that gives your AI coding tool or agent access to documentation for APIs, services, libraries etc. It's your one-stop-shop to keep your agent up-to-date on documentation in a fast and token-efficient way.\n\nFor more see info [ref.tools](https://ref.tools)\n\n## Agentic search for exactly the right context\n\nRef's tools are design to match how models search while using as little context as possible to reduce [context rot](https://research.trychroma.com/context-rot). The goal is to find exactly the context your coding agent needs to be successful while using minimum tokens.\n\nDepending on the complexity of the prompt, LLM coding agents like Claude Code will typically do one or more searches and then choose a few resources to read in more depth.\n\nFor a simple query about Figma's Comment REST API it will make a couple calls to get exactly what it needs:\n```\nSEARCH 'Figma API post comment endpoint documentation' (54 tokens)\nREAD https://www.figma.com/developers/api#post-comments-endpoint (385 tokens)\n```\n\nFor more complex situations, the LLM will try to refine it's prompt as it reads results.",
        "start_pos": 0,
        "end_pos": 1631,
        "token_count_estimate": 407,
        "source_type": "readme",
        "agent_id": "aed3bbc66176a8a3"
      },
      {
        "chunk_id": 1,
        "text": "ic/merging/#merge-data-from-multiple-node-executions (138)\nSEARCH 'n8n Code node multiple inputs best practices when to use' (107)\nREAD https://docs.n8n.io/code/code-node/#usage (80)\nSEARCH 'n8n Code node access multiple inputs from different nodes' (370)\nSEARCH 'n8n Code node $input access multiple node inputs' (372)\nREAD https://docs.n8n.io/code/builtin/output-other-nodes/#output-of-other-nodes (2310)\n  ```\n\nRef takes advantage of MCP sessions to track search trajectory and minimize context usage. There's a lot more ideas cooking but here's what we've implemented so far.\n\n### 1. Filtering search results\nFor repeated similar searches in a session, Ref will never return repeated results. Traditionally, you dig farther in to search results by paging to the next result but this approach allows the agent to page AND adjust the prompt at the same time.\n\n### 2. Fetching the part of the page that matters\nWhen reading a page of documentation, Ref will use the agent's session search history to dropout less relevant sections and return the most relevant 5k tokens. This helps Ref avoid a big problem with standard `fetch()` web scraping which is when it hits a large documentation page you can easily end up pull in 20k+ tokens into context, most of which are irrelevant. \n\n## Why does minimizing tokens from documentation context matter?\n\n### 1. More context makes models dumber\n\nIt's well documented that as of July 2025 that models get dumber as you put in more tokens. You might have heard about how models are great with long context now and that's kind of true but not the whole picture. For a quick primer on some research, [checkout this video from the team at Chroma](https://www.youtube.com/watch?v=TUjQuC4ugak).\n\n### 2. Tokens cost $$$\n\nImagine you are using Claude Opus as a background agent and you start by having the agent pull in documentation context and suppose it pulls in 10000 tokens of context with 4000 being relevant and 6000 being extra noise. At API pricing, that 6k tokens cost about $0.09 PER STEP.",
        "start_pos": 1848,
        "end_pos": 3881,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "aed3bbc66176a8a3"
      },
      {
        "chunk_id": 2,
        "text": "g the agent pull in documentation context and suppose it pulls in 10000 tokens of context with 4000 being relevant and 6000 being extra noise. At API pricing, that 6k tokens cost about $0.09 PER STEP. If one prompt ends up taking 11 steps with Opus, you've spent $1 for no reason. \n\n\n## Setup\n\nThere are two options for setting up Ref as an MCP server, either via the streamable-http server (recommended) or local stdio server (legacy). \n\nThis repo contains the legacy stdio server. \n\n### Streamable HTTP (recommended)\n\n[![Install Ref MCP in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=Ref&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIm1jcC1yZW1vdGVAMC4xLjAtMCIsImh0dHBzOi8vYXBpLnJlZi50b29scy9tY3AiLCItLWhlYWRlcj14LXJlZi1hcGkta2V5OjxzaWduIHVwIHRvIGdldCBhbiBhcGkga2V5PiJdfQ==)\n\n```\n\"Ref\": {\n  \"type\": \"http\",\n  \"url\": \"https://api.ref.tools/mcp?apiKey=YOUR_API_KEY\"\n}\n```\n\n### stdio \n\n[![Install Ref MCP in Cursor (stdio)](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=Ref&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyJyZWYtdG9vbHMtbWNwIl0sImVudiI6eyJSRUZfQVBJX0tFWSI6IjxzaWduIHVwIHRvIGdldCBhbiBhcGkga2V5PiJ9fQ==)\n\n```\n\"Ref\": {\n  \"command\": \"npx\",\n  \"args\": [\"ref-tools-mcp@latest\"],\n  \"env\": {\n    \"REF_API_KEY\": <sign up to get an api key>\n  }\n}\n```\n\n## Tools\n\nRef MCP server provides all the documentation related tools for your agent needs.\n\n### ref_search_documentation\n\nA powerful search tool to check technical documentation. Great for finding facts or code snippets. Can be used to search for public documentation on the web or github as well from private resources like repos and pdfs.\n\n**Parameters:**\n- `query` (required): Query to search for relevant documentation. This should be a full sentence or question.\n\n### ref_read_url\n\nA tool that fetches content from a URL and converts it to markdown for easy reading with Ref. This is powerful when used in conjunction with the ref_search_documentation tool that returns urls of relevant content.",
        "start_pos": 3681,
        "end_pos": 5725,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "aed3bbc66176a8a3"
      },
      {
        "chunk_id": 3,
        "text": "etches content from a URL and converts it to markdown for easy reading with Ref. This is powerful when used in conjunction with the ref_search_documentation tool that returns urls of relevant content.\n\n**Parameters:**\n- `url` (required): The URL of the webpage to read.\n\n\n## OpenAI deep research support\n\nRef can be used as a source for deep research. OpenAI requires specific tool definitions so when used with an OpenAI client, Ref will provide the same tools with slightly different naming.\n\n```\nref_search_documentation(query) -> search(query)\nref_read_url(url) -> fetch(id)\n```\n\n## Development\n\n```\nnpm install\nnpm run dev\n```\n\n### Running with Inspector\n\nFor development and debugging purposes, you can use the MCP Inspector tool. The Inspector provides a visual interface for testing and monitoring MCP server interactions.\n\nVisit the [Inspector documentation](https://modelcontextprotocol.io/docs/tools/inspector) for detailed setup instructions.\n\nTo test locally with Inspector:\n```\nnpm run inspect\n```\n\nOr run both the watcher and inspector:\n```\nnpm run dev\n```\n\n### Local Development\n\n1. Clone the repository\n2. Install dependencies:\n```bash\nnpm install\n```\n3. Build the project:\n```bash\nnpm run build\n```\n4. For development with auto-rebuilding:\n```bash\nnpm run watch\n```\n\n## License\n\nMIT",
        "start_pos": 5525,
        "end_pos": 6826,
        "token_count_estimate": 325,
        "source_type": "readme",
        "agent_id": "aed3bbc66176a8a3"
      }
    ]
  },
  {
    "agent_id": "04172ab4993a68ad",
    "name": "ai.smithery/renCosta2025-context7fork",
    "source": "mcp",
    "source_url": "https://github.com/renCosta2025/context7fork",
    "description": "Get up-to-date, version-specific documentation and code examples from official sources directly in‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-29T10:26:07.372529Z",
    "indexed_at": "2026-02-18T04:08:35.246867",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "![Cover](public/cover.png)\n\n# Context7 MCP - Up-to-date Code Docs For Any Prompt\n\n[![Website](https://img.shields.io/badge/Website-context7.com-blue)](https://context7.com) [![smithery badge](https://smithery.ai/badge/@upstash/context7-mcp)](https://smithery.ai/server/@upstash/context7-mcp) [![NPM Version](https://img.shields.io/npm/v/%40upstash%2Fcontext7-mcp?color=red)](https://www.npmjs.com/package/@upstash/context7-mcp) [![MIT licensed](https://img.shields.io/npm/l/%40upstash%2Fcontext7-mcp)](./LICENSE)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D) [<img alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/Install%20in%20VS%20Code-0098FF?style=for-the-badge&logo=visualstudiocode&logoColor=white\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\n[![ÁπÅÈ´î‰∏≠Êñá](https://img.shields.io/badge/docs-ÁπÅÈ´î‰∏≠Êñá-yellow)](./docs/README.zh-TW.md) [![ÁÆÄ‰Ωì‰∏≠Êñá](https://img.shields.io/badge/docs-ÁÆÄ‰Ωì‰∏≠Êñá-yellow)](./docs/README.zh-CN.md) [![Êó•Êú¨Ë™û](https://img.shields.io/badge/docs-Êó•Êú¨Ë™û-b7003a)](./docs/README.ja.md) [![ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú](https://img.shields.io/badge/docs-ÌïúÍµ≠Ïñ¥-green)](./docs/README.ko.md) [![Documentaci√≥n en Espa√±ol](https://img.shields.io/badge/docs-Espa√±ol-orange)](./docs/README.es.md) [![Documentation en Fran√ßais](https://img.shields.io/badge/docs-Fran√ßais-blue)](./docs/README.fr.md) [![Documenta√ß√£o em Portugu√™s (Brasil)](<https://img.shields.io/badge/docs-Portugu√™s%20(Brasil)-purple>)](./docs/README.pt-BR.md) [![Documentazione in italiano](https://img.shields.io/badge/docs-Italian-red)](./docs/README.it.md) [![Dokumentasi Bahasa Indonesia](https://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./docs/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./docs/README.de.md) [![–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ](https://img.shields.io/badge/docs-–†—É—Å—Å–∫–∏–π-darkblue)](./docs/README.ru.md) [![–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è](https://img.shields.io/badge/docs-–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞-lightblue)](./docs/README.uk.md) [![T√ºrk√ße Dok√ºman](https://img.shields.io/badge/docs-T√ºrk√ße-blue)](./docs/README.tr.md) [![Arabic Documentation](https://img.shields.io/badge/docs-Arabic-white)](./docs/README.ar.md) [![Ti·∫øng Vi·ªát](https://img.shields.io/badge/docs-Ti·∫øng%20Vi·ªát-red)](./docs/README.vi.md)\n\n## ‚ùå Without Context7\n\nLLMs rely on outdated or generic information about the libraries you use. You get:\n\n- ‚ùå Code examples are outdated and based on year-old training data\n- ‚ùå Hallucinated APIs that don't even exist\n- ‚ùå Generic answers for old package versions\n\n## ‚úÖ With Context7\n\nContext7 MCP pulls up-to-date, version-specific documentation and code examples straight from the source ‚Äî and places them directly into your prompt.\n\nAdd `use context7` to your prompt in Cursor:\n\n```txt\nCreate a Next.js middleware that checks for a valid JWT in cookies and redirects unauthenticated users to `/login`. use context7\n```\n\n```txt\nConfigure a Cloudflare Worker script to cache JSON API responses for five minutes. use context7\n```\n\nContext7 fetches up-to-date code examples and documentation right into your LLM's context.\n\n- 1Ô∏è‚É£ Write your prompt naturally\n- 2Ô∏è‚É£ Tell the LLM to `use context7`\n- 3Ô∏è‚É£ Get working code answers\n\nNo tab-switching, no hallucinated APIs that don't exist, no outdated code generation.\n\n## üìö Adding Projects\n\nCheck out our [project addition guide](./docs/adding-projects.md) to learn how to add (or update) your favorite libraries to Context7.\n\n## üõ†Ô∏è Installation\n\n### Requirements\n\n- Node.js >= v18.0.0\n- Cursor, Claude Code, VSCode, Windsurf or another MCP Client\n- Context7 API Key (Optional for higher rate limits) (Get yours by creating an account at [context7.com/dashboard](https://context7.com/dashboard))\n\n> [!WARNING]\n> **SSE Protocol Deprecation Notice**\n>\n> The Server-Sent Events (SSE) transport protocol is deprecated and its endpoint will be removed in upcoming releases. Please use HTTP or stdio transport methods instead.\n\n<details>\n<summary><b>Installing via Smithery</b></summary>\n\nTo install Context7 MCP Server for any client automatically via [Smithery](https://smithery.ai/server/@upstash/context7-mcp):\n\n```bash\nnpx -y @smithery/cli@latest install @upstash/context7-mcp --client <CLIENT_NAME> --key <YOUR_SMITHERY_KEY>\n```\n\nYou can find your Smithery key in the [Smithery.ai webpage](https://smithery.ai/server/@upstash/context7-mcp).\n\n</details>\n\n<details>\n<summary><b>Install in Cursor</b></summary>\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n> Since Cursor 1.0, you can click the install button below for instant one-click installation.\n\n#### Cursor Remote Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJjb21tYW5kIjoibnB4IC15IEB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Claude Code</b></summary>\n\nRun this command. See [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Claude Code Remote Server Connection\n\n```sh\nclaude mcp add --transport http context7 https://mcp.context7.com/mcp --header \"CONTEXT7_API_KEY: YOUR_API_KEY\"\n```\n\n#### Claude Code Local Server Connection\n\n```sh\nclaude mcp add context7 -- npx -y @upstash/context7-mcp --api-key YOUR_API_KEY\n```\n\n</details>\n\n<details>\n<summary><b>Install in Windsurf</b></summary>\n\nAdd this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Windsurf Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"serverUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Windsurf Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in VS Code</b></summary>\n\n[<img alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Context7%20MCP&color=0098FF\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n[<img alt=\"Install in VS Code Insiders (npx)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Context7%20MCP&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### VS Code Remote Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### VS Code Local Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>\n<b>Install in Cline</b>\n</summary>\n\nYou can easily install Context7 through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Use the search bar within the **Marketplace** tab to find _Context7_.\n4. Click the **Install** button.\n\nOr you can directly edit MCP servers configuration:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Choose **Remote Servers** tab.\n4. Click the **Edit Configuration** button.\n5. Add context7 to `mcpServers`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"type\": \"streamableHttp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Zed</b></summary>\n\nIt can be installed via [Zed Extensions](https://zed.dev/extensions?query=Context7) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.\n\n```json\n{\n  \"context_servers\": {\n    \"Context7\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Augment Code</b></summary>\n\nTo configure Context7 MCP in Augment Code, you can use either the graphical interface or manual configuration.\n\n### **A. Using the Augment Code UI**\n\n1. Click the hamburger menu.\n2. Select **Settings**.\n3. Navigate to the **Tools** section.\n4. Click the **+ Add MCP** button.\n5. Enter the following command:\n\n   ```\n   npx -y @upstash/context7-mcp@latest\n   ```\n\n6. Name the MCP: **Context7**.\n7. Click the **Add** button.\n\nOnce the MCP server is added, you can start using Context7's up-to-date code documentation features directly within Augment Code.\n\n---\n\n### **B. Manual Configuration**\n\n1. Press Cmd/Ctrl Shift P or go to the hamburger menu in the Augment panel\n2. Select Edit Settings\n3. Under Advanced, click Edit in settings.json\n4. Add the server configuration to the `mcpServers` array in the `augment.advanced` object\n\n```json\n\"augment.advanced\": {\n  \"mcpServers\": [\n    {\n      \"name\": \"context7\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  ]\n}\n```\n\nOnce the MCP server is added, restart your editor. If you receive any errors, check the syntax to make sure closing brackets or commas are not missing.\n\n</details>\n\n<details>\n<summary><b>Install in Roo Code</b></summary>\n\nAdd this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.\n\n#### Roo Code Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Roo Code Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Gemini CLI</b></summary>\n\nSee [Gemini CLI Configuration](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for details.\n\n1.  Open the Gemini CLI settings file. The location is `~/.gemini/settings.json` (where `~` is your home directory).\n2.  Add the following to the `mcpServers` object in your `settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"httpUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n        \"Accept\": \"application/json, text/event-stream\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nIf the `mcpServers` object does not exist, create it.\n\n</details>\n\n<details>\n<summary><b>Install in Claude Desktop</b></summary>\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to Settings > Connectors > Add Custom Connector. Enter the name as `Context7` and the remote MCP server URL as `https://mcp.context7.com/mcp`.\n\n#### Local Server Connection\n\nOpen Claude Desktop developer settings and edit your `claude_desktop_config.json` file to add the following configuration. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Opencode</b></summary>\n\nAdd this to your Opencode configuration file. See [Opencode MCP docs](https://opencode.ai/docs/mcp-servers) for more info.\n\n#### Opencode Remote Server Connection\n\n```json\n\"mcp\": {\n  \"context7\": {\n    \"type\": \"remote\",\n    \"url\": \"https://mcp.context7.com/mcp\",\n    \"headers\": {\n      \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n    },\n    \"enabled\": true\n  }\n}\n```\n\n#### Opencode Local Server Connection\n\n```json\n{\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"local\",\n      \"command\": [\"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"enabled\": true\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in OpenAI Codex</b></summary>\n\nSee [OpenAI Codex](https://github.com/openai/codex) for more information.\n\nAdd the following configuration to your OpenAI Codex MCP server settings:\n\n```toml\n[mcp_servers.context7]\nargs = [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\ncommand = \"npx\"\n```\n\n‚ö†Ô∏è Windows Notes\n\nOn Windows, some users may encounter request timed out errors with the default configuration.\nIn that case, explicitly configure the MCP server with the full path to Node.js and the installed package:\n\n```toml\n[mcp_servers.context7]\ncommand = \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"\nargs = [\n  \"C:\\\\Users\\\\yourname\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\@upstash\\\\context7-mcp\\\\dist\\\\index.js\",\n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\n\nAlternatively, you can use the following configuration:\n\n```toml\n[mcp_servers.context7]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"@upstash/context7-mcp\",\n    \"--api-key\",\n    \"YOUR_API_KEY\"\n]\nenv = { SystemRoot=\"C:\\\\Windows\" }\nstartup_timeout_ms = 20_000\n```\n\nThis ensures Codex CLI works reliably on Windows.\n\n</details>\n\n<details>\n<summary><b>Install in JetBrains AI Assistant</b></summary>\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.\n\n1. In JetBrains IDEs, go to `Settings` -> `Tools` -> `AI Assistant` -> `Model Context Protocol (MCP)`\n2. Click `+ Add`.\n3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list\n4. Add this configuration and click `OK`\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n5. Click `Apply` to save changes.\n6. The same way context7 could be added for JetBrains Junie in `Settings` -> `Tools` -> `Junie` -> `MCP Settings`\n\n</details>\n\n<details>\n  \n<summary><b>Install in Kiro</b></summary>\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` > `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n</details>\n\n<details>\n<summary><b>Install in Trae</b></summary>\n\nUse the Add manually feature and fill in the JSON configuration information for that MCP server.\nFor more details, visit the [Trae documentation](https://docs.trae.ai/ide/model-context-protocol?_lang=en).\n\n#### Trae Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Trae Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Using Bun or Deno</b></summary>\n\nUse these alternatives to run the local Context7 MCP server with other runtimes. These examples work for any client that supports launching a local MCP server via command + args.\n\n#### Bun\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Deno\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"--allow-env=NO_DEPRECATION,TRACE_DEPRECATION\",\n        \"--allow-net\",\n        \"npm:@upstash/context7-mcp\"\n      ]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Using Docker</b></summary>\n\nIf you prefer to run the MCP server in a Docker container:\n\n1. **Build the Docker Image:**\n\n   First, create a `Dockerfile` in the project root (or anywhere you prefer):\n\n   <details>\n   <summary>Click to see Dockerfile content</summary>\n\n   ```Dockerfile\n   FROM node:18-alpine\n\n   WORKDIR /app\n\n   # Install the latest version globally\n   RUN npm install -g @upstash/context7-mcp\n\n   # Expose default port if needed (optional, depends on MCP client interaction)\n   # EXPOSE 3000\n\n   # Default command to run the server\n   CMD [\"context7-mcp\"]\n   ```\n\n   </details>\n\n   Then, build the image using a tag (e.g., `context7-mcp`). **Make sure Docker Desktop (or the Docker daemon) is running.** Run the following command in the same directory where you saved the `Dockerfile`:\n\n   ```bash\n   docker build -t context7-mcp .\n   ```\n\n2. **Configure Your MCP Client:**\n\n   Update your MCP client's configuration to use the Docker command.\n\n   _Example for a cline_mcp_settings.json:_\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"–°ontext7\": {\n         \"autoApprove\": [],\n         \"disabled\": false,\n         \"timeout\": 60,\n         \"command\": \"docker\",\n         \"args\": [\"run\", \"-i\", \"--rm\", \"context7-mcp\"],\n         \"transportType\": \"stdio\"\n       }\n     }\n   }\n   ```\n\n   _Note: This is an example configuration. Please refer to the specific examples for your MCP client (like Cursor, VS Code, etc.) earlier in this README to adapt the structure (e.g., `mcpServers` vs `servers`). Also, ensure the image name in `args` matches the tag used during the `docker build` command._\n\n</details>\n\n<details>\n<summary><b>Install Using the Desktop Extension</b></summary>\n\nInstall the [context7.mcpb](mcpb/context7.mcpb) file under the mcpb folder and add it to your client. For more information, please check out [MCP bundles docs](https://github.com/anthropics/mcpb#mcp-bundles-mcpb).\n\n</details>\n\n<details>\n<summary><b>Install in Windows</b></summary>\n\nThe configuration on Windows is slightly different compared to Linux or macOS (_`Cline` is used in the example_). The same principle applies to other editors; refer to the configuration of `command` and `args`.\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/upstash/context7-mcp\": {\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Amazon Q Developer CLI</b></summary>\n\nAdd this to your Amazon Q Developer CLI configuration file. See [Amazon Q Developer CLI docs](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) for more details.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Warp</b></summary>\n\nSee [Warp Model Context Protocol Documentation](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server) for details.\n\n1. Navigate `Settings` > `AI` > `Manage MCP servers`.\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"Context7\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n    \"env\": {},\n    \"working_directory\": null,\n    \"start_on_launch\": true\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n</details>\n\n<details>\n\n<summary><b>Install in Copilot Coding Agent</b></summary>\n\n## Using Context7 with Copilot Coding Agent\n\nAdd the following configuration to the `mcp` section of your Copilot Coding Agent configuration file Repository->Settings->Copilot->Coding agent->MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      },\n      \"tools\": [\"get-library-docs\", \"resolve-library-id\"]\n    }\n  }\n}\n```\n\nFor more information, see the [official GitHub documentation](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp).\n\n</details>\n\n<details>\n<summary><b>Install in LM Studio</b></summary>\n\nSee [LM Studio MCP Support](https://lmstudio.ai/blog/lmstudio-v0.3.17) for more information.\n\n#### One-click install:\n\n[![Add MCP Server context7 to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=context7&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkB1cHN0YXNoL2NvbnRleHQ3LW1jcCJdfQ%3D%3D)\n\n#### Manual set-up:\n\n1. Navigate to `Program` (right side) > `Install` > `Edit mcp.json`.\n2. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n3. Click `Save` to apply the changes.\n4. Toggle the MCP server on/off from the right hand side, under `Program`, or by clicking the plug icon at the bottom of the chat box.\n\n</details>\n\n<details>\n<summary><b>Install in Visual Studio 2022</b></summary>\n\nYou can configure Context7 MCP in Visual Studio 2022 by following the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\nAdd this to your Visual Studio MCP config file (see the [Visual Studio docs](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022) for details):\n\n```json\n{\n  \"inputs\": [],\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"context7\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      }\n    }\n  }\n}\n```\n\nFor more information and troubleshooting, refer to the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\n</details>\n\n<details>\n<summary><b>Install in Crush</b></summary>\n\nAdd this to your Crush configuration file. See [Crush MCP docs](https://github.com/charmbracelet/crush#mcps) for more info.\n\n#### Crush Remote Server Connection (HTTP)\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Crush Local Server Connection\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in BoltAI</b></summary>\n\nOpen the \"Settings\" page of the app, navigate to \"Plugins,\" and enter the following JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nOnce saved, enter in the chat `get-library-docs` followed by your Context7 documentation ID (e.g., `get-library-docs /nuxt/ui`). More information is available on [BoltAI's Documentation site](https://docs.boltai.com/docs/plugins/mcp-servers). For BoltAI on iOS, [see this guide](https://docs.boltai.com/docs/boltai-mobile/mcp-servers).\n\n</details>\n\n<details>\n<summary><b>Install in Rovo Dev CLI</b></summary>\n\nEdit your Rovo Dev CLI MCP config by running the command below -\n\n```bash\nacli rovodev mcp\n```\n\nExample config -\n\n#### Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Zencoder</b></summary>\n\nTo configure Context7 MCP in Zencoder, follow these steps:\n\n1. Go to the Zencoder menu (...)\n2. From the dropdown menu, select Agent tools\n3. Click on the Add custom MCP\n4. Add the name and server configuration from below, and make sure to hit the Install button\n\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n}\n```\n\nOnce the MCP server is added, you can easily continue using it.\n\n</details>\n\n<details>\n<summary><b>Install in Qodo Gen</b></summary>\n\nSee [Qodo Gen docs](https://docs.qodo.ai/qodo-documentation/qodo-gen/qodo-gen-chat/agentic-mode/agentic-tools-mcps) for more details.\n\n1. Open Qodo Gen chat panel in VSCode or IntelliJ.\n2. Click Connect more tools.\n3. Click + Add new MCP.\n4. Add the following configuration:\n\n#### Qodo Gen Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Qodo Gen Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Perplexity Desktop</b></summary>\n\nSee [Local and Remote MCPs for Perplexity](https://www.perplexity.ai/help-center/en/articles/11502712-local-and-remote-mcps-for-perplexity) for more information.\n\n1. Navigate `Perplexity` > `Settings`\n2. Select `Connectors`.\n3. Click `Add Connector`.\n4. Select `Advanced`.\n5. Enter Server Name: `Context7`\n6. Paste the following JSON in the text area:\n\n```json\n{\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n  \"command\": \"npx\",\n  \"env\": {}\n}\n```\n\n7. Click `Save`.\n</details>\n\n## üî® Available Tools\n\nContext7 MCP provides the following tools that LLMs can use:\n\n- `resolve-library-id`: Resolves a general library name into a Context7-compatible library ID.\n  - `libraryName` (required): The name of the library to search for\n\n- `get-library-docs`: Fetches documentation for a library using a Context7-compatible library ID.\n  - `context7CompatibleLibraryID` (required): Exact Context7-compatible library ID (e.g., `/mongodb/docs`, `/vercel/next.js`)\n  - `topic` (optional): Focus the docs on a specific topic (e.g., \"routing\", \"hooks\")\n  - `tokens` (optional, default 5000): Max number of tokens to return. Values less than 1000 are automatically increased to 1000.\n\n## üõü Tips\n\n### Add a Rule\n\nIf you don‚Äôt want to add `use context7` to every prompt, you can define a simple rule in your MCP client's rule section:\n\n- For Windsurf, in `.windsurfrules` file\n- For Cursor, from `Cursor Settings > Rules` section\n- For Claude Code, in `CLAUDE.md` file\n\nOr the equivalent in your MCP client to auto-invoke Context7 on any code question.\n\n#### Example Rule\n\n```txt\nAlways use context7 when I need code generation, setup or configuration steps, or\nlibrary/API documentation. This means you should automatically use the Context7 MCP\ntools to resolve library id and get library docs without me having to explicitly ask.\n```\n\nFrom then on, you‚Äôll get Context7‚Äôs docs in any related conversation without typing anything extra. You can alter the rule to match your use cases.\n\n### Use Library Id\n\nIf you already know exactly which library you want to use, add its Context7 ID to your prompt. That way, Context7 MCP server can skip the library-matching step and directly continue with retrieving docs.\n\n```txt\nImplement basic authentication with Supabase. use library /supabase/supabase for API and docs.\n```\n\nThe slash syntax tells the MCP tool exactly which library to load docs for.\n\n### HTTPS Proxy\n\nIf you are behind an HTTP proxy, Context7 uses the standard `https_proxy` / `HTTPS_PROXY` environment variables.\n\n## üíª Development\n\nClone the project and install dependencies:\n\n```bash\nbun i\n```\n\nBuild:\n\n```bash\nbun run build\n```\n\nRun the server:\n\n```bash\nbun run dist/index.js\n```\n\n### CLI Arguments\n\n`context7-mcp` accepts the following CLI flags:\n\n- `--transport <stdio|http>` ‚Äì Transport to use (`stdio` by default). Note that HTTP transport automatically provides both HTTP and SSE endpoints.\n- `--port <number>` ‚Äì Port to listen on when using `http` transport (default `3000`).\n- `--api-key <key>` ‚Äì API key for authentication. You can get your API key by creating an account at [context7.com/dashboard](https://context7.com/dashboard).\n\nExample with HTTP transport and port 8080:\n\n```bash\nbun run dist/index.js --transport http --port 8080\n```\n\nAnother example with stdio transport:\n\n```bash\nbun run dist/index.js --transport stdio --api-key YOUR_API_KEY\n```\n\n<details>\n<summary><b>Local Configuration Example</b></summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"tsx\", \"/path/to/folder/context7/src/index.ts\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Testing with MCP Inspector</b></summary>\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx @upstash/context7-mcp\n```\n\n</details>\n\n## üö® Troubleshooting\n\n<details>\n<summary><b>Module Not Found Errors</b></summary>\n\nIf you encounter `ERR_MODULE_NOT_FOUND`, try using `bunx` instead of `npx`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\nThis often resolves module resolution issues in environments where `npx` doesn't properly install or resolve packages.\n\n</details>\n\n<details>\n<summary><b>ESM Resolution Issues</b></summary>\n\nFor errors like `Error: Cannot find module 'uriTemplate.js'`, try the `--experimental-vm-modules` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-vm-modules\", \"@upstash/context7-mcp@1.0.6\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>TLS/Certificate Issues</b></summary>\n\nUse the `--experimental-fetch` flag to bypass TLS-related problems:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-fetch\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>General MCP Client Errors</b></summary>\n\n1. Try adding `@latest` to the package name\n2. Use `bunx` as an alternative to `npx`\n3. Consider using `deno` as another alternative\n4. Ensure you're using Node.js v18 or higher for native fetch support\n\n</details>\n\n## ‚ö†Ô∏è Disclaimer\n\nContext7 projects are community-contributed and while we strive to maintain high quality, we cannot guarantee the accuracy, completeness, or security of all library documentation. Projects listed in Context7 are developed and maintained by their respective owners, not by Context7. If you encounter any suspicious, inappropriate, or potentially harmful content, please use the \"Report\" button on the project page to notify us immediately. We take all reports seriously and will review flagged content promptly to maintain the integrity and safety of our platform. By using Context7, you acknowledge that you do so at your own discretion and risk.\n\n## ü§ù Connect with Us\n\nStay updated and join our community:\n\n- üì¢ Follow us on [X](https://x.com/context7ai) for the latest news and updates\n- üåê Visit our [Website](https://context7.com)\n- üí¨ Join our [Discord Community](https://upstash.com/discord)\n\n## üì∫ Context7 In Media\n\n- [Better Stack: \"Free Tool Makes Cursor 10x Smarter\"](https://youtu.be/52FC3qObp9E)\n- [Cole Medin: \"This is Hands Down the BEST MCP Server for AI Coding Assistants\"](https://www.youtube.com/watch?v=G7gK8H6u7Rs)\n- [Income Stream Surfers: \"Context7 + SequentialThinking MCPs: Is This AGI?\"](https://www.youtube.com/watch?v=-ggvzyLpK6o)\n- [Julian Goldie SEO: \"Context7: New MCP AI Agent Update\"](https://www.youtube.com/watch?v=CTZm6fBYisc)\n- [JeredBlu: \"Context 7 MCP: Get Documentation Instantly + VS Code Setup\"](https://www.youtube.com/watch?v=-ls0D-rtET4)\n- [Income Stream Surfers: \"Context7: The New MCP Server That Will CHANGE AI Coding\"](https://www.youtube.com/watch?v=PS-2Azb-C3M)\n- [AICodeKing: \"Context7 + Cline & RooCode: This MCP Server Makes CLINE 100X MORE EFFECTIVE!\"](https://www.youtube.com/watch?v=qZfENAPMnyo)\n- [Sean Kochel: \"5 MCP Servers For Vibe Coding Glory (Just Plug-In & Go)\"](https://www.youtube.com/watch?v=LqTQi8qexJM)\n\n## ‚≠ê Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=upstash/context7&type=Date)](https://www.star-history.com/#upstash/context7&Date)\n\n## üìÑ License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Fetch up-to-date, version-specific code documentation and examples from source libraries",
        "Integrate with multiple MCP clients such as Cursor, Claude Code, VS Code, Windsurf, Cline, Zed, Augment Code, Roo Code, and Gemini CLI",
        "Embed current code examples and API documentation directly into LLM prompts to improve code generation accuracy",
        "Support both remote HTTP and local stdio transport methods for MCP server connections",
        "Allow optional use of Context7 API key for higher rate limits",
        "Provide multi-language documentation for ease of use internationally",
        "Enable adding or updating library projects to the Context7 database",
        "Offer one-click installation options for various MCP clients"
      ],
      "limitations": [
        "Does not support the deprecated Server-Sent Events (SSE) transport protocol",
        "Requires Node.js version 18.0.0 or higher",
        "API key is optional but recommended for higher rate limits",
        "Limited to libraries/projects added to Context7; user must add projects to get documentation for unsupported libraries"
      ],
      "requirements": [
        "Node.js version 18.0.0 or higher",
        "An MCP client such as Cursor, Claude Code, VS Code, Windsurf, Cline, Zed, Augment Code, Roo Code, or Gemini CLI",
        "Optional Context7 API key obtainable by creating an account at context7.com/dashboard for higher rate limits",
        "For local server usage, ability to run 'npx' commands"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions for multiple clients, usage examples, detailed configuration snippets, explicit limitations, and requirements, along with multi-language support.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "![Cover](public/cover.png)\n\n# Context7 MCP - Up-to-date Code Docs For Any Prompt\n\n[![Website](https://img.shields.io/badge/Website-context7.com-blue)](https://context7.com) [![smithery badge](https://smithery.ai/badge/@upstash/context7-mcp)](https://smithery.ai/server/@upstash/context7-mcp) [![NPM Version](https://img.shields.io/npm/v/%40upstash%2Fcontext7-mcp?color=red)](https://www.npmjs.com/package/@upstash/context7-mcp) [![MIT licensed](https://img.shields.io/npm/l/%40upstash%2Fcontext7-mcp)](./LICENSE)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D) [<img alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/Install%20in%20VS%20Code-0098FF?style=for-the-badge&logo=visualstudiocode&logoColor=white\">](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\n[![ÁπÅÈ´î‰∏≠Êñá](https://img.shields.io/badge/docs-ÁπÅÈ´î‰∏≠Êñá-yellow)](./docs/README.zh-TW.md) [![ÁÆÄ‰Ωì‰∏≠Êñá](https://img.shields.io/badge/docs-ÁÆÄ‰Ωì‰∏≠Êñá-yellow)](./docs/README.zh-CN.md) [![Êó•Êú¨Ë™û](https://img.shields.io/badge/docs-Êó•Êú¨Ë™û-b7003a)](./docs/README.ja.md) [![ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú](https://img.shields.io/badge/docs-ÌïúÍµ≠Ïñ¥-green)](./docs/README.ko.md) [![Documentaci√≥n en Espa√±ol](https://img.shields.io/badge/docs-Espa√±ol-orange)](./docs/README.es.md) [![Documentation en Fran√ßais](https://img.shields.io/badge/docs-Fran√ßais-blue)](./docs/README.fr.md) [![Documenta√ß√£o em Portugu√™s (Brasil)](<https://img.shields.io/badge/docs-Portugu√™s%20(Brasil)-purple>)](./docs/README.pt-BR.md) [![Documentazione in italiano](https://img.shields.io/badge/docs-Italian-red)](./docs/README.it.md) [![Dokumentasi Bahasa Indonesia](https://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./docs/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./docs/README.de.md) [![–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü",
        "start_pos": 0,
        "end_pos": 2048,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 1,
        "text": "tps://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./docs/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./docs/README.de.md) [![–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ](https://img.shields.io/badge/docs-–†—É—Å—Å–∫–∏–π-darkblue)](./docs/README.ru.md) [![–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è](https://img.shields.io/badge/docs-–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞-lightblue)](./docs/README.uk.md) [![T√ºrk√ße Dok√ºman](https://img.shields.io/badge/docs-T√ºrk√ße-blue)](./docs/README.tr.md) [![Arabic Documentation](https://img.shields.io/badge/docs-Arabic-white)](./docs/README.ar.md) [![Ti·∫øng Vi·ªát](https://img.shields.io/badge/docs-Ti·∫øng%20Vi·ªát-red)](./docs/README.vi.md)\n\n## ‚ùå Without Context7\n\nLLMs rely on outdated or generic information about the libraries you use. You get:\n\n- ‚ùå Code examples are outdated and based on year-old training data\n- ‚ùå Hallucinated APIs that don't even exist\n- ‚ùå Generic answers for old package versions\n\n## ‚úÖ With Context7\n\nContext7 MCP pulls up-to-date, version-specific documentation and code examples straight from the source ‚Äî and places them directly into your prompt.\n\nAdd `use context7` to your prompt in Cursor:\n\n```txt\nCreate a Next.js middleware that checks for a valid JWT in cookies and redirects unauthenticated users to `/login`. use context7\n```\n\n```txt\nConfigure a Cloudflare Worker script to cache JSON API responses for five minutes. use context7\n```\n\nContext7 fetches up-to-date code examples and documentation right into your LLM's context.\n\n- 1Ô∏è‚É£ Write your prompt naturally\n- 2Ô∏è‚É£ Tell the LLM to `use context7`\n- 3Ô∏è‚É£ Get working code answers\n\nNo tab-switching, no hallucinated APIs that don't exist, no outdated code generation.\n\n## üìö Adding Projects\n\nCheck out our [project addition guide](./docs/adding-projects.md) to learn how to add (or update) your favorite libraries to Context7.",
        "start_pos": 1848,
        "end_pos": 3690,
        "token_count_estimate": 460,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 2,
        "text": "Ô∏è Installation\n\n### Requirements\n\n- Node.js >= v18.0.0\n- Cursor, Claude Code, VSCode, Windsurf or another MCP Client\n- Context7 API Key (Optional for higher rate limits) (Get yours by creating an account at [context7.com/dashboard](https://context7.com/dashboard))\n\n> [!WARNING]\n> **SSE Protocol Deprecation Notice**\n>\n> The Server-Sent Events (SSE) transport protocol is deprecated and its endpoint will be removed in upcoming releases. Please use HTTP or stdio transport methods instead.\n\n<details>\n<summary><b>Installing via Smithery</b></summary>\n\nTo install Context7 MCP Server for any client automatically via [Smithery](https://smithery.ai/server/@upstash/context7-mcp):\n\n```bash\nnpx -y @smithery/cli@latest install @upstash/context7-mcp --client <CLIENT_NAME> --key <YOUR_SMITHERY_KEY>\n```\n\nYou can find your Smithery key in the [Smithery.ai webpage](https://smithery.ai/server/@upstash/context7-mcp).\n\n</details>\n\n<details>\n<summary><b>Install in Cursor</b></summary>\n\nGo to: `Settings` -> `Cursor Settings` -> `MCP` -> `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n> Since Cursor 1.0, you can click the install button below for instant one-click installation.",
        "start_pos": 3696,
        "end_pos": 5148,
        "token_count_estimate": 363,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 3,
        "text": "}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJjb21tYW5kIjoibnB4IC15IEB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Claude Code</b></summary>\n\nRun this command. See [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Claude Code Remote Server Connection\n\n```sh\nclaude mcp add --transport http context7 https://mcp.context7.com/mcp --header \"CONTEXT7_API_KEY: YOUR_API_KEY\"\n```\n\n#### Claude Code Local Server Connection\n\n```sh\nclaude mcp add context7 -- npx -y @upstash/context7-mcp --api-key YOUR_API_KEY\n```\n\n</details>\n\n<details>\n<summary><b>Install in Windsurf</b></summary>\n\nAdd this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.",
        "start_pos": 5544,
        "end_pos": 6620,
        "token_count_estimate": 268,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 4,
        "text": "2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n[<img alt=\"Install in VS Code Insiders (npx)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Context7%20MCP&color=24bfa5\">](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### VS Code Remote Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### VS Code Local Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary>\n<b>Install in Cline</b>\n</summary>\n\nYou can easily install Context7 through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Use the search bar within the **Marketplace** tab to find _Context7_.\n4. Click the **Install** button.\n\nOr you can directly edit MCP servers configuration:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Choose **Remote Servers** tab.\n4. Click the **Edit Configuration** button.\n5.",
        "start_pos": 7392,
        "end_pos": 9112,
        "token_count_estimate": 430,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 5,
        "text": "\"type\": \"streamableHttp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Zed</b></summary>\n\nIt can be installed via [Zed Extensions](https://zed.dev/extensions?query=Context7) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.\n\n```json\n{\n  \"context_servers\": {\n    \"Context7\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Augment Code</b></summary>\n\nTo configure Context7 MCP in Augment Code, you can use either the graphical interface or manual configuration.\n\n### **A. Using the Augment Code UI**\n\n1. Click the hamburger menu.\n2. Select **Settings**.\n3. Navigate to the **Tools** section.\n4. Click the **+ Add MCP** button.\n5. Enter the following command:\n\n   ```\n   npx -y @upstash/context7-mcp@latest\n   ```\n\n6. Name the MCP: **Context7**.\n7. Click the **Add** button.\n\nOnce the MCP server is added, you can start using Context7's up-to-date code documentation features directly within Augment Code.\n\n---\n\n### **B. Manual Configuration**\n\n1. Press Cmd/Ctrl Shift P or go to the hamburger menu in the Augment panel\n2. Select Edit Settings\n3. Under Advanced, click Edit in settings.json\n4. Add the server configuration to the `mcpServers` array in the `augment.advanced` object\n\n```json\n\"augment.advanced\": {\n  \"mcpServers\": [\n    {\n      \"name\": \"context7\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  ]\n}\n```\n\nOnce the MCP server is added, restart your editor. If you receive any errors, check the syntax to make sure closing brackets or commas are not missing.\n\n</details>\n\n<details>\n<summary><b>Install in Roo Code</b></summary>\n\nAdd this to your Roo Code MCP configuration file.",
        "start_pos": 9240,
        "end_pos": 11237,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 6,
        "text": "rrors, check the syntax to make sure closing brackets or commas are not missing.\n\n</details>\n\n<details>\n<summary><b>Install in Roo Code</b></summary>\n\nAdd this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.\n\n#### Roo Code Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Roo Code Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Gemini CLI</b></summary>\n\nSee [Gemini CLI Configuration](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for details.\n\n1.  Open the Gemini CLI settings file. The location is `~/.gemini/settings.json` (where `~` is your home directory).\n2.  Add the following to the `mcpServers` object in your `settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"httpUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n        \"Accept\": \"application/json, text/event-stream\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nIf the `mcpServers` object does not exist, create it.\n\n</details>\n\n<details>\n<summary><b>Install in Claude Desktop</b></summary>\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to Settings > Connectors > Add Custom Connector. Enter the name as `Context7` and the remote MCP server URL as `https://mcp.context7.com/mcp`.",
        "start_pos": 11037,
        "end_pos": 12932,
        "token_count_estimate": 473,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 7,
        "text": "emote Server Connection\n\nOpen Claude Desktop and navigate to Settings > Connectors > Add Custom Connector. Enter the name as `Context7` and the remote MCP server URL as `https://mcp.context7.com/mcp`.\n\n#### Local Server Connection\n\nOpen Claude Desktop developer settings and edit your `claude_desktop_config.json` file to add the following configuration. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Opencode</b></summary>\n\nAdd this to your Opencode configuration file. See [Opencode MCP docs](https://opencode.ai/docs/mcp-servers) for more info.\n\n#### Opencode Remote Server Connection\n\n```json\n\"mcp\": {\n  \"context7\": {\n    \"type\": \"remote\",\n    \"url\": \"https://mcp.context7.com/mcp\",\n    \"headers\": {\n      \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n    },\n    \"enabled\": true\n  }\n}\n```\n\n#### Opencode Local Server Connection\n\n```json\n{\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"local\",\n      \"command\": [\"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"enabled\": true\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in OpenAI Codex</b></summary>\n\nSee [OpenAI Codex](https://github.com/openai/codex) for more information.\n\nAdd the following configuration to your OpenAI Codex MCP server settings:\n\n```toml\n[mcp_servers.context7]\nargs = [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\ncommand = \"npx\"\n```\n\n‚ö†Ô∏è Windows Notes\n\nOn Windows, some users may encounter request timed out errors with the default configuration.",
        "start_pos": 12732,
        "end_pos": 14451,
        "token_count_estimate": 429,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 8,
        "text": ".context7]\ncommand = \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"\nargs = [\n  \"C:\\\\Users\\\\yourname\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\@upstash\\\\context7-mcp\\\\dist\\\\index.js\",\n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\n\nAlternatively, you can use the following configuration:\n\n```toml\n[mcp_servers.context7]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"@upstash/context7-mcp\",\n    \"--api-key\",\n    \"YOUR_API_KEY\"\n]\nenv = { SystemRoot=\"C:\\\\Windows\" }\nstartup_timeout_ms = 20_000\n```\n\nThis ensures Codex CLI works reliably on Windows.\n\n</details>\n\n<details>\n<summary><b>Install in JetBrains AI Assistant</b></summary>\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.\n\n1. In JetBrains IDEs, go to `Settings` -> `Tools` -> `AI Assistant` -> `Model Context Protocol (MCP)`\n2. Click `+ Add`.\n3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list\n4. Add this configuration and click `OK`\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n5. Click `Apply` to save changes.\n6. The same way context7 could be added for JetBrains Junie in `Settings` -> `Tools` -> `Junie` -> `MCP Settings`\n\n</details>\n\n<details>\n  \n<summary><b>Install in Kiro</b></summary>\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` > `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.",
        "start_pos": 14580,
        "end_pos": 16512,
        "token_count_estimate": 483,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 9,
        "text": "x\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n</details>\n\n<details>\n<summary><b>Install in Trae</b></summary>\n\nUse the Add manually feature and fill in the JSON configuration information for that MCP server.\nFor more details, visit the [Trae documentation](https://docs.trae.ai/ide/model-context-protocol?_lang=en).\n\n#### Trae Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Trae Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Using Bun or Deno</b></summary>\n\nUse these alternatives to run the local Context7 MCP server with other runtimes. These examples work for any client that supports launching a local MCP server via command + args.\n\n#### Bun\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Deno\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"--allow-env=NO_DEPRECATION,TRACE_DEPRECATION\",\n        \"--allow-net\",\n        \"npm:@upstash/context7-mcp\"\n      ]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Using Docker</b></summary>\n\nIf you prefer to run the MCP server in a Docker container:\n\n1.",
        "start_pos": 16312,
        "end_pos": 17922,
        "token_count_estimate": 402,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 10,
        "text": "# Install the latest version globally\n   RUN npm install -g @upstash/context7-mcp\n\n   # Expose default port if needed (optional, depends on MCP client interaction)\n   # EXPOSE 3000\n\n   # Default command to run the server\n   CMD [\"context7-mcp\"]\n   ```\n\n   </details>\n\n   Then, build the image using a tag (e.g., `context7-mcp`). **Make sure Docker Desktop (or the Docker daemon) is running.** Run the following command in the same directory where you saved the `Dockerfile`:\n\n   ```bash\n   docker build -t context7-mcp .\n   ```\n\n2. **Configure Your MCP Client:**\n\n   Update your MCP client's configuration to use the Docker command.\n\n   _Example for a cline_mcp_settings.json:_\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"–°ontext7\": {\n         \"autoApprove\": [],\n         \"disabled\": false,\n         \"timeout\": 60,\n         \"command\": \"docker\",\n         \"args\": [\"run\", \"-i\", \"--rm\", \"context7-mcp\"],\n         \"transportType\": \"stdio\"\n       }\n     }\n   }\n   ```\n\n   _Note: This is an example configuration. Please refer to the specific examples for your MCP client (like Cursor, VS Code, etc.) earlier in this README to adapt the structure (e.g., `mcpServers` vs `servers`). Also, ensure the image name in `args` matches the tag used during the `docker build` command._\n\n</details>\n\n<details>\n<summary><b>Install Using the Desktop Extension</b></summary>\n\nInstall the [context7.mcpb](mcpb/context7.mcpb) file under the mcpb folder and add it to your client. For more information, please check out [MCP bundles docs](https://github.com/anthropics/mcpb#mcp-bundles-mcpb).\n\n</details>\n\n<details>\n<summary><b>Install in Windows</b></summary>\n\nThe configuration on Windows is slightly different compared to Linux or macOS (_`Cline` is used in the example_). The same principle applies to other editors; refer to the configuration of `command` and `args`.",
        "start_pos": 18160,
        "end_pos": 20012,
        "token_count_estimate": 463,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 11,
        "text": "uration on Windows is slightly different compared to Linux or macOS (_`Cline` is used in the example_). The same principle applies to other editors; refer to the configuration of `command` and `args`.\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/upstash/context7-mcp\": {\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Amazon Q Developer CLI</b></summary>\n\nAdd this to your Amazon Q Developer CLI configuration file. See [Amazon Q Developer CLI docs](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) for more details.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Warp</b></summary>\n\nSee [Warp Model Context Protocol Documentation](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server) for details.\n\n1. Navigate `Settings` > `AI` > `Manage MCP servers`.\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"Context7\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n    \"env\": {},\n    \"working_directory\": null,\n    \"start_on_launch\": true\n  }\n}\n```\n\n4. Click `Save` to apply the changes.",
        "start_pos": 19812,
        "end_pos": 21312,
        "token_count_estimate": 375,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 12,
        "text": "type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      },\n      \"tools\": [\"get-library-docs\", \"resolve-library-id\"]\n    }\n  }\n}\n```\n\nFor more information, see the [official GitHub documentation](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp).\n\n</details>\n\n<details>\n<summary><b>Install in LM Studio</b></summary>\n\nSee [LM Studio MCP Support](https://lmstudio.ai/blog/lmstudio-v0.3.17) for more information.\n\n#### One-click install:\n\n[![Add MCP Server context7 to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=context7&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkB1cHN0YXNoL2NvbnRleHQ3LW1jcCJdfQ%3D%3D)\n\n#### Manual set-up:\n\n1. Navigate to `Program` (right side) > `Install` > `Edit mcp.json`.\n2. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n3. Click `Save` to apply the changes.\n4. Toggle the MCP server on/off from the right hand side, under `Program`, or by clicking the plug icon at the bottom of the chat box.\n\n</details>\n\n<details>\n<summary><b>Install in Visual Studio 2022</b></summary>\n\nYou can configure Context7 MCP in Visual Studio 2022 by following the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).",
        "start_pos": 21660,
        "end_pos": 23220,
        "token_count_estimate": 390,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 13,
        "text": "\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"context7\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      }\n    }\n  }\n}\n```\n\nFor more information and troubleshooting, refer to the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\n</details>\n\n<details>\n<summary><b>Install in Crush</b></summary>\n\nAdd this to your Crush configuration file. See [Crush MCP docs](https://github.com/charmbracelet/crush#mcps) for more info.\n\n#### Crush Remote Server Connection (HTTP)\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Crush Local Server Connection\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in BoltAI</b></summary>\n\nOpen the \"Settings\" page of the app, navigate to \"Plugins,\" and enter the following JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nOnce saved, enter in the chat `get-library-docs` followed by your Context7 documentation ID (e.g., `get-library-docs /nuxt/ui`). More information is available on [BoltAI's Documentation site](https://docs.boltai.com/docs/plugins/mcp-servers). For BoltAI on iOS, [see this guide](https://docs.boltai.com/docs/boltai-mobile/mcp-servers).",
        "start_pos": 23508,
        "end_pos": 25393,
        "token_count_estimate": 471,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 14,
        "text": "nformation is available on [BoltAI's Documentation site](https://docs.boltai.com/docs/plugins/mcp-servers). For BoltAI on iOS, [see this guide](https://docs.boltai.com/docs/boltai-mobile/mcp-servers).\n\n</details>\n\n<details>\n<summary><b>Install in Rovo Dev CLI</b></summary>\n\nEdit your Rovo Dev CLI MCP config by running the command below -\n\n```bash\nacli rovodev mcp\n```\n\nExample config -\n\n#### Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Zencoder</b></summary>\n\nTo configure Context7 MCP in Zencoder, follow these steps:\n\n1. Go to the Zencoder menu (...)\n2. From the dropdown menu, select Agent tools\n3. Click on the Add custom MCP\n4. Add the name and server configuration from below, and make sure to hit the Install button\n\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n}\n```\n\nOnce the MCP server is added, you can easily continue using it.\n\n</details>\n\n<details>\n<summary><b>Install in Qodo Gen</b></summary>\n\nSee [Qodo Gen docs](https://docs.qodo.ai/qodo-documentation/qodo-gen/qodo-gen-chat/agentic-mode/agentic-tools-mcps) for more details.\n\n1. Open Qodo Gen chat panel in VSCode or IntelliJ.\n2. Click Connect more tools.\n3. Click + Add new MCP.\n4.",
        "start_pos": 25193,
        "end_pos": 26728,
        "token_count_estimate": 383,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 15,
        "text": "ext7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Install in Perplexity Desktop</b></summary>\n\nSee [Local and Remote MCPs for Perplexity](https://www.perplexity.ai/help-center/en/articles/11502712-local-and-remote-mcps-for-perplexity) for more information.\n\n1. Navigate `Perplexity` > `Settings`\n2. Select `Connectors`.\n3. Click `Add Connector`.\n4. Select `Advanced`.\n5. Enter Server Name: `Context7`\n6. Paste the following JSON in the text area:\n\n```json\n{\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n  \"command\": \"npx\",\n  \"env\": {}\n}\n```\n\n7. Click `Save`.\n</details>\n\n## üî® Available Tools\n\nContext7 MCP provides the following tools that LLMs can use:\n\n- `resolve-library-id`: Resolves a general library name into a Context7-compatible library ID.\n  - `libraryName` (required): The name of the library to search for\n\n- `get-library-docs`: Fetches documentation for a library using a Context7-compatible library ID.\n  - `context7CompatibleLibraryID` (required): Exact Context7-compatible library ID (e.g., `/mongodb/docs`, `/vercel/next.js`)\n  - `topic` (optional): Focus the docs on a specific topic (e.g., \"routing\", \"hooks\")\n  - `tokens` (optional, default 5000): Max number of tokens to return. Values less than 1000 are automatically increased to 1000.\n\n## üõü Tips\n\n### Add a Rule\n\nIf you don‚Äôt want to add `use context7` to every prompt, you can define a simple rule in your MCP client's rule section:\n\n- For Windsurf, in `.windsurfrules` file\n- For Cursor, from `Cursor Settings > Rules` section\n- For Claude Code, in `CLAUDE.md` file\n\nOr the equivalent in your MCP client to auto-invoke Context7 on any code question.\n\n#### Example Rule\n\n```txt\nAlways use context7 when I need code generation, setup or configuration steps, or\nlibrary/API documentation. This means you should automatically use the Context7 MCP\ntools to resolve library id and get library docs without me having to explicitly ask.",
        "start_pos": 27041,
        "end_pos": 29026,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 16,
        "text": "tup or configuration steps, or\nlibrary/API documentation. This means you should automatically use the Context7 MCP\ntools to resolve library id and get library docs without me having to explicitly ask.\n```\n\nFrom then on, you‚Äôll get Context7‚Äôs docs in any related conversation without typing anything extra. You can alter the rule to match your use cases.\n\n### Use Library Id\n\nIf you already know exactly which library you want to use, add its Context7 ID to your prompt. That way, Context7 MCP server can skip the library-matching step and directly continue with retrieving docs.\n\n```txt\nImplement basic authentication with Supabase. use library /supabase/supabase for API and docs.\n```\n\nThe slash syntax tells the MCP tool exactly which library to load docs for.\n\n### HTTPS Proxy\n\nIf you are behind an HTTP proxy, Context7 uses the standard `https_proxy` / `HTTPS_PROXY` environment variables.\n\n## üíª Development\n\nClone the project and install dependencies:\n\n```bash\nbun i\n```\n\nBuild:\n\n```bash\nbun run build\n```\n\nRun the server:\n\n```bash\nbun run dist/index.js\n```\n\n### CLI Arguments\n\n`context7-mcp` accepts the following CLI flags:\n\n- `--transport <stdio|http>` ‚Äì Transport to use (`stdio` by default). Note that HTTP transport automatically provides both HTTP and SSE endpoints.\n- `--port <number>` ‚Äì Port to listen on when using `http` transport (default `3000`).\n- `--api-key <key>` ‚Äì API key for authentication. You can get your API key by creating an account at [context7.com/dashboard](https://context7.com/dashboard).",
        "start_pos": 28826,
        "end_pos": 30349,
        "token_count_estimate": 380,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 17,
        "text": "ntext7\": {\n      \"command\": \"npx\",\n      \"args\": [\"tsx\", \"/path/to/folder/context7/src/index.ts\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Testing with MCP Inspector</b></summary>\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx @upstash/context7-mcp\n```\n\n</details>\n\n## üö® Troubleshooting\n\n<details>\n<summary><b>Module Not Found Errors</b></summary>\n\nIf you encounter `ERR_MODULE_NOT_FOUND`, try using `bunx` instead of `npx`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\nThis often resolves module resolution issues in environments where `npx` doesn't properly install or resolve packages.\n\n</details>\n\n<details>\n<summary><b>ESM Resolution Issues</b></summary>\n\nFor errors like `Error: Cannot find module 'uriTemplate.js'`, try the `--experimental-vm-modules` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-vm-modules\", \"@upstash/context7-mcp@1.0.6\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>TLS/Certificate Issues</b></summary>\n\nUse the `--experimental-fetch` flag to bypass TLS-related problems:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-fetch\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>General MCP Client Errors</b></summary>\n\n1. Try adding `@latest` to the package name\n2. Use `bunx` as an alternative to `npx`\n3. Consider using `deno` as another alternative\n4. Ensure you're using Node.js v18 or higher for native fetch support\n\n</details>\n\n## ‚ö†Ô∏è Disclaimer\n\nContext7 projects are community-contributed and while we strive to maintain high quality, we cannot guarantee the accuracy, completeness, or security of all library documentation. Projects listed in Context7 are developed and maintained by their respective owners, not by Context7.",
        "start_pos": 30674,
        "end_pos": 32660,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 18,
        "text": "quality, we cannot guarantee the accuracy, completeness, or security of all library documentation. Projects listed in Context7 are developed and maintained by their respective owners, not by Context7. If you encounter any suspicious, inappropriate, or potentially harmful content, please use the \"Report\" button on the project page to notify us immediately. We take all reports seriously and will review flagged content promptly to maintain the integrity and safety of our platform. By using Context7, you acknowledge that you do so at your own discretion and risk.\n\n## ü§ù Connect with Us\n\nStay updated and join our community:\n\n- üì¢ Follow us on [X](https://x.com/context7ai) for the latest news and updates\n- üåê Visit our [Website](https://context7.com)\n- üí¨ Join our [Discord Community](https://upstash.com/discord)\n\n## üì∫ Context7 In Media\n\n- [Better Stack: \"Free Tool Makes Cursor 10x Smarter\"](https://youtu.be/52FC3qObp9E)\n- [Cole Medin: \"This is Hands Down the BEST MCP Server for AI Coding Assistants\"](https://www.youtube.com/watch?v=G7gK8H6u7Rs)\n- [Income Stream Surfers: \"Context7 + SequentialThinking MCPs: Is This AGI?\"](https://www.youtube.com/watch?v=-ggvzyLpK6o)\n- [Julian Goldie SEO: \"Context7: New MCP AI Agent Update\"](https://www.youtube.com/watch?v=CTZm6fBYisc)\n- [JeredBlu: \"Context 7 MCP: Get Documentation Instantly + VS Code Setup\"](https://www.youtube.com/watch?v=-ls0D-rtET4)\n- [Income Stream Surfers: \"Context7: The New MCP Server That Will CHANGE AI Coding\"](https://www.youtube.com/watch?v=PS-2Azb-C3M)\n- [AICodeKing: \"Context7 + Cline & RooCode: This MCP Server Makes CLINE 100X MORE EFFECTIVE!\"](https://www.youtube.com/watch?v=qZfENAPMnyo)\n- [Sean Kochel: \"5 MCP Servers For Vibe Coding Glory (Just Plug-In & Go)\"](https://www.youtube.com/watch?v=LqTQi8qexJM)\n\n## ‚≠ê Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=upstash/context7&type=Date)](https://www.star-history.com/#upstash/context7&Date)\n\n## üìÑ License\n\nMIT",
        "start_pos": 32460,
        "end_pos": 34431,
        "token_count_estimate": 492,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      },
      {
        "chunk_id": 19,
        "text": "h?v=LqTQi8qexJM)\n\n## ‚≠ê Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=upstash/context7&type=Date)](https://www.star-history.com/#upstash/context7&Date)\n\n## üìÑ License\n\nMIT",
        "start_pos": 34231,
        "end_pos": 34431,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "04172ab4993a68ad"
      }
    ]
  },
  {
    "agent_id": "385483b940041c8a",
    "name": "ai.smithery/rfdez-pvpc-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/rfdez/pvpc-mcp-server",
    "description": "Retrieve daily PVPC electricity tariffs for 2.0 TD consumers, published by Red El√©ctrica.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T16:57:17.590562Z",
    "indexed_at": "2026-02-18T04:08:37.632798",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Voluntary Price for the Small Consumer (PVPC) MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![NPM Version](https://img.shields.io/npm/v/%40rfdez%2Fpvpc-mcp-server)](https://www.npmjs.com/package/@rfdez/pvpc-mcp-server)\n\nFetch the Voluntary Price for the Small Consumer (PVPC) published daily by Red El√©ctrica at 8:15 p.m. This includes the hourly electricity tariffs that will apply the following day for consumers billed under the 2.0 TD tariff.\n\n## üß© Components\n\n### Tools\n\n- `fetch_prices`: Fetches the Voluntary Price for the Small Consumer (PVPC) prices for a given date range and geographical area.\n  - Inputs:\n    - `locale`: Get translations for sources. Accepted values: `es`, `en`. Defaults to `es`.\n    - `startDate`: Beginning of the date range to filter indicator values in iso8601 format. E.g. 2025-06-29T00:00:00.000+02:00. Defaults to the start of today.\n    - `endDate`: End of the date range to filter indicator values in iso8601 format. E.g. 2025-06-29T23:59:59.999+02:00. Defaults to the end of today.\n    - `timeAggregation`: How to aggregate indicator values when grouping them by time. Accepted values: `sum`, `average`. Defaults to `sum`.\n    - `timeTruncation`: Tells how to truncate data time series. Accepted values: `hour`, `day`, `month`, `year`. Optional parameter.\n    - `geographicalAggregation`: How to aggregate indicator values when grouping them by geographical ID. Accepted values: `sum`, `average`. Defaults to `sum`.\n    - `geographicalIds`: Tells the geographical IDs to filter indicator values. Accepted values: `3` (Espa√±a), `8741` (Pen√≠nsula), `8742` (Canarias), `8743` (Baleares), `8744` (Ceuta), `8745` (Melilla). Defaults to `8741`, `8742`, `8743`, `8744`, `8745`.\n    - `geographicalTruncation`: Tells how to group data at geographical level when the geographical aggregation is informed. Accepted values: `country`, `electric_system`. Optional parameter.\n  - Returns: Text content with the PVPC prices in JSON format.\n\n## üîß Configuration\n\n### Requirements\n\nYou need to register an API key from [Esios Red El√©ctrica de Espa√±a](https://www.esios.ree.es/es/pagina/api) to access the Esios Red El√©ctrica de Espa√±a API.\n\nYou will find the API documentation at [API e¬∑sios Documentation](https://api.esios.ree.es/).\n\n### Claude Desktop\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to `Settings > Connectors > Add Custom Connector`. Enter the name as `PVPC` and the remote MCP server URL like `https://mcp.example.com/mcp`.\n\n#### Local Server Connection\n\nAdd this to your Claude Desktop `claude_desktop_config.json` file. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"pvpc\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@rfdez/pvpc-mcp-server@latest\", \"--api-key\", \"your_esios_api_key\"]\n    }\n  }\n}\n```\n\n## üíª Development\n\nClone this repository and install the dependencies:\n\n```bash\nnpm install\n```\n\nBuild the project:\n\n```bash\nnpm run build\n```\n\nRun the server:\n\n```bash\nnode dist/index.js\n```\n\n### CLI Arguments\n\n`pvpc-mcp-server` accepts the following CLI flags:\n\n- `--transport <stdio|http>`: Transport to use (`stdio` by default).\n- `--port <number>`: Port to listen on when using `http` transport (default `8080`).\n- `--api-key <key>`: Your e¬∑sios API key for authentication.\n\nExample with http transport and port 8080:\n\n```bash\nnode dist/index.js --transport http --port 8080\n```\n\nExample with stdio transport:\n\n```bash\nnode dist/index.js --transport stdio --api-key YOUR_ESIOS_API_KEY\n```\n\n### Local Configuration Example\n\n```json\n{\n  \"mcpServers\": {\n    \"pvpc\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"tsx\", \"/path/to/folder/pvpc-mcp-server/src/index.ts\", \"--api-key\", \"YOUR_ESIOS_API_KEY\"]\n    }\n  }\n}\n```\n\n### Testing with MCP Inspector\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx -y @rfdez/pvpc-mcp-server\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Fetch the Voluntary Price for the Small Consumer (PVPC) electricity prices",
        "Retrieve hourly electricity tariffs for the following day",
        "Filter PVPC prices by date range using ISO8601 format",
        "Aggregate indicator values by time using sum or average",
        "Truncate data time series by hour, day, month, or year",
        "Aggregate indicator values by geographical ID using sum or average",
        "Filter PVPC prices by specific geographical IDs",
        "Group data at geographical level by country or electric system",
        "Provide PVPC prices in JSON format"
      ],
      "limitations": [
        "Requires a valid API key from Esios Red El√©ctrica de Espa√±a to access data",
        "Geographical filtering limited to predefined IDs (Espa√±a, Pen√≠nsula, Canarias, Baleares, Ceuta, Melilla)",
        "Time aggregation limited to sum or average",
        "Time truncation and geographical truncation are optional and limited to specified values"
      ],
      "requirements": [
        "API key from Esios Red El√©ctrica de Espa√±a",
        "Node.js environment to run the server",
        "NPM to install dependencies",
        "Access to Esios Red El√©ctrica de Espa√±a API"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation includes installation instructions, detailed tool descriptions with input parameters and outputs, usage examples for CLI and integration, configuration details, and explicit requirements and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Voluntary Price for the Small Consumer (PVPC) MCP Server\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![NPM Version](https://img.shields.io/npm/v/%40rfdez%2Fpvpc-mcp-server)](https://www.npmjs.com/package/@rfdez/pvpc-mcp-server)\n\nFetch the Voluntary Price for the Small Consumer (PVPC) published daily by Red El√©ctrica at 8:15 p.m. This includes the hourly electricity tariffs that will apply the following day for consumers billed under the 2.0 TD tariff.\n\n## üß© Components\n\n### Tools\n\n- `fetch_prices`: Fetches the Voluntary Price for the Small Consumer (PVPC) prices for a given date range and geographical area.\n  - Inputs:\n    - `locale`: Get translations for sources. Accepted values: `es`, `en`. Defaults to `es`.\n    - `startDate`: Beginning of the date range to filter indicator values in iso8601 format. E.g. 2025-06-29T00:00:00.000+02:00. Defaults to the start of today.\n    - `endDate`: End of the date range to filter indicator values in iso8601 format. E.g. 2025-06-29T23:59:59.999+02:00. Defaults to the end of today.\n    - `timeAggregation`: How to aggregate indicator values when grouping them by time. Accepted values: `sum`, `average`. Defaults to `sum`.\n    - `timeTruncation`: Tells how to truncate data time series. Accepted values: `hour`, `day`, `month`, `year`. Optional parameter.\n    - `geographicalAggregation`: How to aggregate indicator values when grouping them by geographical ID. Accepted values: `sum`, `average`. Defaults to `sum`.\n    - `geographicalIds`: Tells the geographical IDs to filter indicator values. Accepted values: `3` (Espa√±a), `8741` (Pen√≠nsula), `8742` (Canarias), `8743` (Baleares), `8744` (Ceuta), `8745` (Melilla). Defaults to `8741`, `8742`, `8743`, `8744`, `8745`.\n    - `geographicalTruncation`: Tells how to group data at geographical level when the geographical aggregation is informed. Accepted values: `country`, `electric_system`. Optional parameter.\n  - Returns: Text content with the PVPC prices in JSON format.",
        "start_pos": 0,
        "end_pos": 2042,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "385483b940041c8a"
      },
      {
        "chunk_id": 1,
        "text": "a at geographical level when the geographical aggregation is informed. Accepted values: `country`, `electric_system`. Optional parameter.\n  - Returns: Text content with the PVPC prices in JSON format.\n\n## üîß Configuration\n\n### Requirements\n\nYou need to register an API key from [Esios Red El√©ctrica de Espa√±a](https://www.esios.ree.es/es/pagina/api) to access the Esios Red El√©ctrica de Espa√±a API.\n\nYou will find the API documentation at [API e¬∑sios Documentation](https://api.esios.ree.es/).\n\n### Claude Desktop\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to `Settings > Connectors > Add Custom Connector`. Enter the name as `PVPC` and the remote MCP server URL like `https://mcp.example.com/mcp`.\n\n#### Local Server Connection\n\nAdd this to your Claude Desktop `claude_desktop_config.json` file. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"pvpc\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@rfdez/pvpc-mcp-server@latest\", \"--api-key\", \"your_esios_api_key\"]\n    }\n  }\n}\n```\n\n## üíª Development\n\nClone this repository and install the dependencies:\n\n```bash\nnpm install\n```\n\nBuild the project:\n\n```bash\nnpm run build\n```\n\nRun the server:\n\n```bash\nnode dist/index.js\n```\n\n### CLI Arguments\n\n`pvpc-mcp-server` accepts the following CLI flags:\n\n- `--transport <stdio|http>`: Transport to use (`stdio` by default).\n- `--port <number>`: Port to listen on when using `http` transport (default `8080`).\n- `--api-key <key>`: Your e¬∑sios API key for authentication.",
        "start_pos": 1842,
        "end_pos": 3404,
        "token_count_estimate": 390,
        "source_type": "readme",
        "agent_id": "385483b940041c8a"
      },
      {
        "chunk_id": 2,
        "text": "c\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"tsx\", \"/path/to/folder/pvpc-mcp-server/src/index.ts\", \"--api-key\", \"YOUR_ESIOS_API_KEY\"]\n    }\n  }\n}\n```\n\n### Testing with MCP Inspector\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx -y @rfdez/pvpc-mcp-server\n```\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
        "start_pos": 3690,
        "end_pos": 4068,
        "token_count_estimate": 94,
        "source_type": "readme",
        "agent_id": "385483b940041c8a"
      }
    ]
  },
  {
    "agent_id": "660ecace243a4442",
    "name": "ai.smithery/sachicali-discordmcp-suite",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@sachicali/discordmcp-suite/mcp",
    "description": "Control your Discord community: send/read messages, manage channels and forums, and handle webhook‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T18:19:06.201517Z",
    "indexed_at": "2026-02-18T04:08:39.918231",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Send messages in Discord communities",
        "Read messages in Discord communities",
        "Manage channels in Discord communities",
        "Manage forums in Discord communities",
        "Handle webhooks in Discord communities"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's capabilities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "675965d731bdb666",
    "name": "ai.smithery/saidsef-mcp-github-pr-issue-analyser",
    "source": "mcp",
    "source_url": "https://github.com/saidsef/mcp-github-pr-issue-analyser",
    "description": "A Model Context Protocol (MCP) application for automated GitHub PR analysis and issue management.‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-05T14:58:08.898007Z",
    "indexed_at": "2026-02-18T04:08:42.212746",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# MCP for GitHub PR, Issues, Tags and Releases\n\nThe [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) (MCP) is an open standard that enables seamless integration between Large Language Models (LLMs) and external tools. Whilst it can be implemented in any AI system, including custom LLM setups, the degree of integration and optimisation varies based on the model's architecture and capabilities.\n\n<a href=\"https://glama.ai/mcp/servers/@saidsef/mcp-github-pr-issue-analyser\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@saidsef/mcp-github-pr-issue-analyser/badge\" alt=\"GitHub PR Issue Analyser MCP server\" />\n</a>\n\nThis MCP application serves as a bridge between LLMs and GitHub's repository management features, offering automated analysis of pull requests and comprehensive issue management. It provides a robust set of tools to fetch PR details, create issues, and update issues directly from your desktop LLM. The application is designed with modularity in mind, supporting extensibility via the MCP tool interface and seamless integration with existing workflows.\n\nThe toolset enables automated PR analysis, issue tracking, tagging and release management through a standardised MCP interface, making it ideal for teams seeking to streamline their GitHub workflow automation.\n\n## Features\n\n| Function                                 | Description                                                                                       |\n|------------------------------------------|---------------------------------------------------------------------------------------------------|\n| Analyse GitHub Pull Requests and fetch diffs         | Retrieve the diff/patch for any PR in a repository.                                                |\n| Fetch content and metadata for specific PRs          | Get PR title, description, author, timestamps, and state.                                          |\n| Update PR title and description                      | Change the title and body of any PR.                                                               |\n| Add comments to PRs                                  | Post general comments to a PR thread.                                                              |\n| Add inline review comments to PRs                    | Comment on specific lines in PR files for code review.                                             |\n| Create and update GitHub Issues                      | Open new issues or update existing ones with title, body, labels, and state.                       |\n| Create tags and releases                             | Tag repository commits and publish releases with changelogs.                                       |\n| Retrieve IPv4 and IPv6 information                   | Get public IP address details for both IPv4 and IPv6.                                              |\n| List all open Issues or Pull Requests                | View all open PRs or issues for any user or organisation.                                          |\n\n## Requirements\n\n- Python 3.11+\n- GitHub Personal Access Token (with `repo` scope)\n\n## Architecture Diagram\n\n```ascii\n                                     +------------------------+\n                                     |                        |\n                                     |    MCP Client/User     |\n                                     |                        |\n                                     +------------------------+\n                                              |\n                                              | (stdio/http)\n                                              v\n+--------------------+              +------------------------+\n|                    |              |    PRIssueAnalyser     |\n|   IP Integration   | <------------|    (FastMCP Server)    |\n|   (ipinfo.io)      |              |                        |\n+--------------------+              +------------------------+\n                                              |\n                                              | (API calls)\n                                              v\n                                   +------------------------+\n                                   |   GitHub Integration   |\n                                   +------------------------+\n                                              |\n                                              | (REST API)\n                     +-------------------------+-------------------------+\n                     |                         |                         |\n              +-------------+           +--------------+        +-------------+\n              | GitHub PRs  |           |GitHub Issues |        |GitHub Tags/ |\n              | & Releases  |           |              |        | Releases    |\n              +-------------+           +--------------+        +-------------+\n```\n\n### Features:\n\n1. PR Management: Fetch, analyze, and update\n2. Issue Tracking: Create and update\n3. Release Management: Tags and releases\n4. Network Info: IPv4/IPv6 details\n\n### Main Flows:\n\n- PRIssueAnalyser: Main MCP server handling tool registration and requests\n- GitHub Integration: Manages all GitHub API interactions\n- IP Integration: Handles IPv4/IPv6 information retrieval\n- MCP Client: Interacts via stdio or streamable HTTP (http)\n\n## Local Installation\n\n1. **Clone the repository:**\n```sh\ngit clone https://github.com/saidsef/mcp-github-pr-issue-analyser.git\ncd mcp-github-pr-issue-analyser\n```\n\n2. **Install dependencies:**\n\nLaunch MCP in `stdio` mode.\n```sh\nexport GITHUB_TOKEN=\"<github-token>\"\nuvx ./\n```\n\nAlternatively, launch MCP in `http` mode.\n```sh\nexport GITHUB_TOKEN=\"<github-token>\"\nexport MCP_ENABLE_REMOTE=true\nuvx ./\n```\n> You can access it via `http` i.e. `http(s)://localhost:8080/mcp`\n\n## Local Integration with IDEs and LLMs\n\nTo add an MCP server to your IDE or LLM, you need to add this section to the configuration file. The basic structure involves defining a server name and providing the command and any necessary arguments to run the server.\n\n<details>\n<summary>Claude / Cursor / Windsurf</summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"github_prs_issues\": {\n      \"command\": \"uvx\",\n      \"env\": {\n        \"GITHUB_TOKEN\": \"<your-github-token>\"\n      },\n      \"args\": [\n        \"https://github.com/saidsef/mcp-github-pr-issue-analyser.git\",\n      ]\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary>VS Code</summary>\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github-token\",\n      \"description\": \"Enter your GitHub token\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"github-prs-issues\": {\n      \"type\": \"stdio\",\n      \"command\": \"uvx\",\n      \"args\": [\n        \"https://github.com/saidsef/mcp-github-pr-issue-analyser.git\",\n      ],\n      \"env\": {\n        \"GITHUB_TOKEN\": \"${input:github-token}\"\n      }\n    }\n  }\n}\n```\n</details>\n\n## Source\n\nOur latest and greatest source of *mcp-github-pr-issue-analyser* can be found on [GitHub]. [Fork us](https://github.com/saidsef/mcp-github-pr-issue-analyser/fork)!\n\n## Contributing\n\nWe would :heart: you to contribute by making a [pull request](https://github.com/saidsef/mcp-github-pr-issue-analyser/pulls).\n\nPlease read the official [Contribution Guide](./CONTRIBUTING.md) for more information on how you can contribute."
    },
    "llm_extracted": {
      "capabilities": [
        "Analyse GitHub pull requests and fetch diffs",
        "Fetch content and metadata for specific pull requests",
        "Update pull request titles and descriptions",
        "Add general comments to pull requests",
        "Add inline review comments to pull requests",
        "Create and update GitHub issues with title, body, labels, and state",
        "Create tags and publish releases with changelogs",
        "Retrieve public IPv4 and IPv6 address information",
        "List all open issues or pull requests for any user or organisation"
      ],
      "limitations": [],
      "requirements": [
        "Python 3.11 or higher",
        "GitHub Personal Access Token with 'repo' scope"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed feature descriptions, usage examples for integration with IDEs and LLMs, architecture overview, and explicit requirements, though it does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# MCP for GitHub PR, Issues, Tags and Releases\n\nThe [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) (MCP) is an open standard that enables seamless integration between Large Language Models (LLMs) and external tools. Whilst it can be implemented in any AI system, including custom LLM setups, the degree of integration and optimisation varies based on the model's architecture and capabilities.\n\n<a href=\"https://glama.ai/mcp/servers/@saidsef/mcp-github-pr-issue-analyser\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@saidsef/mcp-github-pr-issue-analyser/badge\" alt=\"GitHub PR Issue Analyser MCP server\" />\n</a>\n\nThis MCP application serves as a bridge between LLMs and GitHub's repository management features, offering automated analysis of pull requests and comprehensive issue management. It provides a robust set of tools to fetch PR details, create issues, and update issues directly from your desktop LLM. The application is designed with modularity in mind, supporting extensibility via the MCP tool interface and seamless integration with existing workflows.\n\nThe toolset enables automated PR analysis, issue tracking, tagging and release management through a standardised MCP interface, making it ideal for teams seeking to streamline their GitHub workflow automation.\n\n## Features\n\n| Function                                 | Description                                                                                       |\n|------------------------------------------|---------------------------------------------------------------------------------------------------|\n| Analyse GitHub Pull Requests and fetch diffs         | Retrieve the diff/patch for any PR in a repository.                                                |\n| Fetch content and metadata for specific PRs          | Get PR title, description, author, timestamps, and state.                                          |\n| Update PR title and description                      | Change the title and body of any PR.",
        "start_pos": 0,
        "end_pos": 2046,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "675965d731bdb666"
      },
      {
        "chunk_id": 1,
        "text": "| Get PR title, description, author, timestamps, and state.                                          |\n| Update PR title and description                      | Change the title and body of any PR.                                                               |\n| Add comments to PRs                                  | Post general comments to a PR thread.                                                              |\n| Add inline review comments to PRs                    | Comment on specific lines in PR files for code review.                                             |\n| Create and update GitHub Issues                      | Open new issues or update existing ones with title, body, labels, and state.                       |\n| Create tags and releases                             | Tag repository commits and publish releases with changelogs.                                       |\n| Retrieve IPv4 and IPv6 information                   | Get public IP address details for both IPv4 and IPv6.                                              |\n| List all open Issues or Pull Requests                | View all open PRs or issues for any user or organisation.",
        "start_pos": 1846,
        "end_pos": 3015,
        "token_count_estimate": 291,
        "source_type": "readme",
        "agent_id": "675965d731bdb666"
      },
      {
        "chunk_id": 2,
        "text": "----------------------+\n|                    |              |    PRIssueAnalyser     |\n|   IP Integration   | <------------|    (FastMCP Server)    |\n|   (ipinfo.io)      |              |                        |\n+--------------------+              +------------------------+\n                                              |\n                                              | (API calls)\n                                              v\n                                   +------------------------+\n                                   |   GitHub Integration   |\n                                   +------------------------+\n                                              |\n                                              | (REST API)\n                     +-------------------------+-------------------------+\n                     |                         |                         |\n              +-------------+           +--------------+        +-------------+\n              | GitHub PRs  |           |GitHub Issues |        |GitHub Tags/ |\n              | & Releases  |           |              |        | Releases    |\n              +-------------+           +--------------+        +-------------+\n```\n\n### Features:\n\n1. PR Management: Fetch, analyze, and update\n2. Issue Tracking: Create and update\n3. Release Management: Tags and releases\n4. Network Info: IPv4/IPv6 details\n\n### Main Flows:\n\n- PRIssueAnalyser: Main MCP server handling tool registration and requests\n- GitHub Integration: Manages all GitHub API interactions\n- IP Integration: Handles IPv4/IPv6 information retrieval\n- MCP Client: Interacts via stdio or streamable HTTP (http)\n\n## Local Installation\n\n1. **Clone the repository:**\n```sh\ngit clone https://github.com/saidsef/mcp-github-pr-issue-analyser.git\ncd mcp-github-pr-issue-analyser\n```\n\n2. **Install dependencies:**\n\nLaunch MCP in `stdio` mode.\n```sh\nexport GITHUB_TOKEN=\"<github-token>\"\nuvx ./\n```\n\nAlternatively, launch MCP in `http` mode.",
        "start_pos": 3694,
        "end_pos": 5656,
        "token_count_estimate": 490,
        "source_type": "readme",
        "agent_id": "675965d731bdb666"
      },
      {
        "chunk_id": 3,
        "text": "yser.git\ncd mcp-github-pr-issue-analyser\n```\n\n2. **Install dependencies:**\n\nLaunch MCP in `stdio` mode.\n```sh\nexport GITHUB_TOKEN=\"<github-token>\"\nuvx ./\n```\n\nAlternatively, launch MCP in `http` mode.\n```sh\nexport GITHUB_TOKEN=\"<github-token>\"\nexport MCP_ENABLE_REMOTE=true\nuvx ./\n```\n> You can access it via `http` i.e. `http(s)://localhost:8080/mcp`\n\n## Local Integration with IDEs and LLMs\n\nTo add an MCP server to your IDE or LLM, you need to add this section to the configuration file. The basic structure involves defining a server name and providing the command and any necessary arguments to run the server.\n\n<details>\n<summary>Claude / Cursor / Windsurf</summary>\n\n```json\n{\n  \"mcpServers\": {\n    \"github_prs_issues\": {\n      \"command\": \"uvx\",\n      \"env\": {\n        \"GITHUB_TOKEN\": \"<your-github-token>\"\n      },\n      \"args\": [\n        \"https://github.com/saidsef/mcp-github-pr-issue-analyser.git\",\n      ]\n    }\n  }\n}\n```\n</details>\n\n<details>\n<summary>VS Code</summary>\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"github-token\",\n      \"description\": \"Enter your GitHub token\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"github-prs-issues\": {\n      \"type\": \"stdio\",\n      \"command\": \"uvx\",\n      \"args\": [\n        \"https://github.com/saidsef/mcp-github-pr-issue-analyser.git\",\n      ],\n      \"env\": {\n        \"GITHUB_TOKEN\": \"${input:github-token}\"\n      }\n    }\n  }\n}\n```\n</details>\n\n## Source\n\nOur latest and greatest source of *mcp-github-pr-issue-analyser* can be found on [GitHub]. [Fork us](https://github.com/saidsef/mcp-github-pr-issue-analyser/fork)!\n\n## Contributing\n\nWe would :heart: you to contribute by making a [pull request](https://github.com/saidsef/mcp-github-pr-issue-analyser/pulls).\n\nPlease read the official [Contribution Guide](./CONTRIBUTING.md) for more information on how you can contribute.",
        "start_pos": 5456,
        "end_pos": 7326,
        "token_count_estimate": 467,
        "source_type": "readme",
        "agent_id": "675965d731bdb666"
      },
      {
        "chunk_id": 4,
        "text": "aking a [pull request](https://github.com/saidsef/mcp-github-pr-issue-analyser/pulls).\n\nPlease read the official [Contribution Guide](./CONTRIBUTING.md) for more information on how you can contribute.",
        "start_pos": 7126,
        "end_pos": 7326,
        "token_count_estimate": 50,
        "source_type": "readme",
        "agent_id": "675965d731bdb666"
      }
    ]
  },
  {
    "agent_id": "bd62fe7150e2804b",
    "name": "ai.smithery/samihalawa-whatsapp-go-mcp",
    "source": "mcp",
    "source_url": "https://github.com/samihalawa/whatsapp-go-mcp",
    "description": "Scan QR codes and go! No more troublesome autos or APIs! Send text messages, images, links, locati‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T00:18:16.874195Z",
    "indexed_at": "2026-02-18T04:08:43.412241",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "<!--\nmarkdownlint-disable MD041\n-->\n\\n\n<!--\nmarkdownlint-disable-next-line MD033\n-->\n\\n<\ndiv\nalign\n=\n\\\"\ncenter\n\\\"\n>\\n\n<!--\nmarkdownlint-disable-next-line MD033\n-->\n\\n  <\nimg\nsrc\n=\n\\\"\nsrc/views/assets/gowa.svg\n\\\"\nalt\n=\n\\\"\nGoWA Logo\n\\\"\nwidth\n=\n\\\"\n200\n\\\"\nheight\n=\n\\\"\n200\n\\\"\n>\\n\\n\n##\nGolang WhatsApp - Built with Go for efficient memory use\n\\n\\n</\ndiv\n>\\n\\n\n[\n![\nsmithery badge\n]\n(\nhttps://smithery.ai/badge/@samihalawa/whatsapp-go-mcp\n)]\n(\nhttps://smithery.ai/server/@samihalawa/whatsapp-go-mcp\n)\n\\n\n[\n![\nPatreon\n]\n(\nhttps://img.shields.io/badge/Support%20on-Patreon-orange.svg\n)]\n(\nhttps://www.patreon.com/c/aldinokemal\n)\n\\n\n**\nIf you're using this tools to generate income, consider supporting its development by becoming a Patreon member!\n**\n\\nYour support helps ensure the library stays maintained and receives regular updates!\\n\n___\n\\n\\n\n![\nrelease version\n]\n(\nhttps://img.shields.io/github/v/release/aldinokemal/go-whatsapp-web-multidevice\n)\n\\n\n![\nBuild Image\n]\n(\nhttps://github.com/aldinokemal/go-whatsapp-web-multidevice/actions/workflows/build-docker-image.yaml/badge.svg\n)\n\\n\n![\nBinary Release\n]\n(\nhttps://github.com/aldinokemal/go-whatsapp-web-multidevice/actions/workflows/release.yml/badge.svg\n)\n\\n\\n\n##\nSupport for\n`\nARM\n`\n&\n`\nAMD\n`\nArchitecture along with\n`\nMCP\n`\nSupport\n\\n\\nDownload:\\n\\n\n-\n[\nRelease\n]\n(\nhttps://github.com/aldinokemal/go-whatsapp-web-multidevice/releases/latest\n)\n\\n\n-\n[\nDocker Hub\n]\n(\nhttps://hub.docker.com/r/aldinokemal2104/go-whatsapp-web-multidevice/tags\n)\n\\n\n-\n[\nGitHub Container Registry\n]\n(\nhttps://github.com/aldinokemal/go-whatsapp-web-multidevice/pkgs/container/go-whatsapp-web-multidevice\n)\n\\n\\n\n##\nSupport n8n package (n8n.io)\n\\n\\n\n-\n[\nn8n package\n]\n(\nhttps://www.npmjs.com/package/@aldinokemal2104/n8n-nodes-gowa\n)\n\\n\n-\nGo to Settings -> Community Nodes -> Input\n`\n@aldinokemal2104/n8n-nodes-gowa\n`\n-> Install\\n\\n\n##\nBreaking Changes\n\\n\\n\n-\n`\nv6\n`\n\\n\n-\nFor REST mode, you need to run\n`\n<binary> rest\n`\ninstead of\n`\n<binary>\n`\n\\n\n-\nfor example:\n`\n./whatsapp rest\n`\ninstead of\n~~\n./whatsapp\n~~\n\\n\n-\nFor MCP mode, you need to run\n`\n<binary> mcp\n`\n\\n\n-\nfor example:\n`\n./whatsapp mcp\n`\n\\n\n-\n`\nv7\n`\n\\n\n-\nStarting version 7.x we are using goreleaser to build the binary, so you can download the binary\\n      from\n[\nrelease\n]\n(\nhttps://github.com/aldinokemal/go-whatsapp-web-multidevice/releases/latest\n)\n\\n\\n\n##\nFeature\n\\n\\n\n-\nSend WhatsApp message via http API,\n[\ndocs/openapi.yml\n]\n(\n./docs/openapi.yaml\n)\nfor more details\\n\n-\n**\nMCP (Model Context Protocol) Server Support\n**\n- Integrate with AI agents and tools using standardized protocol\\n\n-\nMention someone\\n\n-\n`\n@phoneNumber\n`\n\\n\n-\nexample:\n`\nHello @628974812XXXX, @628974812XXXX\n`\n\\n\n-\nPost Whatsapp Status\\n\n-\nCompress image before send\\n\n-\nCompress video before send\\n\n-\nChange OS name become your app (it's the device name when connect via mobile)\\n\n-\n`\n--os=Chrome\n`\nor\n`\n--os=MyApplication\n`\n\\n\n-\nBasic Auth (able to add multi credentials)\\n\n-\n`\n--basic-auth=kemal:secret,toni:password,userName:secretPassword\n`\n, or you can simplify\\n\n-\n`\n-b=kemal:secret,toni:password,userName:secretPassword\n`\n\\n\n-\nSubpath deployment support\\n\n-\n`\n--base-path=\\\"/gowa\\\"\n`\n(allows deployment under a specific path like\n`\n/gowa/sub/path\n`\n)\\n\n-\nCustomizable port and debug mode\\n\n-\n`\n--port 8000\n`\n\\n\n-\n`\n--debug true\n`\n\\n\n-\nAuto reply message\\n\n-\n`\n--autoreply=\\\"Don't reply this message\\\"\n`\n\\n\n-\nAuto mark read incoming messages\\n\n-\n`\n--auto-mark-read=true\n`\n(automatically marks incoming messages as read)\\n\n-\nWebhook for received message\\n\n-\n`\n--webhook=\\\"http://yourwebhook.site/handler\\\"\n`\n, or you can simplify\\n\n-\n`\n-w=\\\"http://yourwebhook.site/handler\\\"\n`\n\\n\n-\nfor more detail, see\n[\nWebhook Payload Documentation\n]\n(\n./docs/webhook-payload.md\n)\n\\n\n-\nWebhook Secret\\n  Our webhook will be sent to you with an HMAC header and a sha256 default key\n`\nsecret\n`\n.\\n\\n  You may modify this by using the option below:\\n\n-\n`\n--webhook-secret=\\\"secret\\\"\n`\n\\n\n-\n**\nWebhook Payload Documentation\n**\n\\n  For detailed webhook payload schemas, security implementation, and integration examples,\\n  see\n[\nWebhook Payload Documentation\n]\n(\n./docs/webhook-payload.md\n)\n\\n\\n\n##\nConfiguration\n\\n\\nYou can configure the application using either command-line flags (shown above) or environment variables. Configuration\\ncan be set in three ways (in order of priority):\\n\\n\n1\n.\nCommand-line flags (highest priority)\\n\n2\n.\nEnvironment variables\\n\n3\n.\n`\n.env\n`\nfile (lowest priority)\\n\\n\n###\nEnvironment Variables\n\\n\\nYou can configure the application using environment variables. Configuration can be set in three ways (in order of\\npriority):\\n\\n\n1\n.\nCommand-line flags (highest priority)\\n\n2\n.\nEnvironment variables\\n\n3\n.\n`\n.env\n`\nfile (lowest priority)\\n\\nTo use environment variables:\\n\\n\n1\n.\nCopy\n`\n.env.example\n`\nto\n`\n.env\n`\nin your project root (\n`\ncp src/.env.example src/.env\n`\n)\\n\n2\n.\nModify the values in\n`\n.env\n`\naccording to your needs\\n\n3\n.\nOr set the same variables as system environment variables\\n\\n\n####\nAvailable Environment Variables\n\\n\\n\n|\nVariable\n|\nDescription\n|\nDefault\n|\nExample\n|\n\\n\n|\n-------------------------------\n|\n---------------------------------------------\n|\n----------------------------------------------\n|\n---------------------------------------------\n|\n\\n\n|\n`\nAPP_PORT\n`\n|\nApplication port\n|\n`\n3000\n`\n|\n`\nAPP_PORT=8080\n`\n|\n\\n\n|\n`\nAPP_DEBUG\n`\n|\nEnable debug logging\n|\n`\nfalse\n`\n|\n`\nAPP_DEBUG=true\n`\n|\n\\n\n|\n`\nAPP_OS\n`\n|\nOS name (device name in WhatsApp)\n|\n`\nChrome\n`\n|\n`\nAPP_OS=MyApp\n`\n|\n\\n\n|\n`\nAPP_BASIC_AUTH\n`\n|\nBasic authentication credentials\n|\n-\n|\n`\nAPP_BASIC_AUTH=user1:pass1,user2:pass2\n`\n|\n\\n\n|\n`\nAPP_BASE_PATH\n`\n|\nBase path for subpath deployment\n|\n-\n|\n`\nAPP_BASE_PATH=/gowa\n`\n|\n\\n\n|\n`\nDB_URI\n`\n|\nDatabase connection URI\n|\n`\nfile:storages/whatsapp.db?_foreign_keys=on\n`\n|\n`\nDB_URI=postgres://user:pass@host/db\n`\n|\n\\n\n|\n`\nWHATSAPP_AUTO_REPLY\n`\n|\nAuto-reply message\n|\n-\n|\n`\nWHATSAPP_AUTO_REPLY=\\\"Auto reply message\\\"\n`\n|\n\\n\n|\n`\nWHATSAPP_AUTO_MARK_READ\n`\n|\nAuto-mark incoming messages as read\n|\n`\nfalse\n`\n|\n`\nWHATSAPP_AUTO_MARK_READ=true\n`\n|\n\\n\n|\n`\nWHATSAPP_WEBHOOK\n`\n|\nWebhook URL(s) for events (comma-separated)\n|\n-\n|\n`\nWHATSAPP_WEBHOOK=https://webhook.site/xxx\n`\n|\n\\n\n|\n`\nWHATSAPP_WEBHOOK_SECRET\n`\n|\nWebhook secret for validation\n|\n`\nsecret\n`\n|\n`\nWHATSAPP_WEBHOOK_SECRET=super-secret-key\n`\n|\n\\n\n|\n`\nWHATSAPP_ACCOUNT_VALIDATION\n`\n|\nEnable account validation\n|\n`\ntrue\n`\n|\n`\nWHATSAPP_ACCOUNT_VALIDATION=false\n`\n|\n\\n\n|\n`\nWHATSAPP_CHAT_STORAGE\n`\n|\nEnable chat storage\n|\n`\ntrue\n`\n|\n`\nWHATSAPP_CHAT_STORAGE=false\n`\n|\n\\n\\nNote: Command-line flags will override any values set in environment variables or\n`\n.env\n`\nfile.\\n\\n\n-\nFor more command\n`\n./whatsapp --help\n`\n\\n\\n\n##\nRequirements\n\\n\\n\n###\nSystem Requirements\n\\n\\n\n-\n**\nGo 1.24.0 or higher\n**\n(for building from source)\\n\n-\n**\nFFmpeg\n**\n(for media processing)\\n\\n\n###\nPlatform Support\n\\n\\n\n-\nLinux (x86_64, ARM64)\\n\n-\nmacOS (Intel, Apple Silicon)\\n\n-\nWindows (x86_64) - WSL recommended\\n\\n\n###\nDependencies (without docker)\n\\n\\n\n-\nMac OS:\\n\n-\n`\nbrew install ffmpeg\n`\n\\n\n-\n`\nexport CGO_CFLAGS_ALLOW=\\\"-Xpreprocessor\\\"\n`\n\\n\n-\nLinux:\\n\n-\n`\nsudo apt update\n`\n\\n\n-\n`\nsudo apt install ffmpeg\n`\n\\n\n-\nWindows (not recomended, prefer using\n[\nWSL\n]\n(\nhttps://docs.microsoft.com/en-us/windows/wsl/install\n)\n):\\n\n-\ninstall ffmpeg,\n[\ndownload here\n]\n(\nhttps://www.ffmpeg.org/download.html#build-windows\n)\n\\n\n-\nadd to ffmpeg to\n[\nenvironment variable\n]\n(\nhttps://www.google.com/search?q=windows+add+to+environment+path\n)\n\\n\\n\n##\nHow to use\n\\n\\n\n###\nBasic\n\\n\\n\n1\n.\nClone this repo:\n`\ngit clone https://github.com/aldinokemal/go-whatsapp-web-multidevice\n`\n\\n\n2\n.\nOpen the folder that was cloned via cmd/terminal.\\n\n3\n.\nrun\n`\ncd src\n`\n\\n\n4\n.\nrun\n`\ngo run . rest\n`\n(for REST API mode)\\n\n5\n.\nOpen\n`\nhttp://localhost:3000\n`\n\\n\\n\n###\nDocker (you don't need to install in required)\n\\n\\n\n1\n.\nClone this repo:\n`\ngit clone https://github.com/aldinokemal/go-whatsapp-web-multidevice\n`\n\\n\n2\n.\nOpen the folder that was cloned via cmd/terminal.\\n\n3\n.\nrun\n`\ndocker-compose up -d --build\n`\n\\n\n4\n.\nopen\n`\nhttp://localhost:3000\n`\n\\n\\n\n###\nBuild your own binary\n\\n\\n\n1\n.\nClone this repo\n`\ngit clone https://github.com/aldinokemal/go-whatsapp-web-multidevice\n`\n\\n\n2\n.\nOpen the folder that was cloned via cmd/terminal.\\n\n3\n.\nrun\n`\ncd src\n`\n\\n\n4\n.\nrun\\n\n1\n.\nLinux & MacOS:\n`\ngo build -o whatsapp\n`\n\\n\n2\n.\nWindows (CMD / PowerShell):\n`\ngo build -o whatsapp.exe\n`\n\\n\n5\n.\nrun\\n\n1\n.\nLinux & MacOS:\n`\n./whatsapp rest\n`\n(for REST API mode)\\n\n1\n.\nrun\n`\n./whatsapp --help\n`\nfor more detail flags\\n\n2\n.\nWindows:\n`\n.\\\\whatsapp.exe rest\n`\n(for REST API mode)\\n\n1\n.\nrun\n`\n.\\\\whatsapp.exe --help\n`\nfor more detail flags\\n\n6\n.\nopen\n`\nhttp://localhost:3000\n`\nin browser\\n\\n\n###\nMCP Server (Model Context Protocol)\n\\n\\nThis application can also run as an MCP server, allowing AI agents and tools to interact with WhatsApp through a\\nstandardized protocol.\\n\\n\n1\n.\nClone this repo\n`\ngit clone https://github.com/aldinokemal/go-whatsapp-web-multidevice\n`\n\\n\n2\n.\nOpen the folder that was cloned via cmd/terminal.\\n\n3\n.\nrun\n`\ncd src\n`\n\\n\n4\n.\nrun\n`\ngo run . mcp\n`\nor build the binary and run\n`\n./whatsapp mcp\n`\n\\n\n5\n.\nThe MCP server will start on\n`\nhttp://localhost:8080\n`\nby default\\n\\n\n####\nMCP Server Options\n\\n\\n\n-\n`\n--host localhost\n`\n- Set the host for MCP server (default: localhost)\\n\n-\n`\n--port 8080\n`\n- Set the port for MCP server (default: 8080)\\n\\n\n####\nAvailable MCP Tools\n\\n\\n\n-\n`\nwhatsapp_send_text\n`\n- Send text messages\\n\n-\n`\nwhatsapp_send_contact\n`\n- Send contact cards\\n\n-\n`\nwhatsapp_send_link\n`\n- Send links with captions\\n\n-\n`\nwhatsapp_send_location\n`\n- Send location coordinates\\n\\n\n####\nMCP Endpoints\n\\n\\n\n-\nSSE endpoint:\n`\nhttp://localhost:8080/sse\n`\n\\n\n-\nMessage endpoint:\n`\nhttp://localhost:8080/message\n`\n\\n\\n\n###\nMCP Configuration\n\\n\\nMake sure you have the MCP server running:\n`\n./whatsapp mcp\n`\n\\n\\nFor AI tools that support MCP with SSE (like Cursor), add this configuration:\\n\\n\n```\njson\n\\n{\\n\n\\\"mcpServers\\\"\n: {\\n\n\\\"whatsapp\\\"\n: {\\n\n\\\"url\\\"\n:\n\\\"\nhttp://localhost:8080/sse\n\\\"\n\\n    }\\n  }\\n}\n\\n\nProduction Mode REST (docker)\n\\n\nUsing Docker Hub:\n\\n\ndocker run --detach --publish=3000:3000 --name=whatsapp --restart=always --volume=\n$(\ndocker volume create --name=whatsapp\n)\n:/app/storages aldinokemal2104/go-whatsapp-web-multidevice rest --autoreply=\n\\\"\nDont't reply this message please\n\\\"\n\\n\nUsing GitHub Container Registry:\n\\n\ndocker run --detach --publish=3000:3000 --name=whatsapp --restart=always --volume=\n$(\ndocker volume create --name=whatsapp\n)\n:/app/storages ghcr.io/aldinokemal/go-whatsapp-web-multidevice rest --autoreply=\n\\\"\nDont't reply this message please\n\\\"\n\\n\nProduction Mode REST (docker compose)\n\\n\ncreate\ndocker-compose.yml\nfile with the following configuration:\n\\n\nUsing Docker Hub:\n\\n\nservices\n:\\n\nwhatsapp\n:\\n\nimage\n:\naldinokemal2104/go-whatsapp-web-multidevice\n\\n\ncontainer_name\n:\nwhatsapp\n\\n\nrestart\n:\nalways\n\\n\nports\n:\\n      -\n\\\"\n3000:3000\n\\\"\n\\n\nvolumes\n:\\n      -\nwhatsapp:/app/storages\n\\n\ncommand\n:\\n      -\nrest\n\\n      -\n--basic-auth=admin:admin\n\\n      -\n--port=3000\n\\n      -\n--debug=true\n\\n      -\n--os=Chrome\n\\n      -\n--account-validation=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nUsing GitHub Container Registry:\n\\n\nservices\n:\\n\nwhatsapp\n:\\n\nimage\n:\nghcr.io/aldinokemal/go-whatsapp-web-multidevice\n\\n\ncontainer_name\n:\nwhatsapp\n\\n\nrestart\n:\nalways\n\\n\nports\n:\\n      -\n\\\"\n3000:3000\n\\\"\n\\n\nvolumes\n:\\n      -\nwhatsapp:/app/storages\n\\n\ncommand\n:\\n      -\nrest\n\\n      -\n--basic-auth=admin:admin\n\\n      -\n--port=3000\n\\n      -\n--debug=true\n\\n      -\n--os=Chrome\n\\n      -\n--account-validation=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nor with env file (Docker Hub):\n\\n\nservices\n:\\n\nwhatsapp\n:\\n\nimage\n:\naldinokemal2104/go-whatsapp-web-multidevice\n\\n\ncontainer_name\n:\nwhatsapp\n\\n\nrestart\n:\nalways\n\\n\nports\n:\\n      -\n\\\"\n3000:3000\n\\\"\n\\n\nvolumes\n:\\n      -\nwhatsapp:/app/storages\n\\n\nenvironment\n:\\n      -\nAPP_BASIC_AUTH=admin:admin\n\\n      -\nAPP_PORT=3000\n\\n      -\nAPP_DEBUG=true\n\\n      -\nAPP_OS=Chrome\n\\n      -\nAPP_ACCOUNT_VALIDATION=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nor with env file (GitHub Container Registry):\n\\n\nservices\n:\\n\nwhatsapp\n:\\n\nimage\n:\nghcr.io/aldinokemal/go-whatsapp-web-multidevice\n\\n\ncontainer_name\n:\nwhatsapp\n\\n\nrestart\n:\nalways\n\\n\nports\n:\\n      -\n\\\"\n3000:3000\n\\\"\n\\n\nvolumes\n:\\n      -\nwhatsapp:/app/storages\n\\n\nenvironment\n:\\n      -\nAPP_BASIC_AUTH=admin:admin\n\\n      -\nAPP_PORT=3000\n\\n      -\nAPP_DEBUG=true\n\\n      -\nAPP_OS=Chrome\n\\n      -\nAPP_ACCOUNT_VALIDATION=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nProduction Mode (binary)\n\\n\n\\n\ndownload binary from\nrelease\n\\n\n\\n\nYou can fork or edit this source code !\n\\n\nCurrent API\n\\n\nMCP (Model Context Protocol) API\n\\n\n\\n\nMCP server provides standardized tools for AI agents to interact with WhatsApp\n\\n\nSupports Server-Sent Events (SSE) transport\n\\n\nAvailable tools:\nwhatsapp_send_text\n,\nwhatsapp_send_contact\n,\nwhatsapp_send_link\n,\nwhatsapp_send_location\n\\n\nCompatible with MCP-enabled AI tools and agents\n\\n\n\\n\nHTTP REST API\n\\n\n\\n\nAPI Specification Document\n.\n\\n\nCheck\ndocs/openapi.yml\nfor detailed API specifications.\n\\n\nUse\nSwaggerEditor\nto visualize the API.\n\\n\nGenerate HTTP clients using\nopenapi-generator\n.\n\\n\n\\n\n\\n\n\\n\n\\n\nFeature\n\\n\nMenu\n\\n\nMethod\n\\n\nURL\n\\n\n\\n\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nLogin with Scan QR\n\\n\nGET\n\\n\n/app/login\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nLogin With Pair Code\n\\n\nGET\n\\n\n/app/login-with-code\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nLogout\n\\n\nGET\n\\n\n/app/logout\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nReconnect\n\\n\nGET\n\\n\n/app/reconnect\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nDevices\n\\n\nGET\n\\n\n/app/devices\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser Info\n\\n\nGET\n\\n\n/user/info\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser Avatar\n\\n\nGET\n\\n\n/user/avatar\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser Change Avatar\n\\n\nPOST\n\\n\n/user/avatar\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser Change PushName\n\\n\nPOST\n\\n\n/user/pushname\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser My Groups\n\\n\nGET\n\\n\n/user/my/groups\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser My Newsletter\n\\n\nGET\n\\n\n/user/my/newsletters\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser My Privacy Setting\n\\n\nGET\n\\n\n/user/my/privacy\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser My Contacts\n\\n\nGET\n\\n\n/user/my/contacts\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser Check\n\\n\nGET\n\\n\n/user/check\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUser Business Profile\n\\n\nGET\n\\n\n/user/business-profile\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Message\n\\n\nPOST\n\\n\n/send/message\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Image\n\\n\nPOST\n\\n\n/send/image\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Audio\n\\n\nPOST\n\\n\n/send/audio\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend File\n\\n\nPOST\n\\n\n/send/file\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Video\n\\n\nPOST\n\\n\n/send/video\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Contact\n\\n\nPOST\n\\n\n/send/contact\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Link\n\\n\nPOST\n\\n\n/send/link\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Location\n\\n\nPOST\n\\n\n/send/location\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Poll / Vote\n\\n\nPOST\n\\n\n/send/poll\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Presence\n\\n\nPOST\n\\n\n/send/presence\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSend Chat Presence (Typing Indicator)\n\\n\nPOST\n\\n\n/send/chat-presence\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nRevoke Message\n\\n\nPOST\n\\n\n/message/:message_id/revoke\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nReact Message\n\\n\nPOST\n\\n\n/message/:message_id/reaction\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nDelete Message\n\\n\nPOST\n\\n\n/message/:message_id/delete\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nEdit Message\n\\n\nPOST\n\\n\n/message/:message_id/update\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nRead Message (DM)\n\\n\nPOST\n\\n\n/message/:message_id/read\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nStar Message\n\\n\nPOST\n\\n\n/message/:message_id/star\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUnstar Message\n\\n\nPOST\n\\n\n/message/:message_id/unstar\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nJoin Group With Link\n\\n\nPOST\n\\n\n/group/join-with-link\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGroup Info From Link\n\\n\nGET\n\\n\n/group/info-from-link\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGroup Info\n\\n\nGET\n\\n\n/group/info\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nLeave Group\n\\n\nPOST\n\\n\n/group/leave\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nCreate Group\n\\n\nPOST\n\\n\n/group\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nAdd Participants in Group\n\\n\nPOST\n\\n\n/group/participants\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nRemove Participant in Group\n\\n\nPOST\n\\n\n/group/participants/remove\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nPromote Participant in Group\n\\n\nPOST\n\\n\n/group/participants/promote\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nDemote Participant in Group\n\\n\nPOST\n\\n\n/group/participants/demote\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nList Requested Participants in Group\n\\n\nGET\n\\n\n/group/participant-requests\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nApprove Requested Participant in Group\n\\n\nPOST\n\\n\n/group/participant-requests/approve\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nReject Requested Participant in Group\n\\n\nPOST\n\\n\n/group/participant-requests/reject\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Photo\n\\n\nPOST\n\\n\n/group/photo\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Name\n\\n\nPOST\n\\n\n/group/name\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Locked\n\\n\nPOST\n\\n\n/group/locked\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Announce\n\\n\nPOST\n\\n\n/group/announce\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Topic\n\\n\nPOST\n\\n\n/group/topic\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGet Group Invite Link\n\\n\nGET\n\\n\n/group/invite-link\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUnfollow Newsletter\n\\n\nPOST\n\\n\n/newsletter/unfollow\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGet Chat List\n\\n\nGET\n\\n\n/chats\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGet Chat Messages\n\\n\nGET\n\\n\n/chat/:chat_jid/messages\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nLabel Chat\n\\n\nPOST\n\\n\n/chat/:chat_jid/label\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nPin Chat\n\\n\nPOST\n\\n\n/chat/:chat_jid/pin\n\\n\n\\n\n\\n\n\\n\n‚úÖ = Available\\n‚ùå = Not Available Yet\n\\n\nUser Interface\n\\n\nMCP UI\n\\n\n\\n\nSetup MCP (tested in cursor)\\n\n\\n\nTest MCP\\n\n\\n\nSuccessfully setup MCP\\n\n\\n\n\\n\nHTTP REST API UI\n\\n\n\\n\n\\n\n\\n\nDescription\n\\n\nImage\n\\n\n\\n\n\\n\n\\n\n\\n\nHomepage\n\\n\n\\n\n\\n\n\\n\nLogin\n\\n\n\\n\n\\n\n\\n\nLogin With Code\n\\n\n\\n\n\\n\n\\n\nSend Message\n\\n\n\\n\n\\n\n\\n\nSend Image\n\\n\n\\n\n\\n\n\\n\nSend File\n\\n\n\\n\n\\n\n\\n\nSend Video\n\\n\n\\n\n\\n\n\\n\nSend Contact\n\\n\n\\n\n\\n\n\\n\nSend Location\n\\n\n\\n\n\\n\n\\n\nSend Audio\n\\n\n\\n\n\\n\n\\n\nSend Poll\n\\n\n\\n\n\\n\n\\n\nSend Presence\n\\n\n\\n\n\\n\n\\n\nSend Link\n\\n\n\\n\n\\n\n\\n\nMy Group\n\\n\n\\n\n\\n\n\\n\nGroup Info From Link\n\\n\n\\n\n\\n\n\\n\nCreate Group\n\\n\n\\n\n\\n\n\\n\nJoin Group with Link\n\\n\n\\n\n\\n\n\\n\nManage Participant\n\\n\n\\n\n\\n\n\\n\nMy Newsletter\n\\n\n\\n\n\\n\n\\n\nMy Contacts\n\\n\n\\n\n\\n\n\\n\nBusiness Profile\n\\n\n\\n\n\\n\n\\n\n\\n\nMac OS NOTE\n\\n\n\\n\nPlease do this if you have an error (invalid flag in pkg-config --cflags: -Xpreprocessor)\\n\nexport CGO_CFLAGS_ALLOW=\\\"-Xpreprocessor\\\"\n\\n\n\\n\nImportant\n\\n\n\\n\nThis project is unofficial and not affiliated with WhatsApp.\n\\n\nPlease use official WhatsApp API to avoid any issues.\n\\n\nWe only able to run MCP or REST API, this is limitation from whatsmeow library. independent MCP will be available in\\nthe future.\n\\n\n\\n\n\\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Send WhatsApp messages via HTTP API",
        "Support MCP (Model Context Protocol) server for AI agent integration",
        "Mention users in messages using phone numbers",
        "Post WhatsApp status updates",
        "Compress images and videos before sending",
        "Customize device OS name for WhatsApp connection",
        "Support Basic Authentication with multiple credentials",
        "Deploy under custom subpaths",
        "Auto-reply to incoming messages",
        "Auto mark incoming messages as read",
        "Send webhooks on received messages with HMAC SHA256 validation",
        "Send various message types via MCP tools including text, contact cards, links, and locations"
      ],
      "limitations": [],
      "requirements": [
        "Go 1.24.0 or higher for building from source",
        "FFmpeg installed for media processing",
        "Supported platforms: Linux (x86_64, ARM64), macOS (Intel, Apple Silicon), Windows (x86_64) with WSL recommended",
        "Basic Auth credentials if authentication is enabled",
        "Optional webhook URL and secret for event handling"
      ]
    },
    "documentation_quality": 0.55,
    "quality_rationale": "The documentation provides structured feature lists, configuration options, environment variables, and usage instructions but lacks detailed usage examples and explicit limitation statements.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<!--\nmarkdownlint-disable MD041\n-->\n\\n\n<!--\nmarkdownlint-disable-next-line MD033\n-->\n\\n<\ndiv\nalign\n=\n\\\"\ncenter\n\\\"\n>\\n\n<!--\nmarkdownlint-disable-next-line MD033\n-->\n\\n  <\nimg\nsrc\n=\n\\\"\nsrc/views/assets/gowa.svg\n\\\"\nalt\n=\n\\\"\nGoWA Logo\n\\\"\nwidth\n=\n\\\"\n200\n\\\"\nheight\n=\n\\\"\n200\n\\\"\n>\\n\\n\n##\nGolang WhatsApp - Built with Go for efficient memory use\n\\n\\n</\ndiv\n>\\n\\n\n[\n![\nsmithery badge\n]\n(\nhttps://smithery.ai/badge/@samihalawa/whatsapp-go-mcp\n)]\n(\nhttps://smithery.ai/server/@samihalawa/whatsapp-go-mcp\n)\n\\n\n[\n![\nPatreon\n]\n(\nhttps://img.shields.io/badge/Support%20on-Patreon-orange.svg\n)]\n(\nhttps://www.patreon.com/c/aldinokemal\n)\n\\n\n**\nIf you're using this tools to generate income, consider supporting its development by becoming a Patreon member!",
        "start_pos": 0,
        "end_pos": 738,
        "token_count_estimate": 184,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 1,
        "text": "nstall\\n\\n\n##\nBreaking Changes\n\\n\\n\n-\n`\nv6\n`\n\\n\n-\nFor REST mode, you need to run\n`\n<binary> rest\n`\ninstead of\n`\n<binary>\n`\n\\n\n-\nfor example:\n`\n./whatsapp rest\n`\ninstead of\n~~\n./whatsapp\n~~\n\\n\n-\nFor MCP mode, you need to run\n`\n<binary> mcp\n`\n\\n\n-\nfor example:\n`\n./whatsapp mcp\n`\n\\n\n-\n`\nv7\n`\n\\n\n-\nStarting version 7.x we are using goreleaser to build the binary, so you can download the binary\\n      from\n[\nrelease\n]\n(\nhttps://github.com/aldinokemal/go-whatsapp-web-multidevice/releases/latest\n)\n\\n\\n\n##\nFeature\n\\n\\n\n-\nSend WhatsApp message via http API,\n[\ndocs/openapi.yml\n]\n(\n./docs/openapi.yaml\n)\nfor more details\\n\n-\n**\nMCP (Model Context Protocol) Server Support\n**\n- Integrate with AI agents and tools using standardized protocol\\n\n-\nMention someone\\n\n-\n`\n@phoneNumber\n`\n\\n\n-\nexample:\n`\nHello @628974812XXXX, @628974812XXXX\n`\n\\n\n-\nPost Whatsapp Status\\n\n-\nCompress image before send\\n\n-\nCompress video before send\\n\n-\nChange OS name become your app (it's the device name when connect via mobile)\\n\n-\n`\n--os=Chrome\n`\nor\n`\n--os=MyApplication\n`\n\\n\n-\nBasic Auth (able to add multi credentials)\\n\n-\n`\n--basic-auth=kemal:secret,toni:password,userName:secretPassword\n`\n, or you can simplify\\n\n-\n`\n-b=kemal:secret,toni:password,userName:secretPassword\n`\n\\n\n-\nSubpath deployment support\\n\n-\n`\n--base-path=\\\"/gowa\\\"\n`\n(allows deployment under a specific path like\n`\n/gowa/sub/path\n`\n)\\n\n-\nCustomizable port and debug mode\\n\n-\n`\n--port 8000\n`\n\\n\n-\n`\n--debug true\n`\n\\n\n-\nAuto reply message\\n\n-\n`\n--autoreply=\\\"Don't reply this message\\\"\n`\n\\n\n-\nAuto mark read incoming messages\\n\n-\n`\n--auto-mark-read=true\n`\n(automatically marks incoming messages as read)\\n\n-\nWebhook for received message\\n\n-\n`\n--webhook=\\\"http://yourwebhook.site/handler\\\"\n`\n, or you can simplify\\n\n-\n`\n-w=\\\"http://yourwebhook.site/handler\\\"\n`\n\\n\n-\nfor more detail, see\n[\nWebhook Payload Documentation\n]\n(\n./docs/webhook-payload.md\n)\n\\n\n-\nWebhook Secret\\n  Our webhook will be sent to you with an HMAC header and a sha256 default key\n`\nsecret\n`\n.\\n\\n  You may modify this by using the opt",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 2,
        "text": "Documentation\n]\n(\n./docs/webhook-payload.md\n)\n\\n\n-\nWebhook Secret\\n  Our webhook will be sent to you with an HMAC header and a sha256 default key\n`\nsecret\n`\n.\\n\\n  You may modify this by using the option below:\\n\n-\n`\n--webhook-secret=\\\"secret\\\"\n`\n\\n\n-\n**\nWebhook Payload Documentation\n**\n\\n  For detailed webhook payload schemas, security implementation, and integration examples,\\n  see\n[\nWebhook Payload Documentation\n]\n(\n./docs/webhook-payload.md\n)\n\\n\\n\n##\nConfiguration\n\\n\\nYou can configure the application using either command-line flags (shown above) or environment variables. Configuration\\ncan be set in three ways (in order of priority):\\n\\n\n1\n.\nCommand-line flags (highest priority)\\n\n2\n.\nEnvironment variables\\n\n3\n.\n`\n.env\n`\nfile (lowest priority)\\n\\n\n###\nEnvironment Variables\n\\n\\nYou can configure the application using environment variables. Configuration can be set in three ways (in order of\\npriority):\\n\\n\n1\n.\nCommand-line flags (highest priority)\\n\n2\n.\nEnvironment variables\\n\n3\n.\n`\n.env\n`\nfile (lowest priority)\\n\\nTo use environment variables:\\n\\n\n1\n.\nCopy\n`\n.env.example\n`\nto\n`\n.env\n`\nin your project root (\n`\ncp src/.env.example src/.env\n`\n)\\n\n2\n.\nModify the values in\n`\n.env\n`\naccording to your needs\\n\n3\n.",
        "start_pos": 3696,
        "end_pos": 4927,
        "token_count_estimate": 307,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 3,
        "text": "redentials\n|\n-\n|\n`\nAPP_BASIC_AUTH=user1:pass1,user2:pass2\n`\n|\n\\n\n|\n`\nAPP_BASE_PATH\n`\n|\nBase path for subpath deployment\n|\n-\n|\n`\nAPP_BASE_PATH=/gowa\n`\n|\n\\n\n|\n`\nDB_URI\n`\n|\nDatabase connection URI\n|\n`\nfile:storages/whatsapp.db?_foreign_keys=on\n`\n|\n`\nDB_URI=postgres://user:pass@host/db\n`\n|\n\\n\n|\n`\nWHATSAPP_AUTO_REPLY\n`\n|\nAuto-reply message\n|\n-\n|\n`\nWHATSAPP_AUTO_REPLY=\\\"Auto reply message\\\"\n`\n|\n\\n\n|\n`\nWHATSAPP_AUTO_MARK_READ\n`\n|\nAuto-mark incoming messages as read\n|\n`\nfalse\n`\n|\n`\nWHATSAPP_AUTO_MARK_READ=true\n`\n|\n\\n\n|\n`\nWHATSAPP_WEBHOOK\n`\n|\nWebhook URL(s) for events (comma-separated)\n|\n-\n|\n`\nWHATSAPP_WEBHOOK=https://webhook.site/xxx\n`\n|\n\\n\n|\n`\nWHATSAPP_WEBHOOK_SECRET\n`\n|\nWebhook secret for validation\n|\n`\nsecret\n`\n|\n`\nWHATSAPP_WEBHOOK_SECRET=super-secret-key\n`\n|\n\\n\n|\n`\nWHATSAPP_ACCOUNT_VALIDATION\n`\n|\nEnable account validation\n|\n`\ntrue\n`\n|\n`\nWHATSAPP_ACCOUNT_VALIDATION=false\n`\n|\n\\n\n|\n`\nWHATSAPP_CHAT_STORAGE\n`\n|\nEnable chat storage\n|\n`\ntrue\n`\n|\n`\nWHATSAPP_CHAT_STORAGE=false\n`\n|\n\\n\\nNote: Command-line flags will override any values set in environment variables or\n`\n.env\n`\nfile.\\n\\n\n-\nFor more command\n`\n./whatsapp --help\n`\n\\n\\n\n##\nRequirements\n\\n\\n\n###\nSystem Requirements\n\\n\\n\n-\n**\nGo 1.24.0 or higher\n**\n(for building from source)\\n\n-\n**\nFFmpeg\n**\n(for media processing)\\n\\n\n###\nPlatform Support\n\\n\\n\n-\nLinux (x86_64, ARM64)\\n\n-\nmacOS (Intel, Apple Silicon)\\n\n-\nWindows (x86_64) - WSL recommended\\n\\n\n###\nDependencies (without docker)\n\\n\\n\n-\nMac OS:\\n\n-\n`\nbrew install ffmpeg\n`\n\\n\n-\n`\nexport CGO_CFLAGS_ALLOW=\\\"-Xpreprocessor\\\"\n`\n\\n\n-\nLinux:\\n\n-\n`\nsudo apt update\n`\n\\n\n-\n`\nsudo apt install ffmpeg\n`\n\\n\n-\nWindows (not recomended, prefer using\n[\nWSL\n]\n(\nhttps://docs.microsoft.com/en-us/windows/wsl/install\n)\n):\\n\n-\ninstall ffmpeg,\n[\ndownload here\n]\n(\nhttps://www.ffmpeg.org/download.html#build-windows\n)\n\\n\n-\nadd to ffmpeg to\n[\nenvironment variable\n]\n(\nhttps://www.google.com/search?q=windows+add+to+environment+path\n)\n\\n\\n\n##\nHow to use\n\\n\\n\n###\nBasic\n\\n\\n\n1\n.",
        "start_pos": 5544,
        "end_pos": 7512,
        "token_count_estimate": 492,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 4,
        "text": "w.ffmpeg.org/download.html#build-windows\n)\n\\n\n-\nadd to ffmpeg to\n[\nenvironment variable\n]\n(\nhttps://www.google.com/search?q=windows+add+to+environment+path\n)\n\\n\\n\n##\nHow to use\n\\n\\n\n###\nBasic\n\\n\\n\n1\n.\nClone this repo:\n`\ngit clone https://github.com/aldinokemal/go-whatsapp-web-multidevice\n`\n\\n\n2\n.\nOpen the folder that was cloned via cmd/terminal.\\n\n3\n.\nrun\n`\ncd src\n`\n\\n\n4\n.\nrun\n`\ngo run . rest\n`\n(for REST API mode)\\n\n5\n.\nOpen\n`\nhttp://localhost:3000\n`\n\\n\\n\n###\nDocker (you don't need to install in required)\n\\n\\n\n1\n.\nClone this repo:\n`\ngit clone https://github.com/aldinokemal/go-whatsapp-web-multidevice\n`\n\\n\n2\n.\nOpen the folder that was cloned via cmd/terminal.\\n\n3\n.\nrun\n`\ndocker-compose up -d --build\n`\n\\n\n4\n.\nopen\n`\nhttp://localhost:3000\n`\n\\n\\n\n###\nBuild your own binary\n\\n\\n\n1\n.\nClone this repo\n`\ngit clone https://github.com/aldinokemal/go-whatsapp-web-multidevice\n`\n\\n\n2\n.\nOpen the folder that was cloned via cmd/terminal.\\n\n3\n.\nrun\n`\ncd src\n`\n\\n\n4\n.\nrun\\n\n1\n.\nLinux & MacOS:\n`\ngo build -o whatsapp\n`\n\\n\n2\n.\nWindows (CMD / PowerShell):\n`\ngo build -o whatsapp.exe\n`\n\\n\n5\n.\nrun\\n\n1\n.\nLinux & MacOS:\n`\n./whatsapp rest\n`\n(for REST API mode)\\n\n1\n.\nrun\n`\n./whatsapp --help\n`\nfor more detail flags\\n\n2\n.\nWindows:\n`\n.\\\\whatsapp.exe rest\n`\n(for REST API mode)\\n\n1\n.\nrun\n`\n.\\\\whatsapp.exe --help\n`\nfor more detail flags\\n\n6\n.\nopen\n`\nhttp://localhost:3000\n`\nin browser\\n\\n\n###\nMCP Server (Model Context Protocol)\n\\n\\nThis application can also run as an MCP server, allowing AI agents and tools to interact with WhatsApp through a\\nstandardized protocol.\\n\\n\n1\n.\nClone this repo\n`\ngit clone https://github.com/aldinokemal/go-whatsapp-web-multidevice\n`\n\\n\n2\n.\nOpen the folder that was cloned via cmd/terminal.\\n\n3\n.\nrun\n`\ncd src\n`\n\\n\n4\n.\nrun\n`\ngo run . mcp\n`\nor build the binary and run\n`\n./whatsapp mcp\n`\n\\n\n5\n.",
        "start_pos": 7312,
        "end_pos": 9122,
        "token_count_estimate": 452,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 5,
        "text": "/localhost:8080\n`\nby default\\n\\n\n####\nMCP Server Options\n\\n\\n\n-\n`\n--host localhost\n`\n- Set the host for MCP server (default: localhost)\\n\n-\n`\n--port 8080\n`\n- Set the port for MCP server (default: 8080)\\n\\n\n####\nAvailable MCP Tools\n\\n\\n\n-\n`\nwhatsapp_send_text\n`\n- Send text messages\\n\n-\n`\nwhatsapp_send_contact\n`\n- Send contact cards\\n\n-\n`\nwhatsapp_send_link\n`\n- Send links with captions\\n\n-\n`\nwhatsapp_send_location\n`\n- Send location coordinates\\n\\n\n####\nMCP Endpoints\n\\n\\n\n-\nSSE endpoint:\n`\nhttp://localhost:8080/sse\n`\n\\n\n-\nMessage endpoint:\n`\nhttp://localhost:8080/message\n`\n\\n\\n\n###\nMCP Configuration\n\\n\\nMake sure you have the MCP server running:\n`\n./whatsapp mcp\n`\n\\n\\nFor AI tools that support MCP with SSE (like Cursor), add this configuration:\\n\\n\n```\njson\n\\n{\\n\n\\\"mcpServers\\\"\n: {\\n\n\\\"whatsapp\\\"\n: {\\n\n\\\"url\\\"\n:\n\\\"\nhttp://localhost:8080/sse\n\\\"\n\\n    }\\n  }\\n}\n\\n\nProduction Mode REST (docker)\n\\n\nUsing Docker Hub:\n\\n\ndocker run --detach --publish=3000:3000 --name=whatsapp --restart=always --volume=\n$(\ndocker volume create --name=whatsapp\n)\n:/app/storages aldinokemal2104/go-whatsapp-web-multidevice rest --autoreply=\n\\\"\nDont't reply this message please\n\\\"\n\\n\nUsing GitHub Container Registry:\n\\n\ndocker run --detach --publish=3000:3000 --name=whatsapp --restart=always --volume=\n$(\ndocker volume create --name=whatsapp\n)\n:/app/storages ghcr.io/aldinokemal/go-whatsapp-web-multidevice rest --autoreply=\n\\\"\nDont't reply this message please\n\\\"\n\\n\nProduction Mode REST (docker compose)\n\\n\ncreate\ndocker-compose.yml\nfile with the following configuration:\n\\n\nUsing Docker Hub:\n\\n\nservices\n:\\n\nwhatsapp\n:\\n\nimage\n:\naldinokemal2104/go-whatsapp-web-multidevice\n\\n\ncontainer_name\n:\nwhatsapp\n\\n\nrestart\n:\nalways\n\\n\nports\n:\\n      -\n\\\"\n3000:3000\n\\\"\n\\n\nvolumes\n:\\n      -\nwhatsapp:/app/storages\n\\n\ncommand\n:\\n      -\nrest\n\\n      -\n--basic-auth=admin:admin\n\\n      -\n--port=3000\n\\n      -\n--debug=true\n\\n      -\n--os=Chrome\n\\n      -\n--account-validation=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nUsing GitHub Container Registry:\n\\n\nservices\n:\\n\nwhatsapp\n:",
        "start_pos": 9160,
        "end_pos": 11208,
        "token_count_estimate": 512,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 6,
        "text": "admin\n\\n      -\n--port=3000\n\\n      -\n--debug=true\n\\n      -\n--os=Chrome\n\\n      -\n--account-validation=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nUsing GitHub Container Registry:\n\\n\nservices\n:\\n\nwhatsapp\n:\\n\nimage\n:\nghcr.io/aldinokemal/go-whatsapp-web-multidevice\n\\n\ncontainer_name\n:\nwhatsapp\n\\n\nrestart\n:\nalways\n\\n\nports\n:\\n      -\n\\\"\n3000:3000\n\\\"\n\\n\nvolumes\n:\\n      -\nwhatsapp:/app/storages\n\\n\ncommand\n:\\n      -\nrest\n\\n      -\n--basic-auth=admin:admin\n\\n      -\n--port=3000\n\\n      -\n--debug=true\n\\n      -\n--os=Chrome\n\\n      -\n--account-validation=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nor with env file (Docker Hub):\n\\n\nservices\n:\\n\nwhatsapp\n:\\n\nimage\n:\naldinokemal2104/go-whatsapp-web-multidevice\n\\n\ncontainer_name\n:\nwhatsapp\n\\n\nrestart\n:\nalways\n\\n\nports\n:\\n      -\n\\\"\n3000:3000\n\\\"\n\\n\nvolumes\n:\\n      -\nwhatsapp:/app/storages\n\\n\nenvironment\n:\\n      -\nAPP_BASIC_AUTH=admin:admin\n\\n      -\nAPP_PORT=3000\n\\n      -\nAPP_DEBUG=true\n\\n      -\nAPP_OS=Chrome\n\\n      -\nAPP_ACCOUNT_VALIDATION=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nor with env file (GitHub Container Registry):\n\\n\nservices\n:\\n\nwhatsapp\n:\\n\nimage\n:\nghcr.io/aldinokemal/go-whatsapp-web-multidevice\n\\n\ncontainer_name\n:\nwhatsapp\n\\n\nrestart\n:\nalways\n\\n\nports\n:\\n      -\n\\\"\n3000:3000\n\\\"\n\\n\nvolumes\n:\\n      -\nwhatsapp:/app/storages\n\\n\nenvironment\n:\\n      -\nAPP_BASIC_AUTH=admin:admin\n\\n      -\nAPP_PORT=3000\n\\n      -\nAPP_DEBUG=true\n\\n      -\nAPP_OS=Chrome\n\\n      -\nAPP_ACCOUNT_VALIDATION=false\n\\n\\n\nvolumes\n:\\n\nwhatsapp\n:\n\\n\nProduction Mode (binary)\n\\n\n\\n\ndownload binary from\nrelease\n\\n\n\\n\nYou can fork or edit this source code !\n\\n\nCurrent API\n\\n\nMCP (Model Context Protocol) API\n\\n\n\\n\nMCP server provides standardized tools for AI agents to interact with WhatsApp\n\\n\nSupports Server-Sent Events (SSE) transport\n\\n\nAvailable tools:\nwhatsapp_send_text\n,\nwhatsapp_send_contact\n,\nwhatsapp_send_link\n,\nwhatsapp_send_location\n\\n\nCompatible with MCP-enabled AI tools and agents\n\\n\n\\n\nHTTP REST API\n\\n\n\\n\nAPI Specification Document\n.\n\\n\nCheck\ndocs/openapi.yml\nfor detailed API specifications.",
        "start_pos": 11008,
        "end_pos": 13051,
        "token_count_estimate": 510,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 7,
        "text": "_send_link\n,\nwhatsapp_send_location\n\\n\nCompatible with MCP-enabled AI tools and agents\n\\n\n\\n\nHTTP REST API\n\\n\n\\n\nAPI Specification Document\n.\n\\n\nCheck\ndocs/openapi.yml\nfor detailed API specifications.\n\\n\nUse\nSwaggerEditor\nto visualize the API.\n\\n\nGenerate HTTP clients using\nopenapi-generator\n.",
        "start_pos": 12851,
        "end_pos": 13145,
        "token_count_estimate": 73,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 8,
        "text": "Message\n\\n\nPOST\n\\n\n/message/:message_id/reaction\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nDelete Message\n\\n\nPOST\n\\n\n/message/:message_id/delete\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nEdit Message\n\\n\nPOST\n\\n\n/message/:message_id/update\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nRead Message (DM)\n\\n\nPOST\n\\n\n/message/:message_id/read\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nStar Message\n\\n\nPOST\n\\n\n/message/:message_id/star\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUnstar Message\n\\n\nPOST\n\\n\n/message/:message_id/unstar\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nJoin Group With Link\n\\n\nPOST\n\\n\n/group/join-with-link\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGroup Info From Link\n\\n\nGET\n\\n\n/group/info-from-link\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGroup Info\n\\n\nGET\n\\n\n/group/info\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nLeave Group\n\\n\nPOST\n\\n\n/group/leave\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nCreate Group\n\\n\nPOST\n\\n\n/group\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nAdd Participants in Group\n\\n\nPOST\n\\n\n/group/participants\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nRemove Participant in Group\n\\n\nPOST\n\\n\n/group/participants/remove\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nPromote Participant in Group\n\\n\nPOST\n\\n\n/group/participants/promote\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nDemote Participant in Group\n\\n\nPOST\n\\n\n/group/participants/demote\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nList Requested Participants in Group\n\\n\nGET\n\\n\n/group/participant-requests\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nApprove Requested Participant in Group\n\\n\nPOST\n\\n\n/group/participant-requests/approve\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nReject Requested Participant in Group\n\\n\nPOST\n\\n\n/group/participant-requests/reject\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Photo\n\\n\nPOST\n\\n\n/group/photo\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Name\n\\n\nPOST\n\\n\n/group/name\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Locked\n\\n\nPOST\n\\n\n/group/locked\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Announce\n\\n\nPOST\n\\n\n/group/announce\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nSet Group Topic\n\\n\nPOST\n\\n\n/group/topic\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGet Group Invite Link\n\\n\nGET\n\\n\n/group/invite-link\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nUnfollow Newsletter\n\\n\nPOST\n\\n\n/newsletter/unfollow\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGet Chat List\n\\n\nGET\n\\n\n/chats\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nGet Chat Messages\n\\n\nGET\n\\n\n/chat/:chat_jid/messages\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nLabel Chat\n\\n\nPOST\n\\n\n/chat/:chat_jid/label\n\\n\n\\n\n\\n\n‚úÖ\n\\n\nPin Chat\n\\n\nPOST\n\\n\n/chat/:chat_jid/pin\n\\n\n\\n\n\\n\n\\n\n‚úÖ = Available\\n‚ùå = Not Available Yet\n\\n\nUser Interface\n\\n\nMCP UI\n\\n\n\\n\nSetup MCP (tested in cursor)\\n\n\\n\nTest MCP\\n\n\\n\nSuccessfully setup MCP\\n",
        "start_pos": 14699,
        "end_pos": 16747,
        "token_count_estimate": 511,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      },
      {
        "chunk_id": 9,
        "text": "‚úÖ\n\\n\nPin Chat\n\\n\nPOST\n\\n\n/chat/:chat_jid/pin\n\\n\n\\n\n\\n\n\\n\n‚úÖ = Available\\n‚ùå = Not Available Yet\n\\n\nUser Interface\n\\n\nMCP UI\n\\n\n\\n\nSetup MCP (tested in cursor)\\n\n\\n\nTest MCP\\n\n\\n\nSuccessfully setup MCP\\n\n\\n\n\\n\nHTTP REST API UI\n\\n\n\\n\n\\n\n\\n\nDescription\n\\n\nImage\n\\n\n\\n\n\\n\n\\n\n\\n\nHomepage\n\\n\n\\n\n\\n\n\\n\nLogin\n\\n\n\\n\n\\n\n\\n\nLogin With Code\n\\n\n\\n\n\\n\n\\n\nSend Message\n\\n\n\\n\n\\n\n\\n\nSend Image\n\\n\n\\n\n\\n\n\\n\nSend File\n\\n\n\\n\n\\n\n\\n\nSend Video\n\\n\n\\n\n\\n\n\\n\nSend Contact\n\\n\n\\n\n\\n\n\\n\nSend Location\n\\n\n\\n\n\\n\n\\n\nSend Audio\n\\n\n\\n\n\\n\n\\n\nSend Poll\n\\n\n\\n\n\\n\n\\n\nSend Presence\n\\n\n\\n\n\\n\n\\n\nSend Link\n\\n\n\\n\n\\n\n\\n\nMy Group\n\\n\n\\n\n\\n\n\\n\nGroup Info From Link\n\\n\n\\n\n\\n\n\\n\nCreate Group\n\\n\n\\n\n\\n\n\\n\nJoin Group with Link\n\\n\n\\n\n\\n\n\\n\nManage Participant\n\\n\n\\n\n\\n\n\\n\nMy Newsletter\n\\n\n\\n\n\\n\n\\n\nMy Contacts\n\\n\n\\n\n\\n\n\\n\nBusiness Profile\n\\n\n\\n\n\\n\n\\n\n\\n\nMac OS NOTE\n\\n\n\\n\nPlease do this if you have an error (invalid flag in pkg-config --cflags: -Xpreprocessor)\\n\nexport CGO_CFLAGS_ALLOW=\\\"-Xpreprocessor\\\"\n\\n\n\\n\nImportant\n\\n\n\\n\nThis project is unofficial and not affiliated with WhatsApp.\n\\n\nPlease use official WhatsApp API to avoid any issues.\n\\n\nWe only able to run MCP or REST API, this is limitation from whatsmeow library. independent MCP will be available in\\nthe future.\n\\n\n\\n\n\\n",
        "start_pos": 16547,
        "end_pos": 17782,
        "token_count_estimate": 308,
        "source_type": "detail_page",
        "agent_id": "bd62fe7150e2804b"
      }
    ]
  },
  {
    "agent_id": "cae1fac4a9089657",
    "name": "ai.smithery/sasabasara-where_is_my_bus_mcp",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@sasabasara/where_is_my_bus_mcp/mcp",
    "description": "Get real-time NYC bus arrivals, live vehicle locations, and service alerts. Plan trips between any‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-11T03:53:53.151653Z",
    "indexed_at": "2026-02-18T04:08:45.516849",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide real-time NYC bus arrival times",
        "Show live vehicle locations for NYC buses",
        "Deliver service alerts for NYC bus routes",
        "Assist in planning trips between locations"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a basic one-liner that outlines core functionalities but lacks detailed examples, structure, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "8ccbac58bf21a28c",
    "name": "ai.smithery/sebastianall1977-gmail-mcp",
    "source": "mcp",
    "source_url": "https://github.com/sebastianall1977/gmail-mcp",
    "description": "Manage Gmail end-to-end: search, read, send, draft, label, and organize threads. Automate workflow‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search",
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T13:55:24.480833Z",
    "indexed_at": "2026-02-18T04:08:47.487797",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<div align=\"center\">\n    <h1 align=\"center\">Gmail MCP Server</h1>\n    <p align=center>\n        <a href=\"https://badge.fury.io/js/@shinzolabs%2Fgmail-mcp\"><img src=\"https://badge.fury.io/js/@shinzolabs%2Fgmail-mcp.svg\" alt=\"NPM Version\"></a>\n        <a href=\"https://github.com/shinzo-labs/gmail-mcp/stargazers\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.github.com%2Frepos%2Fshinzo-labs%2Fgmail-mcp%2Fstargazers&query=%24.length&logo=github&label=stars&color=e3b341\" alt=\"Stars\"></a>\n        <a href=\"https://github.com/shinzo-labs/gmail-mcp/forks\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.github.com%2Frepos%2Fshinzo-labs%2Fgmail-mcp%2Fforks&query=%24.length&logo=github&label=forks&color=8957e5\" alt=\"Forks\"></a>\n        <a href=\"https://smithery.ai/server/@shinzo-labs/gmail-mcp\"><img src=\"https://smithery.ai/badge/@shinzo-labs/gmail-mcp\" alt=\"Smithery Calls\"></a>\n        <a href=\"https://www.npmjs.com/package/@shinzolabs/gmail-mcp\"><img src=\"https://img.shields.io/npm/dm/%40shinzolabs%2Fgmail-mcp\" alt=\"NPM Downloads\"></a>\n</div>\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server implementation for the [Gmail](https://developers.google.com/gmail/api) API, providing a standardized interface for email management, sending, and retrieval.\n\n<p align=\"center\"><img height=\"512\" src=https://github.com/user-attachments/assets/b61db02e-bde4-4386-b5a9-2b1c6a989925></p>\n\n## Features\n\n- Complete Gmail API coverage including messages, threads, labels, drafts, and settings\n- Support for sending, drafting, and managing emails\n- Label management with customizable colors and visibility settings\n- Thread operations for conversation management\n- Settings management including vacation responder, IMAP/POP, and language settings\n- History tracking for mailbox changes\n- Secure OAuth2 authentication using Google Cloud credentials\n\n## Prerequisites\n\n### Dependencies\n\nFor simplest installation, install [Node.js 18+](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm). If you would like to build locally, you will also need to install [pnpm](https://pnpm.io/installation).\n\n### Google Workspace Setup\n\nTo run this MCP server, you will need to set up a Google API Client for your organization, with each user running a script to retrieve their own OAuth refresh token.\n\n#### Google API Client Setup (once per organization)\n\n1. Go to the [Google Cloud Console](https://console.cloud.google.com).\n2. Create a new project or select an existing one.\n3. Enable the Gmail API for your project.\n4. Go to Credentials and create an OAuth 2.0 Client ID. Choose \"Desktop app\" for the client type.\n5. Download and save the OAuth keys JSON as `~/.gmail-mcp/gcp-oauth.keys.json`. ‚ö†Ô∏è NOTE: to create `~/.gmail-mcp/` through MacOS's Finder app you need to [enable hidden files](https://stackoverflow.com/questions/5891365/mac-os-x-doesnt-allow-to-name-files-starting-with-a-dot-how-do-i-name-the-hta) first.\n6. (Optional) For remote server installation (ex. using Smithery CLI), note the `CLIENT_ID` and `CLIENT_SECRET` from this file.\n\n#### Client OAuth (once per user)\n\n1. Have the user copy `~/.gmail-mcp/gcp-oauth.keys.json` to their computer at the same path.\n2. Run `npx @shinzolabs/gmail-mcp auth`.\n3. A browser window will open where the user may select a profile, review the requested scopes, and approve.\n4. (Optional) For remote server installation, note the file path mentioned in the success message (`~/.gmail-mcp/credentials.json` by default). The user's `REFRESH_TOKEN` will be found here.\n\n## Client Configuration\n\nThere are several options to configure your MCP client with the server. For hosted/remote server setup, use Smithery's CLI with a [Smithery API Key](https://smithery.ai/docs/registry#registry-api). For local installation, use `npx` or build from source. Each of these options is explained below.\n\n### Smithery Remote Server (Recommended)\n\nTo add a remote server to your MCP client `config.json`, run the following command from [Smithery CLI](https://github.com/smithery-ai/cli?tab=readme-ov-file#smithery-cli--):\n\n```bash\nnpx -y @smithery/cli install @shinzo-labs/gmail-mcp\n```\n\nEnter your `CLIENT_ID`, `CLIENT_SECRET`, and `REFRESH_TOKEN` when prompted.\n\n### Smithery SDK\n\nIf you are developing your own agent application, you can use the boilerplate code [here](https://smithery.ai/server/@shinzo-labs/gmail-mcp/api).\n\n### NPX Local Install\n\nTo install the server locally with `npx`, add the following to your MCP client `config.json`:\n```javascript\n{\n  \"mcpServers\": {\n    \"gmail\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@shinzolabs/gmail-mcp\"\n      ]\n    }\n  }\n}\n```\n\n### Build from Source\n\n1. Download the repo:\n```bash\ngit clone https://github.com/shinzo-labs/gmail-mcp.git\n```\n\n2. Install packages and build with `pnpm` (inside cloned repo):\n```bash\npnpm i && pnpm build\n```\n\n3. Add the following to your MCP client `config.json`:\n```javascript\n{\n  \"mcpServers\": {\n    \"gmail\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"/path/to/gmail-mcp/dist/index.js\"\n      ]\n    }\n  }\n}\n```\n\n## Config Variables\n\n| Variable                 | Description                                             | Required?                       | Default                              |\n|--------------------------|---------------------------------------------------------|---------------------------------|--------------------------------------|\n| `AUTH_SERVER_PORT`       | Port for the temporary OAuth authentication server      | No                              | `3000`                               |\n| `CLIENT_ID`              | Google API client ID (found in `GMAIL_OAUTH_PATH`)      | Yes if remote server connection | `''`                                 |\n| `CLIENT_SECRET`          | Google API client secret (found in `GMAIL_OAUTH_PATH`)  | Yes if remote server connection | `''`                                 |\n| `GMAIL_CREDENTIALS_PATH` | Path to the user credentials file                       | No                              | `MCP_CONFIG_DIR/credentials.json`    |\n| `GMAIL_OAUTH_PATH`       | Path to the Google API Client file                      | No                              | `MCP_CONFIG_DIR/gcp-oauth.keys.json` |\n| `MCP_CONFIG_DIR`         | Directory for storing configuration files               | No                              | `~/.gmail-mcp`                       |\n| `REFRESH_TOKEN`          | OAuth refresh token (found in `GMAIL_CREDENTIALS_PATH`) | Yes if remote server connection | `''`                                 |\n| `PORT`                   | Port for Streamable HTTP transport method               | No                              | `3000`                               |\n| `TELEMETRY_ENABLED`      | Enable telemetry                                        | No                              | `true`                               |\n\n## Supported Endpoints\n\n### User Management\n- `get_profile`: Get the current user's Gmail profile\n- `stop_mail_watch`: Stop receiving push notifications\n- `watch_mailbox`: Set up push notifications for mailbox changes\n\n### Message Management\n\n#### Managing Messages\n- `list_messages`: List messages with optional filtering\n- `get_message`: Get a specific message\n- `get_attachment`: Get a message attachment\n- `modify_message`: Modify message labels\n- `send_message`: Send an email message to specified recipients\n- `delete_message`: Permanently delete a message\n- `trash_message`: Move message to trash\n- `untrash_message`: Remove message from trash\n- `batch_modify_messages`: Modify multiple messages\n- `batch_delete_messages`: Delete multiple messages\n\n### Label Management\n- `list_labels`: List all labels\n- `get_label`: Get a specific label\n- `create_label`: Create a new label\n- `update_label`: Update a label\n- `patch_label`: Partial update of a label\n- `delete_label`: Delete a label\n\n### Thread Management\n- `list_threads`: List email threads\n- `get_thread`: Get a specific thread\n- `modify_thread`: Modify thread labels\n- `trash_thread`: Move thread to trash\n- `untrash_thread`: Remove thread from trash\n- `delete_thread`: Delete a thread\n\n### Draft Management\n- `list_drafts`: List drafts in the user's mailbox\n- `get_draft`: Get a specific draft by ID\n- `create_draft`: Create a draft email in Gmail\n- `update_draft`: Replace a draft's content\n- `delete_draft`: Delete a draft\n- `send_draft`: Send an existing draft\n\n### Settings Management\n\n#### Auto-Forwarding\n- `get_auto_forwarding`: Get auto-forwarding settings\n- `update_auto_forwarding`: Update auto-forwarding settings\n\n#### IMAP Settings\n- `get_imap`: Get IMAP settings\n- `update_imap`: Update IMAP settings\n\n#### POP Settings\n- `get_pop`: Get POP settings\n- `update_pop`: Update POP settings\n\n#### Vacation Responder\n- `get_vacation`: Get vacation responder settings\n- `update_vacation`: Update vacation responder\n\n#### Language Settings\n- `get_language`: Get language settings\n- `update_language`: Update language settings\n\n#### Delegates\n- `list_delegates`: List account delegates\n- `get_delegate`: Get a specific delegate\n- `add_delegate`: Add a delegate\n- `remove_delegate`: Remove a delegate\n\n#### Filters\n- `list_filters`: List email filters\n- `get_filter`: Get a specific filter\n- `create_filter`: Create a new filter\n- `delete_filter`: Delete a filter\n\n#### Forwarding Addresses\n- `list_forwarding_addresses`: List forwarding addresses\n- `get_forwarding_address`: Get a specific forwarding address\n- `create_forwarding_address`: Create a forwarding address\n- `delete_forwarding_address`: Delete a forwarding address\n\n#### Send-As Settings\n- `list_send_as`: List send-as aliases\n- `get_send_as`: Get a specific send-as alias\n- `create_send_as`: Create a send-as alias\n- `update_send_as`: Update a send-as alias\n- `patch_send_as`: Partial update of a send-as alias\n- `verify_send_as`: Send verification email\n- `delete_send_as`: Delete a send-as alias\n\n#### S/MIME Settings\n- `list_smime_info`: List S/MIME configurations\n- `get_smime_info`: Get a specific S/MIME config\n- `insert_smime_info`: Upload a new S/MIME config\n- `set_default_smime_info`: Set default S/MIME config\n- `delete_smime_info`: Delete an S/MIME config\n\n## Contributing\n\nContributions are welcomed and encouraged! Please read [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on issues, contributions, and contact information.\n\n## Data Collection and Privacy\n\nShinzo Labs collects limited anonymous telemetry from this server to help improve our products and services. No personally identifiable information is collected as part of this process. Please review the [Privacy Policy](./PRIVACY.md) for more details on the types of data collected and how to opt-out of this telemetry.\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Manage Gmail messages including listing, retrieving, sending, modifying, trashing, untrashing, and deleting",
        "Manage email threads including listing, retrieving, modifying, trashing, untrashing, and deleting",
        "Manage email labels including listing, creating, updating, patching, and deleting",
        "Manage drafts including listing, retrieving, creating, updating, deleting, and sending drafts",
        "Manage Gmail settings such as auto-forwarding, IMAP, POP, vacation responder, language, delegates, filters, forwarding addresses, send-as aliases, and S/MIME configurations",
        "Set up and stop push notifications for mailbox changes",
        "Authenticate securely using OAuth2 with Google Cloud credentials",
        "Support complete Gmail API coverage with standardized MCP interface",
        "Provide multiple installation and configuration options including local, remote via Smithery CLI, and build from source"
      ],
      "limitations": [
        "Requires Google Workspace setup with OAuth2 credentials and user refresh tokens",
        "OAuth client setup and token retrieval must be done per organization and per user respectively",
        "No explicit mention of rate limits or quota handling in the documentation",
        "Telemetry is enabled by default but can be disabled; limited anonymous data collection occurs",
        "No direct support for non-Gmail email providers"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "Google Cloud project with Gmail API enabled",
        "OAuth 2.0 Client ID and Client Secret configured as a Desktop app",
        "User-specific OAuth refresh token obtained via authentication flow",
        "For remote server usage, Smithery API Key is required",
        "pnpm package manager if building from source"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed feature and endpoint descriptions, configuration options, prerequisites, usage examples, and notes on telemetry and privacy.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<div align=\"center\">\n    <h1 align=\"center\">Gmail MCP Server</h1>\n    <p align=center>\n        <a href=\"https://badge.fury.io/js/@shinzolabs%2Fgmail-mcp\"><img src=\"https://badge.fury.io/js/@shinzolabs%2Fgmail-mcp.svg\" alt=\"NPM Version\"></a>\n        <a href=\"https://github.com/shinzo-labs/gmail-mcp/stargazers\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.github.com%2Frepos%2Fshinzo-labs%2Fgmail-mcp%2Fstargazers&query=%24.length&logo=github&label=stars&color=e3b341\" alt=\"Stars\"></a>\n        <a href=\"https://github.com/shinzo-labs/gmail-mcp/forks\"><img src=\"https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.github.com%2Frepos%2Fshinzo-labs%2Fgmail-mcp%2Fforks&query=%24.length&logo=github&label=forks&color=8957e5\" alt=\"Forks\"></a>\n        <a href=\"https://smithery.ai/server/@shinzo-labs/gmail-mcp\"><img src=\"https://smithery.ai/badge/@shinzo-labs/gmail-mcp\" alt=\"Smithery Calls\"></a>\n        <a href=\"https://www.npmjs.com/package/@shinzolabs/gmail-mcp\"><img src=\"https://img.shields.io/npm/dm/%40shinzolabs%2Fgmail-mcp\" alt=\"NPM Downloads\"></a>\n</div>\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server implementation for the [Gmail](https://developers.google.com/gmail/api) API, providing a standardized interface for email management, sending, and retrieval.",
        "start_pos": 0,
        "end_pos": 1346,
        "token_count_estimate": 336,
        "source_type": "readme",
        "agent_id": "8ccbac58bf21a28c"
      },
      {
        "chunk_id": 1,
        "text": "or mailbox changes\n- Secure OAuth2 authentication using Google Cloud credentials\n\n## Prerequisites\n\n### Dependencies\n\nFor simplest installation, install [Node.js 18+](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm). If you would like to build locally, you will also need to install [pnpm](https://pnpm.io/installation).\n\n### Google Workspace Setup\n\nTo run this MCP server, you will need to set up a Google API Client for your organization, with each user running a script to retrieve their own OAuth refresh token.\n\n#### Google API Client Setup (once per organization)\n\n1. Go to the [Google Cloud Console](https://console.cloud.google.com).\n2. Create a new project or select an existing one.\n3. Enable the Gmail API for your project.\n4. Go to Credentials and create an OAuth 2.0 Client ID. Choose \"Desktop app\" for the client type.\n5. Download and save the OAuth keys JSON as `~/.gmail-mcp/gcp-oauth.keys.json`. ‚ö†Ô∏è NOTE: to create `~/.gmail-mcp/` through MacOS's Finder app you need to [enable hidden files](https://stackoverflow.com/questions/5891365/mac-os-x-doesnt-allow-to-name-files-starting-with-a-dot-how-do-i-name-the-hta) first.\n6. (Optional) For remote server installation (ex. using Smithery CLI), note the `CLIENT_ID` and `CLIENT_SECRET` from this file.\n\n#### Client OAuth (once per user)\n\n1. Have the user copy `~/.gmail-mcp/gcp-oauth.keys.json` to their computer at the same path.\n2. Run `npx @shinzolabs/gmail-mcp auth`.\n3. A browser window will open where the user may select a profile, review the requested scopes, and approve.\n4. (Optional) For remote server installation, note the file path mentioned in the success message (`~/.gmail-mcp/credentials.json` by default). The user's `REFRESH_TOKEN` will be found here.\n\n## Client Configuration\n\nThere are several options to configure your MCP client with the server. For hosted/remote server setup, use Smithery's CLI with a [Smithery API Key](https://smithery.ai/docs/registry#registry-api). For local installation, use `npx` or build from source.",
        "start_pos": 1848,
        "end_pos": 3882,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "8ccbac58bf21a28c"
      },
      {
        "chunk_id": 2,
        "text": "t with the server. For hosted/remote server setup, use Smithery's CLI with a [Smithery API Key](https://smithery.ai/docs/registry#registry-api). For local installation, use `npx` or build from source. Each of these options is explained below.\n\n### Smithery Remote Server (Recommended)\n\nTo add a remote server to your MCP client `config.json`, run the following command from [Smithery CLI](https://github.com/smithery-ai/cli?tab=readme-ov-file#smithery-cli--):\n\n```bash\nnpx -y @smithery/cli install @shinzo-labs/gmail-mcp\n```\n\nEnter your `CLIENT_ID`, `CLIENT_SECRET`, and `REFRESH_TOKEN` when prompted.\n\n### Smithery SDK\n\nIf you are developing your own agent application, you can use the boilerplate code [here](https://smithery.ai/server/@shinzo-labs/gmail-mcp/api).\n\n### NPX Local Install\n\nTo install the server locally with `npx`, add the following to your MCP client `config.json`:\n```javascript\n{\n  \"mcpServers\": {\n    \"gmail\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@shinzolabs/gmail-mcp\"\n      ]\n    }\n  }\n}\n```\n\n### Build from Source\n\n1. Download the repo:\n```bash\ngit clone https://github.com/shinzo-labs/gmail-mcp.git\n```\n\n2. Install packages and build with `pnpm` (inside cloned repo):\n```bash\npnpm i && pnpm build\n```\n\n3.",
        "start_pos": 3682,
        "end_pos": 4926,
        "token_count_estimate": 311,
        "source_type": "readme",
        "agent_id": "8ccbac58bf21a28c"
      },
      {
        "chunk_id": 3,
        "text": "OAuth authentication server      | No                              | `3000`                               |\n| `CLIENT_ID`              | Google API client ID (found in `GMAIL_OAUTH_PATH`)      | Yes if remote server connection | `''`                                 |\n| `CLIENT_SECRET`          | Google API client secret (found in `GMAIL_OAUTH_PATH`)  | Yes if remote server connection | `''`                                 |\n| `GMAIL_CREDENTIALS_PATH` | Path to the user credentials file                       | No                              | `MCP_CONFIG_DIR/credentials.json`    |\n| `GMAIL_OAUTH_PATH`       | Path to the Google API Client file                      | No                              | `MCP_CONFIG_DIR/gcp-oauth.keys.json` |\n| `MCP_CONFIG_DIR`         | Directory for storing configuration files               | No                              | `~/.gmail-mcp`                       |\n| `REFRESH_TOKEN`          | OAuth refresh token (found in `GMAIL_CREDENTIALS_PATH`) | Yes if remote server connection | `''`                                 |\n| `PORT`                   | Port for Streamable HTTP transport method               | No                              | `3000`                               |\n| `TELEMETRY_ENABLED`      | Enable telemetry                                        | No                              | `true`                               |\n\n## Supported Endpoints\n\n### User Management\n- `get_profile`: Get the current user's Gmail profile\n- `stop_mail_watch`: Stop receiving push notifications\n- `watch_mailbox`: Set up push notifications for mailbox changes\n\n### Message Management\n\n#### Managing Messages\n- `list_messages`: List messages with optional filtering\n- `get_message`: Get a specific message\n- `get_attachment`: Get a message attachment\n- `modify_message`: Modify message labels\n- `send_message`: Send an email message to specified recipients\n- `delete_message`: Permanently delete a message\n- `trash_message`: Move message to trash\n- `untrash_message`: Remove message from trash\n- `batch",
        "start_pos": 5530,
        "end_pos": 7578,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "8ccbac58bf21a28c"
      },
      {
        "chunk_id": 4,
        "text": "essage`: Send an email message to specified recipients\n- `delete_message`: Permanently delete a message\n- `trash_message`: Move message to trash\n- `untrash_message`: Remove message from trash\n- `batch_modify_messages`: Modify multiple messages\n- `batch_delete_messages`: Delete multiple messages\n\n### Label Management\n- `list_labels`: List all labels\n- `get_label`: Get a specific label\n- `create_label`: Create a new label\n- `update_label`: Update a label\n- `patch_label`: Partial update of a label\n- `delete_label`: Delete a label\n\n### Thread Management\n- `list_threads`: List email threads\n- `get_thread`: Get a specific thread\n- `modify_thread`: Modify thread labels\n- `trash_thread`: Move thread to trash\n- `untrash_thread`: Remove thread from trash\n- `delete_thread`: Delete a thread\n\n### Draft Management\n- `list_drafts`: List drafts in the user's mailbox\n- `get_draft`: Get a specific draft by ID\n- `create_draft`: Create a draft email in Gmail\n- `update_draft`: Replace a draft's content\n- `delete_draft`: Delete a draft\n- `send_draft`: Send an existing draft\n\n### Settings Management\n\n#### Auto-Forwarding\n- `get_auto_forwarding`: Get auto-forwarding settings\n- `update_auto_forwarding`: Update auto-forwarding settings\n\n#### IMAP Settings\n- `get_imap`: Get IMAP settings\n- `update_imap`: Update IMAP settings\n\n#### POP Settings\n- `get_pop`: Get POP settings\n- `update_pop`: Update POP settings\n\n#### Vacation Responder\n- `get_vacation`: Get vacation responder settings\n- `update_vacation`: Update vacation responder\n\n#### Language Settings\n- `get_language`: Get language settings\n- `update_language`: Update language settings\n\n#### Delegates\n- `list_delegates`: List account delegates\n- `get_delegate`: Get a specific delegate\n- `add_delegate`: Add a delegate\n- `remove_delegate`: Remove a delegate\n\n#### Filters\n- `list_filters`: List email filters\n- `get_filter`: Get a specific filter\n- `create_filter`: Create a new filter\n- `delete_filter`: Delete a filter\n\n#### Forwarding Addresses\n- `list_forwarding_addresses`: List forwarding a",
        "start_pos": 7378,
        "end_pos": 9426,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "8ccbac58bf21a28c"
      },
      {
        "chunk_id": 5,
        "text": "mail filters\n- `get_filter`: Get a specific filter\n- `create_filter`: Create a new filter\n- `delete_filter`: Delete a filter\n\n#### Forwarding Addresses\n- `list_forwarding_addresses`: List forwarding addresses\n- `get_forwarding_address`: Get a specific forwarding address\n- `create_forwarding_address`: Create a forwarding address\n- `delete_forwarding_address`: Delete a forwarding address\n\n#### Send-As Settings\n- `list_send_as`: List send-as aliases\n- `get_send_as`: Get a specific send-as alias\n- `create_send_as`: Create a send-as alias\n- `update_send_as`: Update a send-as alias\n- `patch_send_as`: Partial update of a send-as alias\n- `verify_send_as`: Send verification email\n- `delete_send_as`: Delete a send-as alias\n\n#### S/MIME Settings\n- `list_smime_info`: List S/MIME configurations\n- `get_smime_info`: Get a specific S/MIME config\n- `insert_smime_info`: Upload a new S/MIME config\n- `set_default_smime_info`: Set default S/MIME config\n- `delete_smime_info`: Delete an S/MIME config\n\n## Contributing\n\nContributions are welcomed and encouraged! Please read [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on issues, contributions, and contact information.\n\n## Data Collection and Privacy\n\nShinzo Labs collects limited anonymous telemetry from this server to help improve our products and services. No personally identifiable information is collected as part of this process. Please review the [Privacy Policy](./PRIVACY.md) for more details on the types of data collected and how to opt-out of this telemetry.\n\n## License\n\nMIT",
        "start_pos": 9226,
        "end_pos": 10766,
        "token_count_estimate": 384,
        "source_type": "readme",
        "agent_id": "8ccbac58bf21a28c"
      }
    ]
  },
  {
    "agent_id": "257fdaefa716975b",
    "name": "ai.smithery/serkan-ozal-driflyte-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/serkan-ozal/driflyte-mcp-server",
    "description": "Discover available topics and explore up-to-date, topic-tagged web content. Search to surface the‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-20T16:15:36.760975Z",
    "indexed_at": "2026-02-18T04:08:48.650743",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Driflyte MCP Server\n\n![Build Status](https://github.com/serkan-ozal/driflyte-mcp-server/actions/workflows/build.yml/badge.svg)\n![NPM Version](https://badge.fury.io/js/%40driflyte%2Fmcp-server.svg)\n![License](https://img.shields.io/badge/license-MIT-blue)\n[![MCP Badge](https://lobehub.com/badge/mcp/serkan-ozal-driflyte-mcp-server)](https://lobehub.com/mcp/serkan-ozal-driflyte-mcp-server)\n\nMCP Server for [Driflyte](http://console.driflyte.com).\n\nThe Driflyte MCP Server exposes tools that allow AI assistants to query and retrieve topic-specific knowledge from recursively crawled and indexed web pages.\nWith this MCP server, Driflyte acts as a bridge between diverse, topic-aware content sources (web, GitHub, and more) and AI-powered reasoning, enabling richer, more accurate answers.\n\n\n## What It Does\n\n- **Deep Web Crawling**: Recursively follows links to crawl and index web pages.\n- **GitHub Integration**: Crawls repositories, issues, and discussions.\n- **Extensible Resource Support**: Future support planned for Slack, Microsoft Teams, Google Docs/Drive, Confluence, JIRA, Zendesk, Salesforce, and more.\n- **Topic-Aware Indexing**: Each document is tagged with one or more topics, enabling targeted, topic-specific retrieval.\n- **Designed for RAG with RAG**: The server itself is built with Retrieval-Augmented Generation (RAG) in mind, and it powers RAG workflows by providing assistants with high-quality, topic-specific documents as grounding context.\n- **Designed for AI with AI**: The system is not just for AI assistants ‚Äî it is also designed and evolved using AI itself, making it an AI-native component for intelligent knowledge retrieval.\n\n\n## Usage & Limits\n\n- **Free Access**: Driflyte is currently free to use.\n- **No Signup Required**: You can start using it immediately ‚Äî no registration or subscription needed.\n- **Rate Limits**: To ensure fair usage, requests are limited by IP:\n  - **`100` API requests** per **`5` minutes** per **IP address**.\n- Future changes to usage policies and limits may be introduced as new features and resource integrations become available.\n\n\n## Prerequisites\n- Node.js 18+\n- An AI assistant (with MCP client) like Cursor, Claude (Desktop or Code), VS Code, Windsurf, etc ...\n\n## Configurations\n\n### CLI Arguments\n\nDriflyte MCP server supports the following CLI arguments for configuration:\n- `--transport <stdio|streamable-http>` - Configures the transport protocol (defaults to `stdio`).\n- `--port <number>` ‚Äì Configures the port number to listen on when using `streamable-http` transport (defaults to `3000`).\n\n\n## Quick Start\n\nThis MCP server (using `STDIO` or `Streamable HTTP` transport) can be added to any MCP Client \nlike VS Code, Claude, Cursor, Windsurf Github Copilot via the `@driflyte/mcp-server` NPM package.\n\n### ChatGPT\n\n- Navigate to `Settings` under your profile and enable `Developer Mode` under the `Connectors` option.\n- In the chat panel, click the `+` icon, and from the dropdown, select `Developer Mode`. \n  You‚Äôll see an option to add sources/connectors.\n- Enter the following MCP Server details and then click `Create`:\n  - `Name`: `Driflyte`\n  - `MCP Server URL`: `https://mcp.driflyte.com/openai`\n  - `Authentication`: `No authentication`\n  - `Trust Setting`: Check `I trust this application`\n\nSee [How to set up a remote MCP server and connect it to ChatGPT deep research](https://community.openai.com/t/how-to-set-up-a-remote-mcp-server-and-connect-it-to-chatgpt-deep-research/1278375) \nand [MCP server tools now in ChatGPT ‚Äì developer mode](https://community.openai.com/t/mcp-server-tools-now-in-chatgpt-developer-mode/1357233) for more info.\n\n### Claude Code\n\nRun the following command.\nSee [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Local Server\n```bash\nclaude mcp add driflyte -- npx -y @driflye/mcp-server\n```\n\n#### Remote Server\n```bash\nclaude mcp add --transport http driflyte https://mcp.driflyte.com/mcp\n```\n\n### Claude Desktop\n\n#### Local Server\nAdd the following configuration into the `claude_desktop_config.json` file.\nSee the [Claude Desktop MCP docs](https://modelcontextprotocol.io/docs/develop/connect-local-servers) for more info.\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\nGo to the `Settings` > `Connectors` > `Add Custom Connector` in the Claude Desktop and add the new MCP server with the following fields: \n- Name: `Driflyte` \n- Remote MCP server URL: `https://mcp.driflyte.com/mcp`\n\n### Copilot Coding Agent\n\nAdd the following configuration to the `mcpServers` section of your Copilot Coding Agent configuration through \n`Repository` > `Settings` > `Copilot` > `Coding agent` > `MCP configuration`.\nSee the [Copilot Coding Agent MCP docs](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"type\": \"local\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd the following configuration into the `~/.cursor/mcp.json` file (or `.cursor/mcp.json` in your project folder).\nOr setup by üñ±Ô∏è[One Click Installation](https://cursor.com/en/install-mcp?name=driflyte&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBkcmlmbHl0ZS9tY3Atc2VydmVyIl19).\nSee the [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"url\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Gemini CLI\n\nAdd the following configuration into the `~/.gemini/settings.json` file.\nSee the [Gemini CLI MCP docs](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"httpUrl\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Smithery\n\nRun the following command.\nYou can find your Smithery API key [here](https://smithery.ai/account/api-keys).\nSee the [Smithery CLI docs](https://smithery.ai/docs/concepts/cli) for more info.\n```bash\nnpx -y @smithery/cli install @serkan-ozal/driflyte-mcp-server --client <SMITHERY-CLIENT-NAME> --key <SMITHERY-API-KEY>\n```\n\n### VS Code\n\nAdd the following configuration into the `.vscode/mcp.json` file.\nOr setup by üñ±Ô∏è[One Click Installation](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22driflyte%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40driflyte%2Fmcp-server%22%5D%7D).\nSee the [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### Local Server\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"driflyte\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n      }\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"driflyte\": {\n        \"type\": \"http\",\n        \"url\": \"https://mcp.driflyte.com/mcp\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nAdd the following configuration into the `~/.codeium/windsurf/mcp_config.json` file. \nSee the [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"serverUrl\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n\n## Components\n\n### Tools\n\n- `list-topics`: Returns a list of topics for which resources (web pages, etc ...) have been crawled and content is available. \n                 This allows AI assistants to discover the most relevant and up-to-date subject areas currently indexed by the crawler.\n  - **Input Schema**: No input parameter supported.\n  - **Output Schema**:\n    - `topics`:\n      - `Optinal`: `false`\n      - `Type`: `Array<string>`\n      - `Description`: List of the supported topics.\n- `search`: Given a list of topics and a user question, this tool retrieves the top-K most relevant documents from the crawled content. \n            It is designed to help AI assistants surface the most contextually appropriate and up-to-date information for a specific topic and query.\n            This enables more informed and accurate responses based on real-world, topic-tagged web content.\n  - **Input Schema**:\n    - `topics`\n      - `Optinal`: `false`\n      - `Type`: `Array<string>`\n      - `Description`: A list of one or more topic identifiers to constrain the search space.\n                       Only documents tagged with at least one of these topics will be considered.\n    - `query`\n      - `Optinal`: `false`\n      - `Type`: `string`\n      - `Description`: The natural language query or question for which relevant information is being sought.\n                       This will be used to rank documents by semantic relevance. \n    - `topK`\n      - `Optinal`: `true`\n      - `Type`: `number`\n      - `Default Value`: `10`\n      - `Min Value`: `1`\n      - `Max Value`: `30`\n      - `Description`: The maximum number of relevant documents to return.\n                       Results are sorted by descending relevance score.\n  - **Output Schema**:\n    - `documents`:\n      - `Optional`: `false`\n      - `Type`: `Array<Document>`\n      - `Description`: Matched documents to the search query.\n      - **Type**: `Document`:\n        - `content`\n          - `Optinal`: `false`\n          - `Type`: `string`\n          - `Description`: Related content (full or partial) of the matched document.\n        - `metadata`\n          - `Optinal`: `false`\n          - `Type`: `Map<string, any>`\n          - `Description`: Metadata of the document and related content in key-value format.\n        - `score`\n          - `Optinal`: `false`\n          - `Type`: `number`\n          - `Min Value`: `0`\n          - `Max Value`: `1`\n          - `Description`: Similarity score (between `0` and `1`) for the content of the document.\n\n### Resources\n\nN/A\n\n\n## Roadmap\n\n- Support more content types (`.pdf`, `.ppt`/`.pptx`, `.doc`/`.docx`, and many others applicable including audio and video file formats ...)\n- Integrate with more data sources (Slack, Teams, Google Docs/Drive, Confluence, JIRA, Zendesk, Salesforce, etc ...))\n- And more topics with their resources\n\n\n## Issues and Feedback\n\n[![Issues](https://img.shields.io/github/issues/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/issues?q=is%3Aopen+is%3Aissue)\n[![Closed issues](https://img.shields.io/github/issues-closed/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/issues?q=is%3Aissue+is%3Aclosed)\n\nPlease use [GitHub Issues](https://github.com/serkan-ozal/driflyte-mcp-server/issues) for any bug report, feature request and support.\n\n\n## Contribution\n\n[![Pull requests](https://img.shields.io/github/issues-pr/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/pulls?q=is%3Aopen+is%3Apr)\n[![Closed pull requests](https://img.shields.io/github/issues-pr-closed/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/pulls?q=is%3Apr+is%3Aclosed)\n[![Contributors](https://img.shields.io/github/contributors/serkan-ozal/driflyte-mcp-server.svg)]()\n\nIf you would like to contribute, please\n- Fork the repository on GitHub and clone your fork.\n- Create a branch for your changes and make your changes on it.\n- Send a pull request by explaining clearly what is your contribution.\n\n> Tip:\n> Please check the existing pull requests for similar contributions and\n> consider submit an issue to discuss the proposed feature before writing code.\n\n## License\n\nLicensed under [MIT](LICENSE).\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Recursively crawl and index web pages with deep web crawling",
        "Crawl GitHub repositories, issues, and discussions",
        "Tag documents with one or more topics for targeted retrieval",
        "Retrieve a list of available topics with crawled content",
        "Search and retrieve top-K relevant documents based on topics and natural language queries",
        "Support integration with multiple MCP clients such as VS Code, Claude, Cursor, Windsurf, ChatGPT, Copilot Coding Agent, Gemini CLI, Smithery",
        "Operate with multiple transport protocols including stdio and streamable-http",
        "Provide free access with no signup required"
      ],
      "limitations": [
        "Rate limited to 100 API requests per 5 minutes per IP address",
        "Currently supports only web and GitHub content sources; other integrations like Slack, Teams, Google Docs/Drive, Confluence, JIRA, Zendesk, Salesforce are planned but not yet available",
        "Does not support document types beyond web pages and GitHub content yet (e.g., PDF, PPT, DOC, audio, video formats are planned)",
        "No authentication required currently, which may limit security or access control options"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "An AI assistant or MCP client compatible with MCP protocol (e.g., Cursor, Claude Desktop or Code, VS Code, Windsurf, etc.)"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples for multiple clients, clear descriptions of tools and their input/output schemas, explicit limitations including rate limits, and environment prerequisites.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Driflyte MCP Server\n\n![Build Status](https://github.com/serkan-ozal/driflyte-mcp-server/actions/workflows/build.yml/badge.svg)\n![NPM Version](https://badge.fury.io/js/%40driflyte%2Fmcp-server.svg)\n![License](https://img.shields.io/badge/license-MIT-blue)\n[![MCP Badge](https://lobehub.com/badge/mcp/serkan-ozal-driflyte-mcp-server)](https://lobehub.com/mcp/serkan-ozal-driflyte-mcp-server)\n\nMCP Server for [Driflyte](http://console.driflyte.com).\n\nThe Driflyte MCP Server exposes tools that allow AI assistants to query and retrieve topic-specific knowledge from recursively crawled and indexed web pages.\nWith this MCP server, Driflyte acts as a bridge between diverse, topic-aware content sources (web, GitHub, and more) and AI-powered reasoning, enabling richer, more accurate answers.\n\n\n## What It Does\n\n- **Deep Web Crawling**: Recursively follows links to crawl and index web pages.\n- **GitHub Integration**: Crawls repositories, issues, and discussions.\n- **Extensible Resource Support**: Future support planned for Slack, Microsoft Teams, Google Docs/Drive, Confluence, JIRA, Zendesk, Salesforce, and more.\n- **Topic-Aware Indexing**: Each document is tagged with one or more topics, enabling targeted, topic-specific retrieval.\n- **Designed for RAG with RAG**: The server itself is built with Retrieval-Augmented Generation (RAG) in mind, and it powers RAG workflows by providing assistants with high-quality, topic-specific documents as grounding context.\n- **Designed for AI with AI**: The system is not just for AI assistants ‚Äî it is also designed and evolved using AI itself, making it an AI-native component for intelligent knowledge retrieval.\n\n\n## Usage & Limits\n\n- **Free Access**: Driflyte is currently free to use.\n- **No Signup Required**: You can start using it immediately ‚Äî no registration or subscription needed.\n- **Rate Limits**: To ensure fair usage, requests are limited by IP:\n  - **`100` API requests** per **`5` minutes** per **IP address**.",
        "start_pos": 0,
        "end_pos": 1974,
        "token_count_estimate": 493,
        "source_type": "readme",
        "agent_id": "257fdaefa716975b"
      },
      {
        "chunk_id": 1,
        "text": "t using it immediately ‚Äî no registration or subscription needed.\n- **Rate Limits**: To ensure fair usage, requests are limited by IP:\n  - **`100` API requests** per **`5` minutes** per **IP address**.\n- Future changes to usage policies and limits may be introduced as new features and resource integrations become available.\n\n\n## Prerequisites\n- Node.js 18+\n- An AI assistant (with MCP client) like Cursor, Claude (Desktop or Code), VS Code, Windsurf, etc ...\n\n## Configurations\n\n### CLI Arguments\n\nDriflyte MCP server supports the following CLI arguments for configuration:\n- `--transport <stdio|streamable-http>` - Configures the transport protocol (defaults to `stdio`).\n- `--port <number>` ‚Äì Configures the port number to listen on when using `streamable-http` transport (defaults to `3000`).\n\n\n## Quick Start\n\nThis MCP server (using `STDIO` or `Streamable HTTP` transport) can be added to any MCP Client \nlike VS Code, Claude, Cursor, Windsurf Github Copilot via the `@driflyte/mcp-server` NPM package.\n\n### ChatGPT\n\n- Navigate to `Settings` under your profile and enable `Developer Mode` under the `Connectors` option.\n- In the chat panel, click the `+` icon, and from the dropdown, select `Developer Mode`. \n  You‚Äôll see an option to add sources/connectors.\n- Enter the following MCP Server details and then click `Create`:\n  - `Name`: `Driflyte`\n  - `MCP Server URL`: `https://mcp.driflyte.com/openai`\n  - `Authentication`: `No authentication`\n  - `Trust Setting`: Check `I trust this application`\n\nSee [How to set up a remote MCP server and connect it to ChatGPT deep research](https://community.openai.com/t/how-to-set-up-a-remote-mcp-server-and-connect-it-to-chatgpt-deep-research/1278375) \nand [MCP server tools now in ChatGPT ‚Äì developer mode](https://community.openai.com/t/mcp-server-tools-now-in-chatgpt-developer-mode/1357233) for more info.\n\n### Claude Code\n\nRun the following command.\nSee [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.",
        "start_pos": 1774,
        "end_pos": 3771,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "257fdaefa716975b"
      },
      {
        "chunk_id": 2,
        "text": "r-tools-now-in-chatgpt-developer-mode/1357233) for more info.\n\n### Claude Code\n\nRun the following command.\nSee [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Local Server\n```bash\nclaude mcp add driflyte -- npx -y @driflye/mcp-server\n```\n\n#### Remote Server\n```bash\nclaude mcp add --transport http driflyte https://mcp.driflyte.com/mcp\n```\n\n### Claude Desktop\n\n#### Local Server\nAdd the following configuration into the `claude_desktop_config.json` file.\nSee the [Claude Desktop MCP docs](https://modelcontextprotocol.io/docs/develop/connect-local-servers) for more info.\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\nGo to the `Settings` > `Connectors` > `Add Custom Connector` in the Claude Desktop and add the new MCP server with the following fields: \n- Name: `Driflyte` \n- Remote MCP server URL: `https://mcp.driflyte.com/mcp`\n\n### Copilot Coding Agent\n\nAdd the following configuration to the `mcpServers` section of your Copilot Coding Agent configuration through \n`Repository` > `Settings` > `Copilot` > `Coding agent` > `MCP configuration`.\nSee the [Copilot Coding Agent MCP docs](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"type\": \"local\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd the following configuration into the `~/.cursor/mcp.json` file (or `.cursor/mcp.json` in your project folder).\nOr setup by üñ±Ô∏è[One Click Installation](https://cursor.com/en/install-mcp?name=driflyte&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBkcmlmbHl0ZS9tY3Atc2VydmVyIl19).",
        "start_pos": 3571,
        "end_pos": 5581,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "257fdaefa716975b"
      },
      {
        "chunk_id": 3,
        "text": ".json` in your project folder).\nOr setup by üñ±Ô∏è[One Click Installation](https://cursor.com/en/install-mcp?name=driflyte&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkBkcmlmbHl0ZS9tY3Atc2VydmVyIl19).\nSee the [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"url\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Gemini CLI\n\nAdd the following configuration into the `~/.gemini/settings.json` file.\nSee the [Gemini CLI MCP docs](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"httpUrl\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n### Smithery\n\nRun the following command.\nYou can find your Smithery API key [here](https://smithery.ai/account/api-keys).\nSee the [Smithery CLI docs](https://smithery.ai/docs/concepts/cli) for more info.\n```bash\nnpx -y @smithery/cli install @serkan-ozal/driflyte-mcp-server --client <SMITHERY-CLIENT-NAME> --key <SMITHERY-API-KEY>\n```\n\n### VS Code\n\nAdd the following configuration into the `.vscode/mcp.json` file.\nOr setup by üñ±Ô∏è[One Click Installation](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22driflyte%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40driflyte%2Fmcp-server%22%5D%7D).\nSee the [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.",
        "start_pos": 5381,
        "end_pos": 7191,
        "token_count_estimate": 452,
        "source_type": "readme",
        "agent_id": "257fdaefa716975b"
      },
      {
        "chunk_id": 4,
        "text": "{\n    \"servers\": {\n      \"driflyte\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n      }\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"driflyte\": {\n        \"type\": \"http\",\n        \"url\": \"https://mcp.driflyte.com/mcp\"\n      }\n    }\n  }\n}\n```\n\n### Windsurf\n\nAdd the following configuration into the `~/.codeium/windsurf/mcp_config.json` file. \nSee the [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Local Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@driflyte/mcp-server\"]\n    }\n  }\n}\n```\n\n#### Remote Server\n```json\n{\n  \"mcpServers\": {\n    \"driflyte\": {\n      \"serverUrl\": \"https://mcp.driflyte.com/mcp\"\n    }\n  }\n}\n```\n\n\n## Components\n\n### Tools\n\n- `list-topics`: Returns a list of topics for which resources (web pages, etc ...) have been crawled and content is available. \n                 This allows AI assistants to discover the most relevant and up-to-date subject areas currently indexed by the crawler.\n  - **Input Schema**: No input parameter supported.\n  - **Output Schema**:\n    - `topics`:\n      - `Optinal`: `false`\n      - `Type`: `Array<string>`\n      - `Description`: List of the supported topics.\n- `search`: Given a list of topics and a user question, this tool retrieves the top-K most relevant documents from the crawled content. \n            It is designed to help AI assistants surface the most contextually appropriate and up-to-date information for a specific topic and query.\n            This enables more informed and accurate responses based on real-world, topic-tagged web content.\n  - **Input Schema**:\n    - `topics`\n      - `Optinal`: `false`\n      - `Type`: `Array<string>`\n      - `Description`: A list of one or more topic identifiers to constrain the search space.\n                       Only documents tagged with at least one of these topics will be considered.",
        "start_pos": 7229,
        "end_pos": 9216,
        "token_count_estimate": 496,
        "source_type": "readme",
        "agent_id": "257fdaefa716975b"
      },
      {
        "chunk_id": 5,
        "text": "tring>`\n      - `Description`: A list of one or more topic identifiers to constrain the search space.\n                       Only documents tagged with at least one of these topics will be considered.\n    - `query`\n      - `Optinal`: `false`\n      - `Type`: `string`\n      - `Description`: The natural language query or question for which relevant information is being sought.\n                       This will be used to rank documents by semantic relevance. \n    - `topK`\n      - `Optinal`: `true`\n      - `Type`: `number`\n      - `Default Value`: `10`\n      - `Min Value`: `1`\n      - `Max Value`: `30`\n      - `Description`: The maximum number of relevant documents to return.\n                       Results are sorted by descending relevance score.\n  - **Output Schema**:\n    - `documents`:\n      - `Optional`: `false`\n      - `Type`: `Array<Document>`\n      - `Description`: Matched documents to the search query.\n      - **Type**: `Document`:\n        - `content`\n          - `Optinal`: `false`\n          - `Type`: `string`\n          - `Description`: Related content (full or partial) of the matched document.\n        - `metadata`\n          - `Optinal`: `false`\n          - `Type`: `Map<string, any>`\n          - `Description`: Metadata of the document and related content in key-value format.\n        - `score`\n          - `Optinal`: `false`\n          - `Type`: `number`\n          - `Min Value`: `0`\n          - `Max Value`: `1`\n          - `Description`: Similarity score (between `0` and `1`) for the content of the document.",
        "start_pos": 9016,
        "end_pos": 10549,
        "token_count_estimate": 383,
        "source_type": "readme",
        "agent_id": "257fdaefa716975b"
      },
      {
        "chunk_id": 6,
        "text": "h their resources\n\n\n## Issues and Feedback\n\n[![Issues](https://img.shields.io/github/issues/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/issues?q=is%3Aopen+is%3Aissue)\n[![Closed issues](https://img.shields.io/github/issues-closed/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/issues?q=is%3Aissue+is%3Aclosed)\n\nPlease use [GitHub Issues](https://github.com/serkan-ozal/driflyte-mcp-server/issues) for any bug report, feature request and support.\n\n\n## Contribution\n\n[![Pull requests](https://img.shields.io/github/issues-pr/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/pulls?q=is%3Aopen+is%3Apr)\n[![Closed pull requests](https://img.shields.io/github/issues-pr-closed/serkan-ozal/driflyte-mcp-server.svg)](https://github.com/serkan-ozal/driflyte-mcp-server/pulls?q=is%3Apr+is%3Aclosed)\n[![Contributors](https://img.shields.io/github/contributors/serkan-ozal/driflyte-mcp-server.svg)]()\n\nIf you would like to contribute, please\n- Fork the repository on GitHub and clone your fork.\n- Create a branch for your changes and make your changes on it.\n- Send a pull request by explaining clearly what is your contribution.\n\n> Tip:\n> Please check the existing pull requests for similar contributions and\n> consider submit an issue to discuss the proposed feature before writing code.\n\n## License\n\nLicensed under [MIT](LICENSE).",
        "start_pos": 10864,
        "end_pos": 12312,
        "token_count_estimate": 361,
        "source_type": "readme",
        "agent_id": "257fdaefa716975b"
      }
    ]
  },
  {
    "agent_id": "4b8da840c7cf0733",
    "name": "ai.smithery/shoumikdc-arxiv-mcp",
    "source": "mcp",
    "source_url": "https://github.com/shoumikdc/arXiv-mcp",
    "description": "Discover the latest arXiv papers by category and keyword. Control how many results you get to spee‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T06:59:26.159502Z",
    "indexed_at": "2026-02-18T04:08:50.799951",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "\n# arXiv-mcp\n\n**arXiv-mcp** is a Model Context Protocol (MCP) server for querying and discovering the latest arXiv papers, built for seamless integration with LLMs and AI agents via the Smithery platform.\n\n## Overview\n\nThis tool enables AI applications to fetch, filter, and summarize new arXiv submissions in any category, making it easy to build research assistants, literature review bots, or custom paper discovery workflows. Powered by Smithery MCP, it supports session-based configuration and is ready for deployment.\n\n## Problem Statement\n\nStaying up-to-date with the latest research on arXiv is challenging due to the volume and frequency of new submissions. arXiv-mcp solves this by providing a programmable interface for LLMs and agents to:\n- Retrieve daily arXiv postings by category\n- Search for papers by keyword\n- Return structured metadata (title, authors, summary, link, published date)\n- Personalize results with session config (in development currently!)\n\n## Example Usage (LLM Chat)\n\n> **User:**  \nFind today's latest papers in the category `quant-ph`. Then curate a list of papers related to quantum computing. \n> \n> **LLM (using arXiv-mcp):**  \n> Calls `fetch_current_arxiv_postings_rss(category=\"quant-ph\")`  \n> Returns a list of new papers with titles, authors, summaries, and links.\n\n<br>\n\n> **User:**  \n> Show me recent arXiv papers about \"fluxonium qubits\" in quantum computing.\n>\n> **LLM (using arXiv-mcp):**  \n> Calls `keyword_search_arxiv_rss(category=\"quant-ph\", keyword=\"fluxonium qubits\")`  \nReturns a list of filtered papers whose title or abstract matches the keyword.\n\n---\n\n## Getting Started\n\n1. **Run the server:**\n   ```bash\n   uv run dev\n   ```\n\n2. **Test interactively:**\n   ```bash\n   uv run playground\n   ```\n\nYour server code is in `src/hello_server/server.py`.  \nThe server capabilities can be modified there.\n\n---\n\n## Prerequisites\n\n- **Smithery API key:** Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n---\n\nThis code can be deployed by pushing to GitHub and then deploying via [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Fetch daily arXiv postings by category",
        "Search arXiv papers by keyword within a category",
        "Return structured metadata including title, authors, summary, link, and published date",
        "Support session-based configuration for personalized results (in development)",
        "Integrate seamlessly with LLMs and AI agents via the Smithery platform",
        "Enable building research assistants and literature review bots",
        "Provide a programmable interface for querying latest arXiv papers"
      ],
      "limitations": [
        "Session-based personalization is currently in development and not fully available"
      ],
      "requirements": [
        "Smithery API key for authentication",
        "Smithery platform account to deploy and manage the server",
        "Python environment capable of running the server with 'uv' command"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, usage examples, tool descriptions, prerequisites, and known limitations, providing a comprehensive overview.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# arXiv-mcp\n\n**arXiv-mcp** is a Model Context Protocol (MCP) server for querying and discovering the latest arXiv papers, built for seamless integration with LLMs and AI agents via the Smithery platform.\n\n## Overview\n\nThis tool enables AI applications to fetch, filter, and summarize new arXiv submissions in any category, making it easy to build research assistants, literature review bots, or custom paper discovery workflows. Powered by Smithery MCP, it supports session-based configuration and is ready for deployment.\n\n## Problem Statement\n\nStaying up-to-date with the latest research on arXiv is challenging due to the volume and frequency of new submissions. arXiv-mcp solves this by providing a programmable interface for LLMs and agents to:\n- Retrieve daily arXiv postings by category\n- Search for papers by keyword\n- Return structured metadata (title, authors, summary, link, published date)\n- Personalize results with session config (in development currently!)\n\n## Example Usage (LLM Chat)\n\n> **User:**  \nFind today's latest papers in the category `quant-ph`. Then curate a list of papers related to quantum computing. \n> \n> **LLM (using arXiv-mcp):**  \n> Calls `fetch_current_arxiv_postings_rss(category=\"quant-ph\")`  \n> Returns a list of new papers with titles, authors, summaries, and links.\n\n<br>\n\n> **User:**  \n> Show me recent arXiv papers about \"fluxonium qubits\" in quantum computing.\n>\n> **LLM (using arXiv-mcp):**  \n> Calls `keyword_search_arxiv_rss(category=\"quant-ph\", keyword=\"fluxonium qubits\")`  \nReturns a list of filtered papers whose title or abstract matches the keyword.\n\n---\n\n## Getting Started\n\n1. **Run the server:**\n   ```bash\n   uv run dev\n   ```\n\n2. **Test interactively:**\n   ```bash\n   uv run playground\n   ```\n\nYour server code is in `src/hello_server/server.py`.  \nThe server capabilities can be modified there.",
        "start_pos": 0,
        "end_pos": 1853,
        "token_count_estimate": 463,
        "source_type": "readme",
        "agent_id": "4b8da840c7cf0733"
      },
      {
        "chunk_id": 1,
        "text": "```bash\n   uv run dev\n   ```\n\n2. **Test interactively:**\n   ```bash\n   uv run playground\n   ```\n\nYour server code is in `src/hello_server/server.py`.  \nThe server capabilities can be modified there.\n\n---\n\n## Prerequisites\n\n- **Smithery API key:** Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n---\n\nThis code can be deployed by pushing to GitHub and then deploying via [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 1653,
        "end_pos": 2103,
        "token_count_estimate": 111,
        "source_type": "readme",
        "agent_id": "4b8da840c7cf0733"
      }
    ]
  },
  {
    "agent_id": "b2c26b34dc68ee5b",
    "name": "ai.smithery/skr-cloudify-clickup-mcp-server-new",
    "source": "mcp",
    "source_url": "https://github.com/skr-cloudify/clickup-mcp-server-new",
    "description": "Manage your ClickUp workspace by creating, updating, and organizing tasks, lists, folders, and tag‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-21T11:44:55.497079Z",
    "indexed_at": "2026-02-18T04:08:52.550409",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create tasks in ClickUp workspace",
        "Update tasks in ClickUp workspace",
        "Organize tasks in ClickUp workspace",
        "Manage lists in ClickUp workspace",
        "Manage folders in ClickUp workspace",
        "Manage tags in ClickUp workspace"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of managing ClickUp workspace elements but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "54660031fc7ea91b",
    "name": "ai.smithery/slhad-aha-mcp",
    "source": "mcp",
    "source_url": "https://github.com/slhad/aha-mcp",
    "description": "A TypeScript MCP server for Home Assistant, enabling programmatic management of entities, automati‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-14T21:53:54.664726Z",
    "indexed_at": "2026-02-18T04:08:57.496413",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# AHA Model Context Protocol (MCP) Server\n\n## Quick install links\n**STDIO**  \n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_AHA--MCP-0098FF?style=for-the-badge&logo=visualstudiocode&logoColor=ffffff)](https://insiders.vscode.dev/redirect/mcp/install?name=aha-mcp&config=%7B%22type%22%3A%20%22stdio%22%2C%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22--pull%22%2C%22always%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22TRANSPORT%3Dstdio%22%2C%22-e%22%2C%22RESOURCES_TO_TOOLS%3Dtrue%22%2C%22-e%22%2C%22HASS_URL%22%2C%22-e%22%2C%22HASS_ACCESS_TOKEN%22%2C%22ghcr.io%2Fslhad%2Faha-mcp%3Alatest%22%5D%2C%22env%22%3A%7B%22HASS_ACCESS_TOKEN%22%3A%22%24%7Binput%3AHASS_ACCESS_TOKEN%7D%22%2C%22HASS_URL%22%3A%22%24%7Binput%3AHASS_URL%7D%22%7D%7D)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=aha-mcp&config=eyJjb21tYW5kIjoiZG9ja2VyIHJ1biAtLXB1bGwgYWx3YXlzIC1pIC0tcm0gLWUgVFJBTlNQT1JUPXN0ZGlvIC1lIFJFU09VUkNFU19UT19UT09MUz10cnVlIC1lIEhBU1NfVVJMPWh0dHBzOi8vaGFfaW5zdGFuY2UgLWUgSEFTU19BQ0NFU1NfVE9LRU49aGFfbG9uZ19saXZlZF9hY2Nlc3NfdG9rZW4gZ2hjci5pby9zbGhhZC9haGEtbWNwOmxhdGVzdCJ9)\n\n**Others**  \n[![smithery badge](https://smithery.ai/badge/@slhad/aha-mcp)](https://smithery.ai/server/@slhad/aha-mcp)\n\nPretty useful to test it without IDE\n\n## AHA stands for Another Home Assistant MCP Server\n\n\nThis repository implements a Model Context Protocol (MCP) server for Home Assistant, providing a bridge between Home Assistant and MCP clients.\n\nWith this server, you can:\n\n- List and query Home Assistant entities (lights, sensors, switches, etc.)\n- Retrieve and update the state of entities\n- Call Home Assistant services (e.g., turn on/off devices)\n- Manage automations: list, create, update, and validate automations\n- Access and update the entity registry\n- Integrate and update Lovelace dashboards\n- Validate Home Assistant configuration\n- Search for entities by prefix or regex\n- Access entity sources and registry information\n\nThe server supports multiple transport methods:\n- **STDIO**: Traditional MCP client communication (default)\n- **Server-Sent Events (SSE)**: Web-based real-time communication\n- **Streamable HTTP**: HTTP-based MCP communication for web integration\n\n## Table of Contents\n\n- [AHA Model Context Protocol (MCP) Server](#aha-model-context-protocol-mcp-server)\n  - [Quick install links](#quick-install-links)\n  - [AHA stands for Another Home Assistant MCP Server](#aha-stands-for-another-home-assistant-mcp-server)\n  - [Table of Contents](#table-of-contents)\n  - [Motivation](#motivation)\n  - [Features](#features)\n  - [Available Tools](#available-tools)\n    - [Generating Tools Documentation](#generating-tools-documentation)\n  - [Project Structure](#project-structure)\n  - [Example: Running the MCP Server](#example-running-the-mcp-server)\n    - [1. Run with STDIO Transport (Default)](#1-run-with-stdio-transport-default)\n    - [2. Run with Server-Sent Events (SSE) Transport](#2-run-with-server-sent-events-sse-transport)\n    - [3. Run with Streamable HTTP Transport](#3-run-with-streamable-http-transport)\n    - [4. Quick SSE Server Startup](#4-quick-sse-server-startup)\n    - [5. Run with Podman/Docker - STDIO Transport](#5-run-with-podmandocker---stdio-transport)\n    - [6. Run with Podman/Docker - HTTP/SSE Server](#6-run-with-podmandocker---httpsse-server)\n    - [Environment Variables](#environment-variables)\n  - [Getting Started](#getting-started)\n    - [Prerequisites](#prerequisites)\n    - [Installation](#installation)\n    - [Running the Server](#running-the-server)\n    - [Running Tests](#running-tests)\n    - [Docker Usage](#docker-usage)\n  - [Contributing](#contributing)\n  - [License](#license)\n\n## Motivation\n\nWhile exploring existing MCP server implementations for Home Assistant, I found that:\n\n- Many repositories did not provide a buildable or working Docker image, making deployment difficult or impossible.\n- Several projects did not actually implement the features or protocol described in their own README files.\n- Some solutions were outdated, unmaintained, or lacked clear documentation and test coverage.\n\n\nThis project aims to address these gaps by providing a reliable and fully functional MCP server with straightforward Docker support and a focus on real-world usability.\n\n> **Special Mention:**\n> Most of the documentation in this project is generated or assisted by AI. As this is a side project (even though I work as a developer), I focus on building fun and working features, and prefer to review and accept generated documentation if it's good enough, rather than writing everything by hand.\n\n## Features\n- Home Assistant client integration\n- Entity registry and management\n- Automation and configuration MCP endpoints\n- Lovelace dashboard support\n- **Multiple transport options**: stdio, Server-Sent Events (SSE), and Streamable HTTP\n- TypeScript codebase with Vitest for testing\n- Docker support for easy deployment\n\n## Available Tools\n\nThis MCP server provides **comprehensive tools** for interacting with Home Assistant, including:\n\n- **Automation Management**: Create, update, delete, and trace automations\n- **Entity Operations**: Query and manipulate Home Assistant entities\n- **Service Calls**: Execute Home Assistant services\n- **Configuration**: Validate and manage Home Assistant configuration\n- **Registry Access**: Access entity and device registries\n- **Config Entry Flow Helpers**: Create and manage Home Assistant integration flows\n\n> **‚ö†Ô∏è TOKEN COST WARNING**\n>\n> **Important:** Each tool definition consumes tokens in your LLM context, even if unused! Some tools (like `update-lovelace-config`) can cost 4,000+ tokens alone. Consider excluding unnecessary tools to minimize token consumption and maximize conversation efficiency.\n\nFor a complete list of all available tools with detailed descriptions and parameters, see the **[Tools Documentation](tools.md)**.\n\n### Generating Tools Documentation\n\nThe tools documentation is automatically generated from the MCP server:\n\n```sh\nnpm run generate-docs\n```\n\nThis command will:\n1. Extract all available tools from the MCP server using the inspector\n2. Generate a comprehensive markdown documentation file (`tools.md`)\n3. Clean up temporary files\n\n**Note:** The tools documentation shows the server currently provides **39 tools** for comprehensive Home Assistant integration.\n\n## Project Structure\n- `src/` - Main source code\n  - `hass/` - Home Assistant client and helpers\n  - `server/` - MCP server implementations\n  - `mcpTransports.ts` - Transport layer implementations (HTTP, SSE)\n- `tests/` - Test files and Home Assistant config examples\n- `scripts/` - Build and documentation generation scripts\n- `Dockerfile` - Containerization support\n- `package.json` - Project dependencies and scripts\n- `tsconfig.json` - TypeScript configuration\n- `vitest.config.ts` - Vitest test runner configuration\n\n\n\n## Example: Running the MCP Server\n\nThe MCP server supports multiple transport methods. You can run it using the same command and argument structure as in your `mcp_settings.json`.\n\n### 1. Run with STDIO Transport (Default)\n\nFor traditional MCP client integration via stdio (server is launched by MCP client):\n\n```jsonc\n{\n  \"type\": \"stdio\",\n  \"command\": \"tsx\",\n  \"args\": [\n    \"c:/dev/git/aha-mcp/src/index.ts\"\n  ],\n  \"env\": {\n    \"LIMIT_RESOURCES\": \"-1\",\n    \"RESOURCES_TO_TOOLS\": \"true\",\n    \"DEBUG\": \"true\",\n    \"HASS_URL\": \"https://your-home-assistant.local:8123\",\n    \"HASS_ACCESS_TOKEN\": \"<your_token_here>\"\n  }\n}\n```\n\n### 2. Run with Server-Sent Events (SSE) Transport\n\nFor SSE-based MCP communication, you need to run the server separately and then configure the MCP client to connect via URL.\n\n**Step 1: Start the SSE server**\n```bash\n# Start the server with SSE transport using npm script\nHASS_URL=https://your-home-assistant.local:8123 HASS_ACCESS_TOKEN=<your_token_here> npm run start:local:sse\n\n# Or with additional configuration\nRESOURCES_TO_TOOLS=true DEBUG=true HASS_URL=https://your-home-assistant.local:8123 HASS_ACCESS_TOKEN=<your_token_here> npm run start:local:sse\n```\n\n**Step 2: Configure MCP client to connect via URL**\n```jsonc\n{\n  \"url\": \"http://localhost:8081/sse\",\n  \"alwaysAllow\": [\n    // your allowed tools here\n  ]\n}\n```\n\n### 3. Run with Streamable HTTP Transport\n\nFor HTTP-based MCP communication, you need to run the server separately and then configure the MCP client to connect via URL.\n\n**Step 1: Start the HTTP server**\n```bash\n# Start the server with streamable HTTP transport using npm script\nHASS_URL=https://your-home-assistant.local:8123 HASS_ACCESS_TOKEN=<your_token_here> npm run start:local:http\n```\n\n**Step 2: Configure MCP client to connect via URL**\n```jsonc\n{\n  \"url\": \"http://localhost:8081/mcp\",\n  \"alwaysAllow\": [\n    // your allowed tools here\n  ]\n}\n```\n\n### 4. Quick SSE Server Startup\n\nStart the SSE server directly with npm scripts:\n\n```bash\n# Set your Home Assistant credentials first\nexport HASS_URL=https://your-home-assistant.local:8123\nexport HASS_ACCESS_TOKEN=<your_token_here>\n\n# Start SSE server with RESOURCES_TO_TOOLS enabled\nRESOURCES_TO_TOOLS=true DEBUG=true npm run start:local:sse\n```\n\nThen configure your MCP client with:\n```jsonc\n{\n  \"url\": \"http://localhost:3000/sse\"\n}\n```\n\n### 5. Run with Podman/Docker - STDIO Transport\n\nFor STDIO transport with containers:\n\n```jsonc\n{\n  \"type\": \"stdio\",\n  \"command\": \"podman\",\n  \"args\": [\n    \"run\",\n    \"-i\",\n    \"--rm\",\n    \"-e\",\n    \"HASS_URL=https://your-home-assistant.local:8123\",\n    \"-e\",\n    \"HASS_ACCESS_TOKEN=<your_token_here>\",\n    \"ghcr.io/slhad/aha-mcp:latest\"\n  ]\n}\n```\n\n### 6. Run with Podman/Docker - HTTP/SSE Server\n\nFor HTTP/SSE transports, run the server separately in a container:\n\n**Step 1: Start the server container**\n```bash\n# For SSE transport\npodman run -p 8081:8081 -e HASS_URL=https://your-home-assistant.local:8123 -e HASS_ACCESS_TOKEN=<your_token_here> ghcr.io/slhad/aha-mcp -- sse\n\n# For HTTP transport  \npodman run -p 8081:8081 -e HASS_URL=https://your-home-assistant.local:8123 -e HASS_ACCESS_TOKEN=<your_token_here> ghcr.io/slhad/aha-mcp -- http\n```\n\n**Step 2: Configure MCP client**\n```jsonc\n{\n  \"url\": \"http://localhost:3000/sse\",  // for SSE\n  // or\n  \"url\": \"http://localhost:3000/mcp\"   // for HTTP\n}\n```\n\nReplace `<your_token_here>` with your actual Home Assistant access token.\n\n### Environment Variables\n\nThe following environment variables can be set to configure the MCP server:\n\n- `HASS_URL` (required): The URL of your Home Assistant instance. Example: `https://your-home-assistant.local:8123` (default in code: `http://localhost:8123`)\n- `HASS_ACCESS_TOKEN` (required): Long-lived access token for Home Assistant. The server will not start without this.\n- `DEBUG`: Set to `true` to enable debug logging. Default: `false`.\n- `RESOURCES_TO_TOOLS`: Set to `true` to enable mapping resources to tools. Default: `false`.\n  - **Detailed explanation:**\n    When enabled, this option exposes Home Assistant resources (such as entities, automations, and services) as individual tools for MCP clients. This is especially useful for clients or agents that can only interact with the server via tool-based interfaces, rather than through generic resource queries. It allows such clients to discover and use Home Assistant features as discrete, callable tools, improving compatibility and usability for tool-limited environments.\n- `LIMIT_RESOURCES`: Set to a number to limit the number of resources returned by the server. Default: unlimited.\n\n**Note:** The server transport method is now controlled by command line arguments (`sse` or `http`) rather than environment variables.\n\n## Getting Started\n\n### Prerequisites\n- Node.js (v18+ recommended)\n- Docker (optional, for containerized deployment)\n\n### Installation\n```sh\nnpm install\n```\n\n### Running the Server\n\n**For STDIO transport (launched by MCP client):**\n```sh\n# Set environment variables and run with npm\nnpm start\n```\n\n**For HTTP/SSE transports (run server separately):**\n```sh\n# Run SSE server on port 8081\nnpm run start:local:sse\n\n# Run HTTP server on port 8081:8081\nnpm run start:local:http\n\n# Quick SSE server startup with environment variables\nRESOURCES_TO_TOOLS=true DEBUG=true npm run start:local:sse\n```\n\nThen configure your MCP client to connect via URL:\n- SSE: `\"url\": \"http://localhost:3000/sse\"`\n- HTTP: `\"url\": \"http://localhost:8080/mcp\"`\n\n\n### Running Tests\n```sh\nnpm run test:short\n```\n\n> **Note:**\n> The unit tests currently run against a real Home Assistant instance and are not mocked yet. This means tests require a live Home Assistant server and valid credentials to execute successfully.\n\n### Docker Usage\nBuild and run the server in a Docker container:\n\n```sh\n# Build the container\nnpm run docker\n```\n\nthen follow :\n\n - [Stdio](#5-run-with-podmandocker---stdio-transport)\n - [HTTP/SSE](#6-run-with-podmandocker---httpsse-server)\n  \nFor HTTP/SSE modes, then configure your MCP client with the appropriate URL:\n- SSE: `\"url\": \"http://localhost:3000/sse\"`\n- HTTP: `\"url\": \"http://localhost:8081/mcp\"`\n\n## Contributing\nPull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.\n\n## License\nSee [LICENSE](LICENSE) for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "List and query Home Assistant entities such as lights, sensors, and switches",
        "Retrieve and update the state of Home Assistant entities",
        "Call Home Assistant services to control devices",
        "Manage automations including listing, creating, updating, and validating them",
        "Access and update the Home Assistant entity registry",
        "Integrate and update Lovelace dashboards",
        "Validate Home Assistant configuration",
        "Search for entities by prefix or regular expression",
        "Support multiple transport methods including STDIO, Server-Sent Events (SSE), and Streamable HTTP"
      ],
      "limitations": [
        "Server requires a valid Home Assistant long-lived access token and URL to operate",
        "Token cost warning: some tools consume large amounts of LLM context tokens, which may impact conversation efficiency",
        "Transport method must be specified via command line arguments; environment variables no longer control transport",
        "No explicit mention of rate limits or concurrency constraints in documentation"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "Home Assistant instance URL (HASS_URL environment variable)",
        "Home Assistant long-lived access token (HASS_ACCESS_TOKEN environment variable)",
        "Docker or Podman for containerized deployment (optional)",
        "MCP client configured to connect via appropriate transport (stdio, SSE, or HTTP)"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples for multiple transports, detailed feature and tool descriptions, environment variable explanations, and prerequisites, but lacks explicit rate limit details.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# AHA Model Context Protocol (MCP) Server\n\n## Quick install links\n**STDIO**  \n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_AHA--MCP-0098FF?style=for-the-badge&logo=visualstudiocode&logoColor=ffffff)](https://insiders.vscode.dev/redirect/mcp/install?name=aha-mcp&config=%7B%22type%22%3A%20%22stdio%22%2C%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22--pull%22%2C%22always%22%2C%22-i%22%2C%22--rm%22%2C%22-e%22%2C%22TRANSPORT%3Dstdio%22%2C%22-e%22%2C%22RESOURCES_TO_TOOLS%3Dtrue%22%2C%22-e%22%2C%22HASS_URL%22%2C%22-e%22%2C%22HASS_ACCESS_TOKEN%22%2C%22ghcr.io%2Fslhad%2Faha-mcp%3Alatest%22%5D%2C%22env%22%3A%7B%22HASS_ACCESS_TOKEN%22%3A%22%24%7Binput%3AHASS_ACCESS_TOKEN%7D%22%2C%22HASS_URL%22%3A%22%24%7Binput%3AHASS_URL%7D%22%7D%7D)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=aha-mcp&config=eyJjb21tYW5kIjoiZG9ja2VyIHJ1biAtLXB1bGwgYWx3YXlzIC1pIC0tcm0gLWUgVFJBTlNQT1JUPXN0ZGlvIC1lIFJFU09VUkNFU19UT19UT09MUz10cnVlIC1lIEhBU1NfVVJMPWh0dHBzOi8vaGFfaW5zdGFuY2UgLWUgSEFTU19BQ0NFU1NfVE9LRU49aGFfbG9uZ19saXZlZF9hY2Nlc3NfdG9rZW4gZ2hjci5pby9zbGhhZC9haGEtbWNwOmxhdGVzdCJ9)\n\n**Others**  \n[![smithery badge](https://smithery.ai/badge/@slhad/aha-mcp)](https://smithery.ai/server/@slhad/aha-mcp)\n\nPretty useful to test it without IDE\n\n## AHA stands for Another Home Assistant MCP Server\n\n\nThis repository implements a Model Context Protocol (MCP) server for Home Assistant, providing a bridge between Home Assistant and MCP clients.",
        "start_pos": 0,
        "end_pos": 1526,
        "token_count_estimate": 381,
        "source_type": "readme",
        "agent_id": "54660031fc7ea91b"
      },
      {
        "chunk_id": 1,
        "text": "egrate and update Lovelace dashboards\n- Validate Home Assistant configuration\n- Search for entities by prefix or regex\n- Access entity sources and registry information\n\nThe server supports multiple transport methods:\n- **STDIO**: Traditional MCP client communication (default)\n- **Server-Sent Events (SSE)**: Web-based real-time communication\n- **Streamable HTTP**: HTTP-based MCP communication for web integration\n\n## Table of Contents\n\n- [AHA Model Context Protocol (MCP) Server](#aha-model-context-protocol-mcp-server)\n  - [Quick install links](#quick-install-links)\n  - [AHA stands for Another Home Assistant MCP Server](#aha-stands-for-another-home-assistant-mcp-server)\n  - [Table of Contents](#table-of-contents)\n  - [Motivation](#motivation)\n  - [Features](#features)\n  - [Available Tools](#available-tools)\n    - [Generating Tools Documentation](#generating-tools-documentation)\n  - [Project Structure](#project-structure)\n  - [Example: Running the MCP Server](#example-running-the-mcp-server)\n    - [1. Run with STDIO Transport (Default)](#1-run-with-stdio-transport-default)\n    - [2. Run with Server-Sent Events (SSE) Transport](#2-run-with-server-sent-events-sse-transport)\n    - [3. Run with Streamable HTTP Transport](#3-run-with-streamable-http-transport)\n    - [4. Quick SSE Server Startup](#4-quick-sse-server-startup)\n    - [5. Run with Podman/Docker - STDIO Transport](#5-run-with-podmandocker---stdio-transport)\n    - [6.",
        "start_pos": 1848,
        "end_pos": 3290,
        "token_count_estimate": 360,
        "source_type": "readme",
        "agent_id": "54660031fc7ea91b"
      },
      {
        "chunk_id": 2,
        "text": "ng)\n  - [License](#license)\n\n## Motivation\n\nWhile exploring existing MCP server implementations for Home Assistant, I found that:\n\n- Many repositories did not provide a buildable or working Docker image, making deployment difficult or impossible.\n- Several projects did not actually implement the features or protocol described in their own README files.\n- Some solutions were outdated, unmaintained, or lacked clear documentation and test coverage.\n\n\nThis project aims to address these gaps by providing a reliable and fully functional MCP server with straightforward Docker support and a focus on real-world usability.\n\n> **Special Mention:**\n> Most of the documentation in this project is generated or assisted by AI. As this is a side project (even though I work as a developer), I focus on building fun and working features, and prefer to review and accept generated documentation if it's good enough, rather than writing everything by hand.\n\n## Features\n- Home Assistant client integration\n- Entity registry and management\n- Automation and configuration MCP endpoints\n- Lovelace dashboard support\n- **Multiple transport options**: stdio, Server-Sent Events (SSE), and Streamable HTTP\n- TypeScript codebase with Vitest for testing\n- Docker support for easy deployment\n\n## Available Tools\n\nThis MCP server provides **comprehensive tools** for interacting with Home Assistant, including:\n\n- **Automation Management**: Create, update, delete, and trace automations\n- **Entity Operations**: Query and manipulate Home Assistant entities\n- **Service Calls**: Execute Home Assistant services\n- **Configuration**: Validate and manage Home Assistant configuration\n- **Registry Access**: Access entity and device registries\n- **Config Entry Flow Helpers**: Create and manage Home Assistant integration flows\n\n> **‚ö†Ô∏è TOKEN COST WARNING**\n>\n> **Important:** Each tool definition consumes tokens in your LLM context, even if unused! Some tools (like `update-lovelace-config`) can cost 4,000+ tokens alone.",
        "start_pos": 3696,
        "end_pos": 5693,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "54660031fc7ea91b"
      },
      {
        "chunk_id": 3,
        "text": "flows\n\n> **‚ö†Ô∏è TOKEN COST WARNING**\n>\n> **Important:** Each tool definition consumes tokens in your LLM context, even if unused! Some tools (like `update-lovelace-config`) can cost 4,000+ tokens alone. Consider excluding unnecessary tools to minimize token consumption and maximize conversation efficiency.\n\nFor a complete list of all available tools with detailed descriptions and parameters, see the **[Tools Documentation](tools.md)**.\n\n### Generating Tools Documentation\n\nThe tools documentation is automatically generated from the MCP server:\n\n```sh\nnpm run generate-docs\n```\n\nThis command will:\n1. Extract all available tools from the MCP server using the inspector\n2. Generate a comprehensive markdown documentation file (`tools.md`)\n3. Clean up temporary files\n\n**Note:** The tools documentation shows the server currently provides **39 tools** for comprehensive Home Assistant integration.\n\n## Project Structure\n- `src/` - Main source code\n  - `hass/` - Home Assistant client and helpers\n  - `server/` - MCP server implementations\n  - `mcpTransports.ts` - Transport layer implementations (HTTP, SSE)\n- `tests/` - Test files and Home Assistant config examples\n- `scripts/` - Build and documentation generation scripts\n- `Dockerfile` - Containerization support\n- `package.json` - Project dependencies and scripts\n- `tsconfig.json` - TypeScript configuration\n- `vitest.config.ts` - Vitest test runner configuration\n\n\n\n## Example: Running the MCP Server\n\nThe MCP server supports multiple transport methods. You can run it using the same command and argument structure as in your `mcp_settings.json`.\n\n### 1.",
        "start_pos": 5493,
        "end_pos": 7104,
        "token_count_estimate": 402,
        "source_type": "readme",
        "agent_id": "54660031fc7ea91b"
      },
      {
        "chunk_id": 4,
        "text": "{\n    \"LIMIT_RESOURCES\": \"-1\",\n    \"RESOURCES_TO_TOOLS\": \"true\",\n    \"DEBUG\": \"true\",\n    \"HASS_URL\": \"https://your-home-assistant.local:8123\",\n    \"HASS_ACCESS_TOKEN\": \"<your_token_here>\"\n  }\n}\n```\n\n### 2. Run with Server-Sent Events (SSE) Transport\n\nFor SSE-based MCP communication, you need to run the server separately and then configure the MCP client to connect via URL.\n\n**Step 1: Start the SSE server**\n```bash\n# Start the server with SSE transport using npm script\nHASS_URL=https://your-home-assistant.local:8123 HASS_ACCESS_TOKEN=<your_token_here> npm run start:local:sse\n\n# Or with additional configuration\nRESOURCES_TO_TOOLS=true DEBUG=true HASS_URL=https://your-home-assistant.local:8123 HASS_ACCESS_TOKEN=<your_token_here> npm run start:local:sse\n```\n\n**Step 2: Configure MCP client to connect via URL**\n```jsonc\n{\n  \"url\": \"http://localhost:8081/sse\",\n  \"alwaysAllow\": [\n    // your allowed tools here\n  ]\n}\n```\n\n### 3. Run with Streamable HTTP Transport\n\nFor HTTP-based MCP communication, you need to run the server separately and then configure the MCP client to connect via URL.\n\n**Step 1: Start the HTTP server**\n```bash\n# Start the server with streamable HTTP transport using npm script\nHASS_URL=https://your-home-assistant.local:8123 HASS_ACCESS_TOKEN=<your_token_here> npm run start:local:http\n```\n\n**Step 2: Configure MCP client to connect via URL**\n```jsonc\n{\n  \"url\": \"http://localhost:8081/mcp\",\n  \"alwaysAllow\": [\n    // your allowed tools here\n  ]\n}\n```\n\n### 4. Quick SSE Server Startup\n\nStart the SSE server directly with npm scripts:\n\n```bash\n# Set your Home Assistant credentials first\nexport HASS_URL=https://your-home-assistant.local:8123\nexport HASS_ACCESS_TOKEN=<your_token_here>\n\n# Start SSE server with RESOURCES_TO_TOOLS enabled\nRESOURCES_TO_TOOLS=true DEBUG=true npm run start:local:sse\n```\n\nThen configure your MCP client with:\n```jsonc\n{\n  \"url\": \"http://localhost:3000/sse\"\n}\n```\n\n### 5.",
        "start_pos": 7341,
        "end_pos": 9270,
        "token_count_estimate": 482,
        "source_type": "readme",
        "agent_id": "54660031fc7ea91b"
      },
      {
        "chunk_id": 5,
        "text": "erver with RESOURCES_TO_TOOLS enabled\nRESOURCES_TO_TOOLS=true DEBUG=true npm run start:local:sse\n```\n\nThen configure your MCP client with:\n```jsonc\n{\n  \"url\": \"http://localhost:3000/sse\"\n}\n```\n\n### 5. Run with Podman/Docker - STDIO Transport\n\nFor STDIO transport with containers:\n\n```jsonc\n{\n  \"type\": \"stdio\",\n  \"command\": \"podman\",\n  \"args\": [\n    \"run\",\n    \"-i\",\n    \"--rm\",\n    \"-e\",\n    \"HASS_URL=https://your-home-assistant.local:8123\",\n    \"-e\",\n    \"HASS_ACCESS_TOKEN=<your_token_here>\",\n    \"ghcr.io/slhad/aha-mcp:latest\"\n  ]\n}\n```\n\n### 6. Run with Podman/Docker - HTTP/SSE Server\n\nFor HTTP/SSE transports, run the server separately in a container:\n\n**Step 1: Start the server container**\n```bash\n# For SSE transport\npodman run -p 8081:8081 -e HASS_URL=https://your-home-assistant.local:8123 -e HASS_ACCESS_TOKEN=<your_token_here> ghcr.io/slhad/aha-mcp -- sse\n\n# For HTTP transport  \npodman run -p 8081:8081 -e HASS_URL=https://your-home-assistant.local:8123 -e HASS_ACCESS_TOKEN=<your_token_here> ghcr.io/slhad/aha-mcp -- http\n```\n\n**Step 2: Configure MCP client**\n```jsonc\n{\n  \"url\": \"http://localhost:3000/sse\",  // for SSE\n  // or\n  \"url\": \"http://localhost:3000/mcp\"   // for HTTP\n}\n```\n\nReplace `<your_token_here>` with your actual Home Assistant access token.\n\n### Environment Variables\n\nThe following environment variables can be set to configure the MCP server:\n\n- `HASS_URL` (required): The URL of your Home Assistant instance. Example: `https://your-home-assistant.local:8123` (default in code: `http://localhost:8123`)\n- `HASS_ACCESS_TOKEN` (required): Long-lived access token for Home Assistant. The server will not start without this.\n- `DEBUG`: Set to `true` to enable debug logging. Default: `false`.\n- `RESOURCES_TO_TOOLS`: Set to `true` to enable mapping resources to tools. Default: `false`.\n  - **Detailed explanation:**\n    When enabled, this option exposes Home Assistant resources (such as entities, automations, and services) as individual tools for MCP clients.",
        "start_pos": 9070,
        "end_pos": 11066,
        "token_count_estimate": 499,
        "source_type": "readme",
        "agent_id": "54660031fc7ea91b"
      },
      {
        "chunk_id": 6,
        "text": "tools. Default: `false`.\n  - **Detailed explanation:**\n    When enabled, this option exposes Home Assistant resources (such as entities, automations, and services) as individual tools for MCP clients. This is especially useful for clients or agents that can only interact with the server via tool-based interfaces, rather than through generic resource queries. It allows such clients to discover and use Home Assistant features as discrete, callable tools, improving compatibility and usability for tool-limited environments.\n- `LIMIT_RESOURCES`: Set to a number to limit the number of resources returned by the server. Default: unlimited.\n\n**Note:** The server transport method is now controlled by command line arguments (`sse` or `http`) rather than environment variables.\n\n## Getting Started\n\n### Prerequisites\n- Node.js (v18+ recommended)\n- Docker (optional, for containerized deployment)\n\n### Installation\n```sh\nnpm install\n```\n\n### Running the Server\n\n**For STDIO transport (launched by MCP client):**\n```sh\n# Set environment variables and run with npm\nnpm start\n```\n\n**For HTTP/SSE transports (run server separately):**\n```sh\n# Run SSE server on port 8081\nnpm run start:local:sse\n\n# Run HTTP server on port 8081:8081\nnpm run start:local:http\n\n# Quick SSE server startup with environment variables\nRESOURCES_TO_TOOLS=true DEBUG=true npm run start:local:sse\n```\n\nThen configure your MCP client to connect via URL:\n- SSE: `\"url\": \"http://localhost:3000/sse\"`\n- HTTP: `\"url\": \"http://localhost:8080/mcp\"`\n\n\n### Running Tests\n```sh\nnpm run test:short\n```\n\n> **Note:**\n> The unit tests currently run against a real Home Assistant instance and are not mocked yet. This means tests require a live Home Assistant server and valid credentials to execute successfully.",
        "start_pos": 10866,
        "end_pos": 12631,
        "token_count_estimate": 441,
        "source_type": "readme",
        "agent_id": "54660031fc7ea91b"
      },
      {
        "chunk_id": 7,
        "text": "he container\nnpm run docker\n```\n\nthen follow :\n\n - [Stdio](#5-run-with-podmandocker---stdio-transport)\n - [HTTP/SSE](#6-run-with-podmandocker---httpsse-server)\n  \nFor HTTP/SSE modes, then configure your MCP client with the appropriate URL:\n- SSE: `\"url\": \"http://localhost:3000/sse\"`\n- HTTP: `\"url\": \"http://localhost:8081/mcp\"`\n\n## Contributing\nPull requests are welcome! For major changes, please open an issue first to discuss what you would like to change.\n\n## License\nSee [LICENSE](LICENSE) for details.",
        "start_pos": 12714,
        "end_pos": 13223,
        "token_count_estimate": 127,
        "source_type": "readme",
        "agent_id": "54660031fc7ea91b"
      }
    ]
  },
  {
    "agent_id": "bee3d39ca5781fe6",
    "name": "ai.smithery/smithery-ai-cookbook-python-quickstart",
    "source": "mcp",
    "source_url": "https://github.com/smithery-ai/smithery-cookbook",
    "description": "A simple MCP server built with FastMCP and python",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T16:07:02.461935Z",
    "indexed_at": "2026-02-18T04:08:59.674549",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Smithery Cookbook\n\nThe Smithery Cookbook provides code examples and guides designed to help developers build MCP (Model Context Protocol) servers and clients, offering copy-able code snippets that you can easily integrate into your own projects.\n\n## Prerequisites\n\nTo make the most of the examples in this cookbook, you'll need:\n\n- **Smithery CLI**: Install with `npm install -g @smithery/cli` to access the interactive playground and development tools\n- **Programming language runtimes** for the examples you want to explore (Python 3.12+, Node.js 18+, etc.)\n- Basic understanding of the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/docs/getting-started/intro)\n\n## Quick Start\n\n1. **Clone this repository:**\n   ```bash\n   git clone https://github.com/smithery-ai/smithery-cookbook.git\n   cd smithery-cookbook\n   ```\n\n2. **Choose an example and follow its README:**\n   ```bash\n   cd servers/python/quickstart\n   # Follow the README.md instructions\n   ```\n\n3. **Test with Smithery Playground:**\n   ```bash\n   npx @smithery/cli playground --port 8081\n   ```\n   \n   *Note: Replace `8081` with the port your server is running on*\n\n4. **Deploy to Smithery (optional):**\n   Ready to share your MCP server? [Deploy it here](https://smithery.ai/new) to host it on Smithery's platform.\n\n## Table of Recipes\n\n### Python Servers\n- **[FastMCP Quickstart](servers/python/quickstart/)** - Basic server with greeting tool\n- **[FastMCP Advanced](servers/python/server_with_session_config/)** - Server with session configuration\n- **[Migrate STDIO to HTTP](servers/python/migrate_stdio_to_http/)** - Server with custom Docker container\n\n### TypeScript Servers\n- **[TypeScript Quickstart](servers/typescript/quickstart/)** - Simple server with character counting tool\n- **[TypeScript Session Config](servers/typescript/server_with_session_config/)** - Server with session configuration\n- **[Migrate STDIO to HTTP - Smithery CLI](servers/typescript/migrate_stdio_to_http/server_with_smithery_cli/)** - Server using Smithery CLI\n- **[Migrate STDIO to HTTP - Custom Container](servers/typescript/migrate_stdio_to_http/server_with_custom_container/)** - Server with custom Docker container\n\n## Development Workflow\n\nBuild and distribute your MCP servers with Smithery:\n\n1. **Build** your MCP server using the language and framework of your choice\n2. **Test** interactively with `npx @smithery/cli playground`\n3. **Debug** with real-time request/response inspection\n4. **Deploy** to Smithery's hosted platform - [Deploy here](https://smithery.ai/new)\n5. **Distribute** your server gets its own page at `smithery.ai/server/{name}` for others to discover and use\n\n## Explore Further\n\nLooking for more resources to enhance your MCP development experience?\n\n- [Smithery Documentation](https://docs.smithery.ai) - Complete guides and API reference\n- [Deploy Your MCP Server](https://smithery.ai/new) - Host your server on Smithery's platform\n- [Model Context Protocol Specification](https://modelcontextprotocol.io/docs/getting-started/intro) - Official MCP documentation\n- [Smithery Discord Community](https://discord.gg/sKd9uycgH9) - Get help and share your projects\n\nIf you have ideas for new examples or guides, share them on the [issues page](https://github.com/smithery-ai/smithery-cookbook/issues).\n\n## License\n\nMIT License\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide code examples and guides for building MCP servers and clients",
        "Offer copy-able code snippets for easy integration into projects",
        "Support multiple programming languages including Python and TypeScript",
        "Enable interactive testing and debugging with Smithery CLI playground",
        "Facilitate deployment of MCP servers to Smithery's hosted platform",
        "Allow distribution and discovery of MCP servers via smithery.ai server pages"
      ],
      "limitations": [
        "Requires specific runtime environments (e.g., Python 3.12+, Node.js 18+)",
        "Does not provide a standalone MCP server implementation but rather examples and recipes",
        "Deployment is optional and tied to Smithery's platform for hosting",
        "Assumes user has basic understanding of the Model Context Protocol"
      ],
      "requirements": [
        "Smithery CLI installed globally via npm",
        "Appropriate programming language runtime installed (Python 3.12+, Node.js 18+)",
        "Basic knowledge of the Model Context Protocol",
        "Git for cloning the repository"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, usage examples, tool descriptions, prerequisites, and links to further resources, providing a comprehensive guide for developers.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Smithery Cookbook\n\nThe Smithery Cookbook provides code examples and guides designed to help developers build MCP (Model Context Protocol) servers and clients, offering copy-able code snippets that you can easily integrate into your own projects.\n\n## Prerequisites\n\nTo make the most of the examples in this cookbook, you'll need:\n\n- **Smithery CLI**: Install with `npm install -g @smithery/cli` to access the interactive playground and development tools\n- **Programming language runtimes** for the examples you want to explore (Python 3.12+, Node.js 18+, etc.)\n- Basic understanding of the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/docs/getting-started/intro)\n\n## Quick Start\n\n1. **Clone this repository:**\n   ```bash\n   git clone https://github.com/smithery-ai/smithery-cookbook.git\n   cd smithery-cookbook\n   ```\n\n2. **Choose an example and follow its README:**\n   ```bash\n   cd servers/python/quickstart\n   # Follow the README.md instructions\n   ```\n\n3. **Test with Smithery Playground:**\n   ```bash\n   npx @smithery/cli playground --port 8081\n   ```\n   \n   *Note: Replace `8081` with the port your server is running on*\n\n4. **Deploy to Smithery (optional):**\n   Ready to share your MCP server? [Deploy it here](https://smithery.ai/new) to host it on Smithery's platform.",
        "start_pos": 0,
        "end_pos": 1296,
        "token_count_estimate": 324,
        "source_type": "readme",
        "agent_id": "bee3d39ca5781fe6"
      },
      {
        "chunk_id": 1,
        "text": ")** - Server with session configuration\n- **[Migrate STDIO to HTTP - Smithery CLI](servers/typescript/migrate_stdio_to_http/server_with_smithery_cli/)** - Server using Smithery CLI\n- **[Migrate STDIO to HTTP - Custom Container](servers/typescript/migrate_stdio_to_http/server_with_custom_container/)** - Server with custom Docker container\n\n## Development Workflow\n\nBuild and distribute your MCP servers with Smithery:\n\n1. **Build** your MCP server using the language and framework of your choice\n2. **Test** interactively with `npx @smithery/cli playground`\n3. **Debug** with real-time request/response inspection\n4. **Deploy** to Smithery's hosted platform - [Deploy here](https://smithery.ai/new)\n5. **Distribute** your server gets its own page at `smithery.ai/server/{name}` for others to discover and use\n\n## Explore Further\n\nLooking for more resources to enhance your MCP development experience?\n\n- [Smithery Documentation](https://docs.smithery.ai) - Complete guides and API reference\n- [Deploy Your MCP Server](https://smithery.ai/new) - Host your server on Smithery's platform\n- [Model Context Protocol Specification](https://modelcontextprotocol.io/docs/getting-started/intro) - Official MCP documentation\n- [Smithery Discord Community](https://discord.gg/sKd9uycgH9) - Get help and share your projects\n\nIf you have ideas for new examples or guides, share them on the [issues page](https://github.com/smithery-ai/smithery-cookbook/issues).\n\n## License\n\nMIT License",
        "start_pos": 1848,
        "end_pos": 3323,
        "token_count_estimate": 368,
        "source_type": "readme",
        "agent_id": "bee3d39ca5781fe6"
      }
    ]
  },
  {
    "agent_id": "d609402dc0960fc6",
    "name": "ai.smithery/smithery-ai-fetch",
    "source": "mcp",
    "source_url": "https://github.com/smithery-ai/mcp-servers",
    "description": "A simple tool that performs a fetch request to a webpage.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-14T03:19:26.387795Z",
    "indexed_at": "2026-02-18T04:09:01.057920",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Perform fetch requests to webpages"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence providing minimal information about the server's functionality.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "7204bb4d8f03273a",
    "name": "ai.smithery/smithery-toolbox",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@smithery/toolbox/mcp",
    "description": "Toolbox dynamically routes to all MCPs in the Smithery registry based on your agent's need. When a‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T20:39:40.188723Z",
    "indexed_at": "2026-02-18T04:09:02.734487",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Dynamically route requests to all MCPs in the Smithery registry based on agent needs"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with minimal detail, providing only a basic idea of dynamic routing without further elaboration.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "8068b8bb6d9f7c8f",
    "name": "ai.smithery/smithery-unicorn",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@smithery/unicorn",
    "description": "A choose your own adventure game where you play as a startup founder trying to build a unicorn again",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-09T15:54:28.824617Z",
    "indexed_at": "2026-02-18T04:09:04.906478",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide an interactive choose your own adventure game experience",
        "Simulate the role of a startup founder",
        "Allow users to make decisions to try to build a unicorn startup"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's purpose but lacks detailed information about features, usage, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "a9d0a436f94998a8",
    "name": "ai.smithery/sunub-obsidian-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/sunub/obsidian-mcp-server",
    "description": "Search your Obsidian vault to quickly find notes by title or keyword, summarize related content, a‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T13:40:45.500067Z",
    "indexed_at": "2026-02-18T04:09:07.139345",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Obsidian MCP Server\n\n`obsidian-mcp-server`Îäî [Model Context Protocol(MCP)](https://modelcontextprotocol.io/docs/getting-started/intro)ÏùÑ Íµ¨ÌòÑÌïú ÏÑúÎ≤ÑÎ°ú, Î°úÏª¨ Obsidian vaultÏùò Î¨∏ÏÑúÎì§ÏùÑ AI ÏóêÏù¥Ï†ÑÌä∏ÎÇò Ïô∏Î∂Ä Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóêÏÑú ÏâΩÍ≤å ÌÉêÏÉâÌïòÍ≥† Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÎèÑÎ°ù Í∞ïÎ†•Ìïú ÎèÑÍµ¨ APIÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.\n\nObsidian VaultÎ•º Ïù¥Ïö©Ìï¥ AIÍ∞Ä ÌôúÏö© Í∞ÄÎä•Ìïú ÏßÄÏãù Î≤†Ïù¥Ïä§(Knowledge Base)Î°ú ÌôïÏû•ÌïòÏó¨ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÍ≤åÎÅî ÌïòÍ≥† Î¨∏ÏÑú Í≤ÄÏÉâ, ÏöîÏïΩ, Ï†ïÎ¶¨ÏôÄ Í∞ôÏùÄ Î∂ÄÍ∞ÄÏ†ÅÏù∏ ÏûëÏóÖÏùÑ ÏûêÎèôÌôîÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä ÌïµÏã¨Ï†ÅÏù∏ \"Í∏ÄÏì∞Í∏∞ ÌôúÎèô\"ÏóêÎßå ÏßëÏ§ëÌï† Ïàò ÏûàÎäî ÌôòÍ≤ΩÏùÑ Íµ¨Ï∂ïÌïòÍ≥†Ïûê Ï†úÏûëÌñàÏäµÎãàÎã§.\n\n## ÌïµÏã¨ ÏïÑÌÇ§ÌÖçÏ≤ò\n\nÎ≥∏ ÏÑúÎ≤ÑÎäî `VaultManager`ÏôÄ `Indexer`Î•º Ï§ëÏã¨ÏúºÎ°ú Íµ¨Ï∂ïÎêòÏñ¥ ÎåÄÍ∑úÎ™® VaultÏóêÏÑúÎèÑ ÎÜíÏùÄ ÏÑ±Îä•Í≥º Î©îÎ™®Î¶¨ Ìö®Ïú®ÏÑ±ÏùÑ Î≥¥Ïû•Ìï©ÎãàÎã§.\n\n- **`Indexer` Í∏∞Î∞ò Í≤ÄÏÉâ**: ÏÑúÎ≤Ñ ÏãúÏûë Ïãú Í∞ÄÎ≤ºÏö¥ Ïó≠ Ïù∏Îç±Ïä§(Inverted Index)Î•º ÏÉùÏÑ±ÌïòÏó¨ ÌÇ§ÏõåÎìú Í≤ÄÏÉâ Ïãú Í±∞Ïùò Ï¶âÍ∞ÅÏ†ÅÏù∏ Í≤∞Í≥ºÎ•º Î∞òÌôòÌï©ÎãàÎã§(O(1)). Ï†ÑÏ≤¥ ÌååÏùº ÎÇ¥Ïö©ÏùÑ Î©îÎ™®Î¶¨Ïóê ÏÉÅÏ£ºÏãúÌÇ§ÏßÄ ÏïäÏïÑ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ ÏµúÏÜåÌôîÌï©ÎãàÎã§.\n- **`VaultManager`**: Vault ÎÇ¥Ïùò Î™®Îì† Î¨∏ÏÑúÎ•º Ìö®Ïú®Ï†ÅÏúºÎ°ú Í¥ÄÎ¶¨ÌïòÎ©∞, ÌååÏùº ÏãúÏä§ÌÖúÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÏó¨ Î¨∏ÏÑúÏùò ÏÉùÏÑ±, ÏàòÏ†ï, ÏÇ≠Ï†úÎ•º Ï≤òÎ¶¨Ìï©ÎãàÎã§.\n\n## Ï£ºÏöî Í∏∞Îä•\n\n- **Í≥†Í∏â Î¨∏ÏÑú ÌÉêÏÉâ**: `vault` ÎèÑÍµ¨Î•º ÌÜµÌï¥ ÌÇ§ÏõåÎìú Í≤ÄÏÉâ, Ï†ÑÏ≤¥ Î™©Î°ù Ï°∞Ìöå, ÌäπÏ†ï Î¨∏ÏÑú ÏùΩÍ∏∞, ÌÜµÍ≥Ñ Î∂ÑÏÑù Îì± Îã§ÏñëÌïú ÌÉêÏÉâ Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n- **Ïª®ÌÖçÏä§Ìä∏ ÏàòÏßë/Í∏∞Ïñµ Ìå®ÌÇ∑ ÏÉùÏÑ±**: `vault collect_context`Î°ú Î¨∏ÏÑú Î∞∞Ïπò ÏàòÏßë, ÏïïÏ∂ï, continuation ÌÜ†ÌÅ∞ Î∞úÍ∏â, Î©îÎ™®Î¶¨ Ìå®ÌÇ∑(JSON canonical)ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n- **Ï†ÄÏû•Îêú Î©îÎ™®Î¶¨ Ïû¨Ìò∏Ï∂ú**: `vault load_memory`Î°ú `memory/context_memory_snapshot.v1.md`Î•º Îπ†Î•¥Í≤å Î°úÎìúÌï¥ Îã§Ïùå ÌÑ¥ Ïª®ÌÖçÏä§Ìä∏Î°ú Ïû¨ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n- **AI Í∏∞Î∞ò ÏÜçÏÑ± ÏÉùÏÑ±**: `generate_property` ÎèÑÍµ¨Îäî Î¨∏ÏÑú Î≥∏Î¨∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ `title`, `tags`, `summary` Îì± Ï†ÅÏ†àÌïú frontmatter ÏÜçÏÑ±ÏùÑ ÏûêÎèôÏúºÎ°ú ÏÉùÏÑ±Ìï©ÎãàÎã§.\n- **ÏïàÏ†ÑÌïú ÏÜçÏÑ± ÏóÖÎç∞Ïù¥Ìä∏**: `write_property` ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÏÉùÏÑ±Îêú ÏÜçÏÑ±ÏùÑ Í∏∞Ï°¥ frontmatterÏôÄ Î≥ëÌï©ÌïòÏó¨ ÌååÏùºÏóê ÏïàÏ†ÑÌïòÍ≤å Í∏∞Î°ùÌï©ÎãàÎã§.\n- **Ï≤®Î∂Ä ÌååÏùº ÏûêÎèô Ï†ïÎ¶¨**: `organize_attachments` ÎèÑÍµ¨Îäî Î¨∏ÏÑúÏôÄ Ïó∞Í≤∞Îêú Ï≤®Î∂Ä ÌååÏùº(Ïòà: Ïù¥ÎØ∏ÏßÄ)ÏùÑ ÏûêÎèôÏúºÎ°ú Í∞êÏßÄÌïòÏó¨ Î¨∏ÏÑú Ï†úÎ™©Ïóê ÎßûÎäî Ìè¥ÎçîÎ°ú Ïù¥ÎèôÏãúÌÇ§Í≥† ÎßÅÌÅ¨Î•º ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§.\n- **ÌÜµÌï© ÏõåÌÅ¨ÌîåÎ°úÏö∞**: `create_document_with_properties`ÏôÄ Í∞ôÏùÄ ÎèÑÍµ¨Î•º ÌÜµÌï¥ Î¨∏ÏÑú Î∂ÑÏÑùÎ∂ÄÌÑ∞ ÏÜçÏÑ± ÏÉùÏÑ±, ÌååÏùº ÏóÖÎç∞Ïù¥Ìä∏ÍπåÏßÄÏùò Ï†ÑÏ≤¥ Í≥ºÏ†ïÏùÑ Îã®Ïùº Î™ÖÎ†πÏúºÎ°ú Ïã§ÌñâÌï©ÎãàÎã§.\n- **Ïã†Î¢∞ÏÑ± Î∞è ÌÖåÏä§Ìä∏**: `vitest`Î•º ÏÇ¨Ïö©Ìïú End-to-End ÌÖåÏä§Ìä∏ÏôÄ GitHub Actions Í∏∞Î∞òÏùò CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÌÜµÌï¥ ÏÑúÎ≤ÑÏùò ÏïàÏ†ïÏÑ±Í≥º Í∞Å ÎèÑÍµ¨ APIÏùò ÏùëÎãµ Ïä§ÌÇ§ÎßàÎ•º Í≤ÄÏ¶ùÌï©ÎãàÎã§.\n\n## ÎèÑÍµ¨ API\n\n`obsidian-mcp-server`Îäî MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î•º ÌÜµÌï¥ Ìò∏Ï∂úÌï† Ïàò ÏûàÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ ÎèÑÍµ¨Îì§ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n\n### `vault`\n\nVault ÎÇ¥ Î¨∏ÏÑúÎ•º ÌÉêÏÉâÌïòÍ≥† Î∂ÑÏÑùÌïòÎäî ÌïµÏã¨ ÎèÑÍµ¨ÏûÖÎãàÎã§. `action` ÌååÎùºÎØ∏ÌÑ∞Î•º ÌÜµÌï¥ Îã§ÏñëÌïú Í∏∞Îä•ÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n- **`list_all`**: Vault ÎÇ¥ Î™®Îì† Î¨∏ÏÑúÏùò Î™©Î°ùÍ≥º Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Î∞òÌôòÌï©ÎãàÎã§.\n- **`search`**: ÌÇ§ÏõåÎìúÎ•º Í∏∞Î∞òÏúºÎ°ú Î¨∏ÏÑú Ï†úÎ™©, ÎÇ¥Ïö©, ÌÉúÍ∑∏Î•º Í≤ÄÏÉâÌï©ÎãàÎã§.\n- **`read`**: ÌäπÏ†ï ÌååÏùºÏùò ÎÇ¥Ïö©ÏùÑ ÏùΩÍ≥† frontmatterÏôÄ Î≥∏Î¨∏ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.\n- **`stats`**: Vault ÎÇ¥ Î™®Îì† Î¨∏ÏÑúÏùò ÌÜµÍ≥Ñ(Îã®Ïñ¥, Í∏ÄÏûê Ïàò Îì±)Î•º Ï†úÍ≥µÌï©ÎãàÎã§.\n- **`collect_context`**: Î¨∏ÏÑúÎ•º Î∞∞Ïπò Ï≤òÎ¶¨ÌïòÏó¨ Î©îÎ™®Î¶¨ Ìå®ÌÇ∑ÏùÑ ÏÉùÏÑ±ÌïòÍ≥†, ÌïÑÏöî Ïãú `memory/context_memory_snapshot.v1.md`Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§.\n- **`load_memory`**: Ï†ÄÏû•Îêú Î©îÎ™®Î¶¨ ÎÖ∏Ìä∏Ïùò canonical JSON Î∏îÎ°ùÏùÑ ÌååÏã±ÌïòÏó¨ Îπ†Î•∏ Ïû¨Ï£ºÏûÖÏö© payloadÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n\n### `generate_property`\n\nÎ¨∏ÏÑú Í≤ΩÎ°ú(`filePath`)Î•º ÏûÖÎ†•Î∞õÏïÑ Ìï¥Îãπ Î¨∏ÏÑúÏùò ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌïòÍ≥†, AIÍ∞Ä Ï∂îÏ≤úÌïòÎäî frontmatter ÏÜçÏÑ±ÏùÑ ÏÉùÏÑ±ÌïòÏó¨ Î∞òÌôòÌï©ÎãàÎã§.\n\n### `write_property`\n\nÌååÏùº Í≤ΩÎ°ú(`filePath`)ÏôÄ JSON ÌòïÏãùÏùò ÏÜçÏÑ±(`properties`)ÏùÑ ÏûÖÎ†•Î∞õÏïÑ, Ìï¥Îãπ ÌååÏùºÏùò frontmatterÎ•º ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§.\n\n### `create_document_with_properties`\n\nÎ¨∏ÏÑú Î∂ÑÏÑù, ÏÜçÏÑ± ÏÉùÏÑ±, ÌååÏùº ÏóÖÎç∞Ïù¥Ìä∏Ïùò Ï†Ñ Í≥ºÏ†ïÏùÑ Ìïú Î≤àÏóê Ï≤òÎ¶¨ÌïòÎäî ÌÜµÌï© ÎèÑÍµ¨ÏûÖÎãàÎã§.\n\n### `organize_attachments`\n\nÌÇ§ÏõåÎìúÎ°ú Î¨∏ÏÑúÎ•º Ï∞æÏïÑ Ìï¥Îãπ Î¨∏ÏÑúÏóê Ïó∞Í≤∞Îêú Î™®Îì† Ï≤®Î∂Ä ÌååÏùºÏùÑ `images/{Î¨∏ÏÑú Ï†úÎ™©}` Ìè¥ÎçîÎ°ú Ïù¥ÎèôÏãúÌÇ§Í≥†, Î¨∏ÏÑú ÎÇ¥Ïùò ÎßÅÌÅ¨Î•º ÏûêÎèôÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§.\n\n## Î©îÎ™®Î¶¨ Ïö¥ÏòÅ ÏõêÏπô\n\n### ÏÑúÎ≤ÑÏôÄ ÏóêÏù¥Ï†ÑÌä∏ Ï±ÖÏûÑ Î∂ÑÎ¶¨\n\n- **MCP ÏÑúÎ≤Ñ(Data Plane)**: Í≤ÄÏÉâ, ÏùΩÍ∏∞, ÏïïÏ∂ï, continuation, memory packet ÏÉùÏÑ±/Ï†ÄÏû•ÍπåÏßÄ Îã¥ÎãπÌï©ÎãàÎã§.\n- **ÏóêÏù¥Ï†ÑÌä∏ Îü∞ÌÉÄÏûÑ(Memory Plane)**: ÏÇ¨Ïö©Ïûê ÏùòÎèÑ Í∞êÏßÄ, `load_memory` ÏûêÎèô Ìò∏Ï∂ú, Îã§Ïùå ÌÑ¥ ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ†Ï£ºÏûÖÏùÑ Îã¥ÎãπÌï©ÎãàÎã§.\n\nÏ§ëÏöî: ÏÑúÎ≤ÑÎßåÏúºÎ°úÎäî \"Îã§Ïùå ÌÑ¥ ÏûêÎèô Í∏∞Ïñµ Î∞òÏòÅ\"ÏùÑ Î≥¥Ïû•Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ ÎèôÏûëÏùÄ Î∞òÎìúÏãú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏/ÏóêÏù¥Ï†ÑÌä∏ Îü∞ÌÉÄÏûÑÏóêÏÑú Íµ¨ÌòÑÌï¥Ïïº Ìï©ÎãàÎã§.\n\n### Î©îÎ™®Î¶¨ ÏÇ∞Ï∂úÎ¨º Ìè¨Îß∑\n\n- Í∏∞Î≥∏ Ï†ÄÏû• Í≤ΩÎ°ú: `memory/context_memory_snapshot.v1.md`\n- Íµ¨ÏÑ±: ÏÇ¨ÎûåÏù¥ ÏùΩÎäî Markdown ÏöîÏïΩ + AI ÌååÏã±Ïö© canonical JSON code block\n- Ïä§ÌÇ§Îßà ÌÇ§: `schema_version`, `generated_at`, `source_hash`, `documents[].doc_hash`, `memory_packet`\n\n## collect_context Ï∂îÏ≤ú ÌîÑÎ¶¨ÏÖã\n\n| Î™©Ï†Å                 | Ï£ºÏöî ÌååÎùºÎØ∏ÌÑ∞                                                         | Í∂åÏû• Í∞í                                 |\n| -------------------- | --------------------------------------------------------------------- | --------------------------------------- |\n| Îπ†Î•∏ ÌÜ†ÌîΩ Ïä§Ï∫î       | `scope`, `maxDocs`, `maxCharsPerDoc`, `compressionMode`               | `topic`, `8`, `700`, `aggressive`       |\n| Ïù¥Î†•ÏÑú Ïª®ÌÖçÏä§Ìä∏ Íµ¨Ï∂ï | `scope`, `maxDocs`, `maxCharsPerDoc`, `memoryMode`, `compressionMode` | `all`, `20`, `1200`, `both`, `balanced` |\n| Ïû•Î¨∏ Vault Îã®Í≥Ñ Ï≤òÎ¶¨ | `maxDocs`, `maxCharsPerDoc`, `maxOutputChars`                         | `10`, `900`, `2800`                     |\n\nÍ∞ÄÎìúÎ†àÏùºÏùÄ Ï∂úÎ†• ÏÉÅÌïú Ï¥àÍ≥º Ïãú Îã§Ïùå ÏàúÏÑúÎ°ú Ï∂ïÏÜåÎê©ÎãàÎã§: `backlinks -> per-doc chars -> doc count -> continuation`.\n\n## ÏòàÏ†ú MCP ÏöîÏ≤≠ (3Í∞ú)\n\nÏïÑÎûòÎäî MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïùò `callTool`Ïóê Ï†ÑÎã¨ÌïòÎäî `arguments` ÏòàÏãúÏûÖÎãàÎã§.\n\n### 1) Ï†ÑÏ≤¥ VaultÏóêÏÑú Î©îÎ™®Î¶¨ Íµ¨Ï∂ï ÏãúÏûë\n\n```json\n{\n  \"action\": \"collect_context\",\n  \"scope\": \"all\",\n  \"maxDocs\": 20,\n  \"maxCharsPerDoc\": 1200,\n  \"memoryMode\": \"both\",\n  \"compressionMode\": \"balanced\"\n}\n```\n\n### 2) continuationTokenÏúºÎ°ú Îã§Ïùå Î∞∞Ïπò Ïù¥Ïñ¥ÏÑú ÏàòÏßë\n\n```json\n{\n  \"action\": \"collect_context\",\n  \"continuationToken\": \"<previous_response.batch.continuation_token>\",\n  \"compressionMode\": \"balanced\"\n}\n```\n\n### 3) Ï†ÄÏû•Îêú Î©îÎ™®Î¶¨ Îπ†Î•∏ Î°úÎìú(quiet)\n\n```json\n{\n  \"action\": \"load_memory\",\n  \"memoryPath\": \"memory/context_memory_snapshot.v1.md\",\n  \"quiet\": true\n}\n```\n\nÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏûêÎèô Ï£ºÏûÖ Í∑úÏπôÏùÄ `docs/CLIENT_INJECTION_GUIDE.md`Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî.\n\n## ÏÑ§Ïπò Î∞è ÏÇ¨Ïö©\n\n### MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï\n\nMCPÎ•º ÏßÄÏõêÌïòÎäî AI ÎèÑÍµ¨(Claude Desktop, Gemini Îì±)Ïùò ÏÑ§Ï†ï ÌååÏùºÏóê Îã§Ïùå Íµ¨ÏÑ±ÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.\n\n#### Claude Desktop\n\n`claude_desktop_config.json` ÌååÏùºÏóê Ï∂îÍ∞ÄÌï¥ÏïºÌï† ÎÇ¥Ïö©:\n\n```json\n{\n  \"mcpServers\": {\n    \"obsidian-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@sunub/obsidian-mcp-server\"],\n      \"env\": {\n        \"VAULT_DIR_PATH\": \"/absolute/path/to/your/obsidian/vault\"\n      }\n    }\n  }\n}\n```\n\n#### Gemini\n\n`gemini_config.json` ÌååÏùºÏóê Ï∂îÍ∞ÄÌï¥ÏïºÌï† ÎÇ¥Ïö©:\n\n```json\n{\n  \"mcpServers\": {\n    \"obsidian-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@sunub/obsidian-mcp-server\"],\n      \"env\": {\n        \"VAULT_DIR_PATH\": \"/absolute/path/to/your/obsidian/vault\"\n      }\n    }\n  }\n}\n```\n\n### ÏÑ§Ï†ï ÌôïÏù∏ÏÇ¨Ìï≠\n\n1. **Vault Í≤ΩÎ°ú**: `VAULT_DIR_PATH`ÏóêÎäî Î∞òÎìúÏãú **Ï†àÎåÄ Í≤ΩÎ°ú**Î•º ÏûÖÎ†•Ìï¥Ïïº Ìï©ÎãàÎã§.\n\n   ```json\n   // ‚úÖ Ïò¨Î∞îÎ•∏ ÏòàÏãú\n   \"VAULT_DIR_PATH\": \"/Users/username/Documents/MyVault\"\n   \"VAULT_DIR_PATH\": \"C:\\\\Users\\\\username\\\\Documents\\\\MyVault\"  // Windows\n   \"VAULT_DIR_PATH\": \"/mnt/c/Users/username/Documents/MyVault\"  // WSL\n\n   // ‚ùå ÏûòÎ™ªÎêú ÏòàÏãú\n   \"VAULT_DIR_PATH\": \"~/Documents/MyVault\"  // ÏÉÅÎåÄ Í≤ΩÎ°ú ÏÇ¨Ïö© Î∂àÍ∞Ä\n   \"VAULT_DIR_PATH\": \"./vault\"              // ÏÉÅÎåÄ Í≤ΩÎ°ú ÏÇ¨Ïö© Î∂àÍ∞Ä\n   ```\n\n2. **Node.js ÏöîÍµ¨ÏÇ¨Ìï≠**: Node.js 22 Ïù¥ÏÉÅÏù¥ ÏÑ§ÏπòÎêòÏñ¥ ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n   ```bash\n   node --version  # v22.0.0 Ïù¥ÏÉÅ ÌôïÏù∏\n   ```\n\n3. **ÏÑ§Ï†ï Ï†ÅÏö©**: ÏÑ§Ï†ï ÌååÏùº Ï†ÄÏû• ÌõÑ AI ÎèÑÍµ¨Î•º Ïû¨ÏãúÏûëÌïòÎ©¥ MCP ÏÑúÎ≤ÑÍ∞Ä ÏûêÎèôÏúºÎ°ú Ïó∞Í≤∞Îê©ÎãàÎã§.\n\n### ÏàòÎèô Ïã§Ìñâ (ÌÖåÏä§Ìä∏Ïö©)\n\nÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏßÅÏ†ë ÏÑúÎ≤ÑÎ•º Ïã§ÌñâÌïòÏó¨ ÌÖåÏä§Ìä∏Ìï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§:\n\n```bash\n# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï ÌõÑ Ïã§Ìñâ\nVAULT_DIR_PATH=/path/to/vault npx -y @sunub/obsidian-mcp-server\n\n# ÎòêÎäî Î™ÖÎ†πÏ§Ñ Ïù∏ÏûêÎ°ú Í≤ΩÎ°ú ÏßÄÏ†ï\nnpx -y @sunub/obsidian-mcp-server --vault-path /path/to/vault\n```\n\n### ÌÖåÏä§Ìä∏\n\n`vitest`Î•º ÏÇ¨Ïö©Ìïú End-to-End ÌÖåÏä§Ìä∏:\n\n```bash\n# ÌÖåÏä§Ìä∏ Ïã§Ìñâ\nnpm test\n\n# Watch Î™®Îìú\nnpm run test:watch\n```\n\n### ÎπÑÏö© Í≥ÑÏ∏°(B1)\n\n`VAULT_METRICS_LOG_PATH`Î•º ÏßÄÏ†ïÌïòÎ©¥ `vault` ÎèÑÍµ¨ ÏùëÎãµÎßàÎã§ ÏïÑÎûò Î©îÌä∏Î¶≠Ïù¥ JSONLÎ°ú Í∏∞Î°ùÎê©ÎãàÎã§.\n\n- `estimated_tokens`\n- `mode`\n- `truncated`\n- `doc_count`\n\nÏòàÏãú:\n\n```bash\n# 1) Î©îÌä∏Î¶≠ Î°úÍ∑∏ Í≤ΩÎ°ú ÏßÄÏ†ï\nexport VAULT_METRICS_LOG_PATH=.tmp/vault-metrics.jsonl\n\n# 2) ÌèâÏÜåÏ≤òÎüº MCP ÏãúÎÇòÎ¶¨Ïò§ Ïã§Ìñâ (search/read/collect_context/load_memory)\nnpm run inspector\n\n# 3) ÏãúÎÇòÎ¶¨Ïò§ Ï¢ÖÎ£å ÌõÑ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±\nnpm run metrics:report -- .tmp/vault-metrics.jsonl\n```\n\nÎ¶¨Ìè¨Ìä∏Îäî Ïï°ÏÖòÎ≥Ñ `count`, `total_tokens`, `avg/p95_tokens`, `avg_doc_count`, `truncated_rate(%)`Î•º Ï∂úÎ†•Ìï©ÎãàÎã§.\n\n### ÏΩîÎìú ÌíàÏßà\n\n```bash\n# Ìè¨Îß∑ÌåÖ\nnpm run format\n\n# Î¶∞ÌåÖ\nnpm run lint\n\n# Ï†ÑÏ≤¥ Ï≤¥ÌÅ¨ (Ìè¨Îß∑ÌåÖ + Î¶∞ÌåÖ)\nnpm run check\n```\n\n### CI/CD\n\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî GitHub ActionsÎ•º ÏÇ¨Ïö©ÌïòÏó¨ CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Íµ¨Ï∂ïÌñàÏäµÎãàÎã§:\n\n- **ÎπåÎìú**: TypeScript Ïª¥ÌååÏùº Î∞è ÎπåÎìú Í≤ÄÏ¶ù\n- **Î¶∞Ìä∏**: BiomeÎ•º ÏÇ¨Ïö©Ìïú ÏΩîÎìú ÌíàÏßà Í≤ÄÏÇ¨\n- **ÌÖåÏä§Ìä∏**: VitestÎ•º ÌÜµÌïú E2E ÌÖåÏä§Ìä∏\n- **Î∞∞Ìè¨**: ÌÉúÍ∑∏ Ìë∏Ïãú Ïãú ÏûêÎèôÏúºÎ°ú npmÏóê Î∞∞Ìè¨\n\n## ÎùºÏù¥ÏÑ†Ïä§\n\nISC License\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search and navigate documents within a local Obsidian vault using keyword and metadata queries",
        "Generate AI-recommended frontmatter properties such as title, tags, and summary for documents",
        "Update and merge frontmatter properties safely into existing documents",
        "Collect, compress, and batch process document contexts into memory packets for AI consumption",
        "Load and parse saved memory snapshots for quick context reuse in subsequent interactions",
        "Automatically organize and relocate attachment files linked to documents, updating links accordingly",
        "Provide integrated workflows to analyze documents, generate properties, and update files in a single command",
        "Support efficient indexing with an inverted index for near-instant search without high memory usage",
        "Enable monitoring and logging of usage metrics such as token counts and document counts for cost tracking"
      ],
      "limitations": [
        "Does not guarantee automatic memory recall or next-turn context injection; this must be implemented on the client/agent side",
        "Requires absolute paths for vault directory configuration; relative paths are not supported",
        "Only supports Node.js version 22 or higher",
        "Memory packets are stored in a specific markdown format and require client-side parsing for reuse",
        "Attachment organization is limited to moving files into folders named after document titles under an images directory"
      ],
      "requirements": [
        "Node.js version 22.0.0 or higher installed",
        "An absolute path to the Obsidian vault directory set in the VAULT_DIR_PATH environment variable",
        "MCP-compatible AI client or tool configured to connect to the obsidian-mcp-server",
        "Optional environment variable VAULT_METRICS_LOG_PATH for enabling usage metrics logging",
        "Permissions to read and write files within the specified Obsidian vault directory"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions, usage examples, configuration guidelines, testing and CI/CD information, and clearly states limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Obsidian MCP Server\n\n`obsidian-mcp-server`Îäî [Model Context Protocol(MCP)](https://modelcontextprotocol.io/docs/getting-started/intro)ÏùÑ Íµ¨ÌòÑÌïú ÏÑúÎ≤ÑÎ°ú, Î°úÏª¨ Obsidian vaultÏùò Î¨∏ÏÑúÎì§ÏùÑ AI ÏóêÏù¥Ï†ÑÌä∏ÎÇò Ïô∏Î∂Ä Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóêÏÑú ÏâΩÍ≤å ÌÉêÏÉâÌïòÍ≥† Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÎèÑÎ°ù Í∞ïÎ†•Ìïú ÎèÑÍµ¨ APIÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.\n\nObsidian VaultÎ•º Ïù¥Ïö©Ìï¥ AIÍ∞Ä ÌôúÏö© Í∞ÄÎä•Ìïú ÏßÄÏãù Î≤†Ïù¥Ïä§(Knowledge Base)Î°ú ÌôïÏû•ÌïòÏó¨ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÍ≤åÎÅî ÌïòÍ≥† Î¨∏ÏÑú Í≤ÄÏÉâ, ÏöîÏïΩ, Ï†ïÎ¶¨ÏôÄ Í∞ôÏùÄ Î∂ÄÍ∞ÄÏ†ÅÏù∏ ÏûëÏóÖÏùÑ ÏûêÎèôÌôîÌïòÏó¨ ÏÇ¨Ïö©ÏûêÍ∞Ä ÌïµÏã¨Ï†ÅÏù∏ \"Í∏ÄÏì∞Í∏∞ ÌôúÎèô\"ÏóêÎßå ÏßëÏ§ëÌï† Ïàò ÏûàÎäî ÌôòÍ≤ΩÏùÑ Íµ¨Ï∂ïÌïòÍ≥†Ïûê Ï†úÏûëÌñàÏäµÎãàÎã§.\n\n## ÌïµÏã¨ ÏïÑÌÇ§ÌÖçÏ≤ò\n\nÎ≥∏ ÏÑúÎ≤ÑÎäî `VaultManager`ÏôÄ `Indexer`Î•º Ï§ëÏã¨ÏúºÎ°ú Íµ¨Ï∂ïÎêòÏñ¥ ÎåÄÍ∑úÎ™® VaultÏóêÏÑúÎèÑ ÎÜíÏùÄ ÏÑ±Îä•Í≥º Î©îÎ™®Î¶¨ Ìö®Ïú®ÏÑ±ÏùÑ Î≥¥Ïû•Ìï©ÎãàÎã§.\n\n- **`Indexer` Í∏∞Î∞ò Í≤ÄÏÉâ**: ÏÑúÎ≤Ñ ÏãúÏûë Ïãú Í∞ÄÎ≤ºÏö¥ Ïó≠ Ïù∏Îç±Ïä§(Inverted Index)Î•º ÏÉùÏÑ±ÌïòÏó¨ ÌÇ§ÏõåÎìú Í≤ÄÏÉâ Ïãú Í±∞Ïùò Ï¶âÍ∞ÅÏ†ÅÏù∏ Í≤∞Í≥ºÎ•º Î∞òÌôòÌï©ÎãàÎã§(O(1)). Ï†ÑÏ≤¥ ÌååÏùº ÎÇ¥Ïö©ÏùÑ Î©îÎ™®Î¶¨Ïóê ÏÉÅÏ£ºÏãúÌÇ§ÏßÄ ÏïäÏïÑ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ ÏµúÏÜåÌôîÌï©ÎãàÎã§.\n- **`VaultManager`**: Vault ÎÇ¥Ïùò Î™®Îì† Î¨∏ÏÑúÎ•º Ìö®Ïú®Ï†ÅÏúºÎ°ú Í¥ÄÎ¶¨ÌïòÎ©∞, ÌååÏùº ÏãúÏä§ÌÖúÍ≥º ÏÉÅÌò∏ÏûëÏö©ÌïòÏó¨ Î¨∏ÏÑúÏùò ÏÉùÏÑ±, ÏàòÏ†ï, ÏÇ≠Ï†úÎ•º Ï≤òÎ¶¨Ìï©ÎãàÎã§.\n\n## Ï£ºÏöî Í∏∞Îä•\n\n- **Í≥†Í∏â Î¨∏ÏÑú ÌÉêÏÉâ**: `vault` ÎèÑÍµ¨Î•º ÌÜµÌï¥ ÌÇ§ÏõåÎìú Í≤ÄÏÉâ, Ï†ÑÏ≤¥ Î™©Î°ù Ï°∞Ìöå, ÌäπÏ†ï Î¨∏ÏÑú ÏùΩÍ∏∞, ÌÜµÍ≥Ñ Î∂ÑÏÑù Îì± Îã§ÏñëÌïú ÌÉêÏÉâ Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n- **Ïª®ÌÖçÏä§Ìä∏ ÏàòÏßë/Í∏∞Ïñµ Ìå®ÌÇ∑ ÏÉùÏÑ±**: `vault collect_context`Î°ú Î¨∏ÏÑú Î∞∞Ïπò ÏàòÏßë, ÏïïÏ∂ï, continuation ÌÜ†ÌÅ∞ Î∞úÍ∏â, Î©îÎ™®Î¶¨ Ìå®ÌÇ∑(JSON canonical)ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.\n- **Ï†ÄÏû•Îêú Î©îÎ™®Î¶¨ Ïû¨Ìò∏Ï∂ú**: `vault load_memory`Î°ú `memory/context_memory_snapshot.v1.md`Î•º Îπ†Î•¥Í≤å Î°úÎìúÌï¥ Îã§Ïùå ÌÑ¥ Ïª®ÌÖçÏä§Ìä∏Î°ú Ïû¨ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n- **AI Í∏∞Î∞ò ÏÜçÏÑ± ÏÉùÏÑ±**: `generate_property` ÎèÑÍµ¨Îäî Î¨∏ÏÑú Î≥∏Î¨∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ `title`, `tags`, `summary` Îì± Ï†ÅÏ†àÌïú frontmatter ÏÜçÏÑ±ÏùÑ ÏûêÎèôÏúºÎ°ú ÏÉùÏÑ±Ìï©ÎãàÎã§.\n- **ÏïàÏ†ÑÌïú ÏÜçÏÑ± ÏóÖÎç∞Ïù¥Ìä∏**: `write_property` ÎèÑÍµ¨Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÏÉùÏÑ±Îêú ÏÜçÏÑ±ÏùÑ Í∏∞Ï°¥ frontmatterÏôÄ Î≥ëÌï©ÌïòÏó¨ ÌååÏùºÏóê ÏïàÏ†ÑÌïòÍ≤å Í∏∞Î°ùÌï©ÎãàÎã§.\n- **Ï≤®Î∂Ä ÌååÏùº ÏûêÎèô Ï†ïÎ¶¨**: `organize_attachments` ÎèÑÍµ¨Îäî Î¨∏ÏÑúÏôÄ Ïó∞Í≤∞Îêú Ï≤®Î∂Ä ÌååÏùº(Ïòà: Ïù¥ÎØ∏ÏßÄ)ÏùÑ ÏûêÎèôÏúºÎ°ú Í∞êÏßÄÌïòÏó¨ Î¨∏ÏÑú Ï†úÎ™©Ïóê ÎßûÎäî Ìè¥ÎçîÎ°ú Ïù¥ÎèôÏãúÌÇ§Í≥† ÎßÅÌÅ¨Î•º ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§.\n- **ÌÜµÌï© ÏõåÌÅ¨ÌîåÎ°úÏö∞**: `create_document_with_properties`ÏôÄ Í∞ôÏùÄ ÎèÑÍµ¨Î•º ÌÜµÌï¥ Î¨∏ÏÑú Î∂ÑÏÑùÎ∂ÄÌÑ∞ ÏÜçÏÑ± ÏÉùÏÑ±, ÌååÏùº ÏóÖÎç∞Ïù¥Ìä∏ÍπåÏßÄÏùò Ï†ÑÏ≤¥ Í≥ºÏ†ïÏùÑ Îã®Ïùº Î™ÖÎ†πÏúºÎ°ú Ïã§ÌñâÌï©ÎãàÎã§.\n- **Ïã†Î¢∞ÏÑ± Î∞è ÌÖåÏä§Ìä∏**: `vitest`Î•º ÏÇ¨Ïö©Ìïú End-to-End ÌÖåÏä§Ìä∏ÏôÄ GitHub Actions Í∏∞Î∞òÏùò CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÌÜµÌï¥ ÏÑúÎ≤ÑÏùò ÏïàÏ†ïÏÑ±Í≥º Í∞Å ÎèÑÍµ¨ APIÏùò ÏùëÎãµ Ïä§ÌÇ§ÎßàÎ•º Í≤ÄÏ¶ùÌï©ÎãàÎã§.\n\n## ÎèÑÍµ¨ API\n\n`obsidian-mcp-server`Îäî MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î•º ÌÜµÌï¥ Ìò∏Ï∂úÌï† Ïàò ÏûàÎäî Îã§ÏùåÍ≥º Í∞ôÏùÄ ÎèÑÍµ¨Îì§ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n\n### `vault`\n\nVault ÎÇ¥ Î¨∏ÏÑúÎ•º ÌÉêÏÉâÌïòÍ≥† Î∂ÑÏÑùÌïòÎäî ÌïµÏã¨ ÎèÑÍµ¨ÏûÖÎãàÎã§. `action` ÌååÎùºÎØ∏ÌÑ∞Î•º ÌÜµÌï¥ Îã§ÏñëÌïú Í∏∞Îä•ÏùÑ ÏàòÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.\n\n- **`list_all`**: Vault ÎÇ¥ Î™®Îì† Î¨∏ÏÑúÏùò Î™©Î°ùÍ≥º Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Î∞òÌôòÌï©ÎãàÎã§.\n- **`search`**: ÌÇ§ÏõåÎìúÎ•º Í∏∞Î∞òÏúºÎ°ú Î¨∏ÏÑú Ï†úÎ™©, ÎÇ¥Ïö©, ÌÉúÍ∑∏Î•º Í≤ÄÏÉâÌï©ÎãàÎã§.\n- **`read`**: ÌäπÏ†ï ÌååÏùºÏùò ÎÇ¥Ïö©ÏùÑ ÏùΩÍ≥† frontmatterÏôÄ Î≥∏Î¨∏ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.\n- **`stats`**: Vault ÎÇ¥ Î™®Îì† Î¨∏ÏÑúÏùò ÌÜµÍ≥Ñ(Îã®Ïñ¥, Í∏ÄÏûê Ïàò Îì±)Î•º Ï†úÍ≥µÌï©ÎãàÎã§.\n- **`collect_context`**: Î¨∏ÏÑúÎ•º Î∞∞Ïπò Ï≤òÎ¶¨ÌïòÏó¨ Î©îÎ™®Î¶¨ Ìå®ÌÇ∑ÏùÑ ÏÉùÏÑ±ÌïòÍ≥†, ÌïÑÏöî Ïãú `memory/context_memory_snapshot.v1.md`Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§.",
        "start_pos": 0,
        "end_pos": 2035,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "a9d0a436f94998a8"
      },
      {
        "chunk_id": 1,
        "text": "ad`**: ÌäπÏ†ï ÌååÏùºÏùò ÎÇ¥Ïö©ÏùÑ ÏùΩÍ≥† frontmatterÏôÄ Î≥∏Î¨∏ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.\n- **`stats`**: Vault ÎÇ¥ Î™®Îì† Î¨∏ÏÑúÏùò ÌÜµÍ≥Ñ(Îã®Ïñ¥, Í∏ÄÏûê Ïàò Îì±)Î•º Ï†úÍ≥µÌï©ÎãàÎã§.\n- **`collect_context`**: Î¨∏ÏÑúÎ•º Î∞∞Ïπò Ï≤òÎ¶¨ÌïòÏó¨ Î©îÎ™®Î¶¨ Ìå®ÌÇ∑ÏùÑ ÏÉùÏÑ±ÌïòÍ≥†, ÌïÑÏöî Ïãú `memory/context_memory_snapshot.v1.md`Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§.\n- **`load_memory`**: Ï†ÄÏû•Îêú Î©îÎ™®Î¶¨ ÎÖ∏Ìä∏Ïùò canonical JSON Î∏îÎ°ùÏùÑ ÌååÏã±ÌïòÏó¨ Îπ†Î•∏ Ïû¨Ï£ºÏûÖÏö© payloadÎ•º Î∞òÌôòÌï©ÎãàÎã§.\n\n### `generate_property`\n\nÎ¨∏ÏÑú Í≤ΩÎ°ú(`filePath`)Î•º ÏûÖÎ†•Î∞õÏïÑ Ìï¥Îãπ Î¨∏ÏÑúÏùò ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌïòÍ≥†, AIÍ∞Ä Ï∂îÏ≤úÌïòÎäî frontmatter ÏÜçÏÑ±ÏùÑ ÏÉùÏÑ±ÌïòÏó¨ Î∞òÌôòÌï©ÎãàÎã§.\n\n### `write_property`\n\nÌååÏùº Í≤ΩÎ°ú(`filePath`)ÏôÄ JSON ÌòïÏãùÏùò ÏÜçÏÑ±(`properties`)ÏùÑ ÏûÖÎ†•Î∞õÏïÑ, Ìï¥Îãπ ÌååÏùºÏùò frontmatterÎ•º ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§.\n\n### `create_document_with_properties`\n\nÎ¨∏ÏÑú Î∂ÑÏÑù, ÏÜçÏÑ± ÏÉùÏÑ±, ÌååÏùº ÏóÖÎç∞Ïù¥Ìä∏Ïùò Ï†Ñ Í≥ºÏ†ïÏùÑ Ìïú Î≤àÏóê Ï≤òÎ¶¨ÌïòÎäî ÌÜµÌï© ÎèÑÍµ¨ÏûÖÎãàÎã§.\n\n### `organize_attachments`\n\nÌÇ§ÏõåÎìúÎ°ú Î¨∏ÏÑúÎ•º Ï∞æÏïÑ Ìï¥Îãπ Î¨∏ÏÑúÏóê Ïó∞Í≤∞Îêú Î™®Îì† Ï≤®Î∂Ä ÌååÏùºÏùÑ `images/{Î¨∏ÏÑú Ï†úÎ™©}` Ìè¥ÎçîÎ°ú Ïù¥ÎèôÏãúÌÇ§Í≥†, Î¨∏ÏÑú ÎÇ¥Ïùò ÎßÅÌÅ¨Î•º ÏûêÎèôÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏Ìï©ÎãàÎã§.\n\n## Î©îÎ™®Î¶¨ Ïö¥ÏòÅ ÏõêÏπô\n\n### ÏÑúÎ≤ÑÏôÄ ÏóêÏù¥Ï†ÑÌä∏ Ï±ÖÏûÑ Î∂ÑÎ¶¨\n\n- **MCP ÏÑúÎ≤Ñ(Data Plane)**: Í≤ÄÏÉâ, ÏùΩÍ∏∞, ÏïïÏ∂ï, continuation, memory packet ÏÉùÏÑ±/Ï†ÄÏû•ÍπåÏßÄ Îã¥ÎãπÌï©ÎãàÎã§.\n- **ÏóêÏù¥Ï†ÑÌä∏ Îü∞ÌÉÄÏûÑ(Memory Plane)**: ÏÇ¨Ïö©Ïûê ÏùòÎèÑ Í∞êÏßÄ, `load_memory` ÏûêÎèô Ìò∏Ï∂ú, Îã§Ïùå ÌÑ¥ ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ†Ï£ºÏûÖÏùÑ Îã¥ÎãπÌï©ÎãàÎã§.\n\nÏ§ëÏöî: ÏÑúÎ≤ÑÎßåÏúºÎ°úÎäî \"Îã§Ïùå ÌÑ¥ ÏûêÎèô Í∏∞Ïñµ Î∞òÏòÅ\"ÏùÑ Î≥¥Ïû•Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ ÎèôÏûëÏùÄ Î∞òÎìúÏãú ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏/ÏóêÏù¥Ï†ÑÌä∏ Îü∞ÌÉÄÏûÑÏóêÏÑú Íµ¨ÌòÑÌï¥Ïïº Ìï©ÎãàÎã§.\n\n### Î©îÎ™®Î¶¨ ÏÇ∞Ï∂úÎ¨º Ìè¨Îß∑\n\n- Í∏∞Î≥∏ Ï†ÄÏû• Í≤ΩÎ°ú: `memory/context_memory_snapshot.v1.md`\n- Íµ¨ÏÑ±: ÏÇ¨ÎûåÏù¥ ÏùΩÎäî Markdown ÏöîÏïΩ + AI ÌååÏã±Ïö© canonical JSON code block\n- Ïä§ÌÇ§Îßà ÌÇ§: `schema_version`, `generated_at`, `source_hash`, `documents[].doc_hash`, `memory_packet`\n\n## collect_context Ï∂îÏ≤ú ÌîÑÎ¶¨ÏÖã\n\n| Î™©Ï†Å                 | Ï£ºÏöî ÌååÎùºÎØ∏ÌÑ∞                                                         | Í∂åÏû• Í∞í                                 |\n| -------------------- | --------------------------------------------------------------------- | --------------------------------------- |\n| Îπ†Î•∏ ÌÜ†ÌîΩ Ïä§Ï∫î       | `scope`, `maxDocs`, `maxCharsPerDoc`, `compressionMode`               | `topic`, `8`, `700`, `aggressive`       |\n| Ïù¥Î†•ÏÑú Ïª®ÌÖçÏä§Ìä∏ Íµ¨Ï∂ï | `scope`, `maxDocs`, `maxCharsPerDoc`, `memoryMode`, `compressionMode` | `all`, `20`, `1200`, `both`, `balanced` |\n| Ïû•Î¨∏ Vault Îã®Í≥Ñ Ï≤òÎ¶¨ | `maxDocs`, `maxCharsPerDoc`, `maxOutputChars`                         | `10`, `900`, `2800`                     |\n\nÍ∞ÄÎìúÎ†àÏùºÏùÄ Ï∂úÎ†• ÏÉÅÌïú Ï¥àÍ≥º Ïãú Îã§Ïùå ÏàúÏÑúÎ°ú Ï∂ïÏÜåÎê©ÎãàÎã§: `backlinks -> per-doc chars -> doc count -> continuation`.",
        "start_pos": 1835,
        "end_pos": 3813,
        "token_count_estimate": 494,
        "source_type": "readme",
        "agent_id": "a9d0a436f94998a8"
      },
      {
        "chunk_id": 2,
        "text": "Docs`, `maxCharsPerDoc`, `maxOutputChars`                         | `10`, `900`, `2800`                     |\n\nÍ∞ÄÎìúÎ†àÏùºÏùÄ Ï∂úÎ†• ÏÉÅÌïú Ï¥àÍ≥º Ïãú Îã§Ïùå ÏàúÏÑúÎ°ú Ï∂ïÏÜåÎê©ÎãàÎã§: `backlinks -> per-doc chars -> doc count -> continuation`.\n\n## ÏòàÏ†ú MCP ÏöîÏ≤≠ (3Í∞ú)\n\nÏïÑÎûòÎäî MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïùò `callTool`Ïóê Ï†ÑÎã¨ÌïòÎäî `arguments` ÏòàÏãúÏûÖÎãàÎã§.\n\n### 1) Ï†ÑÏ≤¥ VaultÏóêÏÑú Î©îÎ™®Î¶¨ Íµ¨Ï∂ï ÏãúÏûë\n\n```json\n{\n  \"action\": \"collect_context\",\n  \"scope\": \"all\",\n  \"maxDocs\": 20,\n  \"maxCharsPerDoc\": 1200,\n  \"memoryMode\": \"both\",\n  \"compressionMode\": \"balanced\"\n}\n```\n\n### 2) continuationTokenÏúºÎ°ú Îã§Ïùå Î∞∞Ïπò Ïù¥Ïñ¥ÏÑú ÏàòÏßë\n\n```json\n{\n  \"action\": \"collect_context\",\n  \"continuationToken\": \"<previous_response.batch.continuation_token>\",\n  \"compressionMode\": \"balanced\"\n}\n```\n\n### 3) Ï†ÄÏû•Îêú Î©îÎ™®Î¶¨ Îπ†Î•∏ Î°úÎìú(quiet)\n\n```json\n{\n  \"action\": \"load_memory\",\n  \"memoryPath\": \"memory/context_memory_snapshot.v1.md\",\n  \"quiet\": true\n}\n```\n\nÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏûêÎèô Ï£ºÏûÖ Í∑úÏπôÏùÄ `docs/CLIENT_INJECTION_GUIDE.md`Î•º Ï∞∏Í≥†ÌïòÏÑ∏Ïöî.\n\n## ÏÑ§Ïπò Î∞è ÏÇ¨Ïö©\n\n### MCP ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï\n\nMCPÎ•º ÏßÄÏõêÌïòÎäî AI ÎèÑÍµ¨(Claude Desktop, Gemini Îì±)Ïùò ÏÑ§Ï†ï ÌååÏùºÏóê Îã§Ïùå Íµ¨ÏÑ±ÏùÑ Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî.\n\n#### Claude Desktop\n\n`claude_desktop_config.json` ÌååÏùºÏóê Ï∂îÍ∞ÄÌï¥ÏïºÌï† ÎÇ¥Ïö©:\n\n```json\n{\n  \"mcpServers\": {\n    \"obsidian-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@sunub/obsidian-mcp-server\"],\n      \"env\": {\n        \"VAULT_DIR_PATH\": \"/absolute/path/to/your/obsidian/vault\"\n      }\n    }\n  }\n}\n```\n\n#### Gemini\n\n`gemini_config.json` ÌååÏùºÏóê Ï∂îÍ∞ÄÌï¥ÏïºÌï† ÎÇ¥Ïö©:\n\n```json\n{\n  \"mcpServers\": {\n    \"obsidian-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@sunub/obsidian-mcp-server\"],\n      \"env\": {\n        \"VAULT_DIR_PATH\": \"/absolute/path/to/your/obsidian/vault\"\n      }\n    }\n  }\n}\n```\n\n### ÏÑ§Ï†ï ÌôïÏù∏ÏÇ¨Ìï≠\n\n1. **Vault Í≤ΩÎ°ú**: `VAULT_DIR_PATH`ÏóêÎäî Î∞òÎìúÏãú **Ï†àÎåÄ Í≤ΩÎ°ú**Î•º ÏûÖÎ†•Ìï¥Ïïº Ìï©ÎãàÎã§.\n\n   ```json\n   // ‚úÖ Ïò¨Î∞îÎ•∏ ÏòàÏãú\n   \"VAULT_DIR_PATH\": \"/Users/username/Documents/MyVault\"\n   \"VAULT_DIR_PATH\": \"C:\\\\Users\\\\username\\\\Documents\\\\MyVault\"  // Windows\n   \"VAULT_DIR_PATH\": \"/mnt/c/Users/username/Documents/MyVault\"  // WSL\n\n   // ‚ùå ÏûòÎ™ªÎêú ÏòàÏãú\n   \"VAULT_DIR_PATH\": \"~/Documents/MyVault\"  // ÏÉÅÎåÄ Í≤ΩÎ°ú ÏÇ¨Ïö© Î∂àÍ∞Ä\n   \"VAULT_DIR_PATH\": \"./vault\"              // ÏÉÅÎåÄ Í≤ΩÎ°ú ÏÇ¨Ïö© Î∂àÍ∞Ä\n   ```\n\n2. **Node.js ÏöîÍµ¨ÏÇ¨Ìï≠**: Node.js 22 Ïù¥ÏÉÅÏù¥ ÏÑ§ÏπòÎêòÏñ¥ ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.",
        "start_pos": 3613,
        "end_pos": 5657,
        "token_count_estimate": 511,
        "source_type": "readme",
        "agent_id": "a9d0a436f94998a8"
      },
      {
        "chunk_id": 3,
        "text": "// WSL\n\n   // ‚ùå ÏûòÎ™ªÎêú ÏòàÏãú\n   \"VAULT_DIR_PATH\": \"~/Documents/MyVault\"  // ÏÉÅÎåÄ Í≤ΩÎ°ú ÏÇ¨Ïö© Î∂àÍ∞Ä\n   \"VAULT_DIR_PATH\": \"./vault\"              // ÏÉÅÎåÄ Í≤ΩÎ°ú ÏÇ¨Ïö© Î∂àÍ∞Ä\n   ```\n\n2. **Node.js ÏöîÍµ¨ÏÇ¨Ìï≠**: Node.js 22 Ïù¥ÏÉÅÏù¥ ÏÑ§ÏπòÎêòÏñ¥ ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n\n   ```bash\n   node --version  # v22.0.0 Ïù¥ÏÉÅ ÌôïÏù∏\n   ```\n\n3. **ÏÑ§Ï†ï Ï†ÅÏö©**: ÏÑ§Ï†ï ÌååÏùº Ï†ÄÏû• ÌõÑ AI ÎèÑÍµ¨Î•º Ïû¨ÏãúÏûëÌïòÎ©¥ MCP ÏÑúÎ≤ÑÍ∞Ä ÏûêÎèôÏúºÎ°ú Ïó∞Í≤∞Îê©ÎãàÎã§.\n\n### ÏàòÎèô Ïã§Ìñâ (ÌÖåÏä§Ìä∏Ïö©)\n\nÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏßÅÏ†ë ÏÑúÎ≤ÑÎ•º Ïã§ÌñâÌïòÏó¨ ÌÖåÏä§Ìä∏Ìï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§:\n\n```bash\n# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï ÌõÑ Ïã§Ìñâ\nVAULT_DIR_PATH=/path/to/vault npx -y @sunub/obsidian-mcp-server\n\n# ÎòêÎäî Î™ÖÎ†πÏ§Ñ Ïù∏ÏûêÎ°ú Í≤ΩÎ°ú ÏßÄÏ†ï\nnpx -y @sunub/obsidian-mcp-server --vault-path /path/to/vault\n```\n\n### ÌÖåÏä§Ìä∏\n\n`vitest`Î•º ÏÇ¨Ïö©Ìïú End-to-End ÌÖåÏä§Ìä∏:\n\n```bash\n# ÌÖåÏä§Ìä∏ Ïã§Ìñâ\nnpm test\n\n# Watch Î™®Îìú\nnpm run test:watch\n```\n\n### ÎπÑÏö© Í≥ÑÏ∏°(B1)\n\n`VAULT_METRICS_LOG_PATH`Î•º ÏßÄÏ†ïÌïòÎ©¥ `vault` ÎèÑÍµ¨ ÏùëÎãµÎßàÎã§ ÏïÑÎûò Î©îÌä∏Î¶≠Ïù¥ JSONLÎ°ú Í∏∞Î°ùÎê©ÎãàÎã§.\n\n- `estimated_tokens`\n- `mode`\n- `truncated`\n- `doc_count`\n\nÏòàÏãú:\n\n```bash\n# 1) Î©îÌä∏Î¶≠ Î°úÍ∑∏ Í≤ΩÎ°ú ÏßÄÏ†ï\nexport VAULT_METRICS_LOG_PATH=.tmp/vault-metrics.jsonl\n\n# 2) ÌèâÏÜåÏ≤òÎüº MCP ÏãúÎÇòÎ¶¨Ïò§ Ïã§Ìñâ (search/read/collect_context/load_memory)\nnpm run inspector\n\n# 3) ÏãúÎÇòÎ¶¨Ïò§ Ï¢ÖÎ£å ÌõÑ Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±\nnpm run metrics:report -- .tmp/vault-metrics.jsonl\n```\n\nÎ¶¨Ìè¨Ìä∏Îäî Ïï°ÏÖòÎ≥Ñ `count`, `total_tokens`, `avg/p95_tokens`, `avg_doc_count`, `truncated_rate(%)`Î•º Ï∂úÎ†•Ìï©ÎãàÎã§.\n\n### ÏΩîÎìú ÌíàÏßà\n\n```bash\n# Ìè¨Îß∑ÌåÖ\nnpm run format\n\n# Î¶∞ÌåÖ\nnpm run lint\n\n# Ï†ÑÏ≤¥ Ï≤¥ÌÅ¨ (Ìè¨Îß∑ÌåÖ + Î¶∞ÌåÖ)\nnpm run check\n```\n\n### CI/CD\n\nÏù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî GitHub ActionsÎ•º ÏÇ¨Ïö©ÌïòÏó¨ CI/CD ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Íµ¨Ï∂ïÌñàÏäµÎãàÎã§:\n\n- **ÎπåÎìú**: TypeScript Ïª¥ÌååÏùº Î∞è ÎπåÎìú Í≤ÄÏ¶ù\n- **Î¶∞Ìä∏**: BiomeÎ•º ÏÇ¨Ïö©Ìïú ÏΩîÎìú ÌíàÏßà Í≤ÄÏÇ¨\n- **ÌÖåÏä§Ìä∏**: VitestÎ•º ÌÜµÌïú E2E ÌÖåÏä§Ìä∏\n- **Î∞∞Ìè¨**: ÌÉúÍ∑∏ Ìë∏Ïãú Ïãú ÏûêÎèôÏúºÎ°ú npmÏóê Î∞∞Ìè¨\n\n## ÎùºÏù¥ÏÑ†Ïä§\n\nISC License",
        "start_pos": 5457,
        "end_pos": 6896,
        "token_count_estimate": 359,
        "source_type": "readme",
        "agent_id": "a9d0a436f94998a8"
      }
    ]
  },
  {
    "agent_id": "177584e064b86b10",
    "name": "ai.smithery/szge-lolwiki-mcp",
    "source": "mcp",
    "source_url": "https://github.com/szge/lolwiki-mcp",
    "description": "Generate friendly greetings for any audience. Toggle Pirate Mode for a playful, swashbuckling styl‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-07T01:41:22.282712Z",
    "indexed_at": "2026-02-18T04:09:08.297076",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# lolwiki-mcp\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server using Smithery CLI",
        "Test server interactions interactively via playground",
        "Customize server capabilities by modifying server.py"
      ],
      "limitations": [],
      "requirements": [
        "Smithery API key from smithery.ai/account/api-keys",
        "Smithery CLI installed to run and deploy the server",
        "GitHub account for repository hosting and deployment"
      ]
    },
    "documentation_quality": 0.65,
    "quality_rationale": "Documentation includes installation steps, usage examples, and deployment instructions but lacks detailed tool descriptions and explicit limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# lolwiki-mcp\n\nAn MCP server built with [Smithery CLI](https://smithery.ai/docs/getting_started/quickstart_build_python)\n\n## Prerequisites\n\n- **Smithery API key**: Get yours at [smithery.ai/account/api-keys](https://smithery.ai/account/api-keys)\n\n## Getting Started\n\n1. Run the server:\n   ```bash\n   uv run dev\n   ```\n\n2. Test interactively:\n\n   ```bash\n   uv run playground\n   ```\n\nTry saying \"Say hello to John\" to test the example tool.\n\n## Development\n\nYour server code is in `src/hello_server/server.py`. Add or update your server capabilities there.\n\n## Deploy\n\nReady to deploy? Push your code to GitHub and deploy to Smithery:\n\n1. Create a new repository at [github.com/new](https://github.com/new)\n\n2. Initialize git and push to GitHub:\n   ```bash\n   git add .\n   git commit -m \"Hello world üëã\"\n   git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO.git\n   git push -u origin main\n   ```\n\n3. Deploy your server to Smithery at [smithery.ai/new](https://smithery.ai/new)",
        "start_pos": 0,
        "end_pos": 989,
        "token_count_estimate": 247,
        "source_type": "readme",
        "agent_id": "177584e064b86b10"
      }
    ]
  },
  {
    "agent_id": "db8b6f0f98acb93d",
    "name": "ai.smithery/truss44-mcp-crypto-price",
    "source": "mcp",
    "source_url": "https://github.com/truss44/mcp-crypto-price",
    "description": "Provide real-time cryptocurrency price data and market analysis.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T23:25:46.643562Z",
    "indexed_at": "2026-02-18T04:09:09.759330",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Crypto Price & Market Analysis MCP Server\n[![smithery badge](https://smithery.ai/badge/@truss44/mcp-crypto-price)](https://smithery.ai/server/@truss44/mcp-crypto-price) [![NPM Downloads](https://img.shields.io/npm/d18m/mcp-crypto-price)](https://www.npmjs.com/package/mcp-crypto-price)\n\n<a href=\"https://glama.ai/mcp/servers/jpqoejojnc\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/jpqoejojnc/badge\" />\n</a>\n\nA Model Context Protocol (MCP) server that provides comprehensive cryptocurrency analysis using the CoinCap API. This server offers real-time price data, market analysis, and historical trends through an easy-to-use interface. Supports both STDIO and Streamable HTTP transports.\n\n## What's New\n\n- Streamable HTTP transport added (while keeping STDIO compatibility)\n- Release workflow signs commits via SSH for Verified releases\n- Smithery CLI scripts to build and run the HTTP server\n\n## Usage\n\nAdd this configuration to your Claude Desktop config file:\n\n- **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-crypto-price\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-crypto-price\"]\n    }\n  }\n}\n```\n\nIf your MCP client requires launching via `cmd.exe` on Windows:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-crypto-price\": {\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"mcp-crypto-price\"]\n    }\n  }\n}\n```\n\n### Run as Streamable HTTP server\n\nYou can run the server over HTTP for environments that support MCP over HTTP streaming.\n\n- Dev server (recommended during development):\n\n```bash\nnpm run dev\n```\n\n- Build and run the HTTP server:\n\n```bash\n# Build the HTTP bundle (outputs to .smithery/)\nnpm run build\n\n# Start the HTTP server\nnpm run start:http\n```\n\n- Build and run the STDIO server:\n\n```bash\n# Build the STDIO bundle (outputs to dist/)\nnpm run build:stdio\n\n# Start the STDIO server\nnpm run start:stdio\n```\n\nThe dev/build commands will print the server address to the console. Use that URL in clients that support MCP over HTTP (for example, Smithery). You can optionally provide an API key via `COINCAP_API_KEY` for higher rate limits.\n\n## Optional: CoinCap API Key\n\nFor higher rate limits, add an API key to your configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-crypto-price\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-crypto-price\"],\n      \"env\": {\n        \"COINCAP_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n## Note for Smithery CLI users\n\nThis MCP server works directly via `npx` (configs above) and does not require Smithery.\n\nIf you do use the Smithery CLI, authenticate with `smithery auth login` or by setting `SMITHERY_API_KEY` in your environment. Recent versions of the Smithery CLI do not support passing API keys via `--key` (or older `--profile` patterns).\n\n> **Important Note**: CoinCap is sunsetting their v2 API. This MCP supports both v2 and v3 APIs:\n> - If you provide a `COINCAP_API_KEY`, it will attempt to use the v3 API first, falling back to v2 if necessary\n> - Without an API key, it will use the v2 API (which will eventually be discontinued)\n> - It's recommended to obtain an API key from [pro.coincap.io/dashboard](https://pro.coincap.io/dashboard) as the v2 API will be completely deactivated in the future\n\nLaunch Claude Desktop to start using the crypto analysis tools.\n\n## Verified commits & SSH signing\n\nThis repository requires Verified (cryptographically signed) commits. CI also includes a job (`Verify commit signatures`) that fails PRs with unsigned commits.\n\n### Create an SSH signing key (once)\n\n```bash\n# Generate a new ed25519 SSH key (no passphrase makes CI easier)\nssh-keygen -t ed25519 -C \"CI signing key for mcp-crypto-price\" -f ~/.ssh/id_ed25519 -N ''\n\n# Your keys will be at:\n#   Private: ~/.ssh/id_ed25519\n#   Public : ~/.ssh/id_ed25519.pub\n```\n\n### Enable SSH signing locally (optional but recommended)\n\n```bash\ngit config --global gpg.format ssh\ngit config --global user.signingkey ~/.ssh/id_ed25519.pub\ngit config --global commit.gpgsign true\n\n# Example signed commit\ngit commit -S -m 'feat: add something'\n```\n\n### Configure GitHub to verify your signatures\n\n1. Add your public key as an SSH Signing Key in your GitHub account:\n   - GitHub ‚Üí Settings ‚Üí SSH and GPG keys ‚Üí New SSH key\n   - Key type: Signing Key (SSH)\n   - Paste contents of `~/.ssh/id_ed25519.pub`\n\n## Tools\n\n#### get-crypto-price\n\nGets current price and 24h stats for any cryptocurrency, including:\n- Current price in USD\n- 24-hour price change\n- Trading volume\n- Market cap\n- Market rank\n\n#### get-market-analysis\n\nProvides detailed market analysis including:\n- Top 5 exchanges by volume\n- Price variations across exchanges\n- Volume distribution analysis\n- VWAP (Volume Weighted Average Price)\n\n#### get-historical-analysis\n\nAnalyzes historical price data with:\n- Customizable time intervals (5min to 1 day)\n- Support for up to 30 days of historical data\n- Price trend analysis\n- Volatility metrics\n- High/low price ranges\n\n## Sample Prompts\n\n- \"What's the current price of Bitcoin?\"\n- \"Show me market analysis for ETH\"\n- \"Give me the 7-day price history for DOGE\"\n- \"What are the top exchanges trading BTC?\"\n- \"Show me the price trends for SOL with 1-hour intervals\"\n\n## Project Inspiration\n\nThis project was inspired by Alex Andru's [coincap-mcp](https://github.com/QuantGeekDev/coincap-mcp) project.\n\n## License\n\nThis project is licensed under the MIT License\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide real-time cryptocurrency price data including current price, 24-hour price change, trading volume, market cap, and market rank",
        "Perform detailed market analysis such as identifying top 5 exchanges by volume, price variations across exchanges, volume distribution, and VWAP calculation",
        "Analyze historical cryptocurrency price data with customizable time intervals from 5 minutes to 1 day and support up to 30 days of data",
        "Calculate price trend analysis, volatility metrics, and high/low price ranges for historical data",
        "Support both STDIO and Streamable HTTP transports for flexible integration",
        "Allow running as a dev server or production server with build scripts",
        "Use CoinCap API v2 and v3 with automatic fallback depending on API key presence",
        "Enable higher API rate limits via optional CoinCap API key configuration"
      ],
      "limitations": [
        "Without an API key, only the deprecated CoinCap v2 API is used which will eventually be discontinued",
        "Historical data analysis is limited to a maximum of 30 days",
        "Requires environment support for either STDIO or HTTP streaming transports",
        "Rate limits depend on CoinCap API key usage and may be restricted without a key"
      ],
      "requirements": [
        "Node.js environment to run the server via npx",
        "Optional CoinCap API key for higher rate limits and access to v3 API",
        "Claude Desktop client or other MCP clients configured to connect via STDIO or HTTP",
        "For Windows, may require launching via cmd.exe for compatibility",
        "Smithery CLI users need to authenticate with smithery auth login or set SMITHERY_API_KEY environment variable"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, API key requirements, transport options, and known limitations, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Crypto Price & Market Analysis MCP Server\n[![smithery badge](https://smithery.ai/badge/@truss44/mcp-crypto-price)](https://smithery.ai/server/@truss44/mcp-crypto-price) [![NPM Downloads](https://img.shields.io/npm/d18m/mcp-crypto-price)](https://www.npmjs.com/package/mcp-crypto-price)\n\n<a href=\"https://glama.ai/mcp/servers/jpqoejojnc\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/jpqoejojnc/badge\" />\n</a>\n\nA Model Context Protocol (MCP) server that provides comprehensive cryptocurrency analysis using the CoinCap API. This server offers real-time price data, market analysis, and historical trends through an easy-to-use interface. Supports both STDIO and Streamable HTTP transports.\n\n## What's New\n\n- Streamable HTTP transport added (while keeping STDIO compatibility)\n- Release workflow signs commits via SSH for Verified releases\n- Smithery CLI scripts to build and run the HTTP server\n\n## Usage\n\nAdd this configuration to your Claude Desktop config file:\n\n- **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n- **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-crypto-price\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-crypto-price\"]\n    }\n  }\n}\n```\n\nIf your MCP client requires launching via `cmd.exe` on Windows:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-crypto-price\": {\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"mcp-crypto-price\"]\n    }\n  }\n}\n```\n\n### Run as Streamable HTTP server\n\nYou can run the server over HTTP for environments that support MCP over HTTP streaming.",
        "start_pos": 0,
        "end_pos": 1601,
        "token_count_estimate": 400,
        "source_type": "readme",
        "agent_id": "db8b6f0f98acb93d"
      },
      {
        "chunk_id": 1,
        "text": "STDIO server:\n\n```bash\n# Build the STDIO bundle (outputs to dist/)\nnpm run build:stdio\n\n# Start the STDIO server\nnpm run start:stdio\n```\n\nThe dev/build commands will print the server address to the console. Use that URL in clients that support MCP over HTTP (for example, Smithery). You can optionally provide an API key via `COINCAP_API_KEY` for higher rate limits.\n\n## Optional: CoinCap API Key\n\nFor higher rate limits, add an API key to your configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-crypto-price\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-crypto-price\"],\n      \"env\": {\n        \"COINCAP_API_KEY\": \"YOUR_API_KEY_HERE\"\n      }\n    }\n  }\n}\n```\n\n## Note for Smithery CLI users\n\nThis MCP server works directly via `npx` (configs above) and does not require Smithery.\n\nIf you do use the Smithery CLI, authenticate with `smithery auth login` or by setting `SMITHERY_API_KEY` in your environment. Recent versions of the Smithery CLI do not support passing API keys via `--key` (or older `--profile` patterns).\n\n> **Important Note**: CoinCap is sunsetting their v2 API. This MCP supports both v2 and v3 APIs:\n> - If you provide a `COINCAP_API_KEY`, it will attempt to use the v3 API first, falling back to v2 if necessary\n> - Without an API key, it will use the v2 API (which will eventually be discontinued)\n> - It's recommended to obtain an API key from [pro.coincap.io/dashboard](https://pro.coincap.io/dashboard) as the v2 API will be completely deactivated in the future\n\nLaunch Claude Desktop to start using the crypto analysis tools.\n\n## Verified commits & SSH signing\n\nThis repository requires Verified (cryptographically signed) commits. CI also includes a job (`Verify commit signatures`) that fails PRs with unsigned commits.",
        "start_pos": 1848,
        "end_pos": 3602,
        "token_count_estimate": 438,
        "source_type": "readme",
        "agent_id": "db8b6f0f98acb93d"
      },
      {
        "chunk_id": 2,
        "text": "e makes CI easier)\nssh-keygen -t ed25519 -C \"CI signing key for mcp-crypto-price\" -f ~/.ssh/id_ed25519 -N ''\n\n# Your keys will be at:\n#   Private: ~/.ssh/id_ed25519\n#   Public : ~/.ssh/id_ed25519.pub\n```\n\n### Enable SSH signing locally (optional but recommended)\n\n```bash\ngit config --global gpg.format ssh\ngit config --global user.signingkey ~/.ssh/id_ed25519.pub\ngit config --global commit.gpgsign true\n\n# Example signed commit\ngit commit -S -m 'feat: add something'\n```\n\n### Configure GitHub to verify your signatures\n\n1. Add your public key as an SSH Signing Key in your GitHub account:\n   - GitHub ‚Üí Settings ‚Üí SSH and GPG keys ‚Üí New SSH key\n   - Key type: Signing Key (SSH)\n   - Paste contents of `~/.ssh/id_ed25519.pub`\n\n## Tools\n\n#### get-crypto-price\n\nGets current price and 24h stats for any cryptocurrency, including:\n- Current price in USD\n- 24-hour price change\n- Trading volume\n- Market cap\n- Market rank\n\n#### get-market-analysis\n\nProvides detailed market analysis including:\n- Top 5 exchanges by volume\n- Price variations across exchanges\n- Volume distribution analysis\n- VWAP (Volume Weighted Average Price)\n\n#### get-historical-analysis\n\nAnalyzes historical price data with:\n- Customizable time intervals (5min to 1 day)\n- Support for up to 30 days of historical data\n- Price trend analysis\n- Volatility metrics\n- High/low price ranges\n\n## Sample Prompts\n\n- \"What's the current price of Bitcoin?\"\n- \"Show me market analysis for ETH\"\n- \"Give me the 7-day price history for DOGE\"\n- \"What are the top exchanges trading BTC?\"\n- \"Show me the price trends for SOL with 1-hour intervals\"\n\n## Project Inspiration\n\nThis project was inspired by Alex Andru's [coincap-mcp](https://github.com/QuantGeekDev/coincap-mcp) project.\n\n## License\n\nThis project is licensed under the MIT License",
        "start_pos": 3696,
        "end_pos": 5490,
        "token_count_estimate": 448,
        "source_type": "readme",
        "agent_id": "db8b6f0f98acb93d"
      }
    ]
  },
  {
    "agent_id": "8422719626d3c708",
    "name": "ai.smithery/turnono-datacommons-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/turnono/datacommons-mcp-server",
    "description": "Discover statistical indicators and topics in Data Commons. Retrieve observations for specific var‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-03T09:53:14.229429Z",
    "indexed_at": "2026-02-18T04:09:11.043427",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# DataCommons MCP Server\n\nA Model Context Protocol (MCP) server for accessing Data Commons API data.\n\n## Features\n\n- Search for indicators and topics\n- Get observations and data\n- Support for various data formats and chart configurations\n- HTTP and stdio transport modes\n\n## Installation\n\n### Using pip\n\n```bash\npip install -r requirements.txt\npip install -e .\n```\n\n### Using Docker\n\n```bash\ndocker build -t datacommons-mcp .\ndocker run -p 8000:8000 datacommons-mcp\n```\n\n## Usage\n\n### CLI Commands\n\nStart the server in HTTP mode:\n\n```bash\npython -m datacommons_mcp.cli serve http --host 0.0.0.0 --port 8000\n```\n\nStart the server in stdio mode:\n\n```bash\npython -m datacommons_mcp.cli serve stdio\n```\n\n### Environment Variables\n\n- `GOOGLE_API_KEY`: Your Google API key for Data Commons access\n\n## Development\n\nInstall development dependencies:\n\n```bash\npip install -e \".[dev]\"\n```\n\nRun tests:\n\n```bash\npytest\n```\n\nFormat code:\n\n```bash\nblack .\nisort .\n```\n\n## License\n\nMIT License\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search for indicators and topics",
        "Get observations and data",
        "Support various data formats and chart configurations",
        "Operate in HTTP transport mode",
        "Operate in stdio transport mode"
      ],
      "limitations": [],
      "requirements": [
        "Google API key for Data Commons access via environment variable GOOGLE_API_KEY",
        "Python environment with dependencies installed via requirements.txt",
        "Docker for containerized deployment (optional)"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, usage examples for different modes, environment variable requirements, and development commands, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# DataCommons MCP Server\n\nA Model Context Protocol (MCP) server for accessing Data Commons API data.\n\n## Features\n\n- Search for indicators and topics\n- Get observations and data\n- Support for various data formats and chart configurations\n- HTTP and stdio transport modes\n\n## Installation\n\n### Using pip\n\n```bash\npip install -r requirements.txt\npip install -e .\n```\n\n### Using Docker\n\n```bash\ndocker build -t datacommons-mcp .\ndocker run -p 8000:8000 datacommons-mcp\n```\n\n## Usage\n\n### CLI Commands\n\nStart the server in HTTP mode:\n\n```bash\npython -m datacommons_mcp.cli serve http --host 0.0.0.0 --port 8000\n```\n\nStart the server in stdio mode:\n\n```bash\npython -m datacommons_mcp.cli serve stdio\n```\n\n### Environment Variables\n\n- `GOOGLE_API_KEY`: Your Google API key for Data Commons access\n\n## Development\n\nInstall development dependencies:\n\n```bash\npip install -e \".[dev]\"\n```\n\nRun tests:\n\n```bash\npytest\n```\n\nFormat code:\n\n```bash\nblack .\nisort .\n```\n\n## License\n\nMIT License",
        "start_pos": 0,
        "end_pos": 979,
        "token_count_estimate": 244,
        "source_type": "readme",
        "agent_id": "8422719626d3c708"
      }
    ]
  },
  {
    "agent_id": "1a2ed362ce9b8916",
    "name": "ai.smithery/wgong-sqlite-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/wgong/sqlite-mcp-server",
    "description": "Explore, query, and inspect SQLite databases with ease. List tables, preview results, and view det‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-10-05T01:52:30.252523Z",
    "indexed_at": "2026-02-18T04:09:12.735299",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "### Prerequisites\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Install FastMCP globally (if not already installed)\npip install fastmcp\n```\n\n### COMMAND CHEATSHEET\n```bash\n# Run FastMCP directly for testing\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp run sqlite_explorer.py\n\n# Test with inspector (if available)\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp inspect sqlite_explorer.py\n\n# To install SQLite Explorer\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp install sqlite_explorer.py --name \"SQLite Explorer\"\n\n# To launch SQLite Explorer via a web-based testing interface. Run with `--transport sse` for HTTP-based communication  \nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp dev sqlite_explorer.py\n\n# To set up the MCP server with Claude Desktop\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp claude-desktop add sqlite_explorer.py --name \"SQLite Explorer\"\n\n# Need to define the SQLITE_DB_PATH variable before running smithery playground \nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db smithery playground\n```\n\nAfter launching Smithery playground, we can now talk to the MCP server using this URL: https://smithery.ai/playground?mcp=https%3A%2F%2Fee09cd8f.ngrok.smithery.ai%2Fmcp\n\n#### For VSCode with Cline\n```bash\n# Add this configuration to Cline MCP settings:\n{\n  \"sqlite-explorer\": {\n    \"command\": \"uv\",\n    \"args\": [\n      \"run\",\n      \"--with\",\n      \"fastmcp\",\n      \"--with\",\n      \"uvicorn\",\n      \"fastmcp\",\n      \"run\",\n      \"/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/sqlite_explorer.py\"\n    ],\n    \"env\": {\n      \"SQLITE_DB_PATH\": \"/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db\"\n    }\n  }\n}\n```\n\n#### Example output. MCP server provides four components. SQLite Explorer provides those tools.\nServer\n  Name:         SQLite Explorer\n  Generation:   2\n\nComponents\n  Tools:        3\n  Prompts:      0\n  Resources:    0\n  Templates:    0\n\nEnvironment\n  FastMCP:      2.12.4\n  MCP:          1.15.0\n\n  This will open an interactive inspector where you can test the MCP tools:\n  - list_tables - to see what tables are in your database\n  - describe_table - to see the structure of a specific table\n  - read_query - to run SELECT queries on your data\n\n\n### Notes\nEven though we're running the MCP locally, still have a web interface\nFor locally deployed MCP server SQLite Explorer, this is the MCP server URL that we can access as a client: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=e01acfa4461c1af5227669c2c0051e7cf9c280fb61a43a146a672463441f4c1b#tools\nIn the future, once deployed, we'll have the same thing - HTTP or HTTPS, but instead of a local host, it'll be my server and the port. The rest will be something else. \nLocal host is the server name, so the server is a local host. But if deployed somewhere, then it'll be a different host name. The port serves as a channel to talk to multiple MCP servers. HTTPS is a secure HTTP. \n\n#### Sample Questions to Ask Your AI Assistant\n\nOnce the MCP server is connected, you can ask questions like:\n\n- \"What tables are in my database?\"\n- \"Show me the structure of my financial data\"\n- \"What are the first 10 rows from [table_name]?\"\n- \"How many records are in each table?\"\n- \"Show me all transactions over $1000\"\n- \"What's the average amount in the transactions table?\"\n\nNOTE: The tables starting with SQLite are called system tables. We never created those tables.\n\n### Troubleshooting\n\n```bash\n# Check FastMCP version and available commands\nfastmcp --help\nfastmcp version\n\n# Verify your database file exists\nls -la /Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db\n\n# Test database directly with sqlite3\nsqlite3 /Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db \".tables\"\n```\n\n### HW 3 Documentation\n\n#### Step 1\nCloned this GitHub repository: https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server\n\n#### Step 2\n\nAsked Claude Code: I was asked to evaluate the following deployment platforms: AWS, Vercel, \nRailway, and Smithery for my MCP server. I want the simplest and cheapest \ndeployment. Which option should I select? \n\n‚è∫ Based on your MCP server and requirements, I recommend Smithery.\n\n  Here's why:\n\n  Simplest: Smithery is specifically designed for MCP servers. It has the\n  most straightforward deployment process for this exact use case.\n\n  Cheapest: Smithery offers a free tier for MCP server hosting, making it\n  the most cost-effective option for your SQLite Explorer MCP server.\n\n  Other options comparison:\n  - AWS: Most complex setup, requires managing infrastructure, not\n  cost-effective for simple MCP servers\n  - Vercel: Designed for web apps/APIs, not optimized for MCP servers,\n  serverless model may not fit well\n  - Railway: Good platform but more expensive than Smithery's free tier,\n  more general-purpose\n\n  For a read-only SQLite MCP server like yours, Smithery's MCP-specific\n  infrastructure is the best match for \"simplest and cheapest.\"\n\n#### Step 3\n\nTested this locally via Cline. In VS Code, we configured the system settings to launch a web-based interface with the following JSON file. See URL: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=a164e503687338cb23938baf05ae738ebe5cd0eaefa629e419cea7ef6ef51563#tools\n\n#### Step 4\n\nsmithery playground URL : https://smithery.ai/playground?mcp=https%3A%2F%2F143c4151.ngrok.smithery.ai%2Fmcp\n\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Run an MCP server to explore SQLite databases",
        "List tables in the connected SQLite database",
        "Describe the structure of specific database tables",
        "Execute SELECT queries on the SQLite database",
        "Provide a web-based interactive inspector interface for testing",
        "Integrate with Claude Desktop and VSCode Cline for development",
        "Deploy MCP server locally or on Smithery platform",
        "Support HTTP/HTTPS communication with optional SSE transport"
      ],
      "limitations": [
        "Read-only access to SQLite database (no mention of write or update capabilities)",
        "Requires local or accessible SQLite database file path",
        "System tables starting with 'SQLite' are not user-created and presumably not modifiable",
        "No explicit mention of support for databases other than SQLite"
      ],
      "requirements": [
        "Python environment with dependencies installed via requirements.txt",
        "FastMCP package installed (version 2.12.4 recommended)",
        "SQLite database file accessible and path set in SQLITE_DB_PATH environment variable",
        "Optional: Smithery account and playground access for deployment and testing",
        "Optional: VSCode with Cline extension configured for local development"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, tool descriptions, environment setup, troubleshooting tips, and deployment recommendations, covering capabilities, limitations, and requirements clearly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "### Prerequisites\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Install FastMCP globally (if not already installed)\npip install fastmcp\n```\n\n### COMMAND CHEATSHEET\n```bash\n# Run FastMCP directly for testing\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp run sqlite_explorer.py\n\n# Test with inspector (if available)\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp inspect sqlite_explorer.py\n\n# To install SQLite Explorer\nSQLITE_DB_PATH=/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db fastmcp install sqlite_explorer.py --name \"SQLite Explorer\"\n\n# To launch SQLite Explorer via a web-based testing interface.",
        "start_pos": 0,
        "end_pos": 838,
        "token_count_estimate": 209,
        "source_type": "readme",
        "agent_id": "1a2ed362ce9b8916"
      },
      {
        "chunk_id": 1,
        "text": ",\n      \"fastmcp\",\n      \"--with\",\n      \"uvicorn\",\n      \"fastmcp\",\n      \"run\",\n      \"/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/sqlite_explorer.py\"\n    ],\n    \"env\": {\n      \"SQLITE_DB_PATH\": \"/Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db\"\n    }\n  }\n}\n```\n\n#### Example output. MCP server provides four components. SQLite Explorer provides those tools.\nServer\n  Name:         SQLite Explorer\n  Generation:   2\n\nComponents\n  Tools:        3\n  Prompts:      0\n  Resources:    0\n  Templates:    0\n\nEnvironment\n  FastMCP:      2.12.4\n  MCP:          1.15.0\n\n  This will open an interactive inspector where you can test the MCP tools:\n  - list_tables - to see what tables are in your database\n  - describe_table - to see the structure of a specific table\n  - read_query - to run SELECT queries on your data\n\n\n### Notes\nEven though we're running the MCP locally, still have a web interface\nFor locally deployed MCP server SQLite Explorer, this is the MCP server URL that we can access as a client: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=e01acfa4461c1af5227669c2c0051e7cf9c280fb61a43a146a672463441f4c1b#tools\nIn the future, once deployed, we'll have the same thing - HTTP or HTTPS, but instead of a local host, it'll be my server and the port. The rest will be something else. \nLocal host is the server name, so the server is a local host. But if deployed somewhere, then it'll be a different host name. The port serves as a channel to talk to multiple MCP servers. HTTPS is a secure HTTP. \n\n#### Sample Questions to Ask Your AI Assistant\n\nOnce the MCP server is connected, you can ask questions like:\n\n- \"What tables are in my database?\"\n- \"Show me the structure of my financial data\"\n- \"What are the first 10 rows from [table_name]?\"\n- \"How many records are in each table?\"\n- \"Show me all transactions over $1000\"\n- \"What's the average amount in the transactions table?\"\n\nNOTE: The tables starting with SQLite are called system tables.",
        "start_pos": 1848,
        "end_pos": 3886,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "1a2ed362ce9b8916"
      },
      {
        "chunk_id": 2,
        "text": "\"How many records are in each table?\"\n- \"Show me all transactions over $1000\"\n- \"What's the average amount in the transactions table?\"\n\nNOTE: The tables starting with SQLite are called system tables. We never created those tables.\n\n### Troubleshooting\n\n```bash\n# Check FastMCP version and available commands\nfastmcp --help\nfastmcp version\n\n# Verify your database file exists\nls -la /Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db\n\n# Test database directly with sqlite3\nsqlite3 /Users/owner/claude-code/agentic-ai-learnings/hw3/sqlite-explorer-fastmcp-mcp-server/financial_data.db \".tables\"\n```\n\n### HW 3 Documentation\n\n#### Step 1\nCloned this GitHub repository: https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server\n\n#### Step 2\n\nAsked Claude Code: I was asked to evaluate the following deployment platforms: AWS, Vercel, \nRailway, and Smithery for my MCP server. I want the simplest and cheapest \ndeployment. Which option should I select? \n\n‚è∫ Based on your MCP server and requirements, I recommend Smithery.\n\n  Here's why:\n\n  Simplest: Smithery is specifically designed for MCP servers. It has the\n  most straightforward deployment process for this exact use case.\n\n  Cheapest: Smithery offers a free tier for MCP server hosting, making it\n  the most cost-effective option for your SQLite Explorer MCP server.\n\n  Other options comparison:\n  - AWS: Most complex setup, requires managing infrastructure, not\n  cost-effective for simple MCP servers\n  - Vercel: Designed for web apps/APIs, not optimized for MCP servers,\n  serverless model may not fit well\n  - Railway: Good platform but more expensive than Smithery's free tier,\n  more general-purpose\n\n  For a read-only SQLite MCP server like yours, Smithery's MCP-specific\n  infrastructure is the best match for \"simplest and cheapest.\"\n\n#### Step 3\n\nTested this locally via Cline. In VS Code, we configured the system settings to launch a web-based interface with the following JSON file.",
        "start_pos": 3686,
        "end_pos": 5695,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "1a2ed362ce9b8916"
      },
      {
        "chunk_id": 3,
        "text": "is the best match for \"simplest and cheapest.\"\n\n#### Step 3\n\nTested this locally via Cline. In VS Code, we configured the system settings to launch a web-based interface with the following JSON file. See URL: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=a164e503687338cb23938baf05ae738ebe5cd0eaefa629e419cea7ef6ef51563#tools\n\n#### Step 4\n\nsmithery playground URL : https://smithery.ai/playground?mcp=https%3A%2F%2F143c4151.ngrok.smithery.ai%2Fmcp",
        "start_pos": 5495,
        "end_pos": 5944,
        "token_count_estimate": 111,
        "source_type": "readme",
        "agent_id": "1a2ed362ce9b8916"
      }
    ]
  },
  {
    "agent_id": "2d3e1c4302f7ff94",
    "name": "ai.smithery/xinkuang-china-stock-mcp",
    "source": "mcp",
    "source_url": "https://github.com/xinkuang/china-stock-mcp",
    "description": "Access real-time and historical market data for China A-shares and Hong Kong stocks, along with ne‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T01:41:03.405268Z",
    "indexed_at": "2026-02-18T04:09:14.158023",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# china-stock-mcp\n[![smithery badge](https://smithery.ai/badge/@xinkuang/china-stock-mcp)](https://smithery.ai/server/@xinkuang/china-stock-mcp)\n‰∏ÄÊ¨æÂü∫‰∫é [akshare-one](https://github.com/zwldarren/akshare-one) ÊûÑÂª∫ÁöÑ MCP (Model Context Protocol) ÊúçÂä°Âô®Ôºå‰∏∫‰∏≠ÂõΩËÇ°Â∏ÇÊï∞ÊçÆÊèê‰æõÊé•Âè£„ÄÇÊèê‰æõ‰∫Ü‰∏ÄÁ≥ªÂàóÂ∑•ÂÖ∑ÔºåÁî®‰∫éËé∑ÂèñË¥¢Âä°‰ø°ÊÅØÔºåÂåÖÊã¨ÂéÜÂè≤ËÇ°Á•®Êï∞ÊçÆ„ÄÅÂÆûÊó∂Êï∞ÊçÆ„ÄÅÊñ∞ÈóªÊï∞ÊçÆ„ÄÅË¥¢Âä°Êä•Ë°®Á≠â„ÄÇ\n\n\n\n## üöÄ Ê†∏ÂøÉÁâπÊÄß\n\n- **ÂèåÊ®°ÂºèËøêË°å**: ÊîØÊåÅ stdio Êú¨Âú∞Ê®°ÂºèÂíå HTTP ÁΩëÁªúÊ®°Âºè\n- **‰∏∞ÂØåÁöÑË¥¢Âä°Êï∞ÊçÆ**: Ê∂µÁõñ A/B/H ËÇ°Êï∞ÊçÆÁöÑÂÖ®Êñπ‰ΩçËé∑Âèñ\n- **ÂÆûÊó∂Êï∞ÊçÆ**: ÊîØÊåÅÂÆûÊó∂ËÇ°‰ª∑„ÄÅ‰∫§Êòì‰ø°ÊÅØÁ≠â\n- **Ë¥¢Âä°Êä•Ë°®**: ËµÑ‰∫ßË¥üÂÄ∫Ë°®„ÄÅÂà©Ê∂¶Ë°®„ÄÅÁé∞ÈáëÊµÅÈáèË°®Á≠â\n- **ÊäÄÊúØÊåáÊ†á**: 30+ ÁßçÊäÄÊúØÊåáÊ†áËá™Âä®ËÆ°ÁÆóÂíåÊ∑ªÂä†\n- **Êñ∞ÈóªÊï∞ÊçÆ**: ËÇ°Á•®Áõ∏ÂÖ≥Êñ∞ÈóªÂíåÂÖ¨Âëä‰ø°ÊÅØ\n- **ÊòìÁî®ÊÄß**: ÁÆÄÂçïÈÖçÁΩÆÂç≥ÂèØÈõÜÊàêÂà∞ AI Âä©Êâã (Claude„ÄÅCursor Á≠â)\n- **Êï∞ÊçÆÁºìÂ≠ò**: ÂÜÖÁΩÆÂÜÖÂ≠òÂíåÁ£ÅÁõòÁºìÂ≠òÊú∫Âà∂ÔºåÊèêÈ´òÊï∞ÊçÆËé∑ÂèñÊïàÁéáÂíåÂìçÂ∫îÈÄüÂ∫¶\n- **ÂÆπÂô®Âåñ**: ÊîØÊåÅ Docker ÈÉ®ÁΩ≤\n\n## üõ†Ô∏è Êû∂ÊûÑÊ¶ÇËßà\n\n### ‰∏ªË¶ÅÁªÑ‰ª∂\n\n- `server.py`: MCP ÊúçÂä°Âô®Ê†∏ÂøÉÔºåÂÆö‰πâÊâÄÊúâÂ∑•ÂÖ∑ÂíåÊï∞ÊçÆÊé•Âè£\n- `__main__.py`: ÂëΩ‰ª§Ë°åÂÖ•Âè£ÔºåÊîØÊåÅÂ§öÁßçËøêË°åÊ®°Âºè\n- FastMCP Ê°ÜÊû∂: Â§ÑÁêÜ MCP ÂçèËÆÆÈÄö‰ø°\n- akshare-one Â∫ì: Êèê‰æõÂ∫ïÂ±ÇÁöÑ‰∏≠ÂõΩËÇ°Â∏ÇÊï∞ÊçÆËé∑ÂèñËÉΩÂäõ\n- `cache_utils.py`: ÁºìÂ≠òÂ∑•ÂÖ∑ÔºåÊèê‰æõÂÜÖÂ≠òÂíåÁ£ÅÁõòÁºìÂ≠òÂäüËÉΩ\n\n### ÊîØÊåÅÁöÑÊï∞ÊçÆÊ∫ê\n\n- \\*\\*Êï∞ÊçÆÊ∫êÊïÖÈöúÂàáÊç¢\\*\\*: ÂÜÖÁΩÆ \\`\\_fetch\\_data\\_with\\_fallback\\` Êú∫Âà∂ÔºåÊîØÊåÅÊåâ‰ºòÂÖàÁ∫ßËá™Âä®ÂàáÊç¢Êï∞ÊçÆÊ∫ê„ÄÇÂΩìÈ¶ñÈÄâÊï∞ÊçÆÊ∫êÂ§±Ë¥•ÊàñËøîÂõûÁ©∫Êï∞ÊçÆÊó∂ÔºåÁ≥ªÁªüÂ∞ÜËá™Âä®Â∞ùËØïÂ§áÁî®Êï∞ÊçÆÊ∫êÔºå‰ªéËÄåÊèêÈ´òÊï∞ÊçÆËé∑ÂèñÁöÑÁ®≥ÂÆöÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ\n\n- ‰∏úÊñπË¥¢ÂØå (eastmoney, eastmoney_direct)\n- Êñ∞Êµ™Ë¥¢Áªè (sina)\n- Èõ™ÁêÉ (xueqiu)\n## üìã ÂèØÁî®Â∑•ÂÖ∑\n\n### 1. `Ëé∑ÂèñËÇ°Á•®ÁöÑÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÔºåÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÊ∫êÂíåÊäÄÊúØÊåáÊ†á` (get_hist_data)\n\nËé∑ÂèñËÇ°Á•®ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `interval` (Literal): Êó∂Èó¥Âë®Êúü: minute, hour, day, week, month, year„ÄÇÈªòËÆ§:day\n- `interval_multiplier` (int): Êó∂Èó¥Âë®Êúü‰πòÊï∞\n- `start_date` (string): ÂºÄÂßãÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYY-MM-DD\n- `end_date` (string): ÁªìÊùüÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYY-MM-DD\n- `adjust` (Literal): Â§çÊùÉÁ±ªÂûã: none, qfq(ÂâçÂ§çÊùÉ), hfq(ÂêéÂ§çÊùÉ)„ÄÇÈªòËÆ§Ôºönone\n- \\`indicators\\_list\\` \\(string\\|list\\): Ë¶ÅÊ∑ªÂä†ÁöÑÊäÄÊúØÊåáÊ†áÔºåÂèØ‰ª•ÊòØÈÄóÂè∑ÂàÜÈöîÁöÑÂ≠óÁ¨¶‰∏≤Ôºà‰æãÂ¶Ç: 'SMA,EMA'ÔºâÊàñÂ≠óÁ¨¶‰∏≤ÂàóË°®Ôºà‰æãÂ¶Ç: \\['SMA', 'EMA'\\]Ôºâ„ÄÇÊîØÊåÅÁöÑÊåáÊ†áÂåÖÊã¨: SMA, EMA, RSI, MACD, BOLL, STOCH, ATR, CCI, ADX, WILLR, AD, ADOSC, OBV, MOM, SAR, TSF, APO, AROON, AROONOSC, BOP, CMO, DX, MFI, MINUS\\_DI, MINUS\\_DM, PLUS\\_DI, PLUS\\_DM, PPO, ROC, ROCP, ROCR, ROCR100, TRIX, ULTOSC„ÄÇÂ∏∏Áî®ÊåáÊ†áÔºöSMA, EMA, RSI, MACD, BOLL, STOCH, OBV, MFI,Âª∫ËÆÆ‰∏çË∂ÖËøá10‰∏™„ÄÇ\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 2. `Ëé∑ÂèñËÇ°Á•®ÁöÑÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÔºåÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÊ∫ê` (get_realtime_data)\n\nËé∑ÂèñÂÆûÊó∂ËÇ°Á•®Ë°åÊÉÖÊï∞ÊçÆÔºåÊîØÊåÅÁöÑÊï∞ÊçÆÊ∫êÂåÖÊã¨Ôºöeastmoney, eastmoney\\_direct, xueqiu„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 3. `Ëé∑ÂèñËÇ°Á•®Áõ∏ÂÖ≥ÁöÑÊñ∞ÈóªÊï∞ÊçÆ` (get_news_data)\n\nËé∑ÂèñËÇ°Á•®Áõ∏ÂÖ≥Êñ∞ÈóªÊï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 4. `Ëé∑ÂèñÂÖ¨Âè∏ÁöÑËµÑ‰∫ßË¥üÂÄ∫Ë°®Êï∞ÊçÆ` (get_balance_sheet)\n\nËé∑ÂèñÂÖ¨Âè∏ËµÑ‰∫ßË¥üÂÄ∫Ë°®Êï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 5. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®‰ª£Á†ÅÁöÑÂÖ¨Âè∏ÁöÑÂà©Ê∂¶Ë°®Êï∞ÊçÆ` (get_income_statement)\n\nËé∑ÂèñÂÖ¨Âè∏Âà©Ê∂¶Ë°®Êï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 6. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®‰ª£Á†ÅÁöÑÂÖ¨Âè∏ÁöÑÁé∞ÈáëÊµÅÈáèË°®Êï∞ÊçÆ` (get_cash_flow)\n\nËé∑ÂèñÂÖ¨Âè∏Áé∞ÈáëÊµÅÈáèË°®Êï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 7. `Ëé∑ÂèñËÇ°Á•®ÁöÑËøë 100 ‰∏™‰∫§ÊòìÊó•ÁöÑËµÑÈáëÊµÅÂêëÊï∞ÊçÆ` (get_fund_flow)\n\nËé∑ÂèñËÇ°Á•®ÁöÑËøë 100 ‰∏™‰∫§ÊòìÊó•ÁöÑËµÑÈáëÊµÅÂêëÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 8. `Ëé∑ÂèñÂÖ¨Âè∏ÁöÑÂÜÖÈÉ®ËÇ°‰∏ú‰∫§ÊòìÊï∞ÊçÆ` (get_inner_trade_data)\n\nËé∑ÂèñÂÖ¨Âè∏ÂÜÖÈÉ®ËÇ°‰∏ú‰∫§ÊòìÊï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 9. `Ëé∑Âèñ‰∏âÂ§ßË¥¢Âä°Êä•Ë°®ÁöÑÂÖ≥ÈîÆË¥¢Âä°ÊåáÊ†á` (get_financial_metrics)\n\nËé∑Âèñ‰∏âÂ§ßË¥¢Âä°Êä•Ë°®ÁöÑÂÖ≥ÈîÆË¥¢Âä°ÊåáÊ†á.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 10. `Ëé∑ÂèñÂΩìÂâçÊó∂Èó¥ÔºàISOÊ†ºÂºè„ÄÅÊó∂Èó¥Êà≥ÔºâÂíåÊúÄËøë‰∏Ä‰∏™‰∫§ÊòìÊó•` (get_time_info)\n\nËé∑ÂèñÂΩìÂâçÊó∂Èó¥ÔºàISOÊ†ºÂºè„ÄÅÊó∂Èó¥Êà≥ÔºâÂíåÊúÄËøë‰∏Ä‰∏™‰∫§ÊòìÊó•.\n\n**ÂèÇÊï∞:** Êó†\n\n### 11. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÂü∫Êú¨Ê¶ÇË¶Å‰ø°ÊÅØ` (get_stock_basic_info)\n\nËé∑ÂèñËÇ°Á•®Âü∫Êú¨Ê¶ÇË¶Å‰ø°ÊÅØÔºåÊîØÊåÅ A ËÇ°ÂíåÊ∏ØËÇ°\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 12. `Ëé∑ÂèñÂçï‰∏™ÂÆèËßÇÁªèÊµéÊåáÊ†áÊï∞ÊçÆ` (get_macro_data)\n\nËé∑ÂèñÂçï‰∏™ÂÆèËßÇÁªèÊµéÊåáÊ†áÊï∞ÊçÆ\n\n**ÂèÇÊï∞:**\n- `indicator` (Literal): Ë¶ÅËé∑ÂèñÁöÑÂÆèËßÇÁªèÊµéÊåáÊ†á„ÄÇÊîØÊåÅÁöÑÊåáÊ†áÂåÖÊã¨: money_supply, gdp, cpi, pmi, stock_summary„ÄÇÈªòËÆ§: 'gdp'\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 13. `ÂàÜÊûêÊï£Êà∑ÂíåÊú∫ÊûÑÊäïËµÑËÄÖÁöÑÊäïËµÑÊÉÖÁª™` (get_investor_sentiment)\n\nÂàÜÊûêÊï£Êà∑ÂíåÊú∫ÊûÑÊäïËµÑËÄÖÁöÑÊäïËµÑÊÉÖÁª™\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 14. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑËÇ°‰∏úÊÉÖÂÜµ` (get_shareholder_info)\n\nËé∑ÂèñËÇ°‰∏úÊÉÖÂÜµ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 15. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®ÂÖ¨Âè∏ÁöÑ‰∏ªË¶Å‰∫ßÂìÅÊàñ‰∏öÂä°ÊûÑÊàê` (get_product_info)\n\nËé∑Âèñ‰∫ßÂìÅÊÉÖÂÜµ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 16. `Ëé∑ÂèñËÇ°Á•®ÁöÑ‰∏öÁª©È¢ÑÊµãÊï∞ÊçÆÔºåÂåÖÊã¨È¢ÑÊµãÂπ¥Êä•ÂáÄÂà©Ê∂¶ÂíåÊØèËÇ°Êî∂Áõä` (get_profit_forecast)\n\nËé∑ÂèñËÇ°Á•®ÁöÑ‰∏öÁª©È¢ÑÊµãÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '600519')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 17. `Ëé∑ÂèñÂàÜÁ∫¢ÈÄÅËÇ°ËØ¶ÊÉÖ` (get_stock_fhps_detail)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÂàÜÁ∫¢ÈÄÅËÇ°ËØ¶ÊÉÖÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n### 18. `Ëé∑ÂèñÁ≠πÁ†ÅÂàÜÂ∏ÉÊï∞ÊçÆ` (get_stock_cyq)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÁ≠πÁ†ÅÂàÜÂ∏ÉÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `date` (string): Êü•ËØ¢Êó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYY-MM-DD\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n### 19. `Ëé∑ÂèñËÇ°Á•®Á†îÁ©∂Êä•Âëä` (get_stock_research_report)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÁ†îÁ©∂Êä•ÂëäÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 20. `Ëé∑ÂèñÊµÅÈÄöËÇ°‰∏úÊï∞ÊçÆ` (get_stock_circulate_stock_holder)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÊµÅÈÄöËÇ°‰∏úÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 21. `Ëé∑ÂèñÈ´òÁÆ°ÂèòÂä®Êï∞ÊçÆ` (get_stock_management_change)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÈ´òÁÆ°ÂèòÂä®Êï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n\n### 22. `Ëé∑ÂèñÈôêÂîÆËß£Á¶ÅÊï∞ÊçÆ` (get_stock_restricted_release_queue)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÈôêÂîÆËß£Á¶ÅÊï∞ÊçÆ„ÄÇ\n\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n\n\n### 23. `Ëé∑Âèñ A ËÇ°‰ª£Á†ÅÂíåÂêçÁß∞` (get_stock_a_code_name)\n\nËé∑ÂèñÊâÄÊúâ A ËÇ°ËÇ°Á•®ÁöÑ‰ª£Á†ÅÂíåÂêçÁß∞„ÄÇ\n\n**ÂèÇÊï∞:**\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n\n### 24. `Ëé∑ÂèñËÇ°Á•®‰º∞ÂÄºÊï∞ÊçÆ` (get_stock_value)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑ‰º∞ÂÄºÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 25. `ËÆ°ÁÆóÊåáÂÆö‰∏™ËÇ°ÁöÑÊ≥¢Âä®ÁéáÊåáÊ†á` (get_stock_volatility)\n\nÈÄöËøáÂàÜÈíüÁ∫ßÂéÜÂè≤Ë°åÊÉÖËÆ°ÁÆóÊåáÂÆö‰∏™ËÇ°ÁöÑÊ≥¢Âä®ÁéáÊåáÊ†á„ÄÇ\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `start_date`(string): ÂºÄÂßãÊó•Êúü\n- `end_date`(string): ÁªìÊùüÊó•Êúü\n- `period` (int): Êó∂Èó¥Âë®ÊúüÔºåÂàÜÈíüÁ∫ßÂà´ (‰æãÂ¶Ç: '1', '5', '15', '30', '60')\")\n- `adjust`(string): Â§çÊùÉÁ±ªÂûã: none, qfq(ÂâçÂ§çÊùÉ), hfq(ÂêéÂ§çÊùÉ)„ÄÇÈªòËÆ§Ôºönone\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 26. `Ëé∑ÂèñÊâÄÊúâÊåáÊï∞ÁöÑ‰ª£Á†ÅÂíåÂü∫Êú¨‰ø°ÊÅØ` (get_all_cni_indices)\n\nËé∑ÂèñÊâÄÊúâÊåáÊï∞ÁöÑ‰ª£Á†ÅÂíåÂü∫Êú¨‰ø°ÊÅØÔºåÂéªÈô§ÂÆûÊó∂ÂèòÂä®Êï∞ÊçÆÂπ∂ÊîØÊåÅÁºìÂ≠ò„ÄÇ\n\n**ÂèÇÊï∞:**\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 27. `Ëé∑ÂèñÊåáÂÆöÊåáÊï∞ÁöÑÊó•È¢ëÁéáÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ` (get_cni_index_hist)\n\nËé∑ÂèñÊåáÂÆöÊåáÊï∞ÁöÑÊó•È¢ëÁéáÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ÊåáÊï∞‰ª£Á†Å (‰æãÂ¶Ç: '399005')\n- `start_date` (string): ÂºÄÂßãÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYYMMDD (‰æãÂ¶Ç: '20230114')\n- `end_date` (string): ÁªìÊùüÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYYMMDD (‰æãÂ¶Ç: '20240114')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 28. `Ëé∑ÂèñÊåáÂÆöÊåáÊï∞ÁöÑÊàêÂàÜËÇ°Ê†∑Êú¨ËØ¶ÊÉÖ` (get_cni_index_detail)\n\nËé∑ÂèñÊåáÂÆöÊåáÊï∞ÁöÑÊàêÂàÜËÇ°Ê†∑Êú¨ËØ¶ÊÉÖ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ÊåáÊï∞‰ª£Á†Å (‰æãÂ¶Ç: '399001')\n- `date` (string): Êó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYYMM (‰æãÂ¶Ç: '202404')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 29. `Ëé∑ÂèñÊäÄÊúØÈÄâËÇ°ÊåáÊ†áÊï∞ÊçÆÔºåÂåÖÊã¨ÂàõÊñ∞È´ò„ÄÅÂàõÊñ∞‰Ωé„ÄÅËøûÁª≠‰∏äÊ∂®„ÄÅËøûÁª≠‰∏ãË∑å„ÄÅÊåÅÁª≠ÊîæÈáè„ÄÅÊåÅÁª≠Áº©Èáè„ÄÅÂêë‰∏äÁ™ÅÁ†¥„ÄÅÂêë‰∏ãÁ™ÅÁ†¥„ÄÅÈáè‰ª∑ÈΩêÂçá„ÄÅÈáè‰ª∑ÈΩêË∑å„ÄÅÈô©ËµÑ‰∏æÁâå„ÄÇ`(get_stock_technical_rank)\n\n**ÂèÇÊï∞:**\n- `indicator_name` (string): Ë¶ÅËé∑ÂèñÁöÑÊäÄÊúØÊåáÊ†áÂêçÁß∞ (‰æãÂ¶Ç: ÂàõÊñ∞È´ò-ÂàõÊúàÊñ∞È´ò,  ÂàõÊñ∞È´ò-ÂçäÂπ¥Êñ∞È´ò,  ÂàõÊñ∞È´ò-‰∏ÄÂπ¥Êñ∞È´ò,  ÂàõÊñ∞È´ò-ÂéÜÂè≤Êñ∞È´ò,  ÂàõÊñ∞‰Ωé-ÂàõÊúàÊñ∞‰Ωé,  ÂàõÊñ∞‰Ωé-ÂçäÂπ¥Êñ∞‰Ωé,  ÂàõÊñ∞‰Ωé-‰∏ÄÂπ¥Êñ∞‰Ωé,  ÂàõÊñ∞‰Ωé-ÂéÜÂè≤Êñ∞‰Ωé,  ËøûÁª≠‰∏äÊ∂®,  ËøûÁª≠‰∏ãË∑å,  ÊåÅÁª≠ÊîæÈáè,  ÊåÅÁª≠Áº©Èáè,  Âêë‰∏äÁ™ÅÁ†¥-5Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-10Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-20Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-30Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-60Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-90Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-250Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-500Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-5Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-10Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-20Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-30Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-60Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-90Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-250Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-500Êó•ÂùáÁ∫ø,  Èáè‰ª∑ÈΩêÂçá,  Èáè‰ª∑ÈΩêË∑å,  Èô©ËµÑ‰∏æÁâå)\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 30. `Ëé∑ÂèñÊâÄÊúâË°å‰∏öÊùøÂùóÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ` (get_stock_board_industry_summary)\n**ÂèÇÊï∞:**\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n## üöÄ ÂÆâË£ÖÂíåËøêË°å\n### ÊñπÊ≥ï‰∏Ä: ‰ΩøÁî® Smithery\n\nÈÄöËøá [Smithery](https://smithery.ai/server/@xinkuang/china-stock-mcp) Ëá™Âä®ÂÆâË£ÖÂà∞ Claude DesktopÔºö\n\n```bash\nnpx -y @smithery/cli install @xinkuang/china-stock-mcp\n```\n\n### ÊñπÊ≥ï‰∫å: ‰ΩøÁî® Docker\n\n#### 1. ÊãâÂèñÈïúÂÉè\n```bash\ndocker pull ghcr.io/xinkuang/china-stock-mcp:latest\n```\n\n#### 2. ËøêË°åÂÆπÂô®\n```bash\ndocker run -p 8081:8081 ghcr.io/xinkuang/china-stock-mcp:latest\n```\n\n### ÊñπÊ≥ï‰∏â: Êú¨Âú∞Ê∫ê‰ª£Á†ÅÂÆâË£Ö\n\n#### 1. ÁéØÂ¢ÉË¶ÅÊ±Ç\n- Python 3.12+\n- Git\n- uv (Êé®ËçêÁöÑ Python ÂåÖÁÆ°ÁêÜÂô®)\n\n#### 2. ÂÖãÈöÜ‰ªìÂ∫ì\n```bash\ngit clone https://github.com/xinkuang/china-stock-mcp\ncd china-stock-mcp\n```\n\n#### 3. ÂÆâË£Ö‰æùËµñ\n```bash\n# Êé®Ëçê‰ΩøÁî® uv ÂåÖÁÆ°ÁêÜÂô®\nuv sync\n\n# ÊàñËÄÖ‰ΩøÁî® pip\npip install -r requirements.txt\n```\n\n#### 4. ËøêË°åÊúçÂä°Âô®\n\n**stdio Ê®°Âºè (ÈªòËÆ§ÔºåÈÄÇÁî®‰∫éÊú¨Âú∞ MCP ÂÆ¢Êà∑Á´Ø):**\n```bash\nuv run -m china_stock_mcp\n```\n\n**HTTP Ê®°Âºè (ÈÄÇÁî®‰∫éËøúÁ®ãËÆøÈóÆ):**\n```bash\nuv run -m china_stock_mcp --streamable-http --host 0.0.0.0 --port 8081\n```\n\nÊúçÂä°Âô®Â∞ÜÂú® `http://localhost:8081/mcp` Êèê‰æõÊúçÂä°„ÄÇ\n\n## ‚öôÔ∏è MCP ÈÖçÁΩÆÁ§∫‰æã\n\n### Claude Desktop ÈÖçÁΩÆ\n\nÁºñËæë `claude_desktop_config.json`Ôºö\n\n**ÊñπÂºè‰∏Ä: Êú¨Âú∞Ê∫ê‰ª£Á†Å**\n```json\n{\n  \"mcpServers\": {\n    \"china-stock-mcp\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"--directory\",\n        \"/path/to/china_stock_mcp\",\n        \"run\",\n        \"china-stock-mcp\"\n      ]\n    }\n  }\n}\n```\n\n**ÊñπÂºè‰∫å: ÈÄöËøá uvx**\n```json\n{\n    \"mcpServers\": {\n        \"china-stock-mcp\": {\n            \"command\": \"uvx\",\n            \"args\": [\n              \"china-stock-mcp\"\n            ]\n        }\n    }\n}\n```\n\n**ÊñπÂºè‰∏â: HTTP Ê®°Âºè**\n```json\n{\n  \"mcpServers\": {\n    \"china-stock-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"china-stock-mcp\", \"--streamable-http\", \"--host\", \"0.0.0.0\", \"--port\", \"8081\"],\n      \"env\": {\n        \"MCP_BASE_URL\": \"http://localhost:8081/mcp\"\n      }\n    }\n  }\n}\n```\n\n### ÂÖ∂‰ªñ AI ÂÆ¢Êà∑Á´ØÈÖçÁΩÆ\n\n**Cursor:**\n```json\n{\n  \"mcpServers\": {\n    \"china-stock-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [ \"china-stock-mcp\"]\n    }\n  }\n}\n```\n\n**Clion with MCP:**\n```json\n{\n  \"mcpServers\": {\n    \"china-stock-mcp\": {\n      \"command\": \"uvx\",\n    \"args\": [ \"china-stock-mcp\"]\n    }\n  }\n}\n```\n\n## üèÉ‚Äç‚ôÇÔ∏è ÂëΩ‰ª§Ë°åÂèÇÊï∞\n\n- `--streamable-http`: ÂêØÁî® HTTP ÂèØÊµÅÂºèÊ®°Âºè (ÈªòËÆ§: stdio Ê®°Âºè)\n- `--host`: HTTP Ê®°Âºè‰∏ãÁöÑÁªëÂÆö‰∏ªÊú∫ (ÈªòËÆ§: 0.0.0.0)\n- `--port`: HTTP Ê®°Âºè‰∏ãÁöÑÁõëÂê¨Á´ØÂè£ (ÈªòËÆ§: 8081)\n\n## üìä Êï∞ÊçÆÊîØÊåÅËåÉÂõ¥\n\n### ËÇ°Á•®Â∏ÇÂú∫\n- AËÇ° (‰∏äËØÅ„ÄÅÊ∑±ËØÅ)\n- BËÇ°\n- HËÇ° (Ê∏ØËÇ°)\n- ‰∏≠Â∞èÊùø„ÄÅÂàõ‰∏öÊùø„ÄÅÊñ∞‰∏âÊùø\n\n### Êï∞ÊçÆÁ±ªÂûã\n- ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ (ÂàÜÈíüÁ∫ß„ÄÅÂ∞èÊó∂Á∫ß„ÄÅÊó•Á∫ß„ÄÅÂë®Á∫ß„ÄÅÊúàÁ∫ß„ÄÅÂπ¥Á∫ß)\n- ÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ\n- ÊäÄÊúØÊåáÊ†áËÆ°ÁÆó\n- Êñ∞ÈóªËµÑËÆØ\n- Ë¥¢Âä°Êä•Ë°® (ËµÑ‰∫ßË¥üÂÄ∫Ë°®„ÄÅÂà©Ê∂¶Ë°®„ÄÅÁé∞ÈáëÊµÅÈáèË°®)\n- Ë¥¢Âä°ÊåáÊ†á\n- ÂÜÖÈÉ®‰∫§ÊòìÊï∞ÊçÆ\n\n## üîß ÂºÄÂèëÂíåË¥°ÁåÆ\n\n### ÂºÄÂèëÁéØÂ¢ÉËÆæÁΩÆ\n\n1. ÂÖãÈöÜ‰ªìÂ∫ì\n```bash\ngit clone https://github.com/xinkuang/china-stock-mcp\ncd china-stock-mcp\n```\n\n2. ÂÆâË£ÖÂºÄÂèë‰æùËµñ\n```bash\nuv sync --dev\n```\n\n3. ËøõÂÖ•ÂºÄÂèëÊ®°Âºè\n```bash\nuv run -m china_stock_mcp\n```\n\n### ‰ª£Á†ÅÁªìÊûÑ\n\n```\nsrc/china_stock_mcp/\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ __main__.py    # ÂëΩ‰ª§Ë°åÂÖ•Âè£ÔºåÂ§ÑÁêÜÂêØÂä®ÂèÇÊï∞\n‚îú‚îÄ‚îÄ server.py      # MCP ÊúçÂä°Âô®Ê†∏ÂøÉÔºåÂÆö‰πâÊâÄÊúâÂ∑•ÂÖ∑\n‚îú‚îÄ‚îÄ mcp.json       # MCP ÈÖçÁΩÆËßÑËåÉ (ÂèØÈÄâ)\n‚îî‚îÄ‚îÄ py.typed       # Á±ªÂûãÊ†áÊ≥®Êñá‰ª∂\n```\n\n### Ê∑ªÂä†Êñ∞Â∑•ÂÖ∑\n\nÂú® `server.py` ‰∏≠‰ΩøÁî® `@mcp.tool` Ë£ÖÈ•∞Âô®Ê∑ªÂä†Êñ∞Â∑•ÂÖ∑Ôºö\n\n```python\n@mcp.tool(name=\"Â∑•ÂÖ∑‰∏≠ÊñáÂêçÁß∞\", description=\"Â∑•ÂÖ∑ÁöÑ‰∏≠ÊñáÊèèËø∞\")\ndef your_tool_name(param1: Annotated[str, Field(description=\"ÂèÇÊï∞ÊèèËø∞\")]) -> str:\n    \"\"\"Â∑•ÂÖ∑ËØ¶ÊÉÖÊèèËø∞\"\"\"\n    # ÂÆûÁé∞ÈÄªËæë\n    pass\n```\n\n## üìù ËÆ∏ÂèØËØÅ\n\nMIT License - ËØ¶ËßÅ [LICENSE](LICENSE) Êñá‰ª∂\n\n## ü§ù Ë¥°ÁåÆ\n\nÊ¨¢ËøéÊèê‰∫§ Issue Âíå Pull RequestÔºÅ\n\n## üôã‚Äç‚ôÇÔ∏è Â∏∏ËßÅÈóÆÈ¢ò\n\n**Q: ‰∏∫‰ªÄ‰πàÊó†Ê≥ïËé∑ÂèñÊï∞ÊçÆÔºü**\nA: ËØ∑Ê£ÄÊü•ÁΩëÁªúËøûÊé•ÂíåÊï∞ÊçÆÊ∫êÂèØÁî®ÊÄß„ÄÇÊüê‰∫õÊï∞ÊçÆÊ∫êÂèØËÉΩÊúâËÆøÈóÆÈôêÂà∂„ÄÇ\n\n**Q: HTTP Ê®°Âºè‰∏ãÊó†Ê≥ïËøûÊé•Ôºü**\nA: Á°ÆËÆ§Á´ØÂè£ 8081 Êú™Ë¢´ÂÖ∂‰ªñÊúçÂä°Âç†Áî®Ôºå‰∏îÈò≤ÁÅ´Â¢ôÂÖÅËÆ∏Áõ∏Â∫îÁ´ØÂè£ÁöÑËÆøÈóÆ„ÄÇ\n\n**Q: Â¶Ç‰ΩïÊõ¥Êñ∞Âà∞ÊúÄÊñ∞ÁâàÊú¨Ôºü**\nA: ‰ΩøÁî® Smithery ÂÆâË£ÖÁöÑÂèØ‰ª•Ëá™Âä®Êõ¥Êñ∞ÔºåÊâãÂä®ÂÆâË£ÖÁöÑËØ∑ÈáçÊñ∞ÊãâÂèñ‰ªìÂ∫ì‰ª£Á†Å„ÄÇ\n## üêû Ë∞ÉËØï\n\nÊúâÂÖ≥Â¶Ç‰Ωï‰ΩøÁî® @modelcontextprotocol/inspector Ë∞ÉËØïÊ≠§ÊúçÂä°Âô®ÁöÑËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖ [DEBUG.md](DEBUG.md)„ÄÇ\n\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide historical stock market data with multiple intervals and technical indicators",
        "Fetch real-time stock market data from multiple data sources",
        "Retrieve stock-related news and announcements",
        "Access comprehensive financial statements including balance sheet, income statement, and cash flow",
        "Calculate and provide over 30 technical indicators automatically",
        "Analyze investor sentiment for retail and institutional investors",
        "Support data caching with memory and disk to improve performance",
        "Operate in both stdio local mode and HTTP network mode",
        "Deploy easily via Docker or local Python environment",
        "Provide extensive stock market data including A/B/H shares, indices, and sector summaries"
      ],
      "limitations": [
        "Data availability depends on external data sources which may have access restrictions or outages",
        "HTTP mode requires port 8081 to be free and accessible through firewalls",
        "Recommended to limit technical indicators to no more than 10 per request for performance",
        "No explicit mention of support for markets outside China (A/B/H shares and related indices only)",
        "No stated support for write or transactional operations, read-only data access"
      ],
      "requirements": [
        "Python 3.12 or higher",
        "Git for source code cloning",
        "uv package manager recommended for installation and running",
        "Network access to external data sources (eastmoney, sina, xueqiu)",
        "Port 8081 open for HTTP mode",
        "Optional: Smithery CLI for easy installation and updates"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with parameters and output formats, usage examples for multiple modes, configuration samples for various AI clients, explicit limitations, and environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# china-stock-mcp\n[![smithery badge](https://smithery.ai/badge/@xinkuang/china-stock-mcp)](https://smithery.ai/server/@xinkuang/china-stock-mcp)\n‰∏ÄÊ¨æÂü∫‰∫é [akshare-one](https://github.com/zwldarren/akshare-one) ÊûÑÂª∫ÁöÑ MCP (Model Context Protocol) ÊúçÂä°Âô®Ôºå‰∏∫‰∏≠ÂõΩËÇ°Â∏ÇÊï∞ÊçÆÊèê‰æõÊé•Âè£„ÄÇÊèê‰æõ‰∫Ü‰∏ÄÁ≥ªÂàóÂ∑•ÂÖ∑ÔºåÁî®‰∫éËé∑ÂèñË¥¢Âä°‰ø°ÊÅØÔºåÂåÖÊã¨ÂéÜÂè≤ËÇ°Á•®Êï∞ÊçÆ„ÄÅÂÆûÊó∂Êï∞ÊçÆ„ÄÅÊñ∞ÈóªÊï∞ÊçÆ„ÄÅË¥¢Âä°Êä•Ë°®Á≠â„ÄÇ\n\n\n\n## üöÄ Ê†∏ÂøÉÁâπÊÄß\n\n- **ÂèåÊ®°ÂºèËøêË°å**: ÊîØÊåÅ stdio Êú¨Âú∞Ê®°ÂºèÂíå HTTP ÁΩëÁªúÊ®°Âºè\n- **‰∏∞ÂØåÁöÑË¥¢Âä°Êï∞ÊçÆ**: Ê∂µÁõñ A/B/H ËÇ°Êï∞ÊçÆÁöÑÂÖ®Êñπ‰ΩçËé∑Âèñ\n- **ÂÆûÊó∂Êï∞ÊçÆ**: ÊîØÊåÅÂÆûÊó∂ËÇ°‰ª∑„ÄÅ‰∫§Êòì‰ø°ÊÅØÁ≠â\n- **Ë¥¢Âä°Êä•Ë°®**: ËµÑ‰∫ßË¥üÂÄ∫Ë°®„ÄÅÂà©Ê∂¶Ë°®„ÄÅÁé∞ÈáëÊµÅÈáèË°®Á≠â\n- **ÊäÄÊúØÊåáÊ†á**: 30+ ÁßçÊäÄÊúØÊåáÊ†áËá™Âä®ËÆ°ÁÆóÂíåÊ∑ªÂä†\n- **Êñ∞ÈóªÊï∞ÊçÆ**: ËÇ°Á•®Áõ∏ÂÖ≥Êñ∞ÈóªÂíåÂÖ¨Âëä‰ø°ÊÅØ\n- **ÊòìÁî®ÊÄß**: ÁÆÄÂçïÈÖçÁΩÆÂç≥ÂèØÈõÜÊàêÂà∞ AI Âä©Êâã (Claude„ÄÅCursor Á≠â)\n- **Êï∞ÊçÆÁºìÂ≠ò**: ÂÜÖÁΩÆÂÜÖÂ≠òÂíåÁ£ÅÁõòÁºìÂ≠òÊú∫Âà∂ÔºåÊèêÈ´òÊï∞ÊçÆËé∑ÂèñÊïàÁéáÂíåÂìçÂ∫îÈÄüÂ∫¶\n- **ÂÆπÂô®Âåñ**: ÊîØÊåÅ Docker ÈÉ®ÁΩ≤\n\n## üõ†Ô∏è Êû∂ÊûÑÊ¶ÇËßà\n\n### ‰∏ªË¶ÅÁªÑ‰ª∂\n\n- `server.py`: MCP ÊúçÂä°Âô®Ê†∏ÂøÉÔºåÂÆö‰πâÊâÄÊúâÂ∑•ÂÖ∑ÂíåÊï∞ÊçÆÊé•Âè£\n- `__main__.py`: ÂëΩ‰ª§Ë°åÂÖ•Âè£ÔºåÊîØÊåÅÂ§öÁßçËøêË°åÊ®°Âºè\n- FastMCP Ê°ÜÊû∂: Â§ÑÁêÜ MCP ÂçèËÆÆÈÄö‰ø°\n- akshare-one Â∫ì: Êèê‰æõÂ∫ïÂ±ÇÁöÑ‰∏≠ÂõΩËÇ°Â∏ÇÊï∞ÊçÆËé∑ÂèñËÉΩÂäõ\n- `cache_utils.py`: ÁºìÂ≠òÂ∑•ÂÖ∑ÔºåÊèê‰æõÂÜÖÂ≠òÂíåÁ£ÅÁõòÁºìÂ≠òÂäüËÉΩ\n\n### ÊîØÊåÅÁöÑÊï∞ÊçÆÊ∫ê\n\n- \\*\\*Êï∞ÊçÆÊ∫êÊïÖÈöúÂàáÊç¢\\*\\*: ÂÜÖÁΩÆ \\`\\_fetch\\_data\\_with\\_fallback\\` Êú∫Âà∂ÔºåÊîØÊåÅÊåâ‰ºòÂÖàÁ∫ßËá™Âä®ÂàáÊç¢Êï∞ÊçÆÊ∫ê„ÄÇÂΩìÈ¶ñÈÄâÊï∞ÊçÆÊ∫êÂ§±Ë¥•ÊàñËøîÂõûÁ©∫Êï∞ÊçÆÊó∂ÔºåÁ≥ªÁªüÂ∞ÜËá™Âä®Â∞ùËØïÂ§áÁî®Êï∞ÊçÆÊ∫êÔºå‰ªéËÄåÊèêÈ´òÊï∞ÊçÆËé∑ÂèñÁöÑÁ®≥ÂÆöÊÄßÂíåÂèØÈù†ÊÄß„ÄÇ\n\n- ‰∏úÊñπË¥¢ÂØå (eastmoney, eastmoney_direct)\n- Êñ∞Êµ™Ë¥¢Áªè (sina)\n- Èõ™ÁêÉ (xueqiu)\n## üìã ÂèØÁî®Â∑•ÂÖ∑\n\n### 1. `Ëé∑ÂèñËÇ°Á•®ÁöÑÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆÔºåÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÊ∫êÂíåÊäÄÊúØÊåáÊ†á` (get_hist_data)\n\nËé∑ÂèñËÇ°Á•®ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `interval` (Literal): Êó∂Èó¥Âë®Êúü: minute, hour, day, week, month, year„ÄÇÈªòËÆ§:day\n- `interval_multiplier` (int): Êó∂Èó¥Âë®Êúü‰πòÊï∞\n- `start_date` (string): ÂºÄÂßãÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYY-MM-DD\n- `end_date` (string): ÁªìÊùüÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYY-MM-DD\n- `adjust` (Literal): Â§çÊùÉÁ±ªÂûã: none, qfq(ÂâçÂ§çÊùÉ), hfq(ÂêéÂ§çÊùÉ)„ÄÇÈªòËÆ§Ôºönone\n- \\`indicators\\_list\\` \\(string\\|list\\): Ë¶ÅÊ∑ªÂä†ÁöÑÊäÄÊúØÊåáÊ†áÔºåÂèØ‰ª•ÊòØÈÄóÂè∑ÂàÜÈöîÁöÑÂ≠óÁ¨¶‰∏≤Ôºà‰æãÂ¶Ç: 'SMA,EMA'ÔºâÊàñÂ≠óÁ¨¶‰∏≤ÂàóË°®Ôºà‰æãÂ¶Ç: \\['SMA', 'EMA'\\]Ôºâ„ÄÇÊîØÊåÅÁöÑÊåáÊ†áÂåÖÊã¨: SMA, EMA, RSI, MACD, BOLL, STOCH, ATR, CCI, ADX, WILLR, AD, ADOSC, OBV, MOM, SAR, TSF, APO, AROON, AROONOSC, BOP, CMO, DX, MFI, MINUS\\_DI, MINUS\\_DM, PLUS\\_DI, PLUS\\_DM, PPO, ROC, ROCP, ROCR, ROCR100, TRIX, ULTOSC„ÄÇÂ∏∏Áî®ÊåáÊ†áÔºöSMA, EMA, RSI, MACD, BOLL, STOCH, OBV, MFI,Âª∫ËÆÆ‰∏çË∂ÖËøá10‰∏™„ÄÇ\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 2.",
        "start_pos": 0,
        "end_pos": 1851,
        "token_count_estimate": 462,
        "source_type": "readme",
        "agent_id": "2d3e1c4302f7ff94"
      },
      {
        "chunk_id": 1,
        "text": "DM, PPO, ROC, ROCP, ROCR, ROCR100, TRIX, ULTOSC„ÄÇÂ∏∏Áî®ÊåáÊ†áÔºöSMA, EMA, RSI, MACD, BOLL, STOCH, OBV, MFI,Âª∫ËÆÆ‰∏çË∂ÖËøá10‰∏™„ÄÇ\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 2. `Ëé∑ÂèñËÇ°Á•®ÁöÑÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆÔºåÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÊ∫ê` (get_realtime_data)\n\nËé∑ÂèñÂÆûÊó∂ËÇ°Á•®Ë°åÊÉÖÊï∞ÊçÆÔºåÊîØÊåÅÁöÑÊï∞ÊçÆÊ∫êÂåÖÊã¨Ôºöeastmoney, eastmoney\\_direct, xueqiu„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 3. `Ëé∑ÂèñËÇ°Á•®Áõ∏ÂÖ≥ÁöÑÊñ∞ÈóªÊï∞ÊçÆ` (get_news_data)\n\nËé∑ÂèñËÇ°Á•®Áõ∏ÂÖ≥Êñ∞ÈóªÊï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 4. `Ëé∑ÂèñÂÖ¨Âè∏ÁöÑËµÑ‰∫ßË¥üÂÄ∫Ë°®Êï∞ÊçÆ` (get_balance_sheet)\n\nËé∑ÂèñÂÖ¨Âè∏ËµÑ‰∫ßË¥üÂÄ∫Ë°®Êï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 5. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®‰ª£Á†ÅÁöÑÂÖ¨Âè∏ÁöÑÂà©Ê∂¶Ë°®Êï∞ÊçÆ` (get_income_statement)\n\nËé∑ÂèñÂÖ¨Âè∏Âà©Ê∂¶Ë°®Êï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 6. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®‰ª£Á†ÅÁöÑÂÖ¨Âè∏ÁöÑÁé∞ÈáëÊµÅÈáèË°®Êï∞ÊçÆ` (get_cash_flow)\n\nËé∑ÂèñÂÖ¨Âè∏Áé∞ÈáëÊµÅÈáèË°®Êï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 7. `Ëé∑ÂèñËÇ°Á•®ÁöÑËøë 100 ‰∏™‰∫§ÊòìÊó•ÁöÑËµÑÈáëÊµÅÂêëÊï∞ÊçÆ` (get_fund_flow)\n\nËé∑ÂèñËÇ°Á•®ÁöÑËøë 100 ‰∏™‰∫§ÊòìÊó•ÁöÑËµÑÈáëÊµÅÂêëÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 8. `Ëé∑ÂèñÂÖ¨Âè∏ÁöÑÂÜÖÈÉ®ËÇ°‰∏ú‰∫§ÊòìÊï∞ÊçÆ` (get_inner_trade_data)\n\nËé∑ÂèñÂÖ¨Âè∏ÂÜÖÈÉ®ËÇ°‰∏ú‰∫§ÊòìÊï∞ÊçÆ.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 9. `Ëé∑Âèñ‰∏âÂ§ßË¥¢Âä°Êä•Ë°®ÁöÑÂÖ≥ÈîÆË¥¢Âä°ÊåáÊ†á` (get_financial_metrics)\n\nËé∑Âèñ‰∏âÂ§ßË¥¢Âä°Êä•Ë°®ÁöÑÂÖ≥ÈîÆË¥¢Âä°ÊåáÊ†á.\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 10. `Ëé∑ÂèñÂΩìÂâçÊó∂Èó¥ÔºàISOÊ†ºÂºè„ÄÅÊó∂Èó¥Êà≥ÔºâÂíåÊúÄËøë‰∏Ä‰∏™‰∫§ÊòìÊó•` (get_time_info)\n\nËé∑ÂèñÂΩìÂâçÊó∂Èó¥ÔºàISOÊ†ºÂºè„ÄÅÊó∂Èó¥Êà≥ÔºâÂíåÊúÄËøë‰∏Ä‰∏™‰∫§ÊòìÊó•.\n\n**ÂèÇÊï∞:** Êó†\n\n### 11.",
        "start_pos": 1651,
        "end_pos": 3584,
        "token_count_estimate": 483,
        "source_type": "readme",
        "agent_id": "2d3e1c4302f7ff94"
      },
      {
        "chunk_id": 2,
        "text": "Â¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 10. `Ëé∑ÂèñÂΩìÂâçÊó∂Èó¥ÔºàISOÊ†ºÂºè„ÄÅÊó∂Èó¥Êà≥ÔºâÂíåÊúÄËøë‰∏Ä‰∏™‰∫§ÊòìÊó•` (get_time_info)\n\nËé∑ÂèñÂΩìÂâçÊó∂Èó¥ÔºàISOÊ†ºÂºè„ÄÅÊó∂Èó¥Êà≥ÔºâÂíåÊúÄËøë‰∏Ä‰∏™‰∫§ÊòìÊó•.\n\n**ÂèÇÊï∞:** Êó†\n\n### 11. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÂü∫Êú¨Ê¶ÇË¶Å‰ø°ÊÅØ` (get_stock_basic_info)\n\nËé∑ÂèñËÇ°Á•®Âü∫Êú¨Ê¶ÇË¶Å‰ø°ÊÅØÔºåÊîØÊåÅ A ËÇ°ÂíåÊ∏ØËÇ°\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 12. `Ëé∑ÂèñÂçï‰∏™ÂÆèËßÇÁªèÊµéÊåáÊ†áÊï∞ÊçÆ` (get_macro_data)\n\nËé∑ÂèñÂçï‰∏™ÂÆèËßÇÁªèÊµéÊåáÊ†áÊï∞ÊçÆ\n\n**ÂèÇÊï∞:**\n- `indicator` (Literal): Ë¶ÅËé∑ÂèñÁöÑÂÆèËßÇÁªèÊµéÊåáÊ†á„ÄÇÊîØÊåÅÁöÑÊåáÊ†áÂåÖÊã¨: money_supply, gdp, cpi, pmi, stock_summary„ÄÇÈªòËÆ§: 'gdp'\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 13. `ÂàÜÊûêÊï£Êà∑ÂíåÊú∫ÊûÑÊäïËµÑËÄÖÁöÑÊäïËµÑÊÉÖÁª™` (get_investor_sentiment)\n\nÂàÜÊûêÊï£Êà∑ÂíåÊú∫ÊûÑÊäïËµÑËÄÖÁöÑÊäïËµÑÊÉÖÁª™\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 14. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑËÇ°‰∏úÊÉÖÂÜµ` (get_shareholder_info)\n\nËé∑ÂèñËÇ°‰∏úÊÉÖÂÜµ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 15. `Ëé∑ÂèñÊåáÂÆöËÇ°Á•®ÂÖ¨Âè∏ÁöÑ‰∏ªË¶Å‰∫ßÂìÅÊàñ‰∏öÂä°ÊûÑÊàê` (get_product_info)\n\nËé∑Âèñ‰∫ßÂìÅÊÉÖÂÜµ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 16. `Ëé∑ÂèñËÇ°Á•®ÁöÑ‰∏öÁª©È¢ÑÊµãÊï∞ÊçÆÔºåÂåÖÊã¨È¢ÑÊµãÂπ¥Êä•ÂáÄÂà©Ê∂¶ÂíåÊØèËÇ°Êî∂Áõä` (get_profit_forecast)\n\nËé∑ÂèñËÇ°Á•®ÁöÑ‰∏öÁª©È¢ÑÊµãÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '600519')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 17. `Ëé∑ÂèñÂàÜÁ∫¢ÈÄÅËÇ°ËØ¶ÊÉÖ` (get_stock_fhps_detail)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÂàÜÁ∫¢ÈÄÅËÇ°ËØ¶ÊÉÖÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n### 18. `Ëé∑ÂèñÁ≠πÁ†ÅÂàÜÂ∏ÉÊï∞ÊçÆ` (get_stock_cyq)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÁ≠πÁ†ÅÂàÜÂ∏ÉÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `date` (string): Êü•ËØ¢Êó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYY-MM-DD\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n### 19.",
        "start_pos": 3384,
        "end_pos": 5267,
        "token_count_estimate": 470,
        "source_type": "readme",
        "agent_id": "2d3e1c4302f7ff94"
      },
      {
        "chunk_id": 3,
        "text": "Ëé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÁ≠πÁ†ÅÂàÜÂ∏ÉÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `date` (string): Êü•ËØ¢Êó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYY-MM-DD\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n### 19. `Ëé∑ÂèñËÇ°Á•®Á†îÁ©∂Êä•Âëä` (get_stock_research_report)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÁ†îÁ©∂Êä•ÂëäÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 20. `Ëé∑ÂèñÊµÅÈÄöËÇ°‰∏úÊï∞ÊçÆ` (get_stock_circulate_stock_holder)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÊµÅÈÄöËÇ°‰∏úÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 21. `Ëé∑ÂèñÈ´òÁÆ°ÂèòÂä®Êï∞ÊçÆ` (get_stock_management_change)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÈ´òÁÆ°ÂèòÂä®Êï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n\n### 22. `Ëé∑ÂèñÈôêÂîÆËß£Á¶ÅÊï∞ÊçÆ` (get_stock_restricted_release_queue)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑÈôêÂîÆËß£Á¶ÅÊï∞ÊçÆ„ÄÇ\n\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n\n\n### 23. `Ëé∑Âèñ A ËÇ°‰ª£Á†ÅÂíåÂêçÁß∞` (get_stock_a_code_name)\n\nËé∑ÂèñÊâÄÊúâ A ËÇ°ËÇ°Á•®ÁöÑ‰ª£Á†ÅÂíåÂêçÁß∞„ÄÇ\n\n**ÂèÇÊï∞:**\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n\n### 24. `Ëé∑ÂèñËÇ°Á•®‰º∞ÂÄºÊï∞ÊçÆ` (get_stock_value)\n\nËé∑ÂèñÊåáÂÆöËÇ°Á•®ÁöÑ‰º∞ÂÄºÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 25. `ËÆ°ÁÆóÊåáÂÆö‰∏™ËÇ°ÁöÑÊ≥¢Âä®ÁéáÊåáÊ†á` (get_stock_volatility)\n\nÈÄöËøáÂàÜÈíüÁ∫ßÂéÜÂè≤Ë°åÊÉÖËÆ°ÁÆóÊåáÂÆö‰∏™ËÇ°ÁöÑÊ≥¢Âä®ÁéáÊåáÊ†á„ÄÇ\n**ÂèÇÊï∞:**\n- `symbol` (string): ËÇ°Á•®‰ª£Á†Å (‰æãÂ¶Ç: '000001')\n- `start_date`(string): ÂºÄÂßãÊó•Êúü\n- `end_date`(string): ÁªìÊùüÊó•Êúü\n- `period` (int): Êó∂Èó¥Âë®ÊúüÔºåÂàÜÈíüÁ∫ßÂà´ (‰æãÂ¶Ç: '1', '5', '15', '30', '60')\")\n- `adjust`(string): Â§çÊùÉÁ±ªÂûã: none, qfq(ÂâçÂ§çÊùÉ), hfq(ÂêéÂ§çÊùÉ)„ÄÇÈªòËÆ§Ôºönone\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 26. `Ëé∑ÂèñÊâÄÊúâÊåáÊï∞ÁöÑ‰ª£Á†ÅÂíåÂü∫Êú¨‰ø°ÊÅØ` (get_all_cni_indices)\n\nËé∑ÂèñÊâÄÊúâÊåáÊï∞ÁöÑ‰ª£Á†ÅÂíåÂü∫Êú¨‰ø°ÊÅØÔºåÂéªÈô§ÂÆûÊó∂ÂèòÂä®Êï∞ÊçÆÂπ∂ÊîØÊåÅÁºìÂ≠ò„ÄÇ\n\n**ÂèÇÊï∞:**\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 27.",
        "start_pos": 5067,
        "end_pos": 7005,
        "token_count_estimate": 484,
        "source_type": "readme",
        "agent_id": "2d3e1c4302f7ff94"
      },
      {
        "chunk_id": 4,
        "text": "html„ÄÇÈªòËÆ§:markdown\n\n### 26. `Ëé∑ÂèñÊâÄÊúâÊåáÊï∞ÁöÑ‰ª£Á†ÅÂíåÂü∫Êú¨‰ø°ÊÅØ` (get_all_cni_indices)\n\nËé∑ÂèñÊâÄÊúâÊåáÊï∞ÁöÑ‰ª£Á†ÅÂíåÂü∫Êú¨‰ø°ÊÅØÔºåÂéªÈô§ÂÆûÊó∂ÂèòÂä®Êï∞ÊçÆÂπ∂ÊîØÊåÅÁºìÂ≠ò„ÄÇ\n\n**ÂèÇÊï∞:**\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 27. `Ëé∑ÂèñÊåáÂÆöÊåáÊï∞ÁöÑÊó•È¢ëÁéáÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ` (get_cni_index_hist)\n\nËé∑ÂèñÊåáÂÆöÊåáÊï∞ÁöÑÊó•È¢ëÁéáÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ÊåáÊï∞‰ª£Á†Å (‰æãÂ¶Ç: '399005')\n- `start_date` (string): ÂºÄÂßãÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYYMMDD (‰æãÂ¶Ç: '20230114')\n- `end_date` (string): ÁªìÊùüÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYYMMDD (‰æãÂ¶Ç: '20240114')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 28. `Ëé∑ÂèñÊåáÂÆöÊåáÊï∞ÁöÑÊàêÂàÜËÇ°Ê†∑Êú¨ËØ¶ÊÉÖ` (get_cni_index_detail)\n\nËé∑ÂèñÊåáÂÆöÊåáÊï∞ÁöÑÊàêÂàÜËÇ°Ê†∑Êú¨ËØ¶ÊÉÖ„ÄÇ\n\n**ÂèÇÊï∞:**\n- `symbol` (string): ÊåáÊï∞‰ª£Á†Å (‰æãÂ¶Ç: '399001')\n- `date` (string): Êó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYYMM (‰æãÂ¶Ç: '202404')\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 29. `Ëé∑ÂèñÊäÄÊúØÈÄâËÇ°ÊåáÊ†áÊï∞ÊçÆÔºåÂåÖÊã¨ÂàõÊñ∞È´ò„ÄÅÂàõÊñ∞‰Ωé„ÄÅËøûÁª≠‰∏äÊ∂®„ÄÅËøûÁª≠‰∏ãË∑å„ÄÅÊåÅÁª≠ÊîæÈáè„ÄÅÊåÅÁª≠Áº©Èáè„ÄÅÂêë‰∏äÁ™ÅÁ†¥„ÄÅÂêë‰∏ãÁ™ÅÁ†¥„ÄÅÈáè‰ª∑ÈΩêÂçá„ÄÅÈáè‰ª∑ÈΩêË∑å„ÄÅÈô©ËµÑ‰∏æÁâå„ÄÇ`(get_stock_technical_rank)\n\n**ÂèÇÊï∞:**\n- `indicator_name` (string): Ë¶ÅËé∑ÂèñÁöÑÊäÄÊúØÊåáÊ†áÂêçÁß∞ (‰æãÂ¶Ç: ÂàõÊñ∞È´ò-ÂàõÊúàÊñ∞È´ò,  ÂàõÊñ∞È´ò-ÂçäÂπ¥Êñ∞È´ò,  ÂàõÊñ∞È´ò-‰∏ÄÂπ¥Êñ∞È´ò,  ÂàõÊñ∞È´ò-ÂéÜÂè≤Êñ∞È´ò,  ÂàõÊñ∞‰Ωé-ÂàõÊúàÊñ∞‰Ωé,  ÂàõÊñ∞‰Ωé-ÂçäÂπ¥Êñ∞‰Ωé,  ÂàõÊñ∞‰Ωé-‰∏ÄÂπ¥Êñ∞‰Ωé,  ÂàõÊñ∞‰Ωé-ÂéÜÂè≤Êñ∞‰Ωé,  ËøûÁª≠‰∏äÊ∂®,  ËøûÁª≠‰∏ãË∑å,  ÊåÅÁª≠ÊîæÈáè,  ÊåÅÁª≠Áº©Èáè,  Âêë‰∏äÁ™ÅÁ†¥-5Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-10Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-20Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-30Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-60Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-90Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-250Êó•ÂùáÁ∫ø,  Âêë‰∏äÁ™ÅÁ†¥-500Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-5Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-10Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-20Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-30Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-60Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-90Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-250Êó•ÂùáÁ∫ø,  Âêë‰∏ãÁ™ÅÁ†¥-500Êó•ÂùáÁ∫ø,  Èáè‰ª∑ÈΩêÂçá,  Èáè‰ª∑ÈΩêË∑å,  Èô©ËµÑ‰∏æÁâå)\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n### 30. `Ëé∑ÂèñÊâÄÊúâË°å‰∏öÊùøÂùóÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ` (get_stock_board_industry_summary)\n**ÂèÇÊï∞:**\n- `output_format` (Literal): ËæìÂá∫Êï∞ÊçÆÊ†ºÂºè: json, csv, xml, excel, markdown, html„ÄÇÈªòËÆ§:markdown\n\n## üöÄ ÂÆâË£ÖÂíåËøêË°å\n### ÊñπÊ≥ï‰∏Ä: ‰ΩøÁî® Smithery\n\nÈÄöËøá [Smithery](https://smithery.ai/server/@xinkuang/china-stock-mcp) Ëá™Âä®ÂÆâË£ÖÂà∞ Claude DesktopÔºö\n\n```bash\nnpx -y @smithery/cli install @xinkuang/china-stock-mcp\n```\n\n### ÊñπÊ≥ï‰∫å: ‰ΩøÁî® Docker\n\n#### 1. ÊãâÂèñÈïúÂÉè\n```bash\ndocker pull ghcr.io/xinkuang/china-stock-mcp:latest\n```\n\n#### 2. ËøêË°åÂÆπÂô®\n```bash\ndocker run -p 8081:8081 ghcr.io/xinkuang/china-stock-mcp:latest\n```\n\n### ÊñπÊ≥ï‰∏â: Êú¨Âú∞Ê∫ê‰ª£Á†ÅÂÆâË£Ö\n\n#### 1. ÁéØÂ¢ÉË¶ÅÊ±Ç\n- Python 3.12+\n- Git\n- uv (Êé®ËçêÁöÑ Python ÂåÖÁÆ°ÁêÜÂô®)\n\n#### 2.",
        "start_pos": 6805,
        "end_pos": 8782,
        "token_count_estimate": 494,
        "source_type": "readme",
        "agent_id": "2d3e1c4302f7ff94"
      },
      {
        "chunk_id": 5,
        "text": "hina-stock-mcp:latest\n```\n\n#### 2. ËøêË°åÂÆπÂô®\n```bash\ndocker run -p 8081:8081 ghcr.io/xinkuang/china-stock-mcp:latest\n```\n\n### ÊñπÊ≥ï‰∏â: Êú¨Âú∞Ê∫ê‰ª£Á†ÅÂÆâË£Ö\n\n#### 1. ÁéØÂ¢ÉË¶ÅÊ±Ç\n- Python 3.12+\n- Git\n- uv (Êé®ËçêÁöÑ Python ÂåÖÁÆ°ÁêÜÂô®)\n\n#### 2. ÂÖãÈöÜ‰ªìÂ∫ì\n```bash\ngit clone https://github.com/xinkuang/china-stock-mcp\ncd china-stock-mcp\n```\n\n#### 3. ÂÆâË£Ö‰æùËµñ\n```bash\n# Êé®Ëçê‰ΩøÁî® uv ÂåÖÁÆ°ÁêÜÂô®\nuv sync\n\n# ÊàñËÄÖ‰ΩøÁî® pip\npip install -r requirements.txt\n```\n\n#### 4.",
        "start_pos": 8582,
        "end_pos": 8974,
        "token_count_estimate": 98,
        "source_type": "readme",
        "agent_id": "2d3e1c4302f7ff94"
      },
      {
        "chunk_id": 6,
        "text": "1)\n\n## üìä Êï∞ÊçÆÊîØÊåÅËåÉÂõ¥\n\n### ËÇ°Á•®Â∏ÇÂú∫\n- AËÇ° (‰∏äËØÅ„ÄÅÊ∑±ËØÅ)\n- BËÇ°\n- HËÇ° (Ê∏ØËÇ°)\n- ‰∏≠Â∞èÊùø„ÄÅÂàõ‰∏öÊùø„ÄÅÊñ∞‰∏âÊùø\n\n### Êï∞ÊçÆÁ±ªÂûã\n- ÂéÜÂè≤Ë°åÊÉÖÊï∞ÊçÆ (ÂàÜÈíüÁ∫ß„ÄÅÂ∞èÊó∂Á∫ß„ÄÅÊó•Á∫ß„ÄÅÂë®Á∫ß„ÄÅÊúàÁ∫ß„ÄÅÂπ¥Á∫ß)\n- ÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ\n- ÊäÄÊúØÊåáÊ†áËÆ°ÁÆó\n- Êñ∞ÈóªËµÑËÆØ\n- Ë¥¢Âä°Êä•Ë°® (ËµÑ‰∫ßË¥üÂÄ∫Ë°®„ÄÅÂà©Ê∂¶Ë°®„ÄÅÁé∞ÈáëÊµÅÈáèË°®)\n- Ë¥¢Âä°ÊåáÊ†á\n- ÂÜÖÈÉ®‰∫§ÊòìÊï∞ÊçÆ\n\n## üîß ÂºÄÂèëÂíåË¥°ÁåÆ\n\n### ÂºÄÂèëÁéØÂ¢ÉËÆæÁΩÆ\n\n1. ÂÖãÈöÜ‰ªìÂ∫ì\n```bash\ngit clone https://github.com/xinkuang/china-stock-mcp\ncd china-stock-mcp\n```\n\n2. ÂÆâË£ÖÂºÄÂèë‰æùËµñ\n```bash\nuv sync --dev\n```\n\n3. ËøõÂÖ•ÂºÄÂèëÊ®°Âºè\n```bash\nuv run -m china_stock_mcp\n```\n\n### ‰ª£Á†ÅÁªìÊûÑ\n\n```\nsrc/china_stock_mcp/\n‚îú‚îÄ‚îÄ __init__.py\n‚îú‚îÄ‚îÄ __main__.py    # ÂëΩ‰ª§Ë°åÂÖ•Âè£ÔºåÂ§ÑÁêÜÂêØÂä®ÂèÇÊï∞\n‚îú‚îÄ‚îÄ server.py      # MCP ÊúçÂä°Âô®Ê†∏ÂøÉÔºåÂÆö‰πâÊâÄÊúâÂ∑•ÂÖ∑\n‚îú‚îÄ‚îÄ mcp.json       # MCP ÈÖçÁΩÆËßÑËåÉ (ÂèØÈÄâ)\n‚îî‚îÄ‚îÄ py.typed       # Á±ªÂûãÊ†áÊ≥®Êñá‰ª∂\n```\n\n### Ê∑ªÂä†Êñ∞Â∑•ÂÖ∑\n\nÂú® `server.py` ‰∏≠‰ΩøÁî® `@mcp.tool` Ë£ÖÈ•∞Âô®Ê∑ªÂä†Êñ∞Â∑•ÂÖ∑Ôºö\n\n```python\n@mcp.tool(name=\"Â∑•ÂÖ∑‰∏≠ÊñáÂêçÁß∞\", description=\"Â∑•ÂÖ∑ÁöÑ‰∏≠ÊñáÊèèËø∞\")\ndef your_tool_name(param1: Annotated[str, Field(description=\"ÂèÇÊï∞ÊèèËø∞\")]) -> str:\n    \"\"\"Â∑•ÂÖ∑ËØ¶ÊÉÖÊèèËø∞\"\"\"\n    # ÂÆûÁé∞ÈÄªËæë\n    pass\n```\n\n## üìù ËÆ∏ÂèØËØÅ\n\nMIT License - ËØ¶ËßÅ [LICENSE](LICENSE) Êñá‰ª∂\n\n## ü§ù Ë¥°ÁåÆ\n\nÊ¨¢ËøéÊèê‰∫§ Issue Âíå Pull RequestÔºÅ\n\n## üôã‚Äç‚ôÇÔ∏è Â∏∏ËßÅÈóÆÈ¢ò\n\n**Q: ‰∏∫‰ªÄ‰πàÊó†Ê≥ïËé∑ÂèñÊï∞ÊçÆÔºü**\nA: ËØ∑Ê£ÄÊü•ÁΩëÁªúËøûÊé•ÂíåÊï∞ÊçÆÊ∫êÂèØÁî®ÊÄß„ÄÇÊüê‰∫õÊï∞ÊçÆÊ∫êÂèØËÉΩÊúâËÆøÈóÆÈôêÂà∂„ÄÇ\n\n**Q: HTTP Ê®°Âºè‰∏ãÊó†Ê≥ïËøûÊé•Ôºü**\nA: Á°ÆËÆ§Á´ØÂè£ 8081 Êú™Ë¢´ÂÖ∂‰ªñÊúçÂä°Âç†Áî®Ôºå‰∏îÈò≤ÁÅ´Â¢ôÂÖÅËÆ∏Áõ∏Â∫îÁ´ØÂè£ÁöÑËÆøÈóÆ„ÄÇ\n\n**Q: Â¶Ç‰ΩïÊõ¥Êñ∞Âà∞ÊúÄÊñ∞ÁâàÊú¨Ôºü**\nA: ‰ΩøÁî® Smithery ÂÆâË£ÖÁöÑÂèØ‰ª•Ëá™Âä®Êõ¥Êñ∞ÔºåÊâãÂä®ÂÆâË£ÖÁöÑËØ∑ÈáçÊñ∞ÊãâÂèñ‰ªìÂ∫ì‰ª£Á†Å„ÄÇ\n## üêû Ë∞ÉËØï\n\nÊúâÂÖ≥Â¶Ç‰Ωï‰ΩøÁî® @modelcontextprotocol/inspector Ë∞ÉËØïÊ≠§ÊúçÂä°Âô®ÁöÑËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇÈòÖ [DEBUG.md](DEBUG.md)„ÄÇ",
        "start_pos": 10430,
        "end_pos": 11588,
        "token_count_estimate": 289,
        "source_type": "readme",
        "agent_id": "2d3e1c4302f7ff94"
      }
    ]
  },
  {
    "agent_id": "cacfd475ad71c039",
    "name": "ai.smithery/yuhuison-mediawiki-mcp-server-auth",
    "source": "mcp",
    "source_url": "https://server.smithery.ai/@yuhuison/mediawiki-mcp-server-auth/mcp",
    "description": "Connect to your MediaWiki using simple credentials and manage content without OAuth. Search, read,‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "search",
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-16T11:19:24.929803Z",
    "indexed_at": "2026-02-18T04:09:15.950452",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Connect to MediaWiki using simple credentials",
        "Manage MediaWiki content without OAuth",
        "Search MediaWiki content",
        "Read MediaWiki content"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of capabilities but lacks detailed examples, structure, or limitations.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "bb6540949a12966f",
    "name": "ai.smithery/yuna0x0-anilist-mcp",
    "source": "mcp",
    "source_url": "https://github.com/yuna0x0/anilist-mcp",
    "description": "Access and interact with anime and manga data seamlessly. Retrieve detailed information about your‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-29T12:06:58.363545Z",
    "indexed_at": "2026-02-18T04:09:17.713857",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# AniList MCP Server\n\nA Model Context Protocol (MCP) server that interfaces with the AniList API, allowing LLM clients to access and interact with anime, manga, character, staff, and user data from AniList.\n\n## Features\n\n- Search for anime, manga, characters, staff, and studios\n- Get detailed information about specific anime, manga, characters, and staff members\n- Access user profiles and lists\n- Support for advanced filtering options\n- Retrieve genres and media tags\n- **Dual transport support**: Both HTTP and STDIO transports\n- **Cloud deployment ready**: Support Smithery and other platforms\n\n## Requirements\n\n- Node.js 18+\n\n## Local Installation (STDIO Transport)\n\n1. Add this server to your `mcp.json` / `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"anilist\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"anilist-mcp\"],\n      \"env\": {\n        \"ANILIST_TOKEN\": \"your_api_token\"\n      }\n    }\n  }\n}\n```\n\nYou may remove the `env` object entirely, if you are not planning to use the AniList Token for operations that require login.\n\n2. Restart your MCP client (e.g., Claude Desktop)\n3. Use the tools to interact with AniList\n\n## Server Deployment (HTTP Transport)\n\n### Self-Hosting\nFollow the [Local Development](#local-development) instructions to set up the project locally, then run:\n```bash\npnpm run start:http\n```\nThis will start the server on port 8081 by default. You can change the port by setting the `PORT` environment variable.\n\n### Cloud Deployment\n\nYou can deploy this MCP server to any cloud platform that supports Node.js server applications.\n\nYou can also deploy via MCP platforms like [Smithery](https://smithery.ai/server/@yuna0x0/anilist-mcp).\n\n## Configuration\n### Environment Variables (STDIO Transport and HTTP Transport server where host provides the config)\n\nWhen using the STDIO transport or hosting the HTTP transport server, you can pass configuration via environment variables:\n- `ANILIST_TOKEN`: (Optional) AniList API Token (Only needed for operations that require login)\n\n> [!CAUTION]\n> If you are hosting the HTTP transport server with token pre-configured, you should protect your endpoint and implement authentication before allowing users to access it. Otherwise, anyone can access your MCP server while using your AniList token.\n\n### HTTP Headers (HTTP Transport where user provides the config)\n\nWhen using the HTTP transport, user can pass configuration via HTTP headers:\n- `Anilist-Token`: (Optional) AniList API Token (Only needed for operations that require login)\n\nIf the user provides the token in the header, while the server also has `ANILIST_TOKEN` set, the header value will take precedence.\n\n### Get an AniList API Token (Optional)\n\nTo get an API token, follow these steps:\n\n1. Go to [AniList settings](https://anilist.co/settings/developer).\n2. Click on \"Create New Client\".\n3. Use this URL as your client's \"Redirect URL\":\n```\nhttps://anilist.co/api/v2/oauth/pin\n```\n\n4. Click \"Save\"\n5. Then go to https://anilist.co/api/v2/oauth/authorize?client_id={clientID}&response_type=token, replace the `{clientID}` with the client ID you get. It will ask you to log in and then provide you with the token to use.\n6. Copy the generated token and use it in your `.env` file or environment variables.\n\n## Available Tools\n\n### Misc Tools\n- **get_genres**: Get all available genres on AniList\n- **get_media_tags**: Get all available media tags on AniList\n- **get_site_statistics**: Get AniList site statistics over the last seven days\n- **get_studio**: Get information about a studio by its AniList ID or name\n- **favourite_studio**: [Requires Login] Favourite or unfavourite a studio by its ID\n\n### Activity Tools\n- **delete_activity**: [Requires Login] Delete the current authorized user's activity post\n- **get_activity**: Get a specific AniList activity by its ID\n- **get_user_activity**: Fetch activities from a user\n- **post_message_activity**: [Requires Login] Post a new message activity or update an existing one\n- **post_text_activity**: [Requires Login] Post a new text activity or update an existing one\n\n### List Tools\n- **get_user_anime_list**: Get a user's anime list\n- **get_user_manga_list**: Get a user's manga list\n- **add_list_entry**: [Requires Login] Add an entry to the authorized user's list\n- **remove_list_entry**: [Requires Login] Remove an entry from the authorized user's list\n- **update_list_entry**: [Requires Login] Update an entry on the authorized user's list\n\n### Media Tools\n- **get_anime**: Get detailed information about an anime by its AniList ID\n- **get_manga**: Get detailed information about a manga by its AniList ID\n- **favourite_anime**: [Requires Login] Favourite or unfavourite an anime by its ID\n- **favourite_manga**: [Requires Login] Favourite or unfavourite a manga by its ID\n\n### People Tools\n- **get_character**: Get information about a character by their AniList ID\n- **get_staff**: Get information about staff member by their AniList ID\n- **favourite_character**: [Requires Login] Favourite or unfavourite a character by its ID\n- **favourite_staff**: [Requires Login] Favourite or unfavourite a staff member by their ID\n- **get_todays_birthday_characters**: Get all characters whose birthday is today\n- **get_todays_birthday_staff**: Get all staff members whose birthday is today\n\n### Recommendation Tools\n- **get_recommendation**: Get an AniList recommendation by its ID\n- **get_recommendations_for_media**: Get AniList recommendations for a specific media\n\n### Search Tools\n- **search_activity**: Search for activities on AniList\n- **search_anime**: Search for anime with query term and filters\n- **search_manga**: Search for manga with query term and filters\n- **search_character**: Search for characters based on a query term\n- **search_staff**: Search for staff members based on a query term\n- **search_studio**: Search for studios based on a query term\n- **search_user**: Search for users on AniList\n\n### Thread Tools\n- **get_thread**: Get a specific thread by its AniList ID\n- **get_thread_comments**: Get comments for a specific thread\n- **delete_thread**: [Requires Login] Delete a thread by its ID\n\n### User Tools\n- **get_user_profile**: Get a user's AniList profile\n- **get_user_stats**: Get a user's AniList statistics\n- **get_full_user_info**: Get a user's complete profile and stats information\n- **get_user_recent_activity**: Get recent activity from a user\n- **get_authorized_user**: [Requires Login] Get profile information of the currently authorized user\n- **follow_user**: [Requires Login] Follow or unfollow a user by their ID\n- **update_user**: [Requires Login] Update user settings\n\n## Example Usage\n\n### Basic Anime Search\n\n```\nCan you search for anime similar to \"Bocchi the Rock!\"?\n```\n\n### Get Character Info\n\n```\nCan you tell me about the character Hitori Gotou? Use the AniList tools to find information.\n```\n\n### Compare Studio Works\n\n```\nWhat anime has Studio Ghibli produced? Can you list their most popular works?\n```\n\n## Local Development\n\nThis project uses [pnpm](https://pnpm.io) as its package manager.\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/yuna0x0/anilist-mcp.git\ncd anilist-mcp\npnpm install\n```\n\n### Configuration (Optional)\n\n1. Create a `.env` file by copying the example:\n```bash\ncp env.example .env\n```\n\n2. Edit the `.env` file and add your AniList API token:\n```\nANILIST_TOKEN=your_api_token\n```\n\n## Debugging with MCP Inspector\n\nYou can use the MCP Inspector to test and debug the AniList MCP server:\n\n```bash\nnpx @modelcontextprotocol/inspector -e ANILIST_TOKEN=your_api_token npx anilist-mcp\n\n# Use this instead when Local Development\npnpm run inspector\n```\n\nThen open your browser to the provided URL (usually http://localhost:6274) to access the MCP Inspector interface. From there, you can:\n\n1. Connect to your running AniList MCP server\n2. Browse available tools\n3. Run tools with custom parameters\n4. View the responses\n\nThis is particularly useful for testing your setup before connecting it to MCP clients like Claude Desktop.\n\n## Docker\n\nPull from GitHub Container Registry:\n```bash\ndocker pull ghcr.io/yuna0x0/anilist-mcp\n```\n\nDocker build (Local Development):\n```bash\ndocker build -t ghcr.io/yuna0x0/anilist-mcp .\n```\n\nDocker multi-platform build (Local Development):\n```bash\ndocker buildx build --platform linux/amd64,linux/arm64 -t ghcr.io/yuna0x0/anilist-mcp .\n```\n\n## MCP Bundles (MCPB)\n\nTo create an MCP Bundle for this server, run:\n```bash\npnpm run pack:mcpb\n```\n\n## Security Notice\n\nThis MCP server accepts your AniList API token in the .env file, environment variable or HTTP header. Keep this information secure and never commit it to version control.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search for anime, manga, characters, staff, and studios",
        "Retrieve detailed information about specific anime, manga, characters, and staff members",
        "Access and manage user profiles and lists",
        "Apply advanced filtering options for searches",
        "Retrieve genres and media tags from AniList",
        "Support both HTTP and STDIO transport protocols",
        "Deploy the server locally or on cloud platforms including Smithery",
        "Perform user-specific actions such as favoriting media, posting activities, and updating lists with authentication",
        "Fetch AniList site statistics and recommendations",
        "Debug and test the server using MCP Inspector"
      ],
      "limitations": [
        "Operations requiring login need a valid AniList API token",
        "Users must secure the HTTP endpoint if the server is deployed with a pre-configured token to prevent unauthorized access",
        "No built-in authentication for HTTP transport server; users must implement their own",
        "Requires Node.js version 18 or higher",
        "Rate limits and API usage constraints are subject to AniList API policies (not explicitly detailed)"
      ],
      "requirements": [
        "Node.js 18 or higher installed",
        "Optional AniList API token for login-required operations",
        "Environment variable or HTTP header configuration for AniList token",
        "MCP client or compatible environment to interact with the server",
        "pnpm package manager for local development and building",
        "Docker for containerized deployment (optional)"
      ]
    },
    "documentation_quality": 0.92,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with login requirements, usage examples, configuration options, deployment methods, security notices, and debugging guidance.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# AniList MCP Server\n\nA Model Context Protocol (MCP) server that interfaces with the AniList API, allowing LLM clients to access and interact with anime, manga, character, staff, and user data from AniList.\n\n## Features\n\n- Search for anime, manga, characters, staff, and studios\n- Get detailed information about specific anime, manga, characters, and staff members\n- Access user profiles and lists\n- Support for advanced filtering options\n- Retrieve genres and media tags\n- **Dual transport support**: Both HTTP and STDIO transports\n- **Cloud deployment ready**: Support Smithery and other platforms\n\n## Requirements\n\n- Node.js 18+\n\n## Local Installation (STDIO Transport)\n\n1. Add this server to your `mcp.json` / `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"anilist\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"anilist-mcp\"],\n      \"env\": {\n        \"ANILIST_TOKEN\": \"your_api_token\"\n      }\n    }\n  }\n}\n```\n\nYou may remove the `env` object entirely, if you are not planning to use the AniList Token for operations that require login.\n\n2. Restart your MCP client (e.g., Claude Desktop)\n3. Use the tools to interact with AniList\n\n## Server Deployment (HTTP Transport)\n\n### Self-Hosting\nFollow the [Local Development](#local-development) instructions to set up the project locally, then run:\n```bash\npnpm run start:http\n```\nThis will start the server on port 8081 by default. You can change the port by setting the `PORT` environment variable.\n\n### Cloud Deployment\n\nYou can deploy this MCP server to any cloud platform that supports Node.js server applications.\n\nYou can also deploy via MCP platforms like [Smithery](https://smithery.ai/server/@yuna0x0/anilist-mcp).",
        "start_pos": 0,
        "end_pos": 1689,
        "token_count_estimate": 422,
        "source_type": "readme",
        "agent_id": "bb6540949a12966f"
      },
      {
        "chunk_id": 1,
        "text": "ing the HTTP transport server, you can pass configuration via environment variables:\n- `ANILIST_TOKEN`: (Optional) AniList API Token (Only needed for operations that require login)\n\n> [!CAUTION]\n> If you are hosting the HTTP transport server with token pre-configured, you should protect your endpoint and implement authentication before allowing users to access it. Otherwise, anyone can access your MCP server while using your AniList token.\n\n### HTTP Headers (HTTP Transport where user provides the config)\n\nWhen using the HTTP transport, user can pass configuration via HTTP headers:\n- `Anilist-Token`: (Optional) AniList API Token (Only needed for operations that require login)\n\nIf the user provides the token in the header, while the server also has `ANILIST_TOKEN` set, the header value will take precedence.\n\n### Get an AniList API Token (Optional)\n\nTo get an API token, follow these steps:\n\n1. Go to [AniList settings](https://anilist.co/settings/developer).\n2. Click on \"Create New Client\".\n3. Use this URL as your client's \"Redirect URL\":\n```\nhttps://anilist.co/api/v2/oauth/pin\n```\n\n4. Click \"Save\"\n5. Then go to https://anilist.co/api/v2/oauth/authorize?client_id={clientID}&response_type=token, replace the `{clientID}` with the client ID you get. It will ask you to log in and then provide you with the token to use.\n6. Copy the generated token and use it in your `.env` file or environment variables.",
        "start_pos": 1848,
        "end_pos": 3265,
        "token_count_estimate": 354,
        "source_type": "readme",
        "agent_id": "bb6540949a12966f"
      },
      {
        "chunk_id": 2,
        "text": "y**: [Requires Login] Delete the current authorized user's activity post\n- **get_activity**: Get a specific AniList activity by its ID\n- **get_user_activity**: Fetch activities from a user\n- **post_message_activity**: [Requires Login] Post a new message activity or update an existing one\n- **post_text_activity**: [Requires Login] Post a new text activity or update an existing one\n\n### List Tools\n- **get_user_anime_list**: Get a user's anime list\n- **get_user_manga_list**: Get a user's manga list\n- **add_list_entry**: [Requires Login] Add an entry to the authorized user's list\n- **remove_list_entry**: [Requires Login] Remove an entry from the authorized user's list\n- **update_list_entry**: [Requires Login] Update an entry on the authorized user's list\n\n### Media Tools\n- **get_anime**: Get detailed information about an anime by its AniList ID\n- **get_manga**: Get detailed information about a manga by its AniList ID\n- **favourite_anime**: [Requires Login] Favourite or unfavourite an anime by its ID\n- **favourite_manga**: [Requires Login] Favourite or unfavourite a manga by its ID\n\n### People Tools\n- **get_character**: Get information about a character by their AniList ID\n- **get_staff**: Get information about staff member by their AniList ID\n- **favourite_character**: [Requires Login] Favourite or unfavourite a character by its ID\n- **favourite_staff**: [Requires Login] Favourite or unfavourite a staff member by their ID\n- **get_todays_birthday_characters**: Get all characters whose birthday is today\n- **get_todays_birthday_staff**: Get all staff members whose birthday is today\n\n### Recommendation Tools\n- **get_recommendation**: Get an AniList recommendation by its ID\n- **get_recommendations_for_media**: Get AniList recommendations for a specific media\n\n### Search Tools\n- **search_activity**: Search for activities on AniList\n- **search_anime**: Search for anime with query term and filters\n- **search_manga**: Search for manga with query term and filters\n- **search_character**: Search for characters based on a query t",
        "start_pos": 3696,
        "end_pos": 5744,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "bb6540949a12966f"
      },
      {
        "chunk_id": 3,
        "text": "iList\n- **search_anime**: Search for anime with query term and filters\n- **search_manga**: Search for manga with query term and filters\n- **search_character**: Search for characters based on a query term\n- **search_staff**: Search for staff members based on a query term\n- **search_studio**: Search for studios based on a query term\n- **search_user**: Search for users on AniList\n\n### Thread Tools\n- **get_thread**: Get a specific thread by its AniList ID\n- **get_thread_comments**: Get comments for a specific thread\n- **delete_thread**: [Requires Login] Delete a thread by its ID\n\n### User Tools\n- **get_user_profile**: Get a user's AniList profile\n- **get_user_stats**: Get a user's AniList statistics\n- **get_full_user_info**: Get a user's complete profile and stats information\n- **get_user_recent_activity**: Get recent activity from a user\n- **get_authorized_user**: [Requires Login] Get profile information of the currently authorized user\n- **follow_user**: [Requires Login] Follow or unfollow a user by their ID\n- **update_user**: [Requires Login] Update user settings\n\n## Example Usage\n\n### Basic Anime Search\n\n```\nCan you search for anime similar to \"Bocchi the Rock!\"?\n```\n\n### Get Character Info\n\n```\nCan you tell me about the character Hitori Gotou? Use the AniList tools to find information.\n```\n\n### Compare Studio Works\n\n```\nWhat anime has Studio Ghibli produced? Can you list their most popular works?\n```\n\n## Local Development\n\nThis project uses [pnpm](https://pnpm.io) as its package manager.\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/yuna0x0/anilist-mcp.git\ncd anilist-mcp\npnpm install\n```\n\n### Configuration (Optional)\n\n1. Create a `.env` file by copying the example:\n```bash\ncp env.example .env\n```\n\n2.",
        "start_pos": 5544,
        "end_pos": 7314,
        "token_count_estimate": 442,
        "source_type": "readme",
        "agent_id": "bb6540949a12966f"
      },
      {
        "chunk_id": 4,
        "text": "pi_token\n```\n\n## Debugging with MCP Inspector\n\nYou can use the MCP Inspector to test and debug the AniList MCP server:\n\n```bash\nnpx @modelcontextprotocol/inspector -e ANILIST_TOKEN=your_api_token npx anilist-mcp\n\n# Use this instead when Local Development\npnpm run inspector\n```\n\nThen open your browser to the provided URL (usually http://localhost:6274) to access the MCP Inspector interface. From there, you can:\n\n1. Connect to your running AniList MCP server\n2. Browse available tools\n3. Run tools with custom parameters\n4. View the responses\n\nThis is particularly useful for testing your setup before connecting it to MCP clients like Claude Desktop.\n\n## Docker\n\nPull from GitHub Container Registry:\n```bash\ndocker pull ghcr.io/yuna0x0/anilist-mcp\n```\n\nDocker build (Local Development):\n```bash\ndocker build -t ghcr.io/yuna0x0/anilist-mcp .\n```\n\nDocker multi-platform build (Local Development):\n```bash\ndocker buildx build --platform linux/amd64,linux/arm64 -t ghcr.io/yuna0x0/anilist-mcp .\n```\n\n## MCP Bundles (MCPB)\n\nTo create an MCP Bundle for this server, run:\n```bash\npnpm run pack:mcpb\n```\n\n## Security Notice\n\nThis MCP server accepts your AniList API token in the .env file, environment variable or HTTP header. Keep this information secure and never commit it to version control.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
        "start_pos": 7392,
        "end_pos": 8790,
        "token_count_estimate": 349,
        "source_type": "readme",
        "agent_id": "bb6540949a12966f"
      }
    ]
  },
  {
    "agent_id": "87ff6c886e7e72bf",
    "name": "ai.smithery/yuna0x0-hackmd-mcp",
    "source": "mcp",
    "source_url": "https://github.com/yuna0x0/hackmd-mcp",
    "description": "Interact with your HackMD notes and teams seamlessly. Manage your notes, view reading history, and‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-15T03:33:42.700252Z",
    "indexed_at": "2026-02-18T04:09:19.767693",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# HackMD MCP Server\n\nA Model Context Protocol (MCP) server that interfaces with the [HackMD API](https://hackmd.io/@hackmd-api/developer-portal), allowing LLM clients to access and interact with HackMD notes, teams, user profiles, and history data.\n\n## Features\n\n- Get user profile information\n- Create, read, update, and delete notes\n- Manage team notes and collaborate with team members\n- Access reading history\n- List and manage teams\n- **Dual transport support**: Both HTTP and STDIO transports\n- **Cloud deployment ready**: Support Smithery and other platforms\n\n## Requirements\n\n- Node.js 18+\n\n## Local Installation (STDIO Transport)\n\n1. Add this server to your `mcp.json` / `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"hackmd\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"hackmd-mcp\"],\n      \"env\": {\n        \"HACKMD_API_TOKEN\": \"your_api_token\"\n      }\n    }\n  }\n}\n```\n\nYou may also optionally set the `HACKMD_API_URL` environment variable if you need to use a different HackMD API endpoint.\n\n2. Restart your MCP client (e.g., Claude Desktop)\n3. Use the tools to interact with HackMD\n\n## Server Deployment (HTTP Transport)\n\n### Self-Hosting\nFollow the [Local Development](#local-development) instructions to set up the project locally, then run:\n```bash\npnpm run start:http\n```\nThis will start the server on port 8081 by default. You can change the port by setting the `PORT` environment variable.\n\n### Cloud Deployment\n\nYou can deploy this MCP server to any cloud platform that supports Node.js server applications.\n\nYou can also deploy via MCP platforms like [Smithery](https://smithery.ai/server/@yuna0x0/hackmd-mcp).\n\n## Configuration\n### Environment Variables (STDIO Transport and HTTP Transport server where host provides the config)\n\nWhen using the STDIO transport or hosting the HTTP transport server, you can pass configuration via environment variables:\n- `HACKMD_API_TOKEN`: HackMD API Token (Required for all operations)\n- `HACKMD_API_URL`: (Optional) HackMD API URL (Defaults to https://api.hackmd.io/v1)\n\nEnvironment variables applied only for the HTTP transport server:\n- `ALLOWED_HACKMD_API_URLS`: (Optional) A comma-separated list of allowed HackMD API URLs. The server will reject requests if the provide HackMD API URL is not in this list. If not set, only the default URL (https://api.hackmd.io/v1) is allowed.\n\n> [!CAUTION]\n> If you are hosting the HTTP transport server with token pre-configured, you should protect your endpoint and implement authentication before allowing users to access it. Otherwise, anyone can access your MCP server while using your HackMD token.\n\n### HTTP Headers (HTTP Transport where user provides the config)\n\nWhen using the HTTP transport, user can pass configuration via HTTP headers:\n- `Hackmd-Api-Token`: HackMD API Token (Required for all operations)\n- `Hackmd-Api-Url`: (Optional) HackMD API URL (Defaults to https://api.hackmd.io/v1)\n\nIf the user provides the token in the header, while the server also has `HACKMD_API_TOKEN` set, the header value will take precedence.\n\n### Get a HackMD API Token\n\nTo get an API token, follow these steps:\n\n1. Go to [HackMD settings](https://hackmd.io/settings#api).\n2. Click on \"Create API Token\".\n3. Copy the generated token and use it in your `.env` file or environment variables.\n\n## Available Tools\n\n### Profile Tools\n- **get_user_info**: Get information about the authenticated user\n\n### Teams Tools\n- **list_teams**: List all teams accessible to the user\n\n### History Tools\n- **get_history**: Get user's reading history\n\n### Team Notes Tools\n- **list_team_notes**: List all notes in a team\n- **create_team_note**: Create a new note in a team\n- **update_team_note**: Update an existing note in a team\n- **delete_team_note**: Delete a note in a team\n\n### User Notes Tools\n- **list_user_notes**: List all notes owned by the user\n- **get_note**: Get a note by its ID\n- **create_note**: Create a new note\n- **update_note**: Update an existing note\n- **delete_note**: Delete a note\n\n## Example Usage\n\n### Basic Note Management\n\n```\nCan you help me manage my HackMD notes?\n```\n\n### List Notes\n\n```\nPlease list all my notes.\n```\n\n### Create a New Note\n\n````\nCreate a new note with the title \"Meeting Notes\" and content:\n```\n# Meeting Notes\n\nDiscussion points:\n- Item 1\n- Item 2\n```\n````\n\n### Team Collaboration\n\n```\nShow me all the teams I'm part of and list the notes in the first team.\n```\n\n## Local Development\n\nThis project uses [pnpm](https://pnpm.io) as its package manager.\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/yuna0x0/hackmd-mcp.git\ncd hackmd-mcp\npnpm install\n```\n\n### Configuration\n\n1. Create a `.env` file by copying the example:\n```bash\ncp env.example .env\n```\n\n2. Edit the `.env` file and add your HackMD API token:\n```\nHACKMD_API_TOKEN=your_api_token\n```\n\n## Debugging with MCP Inspector\n\nYou can use the MCP Inspector to test and debug the HackMD MCP server:\n\n```bash\nnpx @modelcontextprotocol/inspector -e HACKMD_API_TOKEN=your_api_token npx hackmd-mcp\n\n# Use this instead when Local Development\npnpm run inspector\n```\n\nThen open your browser to the provided URL (usually http://localhost:6274) to access the MCP Inspector interface. From there, you can:\n\n1. Connect to your running HackMD MCP server\n2. Browse available tools\n3. Run tools with custom parameters\n4. View the responses\n\nThis is particularly useful for testing your setup before connecting it to MCP clients like Claude Desktop.\n\n## Docker\n\nPull from GitHub Container Registry:\n```bash\ndocker pull ghcr.io/yuna0x0/hackmd-mcp\n```\n\nDocker build (Local Development):\n```bash\ndocker build -t ghcr.io/yuna0x0/hackmd-mcp .\n```\n\nDocker multi-platform build (Local Development):\n```bash\ndocker buildx build --platform linux/amd64,linux/arm64 -t ghcr.io/yuna0x0/hackmd-mcp .\n```\n\n## MCP Bundles (MCPB)\n\nTo create an MCP Bundle for this server, run:\n```bash\npnpm run pack:mcpb\n```\n\n## Security Notice\n\nThis MCP server accepts your HackMD API token in the .env file, environment variable or HTTP header. Keep this information secure and never commit it to version control.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Get user profile information",
        "Create, read, update, and delete user notes",
        "List and manage teams",
        "Create, read, update, and delete team notes",
        "Access user's reading history",
        "Support both HTTP and STDIO transports for communication",
        "Deploy on cloud platforms and MCP platforms like Smithery"
      ],
      "limitations": [
        "Requires a valid HackMD API token for all operations",
        "HTTP transport server must be secured with authentication to prevent unauthorized access",
        "Only allows HackMD API URLs specified in ALLOWED_HACKMD_API_URLS environment variable or defaults to https://api.hackmd.io/v1",
        "No mention of support for real-time collaboration or WebSocket connections",
        "No explicit rate limit information provided"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "A valid HackMD API token obtained from HackMD settings",
        "Environment variables or HTTP headers to provide API token and optionally API URL",
        "For HTTP transport, recommended to implement authentication to protect the server endpoint"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear tool descriptions, environment configuration, deployment options, and security considerations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# HackMD MCP Server\n\nA Model Context Protocol (MCP) server that interfaces with the [HackMD API](https://hackmd.io/@hackmd-api/developer-portal), allowing LLM clients to access and interact with HackMD notes, teams, user profiles, and history data.\n\n## Features\n\n- Get user profile information\n- Create, read, update, and delete notes\n- Manage team notes and collaborate with team members\n- Access reading history\n- List and manage teams\n- **Dual transport support**: Both HTTP and STDIO transports\n- **Cloud deployment ready**: Support Smithery and other platforms\n\n## Requirements\n\n- Node.js 18+\n\n## Local Installation (STDIO Transport)\n\n1. Add this server to your `mcp.json` / `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"hackmd\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"hackmd-mcp\"],\n      \"env\": {\n        \"HACKMD_API_TOKEN\": \"your_api_token\"\n      }\n    }\n  }\n}\n```\n\nYou may also optionally set the `HACKMD_API_URL` environment variable if you need to use a different HackMD API endpoint.\n\n2. Restart your MCP client (e.g., Claude Desktop)\n3. Use the tools to interact with HackMD\n\n## Server Deployment (HTTP Transport)\n\n### Self-Hosting\nFollow the [Local Development](#local-development) instructions to set up the project locally, then run:\n```bash\npnpm run start:http\n```\nThis will start the server on port 8081 by default. You can change the port by setting the `PORT` environment variable.\n\n### Cloud Deployment\n\nYou can deploy this MCP server to any cloud platform that supports Node.js server applications.\n\nYou can also deploy via MCP platforms like [Smithery](https://smithery.ai/server/@yuna0x0/hackmd-mcp).",
        "start_pos": 0,
        "end_pos": 1650,
        "token_count_estimate": 412,
        "source_type": "readme",
        "agent_id": "87ff6c886e7e72bf"
      },
      {
        "chunk_id": 1,
        "text": "pass configuration via environment variables:\n- `HACKMD_API_TOKEN`: HackMD API Token (Required for all operations)\n- `HACKMD_API_URL`: (Optional) HackMD API URL (Defaults to https://api.hackmd.io/v1)\n\nEnvironment variables applied only for the HTTP transport server:\n- `ALLOWED_HACKMD_API_URLS`: (Optional) A comma-separated list of allowed HackMD API URLs. The server will reject requests if the provide HackMD API URL is not in this list. If not set, only the default URL (https://api.hackmd.io/v1) is allowed.\n\n> [!CAUTION]\n> If you are hosting the HTTP transport server with token pre-configured, you should protect your endpoint and implement authentication before allowing users to access it. Otherwise, anyone can access your MCP server while using your HackMD token.\n\n### HTTP Headers (HTTP Transport where user provides the config)\n\nWhen using the HTTP transport, user can pass configuration via HTTP headers:\n- `Hackmd-Api-Token`: HackMD API Token (Required for all operations)\n- `Hackmd-Api-Url`: (Optional) HackMD API URL (Defaults to https://api.hackmd.io/v1)\n\nIf the user provides the token in the header, while the server also has `HACKMD_API_TOKEN` set, the header value will take precedence.\n\n### Get a HackMD API Token\n\nTo get an API token, follow these steps:\n\n1. Go to [HackMD settings](https://hackmd.io/settings#api).\n2. Click on \"Create API Token\".\n3. Copy the generated token and use it in your `.env` file or environment variables.",
        "start_pos": 1848,
        "end_pos": 3304,
        "token_count_estimate": 364,
        "source_type": "readme",
        "agent_id": "87ff6c886e7e72bf"
      },
      {
        "chunk_id": 2,
        "text": "date an existing note in a team\n- **delete_team_note**: Delete a note in a team\n\n### User Notes Tools\n- **list_user_notes**: List all notes owned by the user\n- **get_note**: Get a note by its ID\n- **create_note**: Create a new note\n- **update_note**: Update an existing note\n- **delete_note**: Delete a note\n\n## Example Usage\n\n### Basic Note Management\n\n```\nCan you help me manage my HackMD notes?\n```\n\n### List Notes\n\n```\nPlease list all my notes.\n```\n\n### Create a New Note\n\n````\nCreate a new note with the title \"Meeting Notes\" and content:\n```\n# Meeting Notes\n\nDiscussion points:\n- Item 1\n- Item 2\n```\n````\n\n### Team Collaboration\n\n```\nShow me all the teams I'm part of and list the notes in the first team.\n```\n\n## Local Development\n\nThis project uses [pnpm](https://pnpm.io) as its package manager.\n\nClone the repository and install dependencies:\n\n```bash\ngit clone https://github.com/yuna0x0/hackmd-mcp.git\ncd hackmd-mcp\npnpm install\n```\n\n### Configuration\n\n1. Create a `.env` file by copying the example:\n```bash\ncp env.example .env\n```\n\n2. Edit the `.env` file and add your HackMD API token:\n```\nHACKMD_API_TOKEN=your_api_token\n```\n\n## Debugging with MCP Inspector\n\nYou can use the MCP Inspector to test and debug the HackMD MCP server:\n\n```bash\nnpx @modelcontextprotocol/inspector -e HACKMD_API_TOKEN=your_api_token npx hackmd-mcp\n\n# Use this instead when Local Development\npnpm run inspector\n```\n\nThen open your browser to the provided URL (usually http://localhost:6274) to access the MCP Inspector interface. From there, you can:\n\n1. Connect to your running HackMD MCP server\n2. Browse available tools\n3. Run tools with custom parameters\n4. View the responses\n\nThis is particularly useful for testing your setup before connecting it to MCP clients like Claude Desktop.\n\n## Docker\n\nPull from GitHub Container Registry:\n```bash\ndocker pull ghcr.io/yuna0x0/hackmd-mcp\n```\n\nDocker build (Local Development):\n```bash\ndocker build -t ghcr.io/yuna0x0/hackmd-mcp .",
        "start_pos": 3696,
        "end_pos": 5665,
        "token_count_estimate": 492,
        "source_type": "readme",
        "agent_id": "87ff6c886e7e72bf"
      },
      {
        "chunk_id": 3,
        "text": "ude Desktop.\n\n## Docker\n\nPull from GitHub Container Registry:\n```bash\ndocker pull ghcr.io/yuna0x0/hackmd-mcp\n```\n\nDocker build (Local Development):\n```bash\ndocker build -t ghcr.io/yuna0x0/hackmd-mcp .\n```\n\nDocker multi-platform build (Local Development):\n```bash\ndocker buildx build --platform linux/amd64,linux/arm64 -t ghcr.io/yuna0x0/hackmd-mcp .\n```\n\n## MCP Bundles (MCPB)\n\nTo create an MCP Bundle for this server, run:\n```bash\npnpm run pack:mcpb\n```\n\n## Security Notice\n\nThis MCP server accepts your HackMD API token in the .env file, environment variable or HTTP header. Keep this information secure and never commit it to version control.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.",
        "start_pos": 5465,
        "end_pos": 6218,
        "token_count_estimate": 188,
        "source_type": "readme",
        "agent_id": "87ff6c886e7e72bf"
      }
    ]
  },
  {
    "agent_id": "ae94beecf5ae8717",
    "name": "ai.smithery/zeta-chain-cli",
    "source": "mcp",
    "source_url": "https://github.com/zeta-chain/cli",
    "description": "Create friendly, customizable greetings for any name or audience. Break the ice in demos, onboardi‚Ä¶",
    "tools": [],
    "detected_capabilities": [
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-19T16:55:43.436334Z",
    "indexed_at": "2026-02-18T04:09:20.995062",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# ZetaChain CLI\n\nA command-line interface for building and interacting with\n[ZetaChain](https://www.zetachain.com) universal applications. Seamlessly\ninteract with EVM, Solana, Bitcoin, Sui, and TON, all from one CLI.\n\n## ‚ú® Features\n\n- Scaffold new ZetaChain universal apps from templates\n- Spin up a local multi-chain development environment (EVM, Solana, etc.) in one\n  command\n- Query cross-chain fees, contracts, balances, cross-chain transaction, tokens,\n  and more\n- Make cross-chain calls between Solana, Sui, Bitcoin, TON, and universal apps\n  on ZetaChain\n- Transfer supported tokens across connected chains\n\n## ‚úÖ Prerequisites\n\n- Node.js ‚â• 18\n- Git (for template cloning)\n- (Optional) Docker ‚â• 24 for `localnet`\n\n## üöÄ Quick Start\n\nRun without installing:\n\n```bash\nnpx zetachain@next new\n```\n\nOr install globally:\n\n```bash\nnpm install -g zetachain@latest\n```\n\nUse `zetachain@next` for bleeding-edge builds.\n\n## üìò Examples\n\nCreate a new project:\n\n```bash\nzetachain new\n```\n\nStart localnet:\n\n```bash\nzetachain localnet start\n```\n\nQuery cross-chain balances:\n\n```bash\nzetachain query balances\n```\n\n## ü§ñ MCP Server Installation\n\nThe ZetaChain CLI can be used as an MCP (Model Context Protocol) server, allowing AI assistants like Claude Code and Cursor to execute ZetaChain commands.\n\n### Local Installation (Recommended)\n\nInstall locally for full access to your filesystem, accounts, and localnet:\n\n```bash\nnpm install -g zetachain\nzetachain mcp install --client claude    # for Claude Code\n# or\nzetachain mcp install --client cursor    # for Cursor\n```\n\nThen restart your AI editor to activate the MCP server.\n\n**Check installation status:**\n```bash\nzetachain mcp list\n```\n\n**Remove:**\n```bash\nzetachain mcp remove --client claude\n```\n\n### Cloud Installation (Smithery)\n\nFor quick setup without local installation, visit [Smithery](https://smithery.ai/server/@zeta-chain/cli) and click \"One-Click Install\".\n\n‚ö†Ô∏è **Note**: The cloud version runs on remote servers and cannot access your local files, accounts, or localnet.\n\n## üß≠ CLI Reference\n\nFor full command documentation:\n\n```bash\nzetachain docs\n```\n\nOr use `--help` with any command:\n\n```bash\nzetachain accounts --help\n```\n\n## ü§ù Contributing\n\nWe welcome contributions! Please open issues or submit pull requests.\n\n## üìö Learn More\n\n- [ZetaChain Docs](https://www.zetachain.com/docs)\n- [CLI Docs](https://www.zetachain.com/docs/reference/cli/)\n- [Join Discord](https://discord.gg/zetachain)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Scaffold new ZetaChain universal applications from templates",
        "Spin up a local multi-chain development environment supporting EVM, Solana, Bitcoin, Sui, and TON",
        "Query cross-chain fees, contracts, balances, transactions, and tokens",
        "Make cross-chain calls between Solana, Sui, Bitcoin, TON, and universal apps on ZetaChain",
        "Transfer supported tokens across connected chains",
        "Operate as an MCP server enabling AI assistants to execute ZetaChain commands",
        "Install and manage MCP server clients for AI tools like Claude Code and Cursor"
      ],
      "limitations": [
        "Cloud MCP server installation cannot access local files, accounts, or localnet",
        "Requires Node.js version 18 or higher",
        "Docker is optional but required for localnet functionality",
        "Local installation needed for full filesystem and account access"
      ],
      "requirements": [
        "Node.js version 18 or higher",
        "Git installed for cloning templates",
        "Optional Docker version 24 or higher for running localnet",
        "Permissions to install global npm packages for CLI and MCP server setup"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed feature descriptions, MCP server setup guidance, and clearly states limitations and prerequisites.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# ZetaChain CLI\n\nA command-line interface for building and interacting with\n[ZetaChain](https://www.zetachain.com) universal applications. Seamlessly\ninteract with EVM, Solana, Bitcoin, Sui, and TON, all from one CLI.\n\n## ‚ú® Features\n\n- Scaffold new ZetaChain universal apps from templates\n- Spin up a local multi-chain development environment (EVM, Solana, etc.) in one\n  command\n- Query cross-chain fees, contracts, balances, cross-chain transaction, tokens,\n  and more\n- Make cross-chain calls between Solana, Sui, Bitcoin, TON, and universal apps\n  on ZetaChain\n- Transfer supported tokens across connected chains\n\n## ‚úÖ Prerequisites\n\n- Node.js ‚â• 18\n- Git (for template cloning)\n- (Optional) Docker ‚â• 24 for `localnet`\n\n## üöÄ Quick Start\n\nRun without installing:\n\n```bash\nnpx zetachain@next new\n```\n\nOr install globally:\n\n```bash\nnpm install -g zetachain@latest\n```\n\nUse `zetachain@next` for bleeding-edge builds.\n\n## üìò Examples\n\nCreate a new project:\n\n```bash\nzetachain new\n```\n\nStart localnet:\n\n```bash\nzetachain localnet start\n```\n\nQuery cross-chain balances:\n\n```bash\nzetachain query balances\n```\n\n## ü§ñ MCP Server Installation\n\nThe ZetaChain CLI can be used as an MCP (Model Context Protocol) server, allowing AI assistants like Claude Code and Cursor to execute ZetaChain commands.\n\n### Local Installation (Recommended)\n\nInstall locally for full access to your filesystem, accounts, and localnet:\n\n```bash\nnpm install -g zetachain\nzetachain mcp install --client claude    # for Claude Code\n# or\nzetachain mcp install --client cursor    # for Cursor\n```\n\nThen restart your AI editor to activate the MCP server.\n\n**Check installation status:**\n```bash\nzetachain mcp list\n```\n\n**Remove:**\n```bash\nzetachain mcp remove --client claude\n```\n\n### Cloud Installation (Smithery)\n\nFor quick setup without local installation, visit [Smithery](https://smithery.ai/server/@zeta-chain/cli) and click \"One-Click Install\".\n\n‚ö†Ô∏è **Note**: The cloud version runs on remote servers and cannot access your local files, accounts, or localnet.",
        "start_pos": 0,
        "end_pos": 2027,
        "token_count_estimate": 506,
        "source_type": "readme",
        "agent_id": "ae94beecf5ae8717"
      },
      {
        "chunk_id": 1,
        "text": "[Smithery](https://smithery.ai/server/@zeta-chain/cli) and click \"One-Click Install\".\n\n‚ö†Ô∏è **Note**: The cloud version runs on remote servers and cannot access your local files, accounts, or localnet.\n\n## üß≠ CLI Reference\n\nFor full command documentation:\n\n```bash\nzetachain docs\n```\n\nOr use `--help` with any command:\n\n```bash\nzetachain accounts --help\n```\n\n## ü§ù Contributing\n\nWe welcome contributions! Please open issues or submit pull requests.\n\n## üìö Learn More\n\n- [ZetaChain Docs](https://www.zetachain.com/docs)\n- [CLI Docs](https://www.zetachain.com/docs/reference/cli/)\n- [Join Discord](https://discord.gg/zetachain)",
        "start_pos": 1827,
        "end_pos": 2449,
        "token_count_estimate": 155,
        "source_type": "readme",
        "agent_id": "ae94beecf5ae8717"
      }
    ]
  },
  {
    "agent_id": "39a886a3474fd16e",
    "name": "ai.smithery/zhaoganghao-hellomcp",
    "source": "mcp",
    "source_url": "https://github.com/zhaoganghao/hellomcp",
    "description": "Greet people by name with friendly, concise messages. Explore the origin of 'Hello, World' for fun‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-30T10:35:36.775049Z",
    "indexed_at": "2026-02-18T04:09:22.534264",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Greet people by name with friendly, concise messages",
        "Provide information about the origin of 'Hello, World'"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of greeting functionality and a fun exploration topic but lacks detailed structure, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "e212b37d0f975ed6",
    "name": "ai.smithery/zwldarren-akshare-one-mcp",
    "source": "mcp",
    "source_url": "https://github.com/zwldarren/akshare-one-mcp",
    "description": "Provide access to Chinese stock market data including historical prices, real-time data, news, and‚Ä¶",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-11T19:15:39.84882Z",
    "indexed_at": "2026-02-18T04:09:26.559351",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# AKShare One MCP Server\n\n<div align=\"center\">\n  <a href=\"README.md\">English</a> | \n  <a href=\"README_zh.md\">‰∏≠Êñá</a>\n</div>\n\n<!-- mcp-name: io.github.zwldarren/akshare-one-mcp -->\n\n[![smithery badge](https://smithery.ai/badge/@zwldarren/akshare-one-mcp)](https://smithery.ai/server/@zwldarren/akshare-one-mcp)\n\n## Overview\n\nAn MCP server based on [akshare-one](https://github.com/zwldarren/akshare-one), providing comprehensive interfaces for China stock market data. It offers a set of powerful tools for retrieving financial information including historical stock data, real-time data, news data, and financial statements.\n\n<a href=\"https://glama.ai/mcp/servers/@zwldarren/akshare-one-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@zwldarren/akshare-one-mcp/badge\" alt=\"akshare-one-mcp MCP server\" />\n</a>\n\n## Available Tools\n\n### Market Data Tools\n\n#### `get_hist_data`\nGet historical stock market data with support for multiple time periods and adjustment methods.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code (e.g. '000001')\n- `interval` (string, optional): Time interval ('minute','hour','day','week','month','year') (default: 'day')\n- `interval_multiplier` (number, optional): Interval multiplier (default: 1)\n- `start_date` (string, optional): Start date in YYYY-MM-DD format (default: '1970-01-01')\n- `end_date` (string, optional): End date in YYYY-MM-DD format (default: '2030-12-31')\n- `adjust` (string, optional): Adjustment type ('none', 'qfq', 'hfq') (default: 'none')\n- `source` (string, optional): Data source ('eastmoney', 'eastmoney_direct', 'sina') (default: 'eastmoney')\n- `indicators_list` (list, optional): Technical indicators to add\n- `recent_n` (number, optional): Number of most recent records to return (default: 100)\n\n</details>\n\n#### `get_realtime_data`\nGet real-time stock market data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, optional): Stock code\n- `source` (string, optional): Data source ('xueqiu', 'eastmoney', 'eastmoney_direct') (default: 'eastmoney_direct')\n\n</details>\n\n### News & Information Tools\n\n#### `get_news_data`\nGet stock-related news data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n### Financial Statement Tools\n\n#### `get_balance_sheet`\nGet company balance sheet data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n#### `get_income_statement`\nGet company income statement data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n#### `get_cash_flow`\nGet company cash flow statement data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `source` (string, optional): Data source (default: 'sina')\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n### Analysis & Metrics Tools\n\n#### `get_inner_trade_data`\nGet company insider trading data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n\n</details>\n\n#### `get_financial_metrics`\nGet key financial metrics from the three major financial statements.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n#### `get_time_info`\nGet current time with ISO format, timestamp, and the last trading day.\n\n## Installation & Setup\n\n### Running Modes\n\nThe server supports two modes: stdio and streamable-http\n\n**Command Line Arguments:**\n- `--streamable-http`: Enable HTTP mode (default: stdio mode)\n- `--host`: Host to bind to in HTTP mode (default: 0.0.0.0)\n- `--port`: Port to listen on in HTTP mode (default: 8081)\n\n> **Note:** When using streamable-http mode, the MCP server will be available at `http://{host}:{port}/mcp`. For the default configuration, this would be `http://0.0.0.0:8081/mcp`.\n\n### Installation Options\n\n#### Option 1: Via Smithery\nTo install akshare-one-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@zwldarren/akshare-one-mcp):\n\n```bash\nnpx -y @smithery/cli install @zwldarren/akshare-one-mcp --client claude\n```\n\n#### Option 2: Via `uv`\nInstall [uv](<https://docs.astral.sh/uv/getting-started/installation/>) if you haven't already.\n\nAdd the following configuration to your MCP Client settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"akshare-one-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"akshare-one-mcp\"]\n    }\n  }\n}\n```\n\n#### Option 3: Local Development Setup\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/zwldarren/akshare-one-mcp.git\n   cd akshare-one-mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   uv sync\n   ```\n\n3. Add the following configuration to your MCP Client settings:\n   ```json\n   {\n     \"mcpServers\": {\n       \"akshare-one-mcp\": {\n         \"command\": \"uv\",\n         \"args\": [\n           \"--directory\",\n           \"/path/to/akshare-one-mcp\",\n           \"run\",\n           \"akshare-one-mcp\"\n         ]\n       }\n     }\n   }\n   ```\n\n## Technical Indicators Reference\n\nThe `get_hist_data` tool supports the following technical indicators:\n\n### Trend Indicators\n- **Moving Averages**: SMA (Simple Moving Average), EMA (Exponential Moving Average)\n- **Trend Tracking**: MACD (Moving Average Convergence Divergence), APO (Absolute Price Oscillator), PPO (Percentage Price Oscillator)\n- **Rate of Change**: ROC (Rate of Change), ROCP (Rate of Change Percentage), ROCR (Rate of Change Ratio), ROCR100\n- **Other**: TRIX (Triple Exponential Moving Average), ULTOSC (Ultimate Oscillator)\n\n### Momentum Indicators\n- **Relative Strength**: RSI (Relative Strength Index), CCI (Commodity Channel Index)\n- **Trend Strength**: ADX (Average Directional Index), DX (Directional Index)\n- **Money Flow**: MFI (Money Flow Index), MOM (Momentum), CMO (Chande Momentum Oscillator), WILLR (Williams %R)\n\n### Volatility Indicators\n- **Bollinger Bands**: BOLL (Bollinger Bands)\n- **Average True Range**: ATR (Average True Range)\n- **Parabolic SAR**: SAR (Parabolic Stop and Reverse)\n\n### Volume Indicators\n- **Volume**: OBV (On-Balance Volume), AD (Accumulation/Distribution Line), ADOSC (Accumulation/Distribution Oscillator)\n\n### Other Indicators\n- **Stochastic**: STOCH (Stochastic Oscillator)\n- **Aroon**: AROON (Aroon Indicator), AROONOSC (Aroon Oscillator)\n- **Balance of Power**: BOP (Balance of Power)\n- **Directional Indicators**: MINUS_DI, MINUS_DM, PLUS_DI, PLUS_DM\n- **Time Series Forecast**: TSF (Time Series Forecast)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Retrieve historical stock market data with customizable intervals and adjustment methods",
        "Fetch real-time stock market data from multiple sources",
        "Obtain stock-related news data",
        "Access company financial statements including balance sheet, income statement, and cash flow",
        "Get insider trading data for companies",
        "Retrieve key financial metrics derived from major financial statements",
        "Provide current time information including ISO format, timestamp, and last trading day",
        "Support multiple technical indicators for historical data analysis",
        "Operate in both stdio and streamable HTTP server modes"
      ],
      "limitations": [],
      "requirements": [
        "Installation of dependencies via uv sync for local development",
        "Use of MCP client configuration to connect to the server",
        "Optional use of Smithery CLI or uv tool for installation and running",
        "No explicit mention of API keys or authentication tokens required"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions with parameters, usage modes, and a thorough list of supported technical indicators, but does not explicitly mention limitations or rate limits.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# AKShare One MCP Server\n\n<div align=\"center\">\n  <a href=\"README.md\">English</a> | \n  <a href=\"README_zh.md\">‰∏≠Êñá</a>\n</div>\n\n<!-- mcp-name: io.github.zwldarren/akshare-one-mcp -->\n\n[![smithery badge](https://smithery.ai/badge/@zwldarren/akshare-one-mcp)](https://smithery.ai/server/@zwldarren/akshare-one-mcp)\n\n## Overview\n\nAn MCP server based on [akshare-one](https://github.com/zwldarren/akshare-one), providing comprehensive interfaces for China stock market data. It offers a set of powerful tools for retrieving financial information including historical stock data, real-time data, news data, and financial statements.\n\n<a href=\"https://glama.ai/mcp/servers/@zwldarren/akshare-one-mcp\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@zwldarren/akshare-one-mcp/badge\" alt=\"akshare-one-mcp MCP server\" />\n</a>\n\n## Available Tools\n\n### Market Data Tools\n\n#### `get_hist_data`\nGet historical stock market data with support for multiple time periods and adjustment methods.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code (e.g. '000001')\n- `interval` (string, optional): Time interval ('minute','hour','day','week','month','year') (default: 'day')\n- `interval_multiplier` (number, optional): Interval multiplier (default: 1)\n- `start_date` (string, optional): Start date in YYYY-MM-DD format (default: '1970-01-01')\n- `end_date` (string, optional): End date in YYYY-MM-DD format (default: '2030-12-31')\n- `adjust` (string, optional): Adjustment type ('none', 'qfq', 'hfq') (default: 'none')\n- `source` (string, optional): Data source ('eastmoney', 'eastmoney_direct', 'sina') (default: 'eastmoney')\n- `indicators_list` (list, optional): Technical indicators to add\n- `recent_n` (number, optional): Number of most recent records to return (default: 100)\n\n</details>\n\n#### `get_realtime_data`\nGet real-time stock market data.",
        "start_pos": 0,
        "end_pos": 1879,
        "token_count_estimate": 469,
        "source_type": "readme",
        "agent_id": "e212b37d0f975ed6"
      },
      {
        "chunk_id": 1,
        "text": "t, optional): Technical indicators to add\n- `recent_n` (number, optional): Number of most recent records to return (default: 100)\n\n</details>\n\n#### `get_realtime_data`\nGet real-time stock market data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, optional): Stock code\n- `source` (string, optional): Data source ('xueqiu', 'eastmoney', 'eastmoney_direct') (default: 'eastmoney_direct')\n\n</details>\n\n### News & Information Tools\n\n#### `get_news_data`\nGet stock-related news data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n### Financial Statement Tools\n\n#### `get_balance_sheet`\nGet company balance sheet data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n#### `get_income_statement`\nGet company income statement data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n#### `get_cash_flow`\nGet company cash flow statement data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n- `source` (string, optional): Data source (default: 'sina')\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n### Analysis & Metrics Tools\n\n#### `get_inner_trade_data`\nGet company insider trading data.\n\n<details>\n<summary>Parameters</summary>\n\n- `symbol` (string, required): Stock code\n\n</details>\n\n#### `get_financial_metrics`\nGet key financial metrics from the three major financial statements.",
        "start_pos": 1679,
        "end_pos": 3467,
        "token_count_estimate": 447,
        "source_type": "readme",
        "agent_id": "e212b37d0f975ed6"
      },
      {
        "chunk_id": 2,
        "text": "g, required): Stock code\n- `recent_n` (number, optional): Number of most recent records to return (default: 10)\n\n</details>\n\n#### `get_time_info`\nGet current time with ISO format, timestamp, and the last trading day.\n\n## Installation & Setup\n\n### Running Modes\n\nThe server supports two modes: stdio and streamable-http\n\n**Command Line Arguments:**\n- `--streamable-http`: Enable HTTP mode (default: stdio mode)\n- `--host`: Host to bind to in HTTP mode (default: 0.0.0.0)\n- `--port`: Port to listen on in HTTP mode (default: 8081)\n\n> **Note:** When using streamable-http mode, the MCP server will be available at `http://{host}:{port}/mcp`. For the default configuration, this would be `http://0.0.0.0:8081/mcp`.\n\n### Installation Options\n\n#### Option 1: Via Smithery\nTo install akshare-one-mcp for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@zwldarren/akshare-one-mcp):\n\n```bash\nnpx -y @smithery/cli install @zwldarren/akshare-one-mcp --client claude\n```\n\n#### Option 2: Via `uv`\nInstall [uv](<https://docs.astral.sh/uv/getting-started/installation/>) if you haven't already.\n\nAdd the following configuration to your MCP Client settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"akshare-one-mcp\": {\n      \"command\": \"uvx\",\n      \"args\": [\"akshare-one-mcp\"]\n    }\n  }\n}\n```\n\n#### Option 3: Local Development Setup\n\n1. Clone this repository:\n   ```bash\n   git clone https://github.com/zwldarren/akshare-one-mcp.git\n   cd akshare-one-mcp\n   ```\n\n2. Install dependencies:\n   ```bash\n   uv sync\n   ```\n\n3.",
        "start_pos": 3527,
        "end_pos": 5047,
        "token_count_estimate": 380,
        "source_type": "readme",
        "agent_id": "e212b37d0f975ed6"
      },
      {
        "chunk_id": 3,
        "text": "echnical Indicators Reference\n\nThe `get_hist_data` tool supports the following technical indicators:\n\n### Trend Indicators\n- **Moving Averages**: SMA (Simple Moving Average), EMA (Exponential Moving Average)\n- **Trend Tracking**: MACD (Moving Average Convergence Divergence), APO (Absolute Price Oscillator), PPO (Percentage Price Oscillator)\n- **Rate of Change**: ROC (Rate of Change), ROCP (Rate of Change Percentage), ROCR (Rate of Change Ratio), ROCR100\n- **Other**: TRIX (Triple Exponential Moving Average), ULTOSC (Ultimate Oscillator)\n\n### Momentum Indicators\n- **Relative Strength**: RSI (Relative Strength Index), CCI (Commodity Channel Index)\n- **Trend Strength**: ADX (Average Directional Index), DX (Directional Index)\n- **Money Flow**: MFI (Money Flow Index), MOM (Momentum), CMO (Chande Momentum Oscillator), WILLR (Williams %R)\n\n### Volatility Indicators\n- **Bollinger Bands**: BOLL (Bollinger Bands)\n- **Average True Range**: ATR (Average True Range)\n- **Parabolic SAR**: SAR (Parabolic Stop and Reverse)\n\n### Volume Indicators\n- **Volume**: OBV (On-Balance Volume), AD (Accumulation/Distribution Line), ADOSC (Accumulation/Distribution Oscillator)\n\n### Other Indicators\n- **Stochastic**: STOCH (Stochastic Oscillator)\n- **Aroon**: AROON (Aroon Indicator), AROONOSC (Aroon Oscillator)\n- **Balance of Power**: BOP (Balance of Power)\n- **Directional Indicators**: MINUS_DI, MINUS_DM, PLUS_DI, PLUS_DM\n- **Time Series Forecast**: TSF (Time Series Forecast)",
        "start_pos": 5375,
        "end_pos": 6845,
        "token_count_estimate": 367,
        "source_type": "readme",
        "agent_id": "e212b37d0f975ed6"
      }
    ]
  },
  {
    "agent_id": "60dbc59e0e1e6f79",
    "name": "ai.specproof/specproof-mcp",
    "source": "mcp",
    "source_url": "https://github.com/ibouazizi/specproof.git",
    "description": "SpecProof: Search standards specs with MCP-ready precision.",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-01T20:43:12.895842Z",
    "indexed_at": "2026-02-18T04:09:27.675714",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search standards specifications with precision",
        "Integrate with MCP for context-aware querying"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence providing a basic idea of the server's function without detailed capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b3c860113849c4a6",
    "name": "ai.spritecook/generate",
    "source": "mcp",
    "source_url": "https://github.com/SpriteCook/spritecook-mcp",
    "description": "Generate game sprites and assets from text prompts for game development.",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-13T23:29:03.256165Z",
    "indexed_at": "2026-02-18T04:09:32.764692",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Generate game sprites from text prompts",
        "Generate game assets from text prompts",
        "Support game development asset creation"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's function but lacks detail, examples, or information on limitations and requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b3c860113849c4a6",
    "name": "ai.spritecook/generate",
    "source": "mcp",
    "source_url": "https://github.com/SpriteCook/skills",
    "description": "Generate game sprites and assets from text prompts for game development.",
    "tools": [],
    "detected_capabilities": [
      "generate"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-13T23:37:36.630187Z",
    "indexed_at": "2026-02-18T04:09:36.817550",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# SpriteCook Agent Skills\n\nAgent skills for [SpriteCook](https://spritecook.ai) - AI-powered game asset generation. These skills teach AI coding agents how to generate pixel art and detailed/HD game art including sprites, characters, items, tilesets, and more using SpriteCook's MCP tools.\n\nSkills follow the open [Agent Skills](https://agentskills.io/) standard and work across all compatible editors.\n\n## Available Skills\n\n### spritecook-generate-sprites\n\nGenerate sprites and game assets from text prompts. Teaches the agent to:\n\n- Generate production-ready pixel art or detailed/HD game art (characters, items, tilesets, UI, backgrounds)\n- Choose the right art style (pixel vs detailed) based on the game's needs\n- Maintain visual consistency across assets using style references\n- Download and save generated assets into your project\n- Autonomously identify and create all assets needed for a game\n\n## Installation\n\n### Quick install (all compatible editors)\n\n```bash\nnpx skills add spritecook/skills\n```\n\n### With SpriteCook CLI (also sets up MCP connection)\n\n```bash\nnpx spritecook-mcp setup\n```\n\n## Prerequisites\n\nYou need the SpriteCook MCP server connected to your editor. The skill tells your AI agent *how* to use SpriteCook, but the MCP connection provides the actual tools.\n\n**Set up MCP + skill in one step:**\n\n```bash\nnpx spritecook-mcp setup\n```\n\nOr see [spritecook.ai](https://spritecook.ai) for manual setup instructions.\n\n## Supported Editors\n\nWorks with any editor that supports the Agent Skills standard:\n\n- Cursor\n- VS Code (GitHub Copilot)\n- Claude Code\n- Claude Desktop\n- Antigravity\n- Codex\n- Windsurf\n- And more\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Generate production-ready pixel art and detailed/HD game art including characters, items, tilesets, UI, and backgrounds from text prompts",
        "Choose appropriate art style (pixel vs detailed) based on game requirements",
        "Maintain visual consistency across generated assets using style references",
        "Download and save generated assets directly into the user's project",
        "Autonomously identify and create all necessary game assets"
      ],
      "limitations": [],
      "requirements": [
        "SpriteCook MCP server connection to the editor",
        "Compatible editor that supports the Agent Skills standard",
        "Node.js environment to run npx commands for installation and setup"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation instructions, usage examples, detailed capability descriptions, supported editors, and prerequisites, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# SpriteCook Agent Skills\n\nAgent skills for [SpriteCook](https://spritecook.ai) - AI-powered game asset generation. These skills teach AI coding agents how to generate pixel art and detailed/HD game art including sprites, characters, items, tilesets, and more using SpriteCook's MCP tools.\n\nSkills follow the open [Agent Skills](https://agentskills.io/) standard and work across all compatible editors.\n\n## Available Skills\n\n### spritecook-generate-sprites\n\nGenerate sprites and game assets from text prompts. Teaches the agent to:\n\n- Generate production-ready pixel art or detailed/HD game art (characters, items, tilesets, UI, backgrounds)\n- Choose the right art style (pixel vs detailed) based on the game's needs\n- Maintain visual consistency across assets using style references\n- Download and save generated assets into your project\n- Autonomously identify and create all assets needed for a game\n\n## Installation\n\n### Quick install (all compatible editors)\n\n```bash\nnpx skills add spritecook/skills\n```\n\n### With SpriteCook CLI (also sets up MCP connection)\n\n```bash\nnpx spritecook-mcp setup\n```\n\n## Prerequisites\n\nYou need the SpriteCook MCP server connected to your editor. The skill tells your AI agent *how* to use SpriteCook, but the MCP connection provides the actual tools.\n\n**Set up MCP + skill in one step:**\n\n```bash\nnpx spritecook-mcp setup\n```\n\nOr see [spritecook.ai](https://spritecook.ai) for manual setup instructions.\n\n## Supported Editors\n\nWorks with any editor that supports the Agent Skills standard:\n\n- Cursor\n- VS Code (GitHub Copilot)\n- Claude Code\n- Claude Desktop\n- Antigravity\n- Codex\n- Windsurf\n- And more\n\n## License\n\nMIT",
        "start_pos": 0,
        "end_pos": 1656,
        "token_count_estimate": 413,
        "source_type": "readme",
        "agent_id": "b3c860113849c4a6"
      }
    ]
  },
  {
    "agent_id": "45611072ae37ea7b",
    "name": "ai.stumpy/agents",
    "source": "mcp",
    "source_url": "https://stumpy.ai",
    "description": "Persistent AI agents that run 24/7 in your Slack, Telegram, SMS, or email.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2026-01-31T00:02:06.764828Z",
    "indexed_at": "2026-02-18T04:09:38.019199",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "Stop doing everything yourself.\nCreate AI agents that show up where you already work ‚Äî Telegram, texts, email. They handle the job so you can focus on what matters.\nCreate your first agent\nFree to start. No credit card required.\nInbox manager\nChecks your email every morning, flags what needs attention, and drafts replies. You just review and send.\nCommunity support\nLives in your Telegram or Discord. Answers customer questions, points people to docs, escalates the hard stuff to you.\nData keeper\nMaintains your spreadsheets, tracks your numbers, and gives you answers when you ask. Like having a bookkeeper on call.\nSocial media presence\nMonitors what people say about your brand, drafts posts, and keeps a steady trickle of content going out.\nUp and running in 60 seconds.\n1\nCreate an agent\nPick a name and what you want it to do.\n2\nConnect a channel\nSMS, Telegram, email, or just web chat.\n3\nStart talking\nTell it what you need. It takes it from there.\nYour next hire is an agent.\nNo interview. No onboarding. No payroll. Just results.\nGet started"
    },
    "llm_extracted": {
      "capabilities": [
        "Create AI agents that integrate with Telegram, texts, and email",
        "Manage email inbox by flagging important messages and drafting replies",
        "Provide community support by answering questions and escalating issues in Telegram or Discord",
        "Maintain spreadsheets and track numerical data with on-demand answers",
        "Monitor social media mentions, draft posts, and maintain consistent content output"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.55,
    "quality_rationale": "The documentation provides a clear list of capabilities and a simple usage flow but lacks detailed examples, installation instructions, or explicit requirements.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "Stop doing everything yourself.\nCreate AI agents that show up where you already work ‚Äî Telegram, texts, email. They handle the job so you can focus on what matters.\nCreate your first agent\nFree to start. No credit card required.\nInbox manager\nChecks your email every morning, flags what needs attention, and drafts replies. You just review and send.\nCommunity support\nLives in your Telegram or Discord. Answers customer questions, points people to docs, escalates the hard stuff to you.\nData keeper\nMaintains your spreadsheets, tracks your numbers, and gives you answers when you ask. Like having a bookkeeper on call.\nSocial media presence\nMonitors what people say about your brand, drafts posts, and keeps a steady trickle of content going out.\nUp and running in 60 seconds.\n1\nCreate an agent\nPick a name and what you want it to do.\n2\nConnect a channel\nSMS, Telegram, email, or just web chat.\n3\nStart talking\nTell it what you need. It takes it from there.\nYour next hire is an agent.\nNo interview. No onboarding. No payroll. Just results.\nGet started",
        "start_pos": 0,
        "end_pos": 1052,
        "token_count_estimate": 263,
        "source_type": "detail_page",
        "agent_id": "45611072ae37ea7b"
      }
    ]
  },
  {
    "agent_id": "2ca0d23e87144e1d",
    "name": "ai.tickettailor/mcp",
    "source": "mcp",
    "source_url": "https://mcp.tickettailor.ai/mcp",
    "description": "Provides event organisers with tools to interact with a Ticket Tailor box office account.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-12T10:10:54.188764Z",
    "indexed_at": "2026-02-18T04:09:38.591828",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Interact with a Ticket Tailor box office account",
        "Provide tools for event organisers"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.25,
    "quality_rationale": "The description is a basic one-liner that indicates the server's general purpose but lacks detail, examples, or specific capabilities.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "ab498a3397e881fe",
    "name": "ai.tinyfish.agent/web-agent",
    "source": "mcp",
    "source_url": "https://agent.tinyfish.ai/mcp",
    "description": "AI-powered web automation. Navigate websites using AI agents for one page or a thousand",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-17T03:00:36.680724Z",
    "indexed_at": "2026-02-18T04:09:38.882683",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Navigate websites using AI agents",
        "Automate web interactions across multiple pages"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of AI-powered web navigation and automation but lacks detailed features, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "ff18cabf8f93ca6d",
    "name": "ai.toolprint/hypertool-mcp",
    "source": "mcp",
    "source_url": "https://github.com/toolprint/hypertool-mcp",
    "description": "Dynamically expose tools from proxied servers based on an Agent Persona",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T22:05:20.65584Z",
    "indexed_at": "2026-02-18T04:09:40.222175",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/hypertool_darkmode_wordmark_horizontal.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/hypertool_lightmode_wordmark_horizontal.png\">\n  <img alt=\"Shows a darkmode hypertool-mcp Logo in light color mode and a white one in dark color mode.\" src=\"./assets/hypertool_lightmode_wordmark_horizontal.png\"  width=\"full\">\n</picture>\n\n<h1 align=\"center\">Give your AI the best tools from all your MCPs üéØ</h1>\n\n[![Version](https://img.shields.io/npm/v/@toolprint/hypertool-mcp)](https://npmjs.com/package/@toolprint/hypertool-mcp)\n[![Downloads](https://img.shields.io/npm/dm/@toolprint/hypertool-mcp)](https://npmjs.com/package/@toolprint/hypertool-mcp)\n[![Discord](https://img.shields.io/discord/1379961140259459113?logo=discord&logoColor=white&label=Discord&color=5865F2)](https://discord.gg/MbvndnJ45W)\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green)](https://modelcontextprotocol.io)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n\n## ‚ö° Features\n\n### üîì **Break Free from Tool Limits**\n\nConnect unlimited MCP servers. Use 10, 50, or 500+ tools total - your AI only sees what it needs.\n\n### üéØ **Task-Specific Toolsets**\n\nBuild \"git-essentials\" with 5 tools instead of drowning in 47 Git commands. Switch contexts instantly.\n\n### üß† **Smart Tool Descriptions**\n\nEnhance tools with examples and context. Watch your AI pick the right tool 89% more often.\n\n## üöÄ Quick Start\n\n### Step 1: Copy Your Existing Config\n\n```bash\n# In your project directory\ncp .mcp.json .mcp.hypertool.json\n```\n\n### Step 2: Point Your AI to HyperTool\n\nReplace your `.mcp.json` with:\n\n```json\n{\n  \"mcpServers\": {\n    \"hypertool\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@toolprint/hypertool-mcp\", \"mcp\", \"run\", \"--mcp-config\", \".mcp.hypertool.json\"]\n    }\n  }\n}\n```\n\n### Step 3: Create Your First Toolset\n\nRestart your AI and try:\n\n```\nYou: \"Create a toolset called 'coding' with git and docker tools\"\nAI: \"Created 'coding' toolset with 15 focused tools\"\n\nYou: \"Switch to coding toolset\"\nAI: \"Equipped! I now have just the tools needed for development\"\n```\n\n**That's it!** Your AI is now focused and effective. üéâ\n\nüí° **Want automated setup?** Try our interactive `setup` command - see [Advanced Guide](guides/ADVANCED.md#setup-command) for details.\n\nüìö **Configuration Mode:** HyperTool uses a smart Configuration Mode to keep toolset management separate from your operational tools. Learn more in the [Configuration Mode Guide](guides/CONFIGURATION_MODE.md).\n\n## üé≠ Personas: Pre-configured Tool Bundles (NEW!)\n\nDon't want to configure from scratch? Use personas - ready-to-use MCP server bundles with pre-built toolsets.\n\n### What are Personas?\n\nThink of personas as \"app bundles\" for your AI - they come with:\n- ‚úÖ Pre-configured MCP servers\n- ‚úÖ Curated toolsets for specific workflows\n- ‚úÖ Everything you need to get started instantly\n\n### Quick Start with Personas\n\n```bash\n# 1. Clone the persona collection\ngit clone https://github.com/toolprint/awesome-mcp-personas\n\n# 2. Add a persona (e.g., web-dev persona)\nhypertool-mcp persona add awesome-mcp-personas/personas/web-dev\n\n# 3. Run with the persona\nnpx -y @toolprint/hypertool-mcp mcp run --persona web-dev\n```\n\nThat's it! No server configuration needed. The persona brings its own servers and toolsets.\n\nüì¶ **Browse all available personas**: [awesome-mcp-personas](https://github.com/toolprint/awesome-mcp-personas)\n\n### Available Personas\n\n| Persona | Included Servers | Best For |\n|---------|-----------------|----------|\n| **web-dev** | Git, Docker, Filesystem, Browser, Testing | Full-stack web development |\n| **data-scientist** | Python, Jupyter, Database, Filesystem, Plotting | Data analysis & ML workflows |\n| **devops** | Docker, Kubernetes, AWS, Terraform, Monitoring | Infrastructure & deployment |\n| **content-creator** | Notion, Slack, Grammar, SEO, Social | Writing & content management |\n| **researcher** | Perplexity, Arxiv, Wikipedia, Filesystem | Research & knowledge work |\n\n### Persona vs Standard Mode\n\n```bash\n# Standard Mode (use your existing MCP servers):\nnpx -y @toolprint/hypertool-mcp mcp run --mcp-config .mcp.hypertool.json\n\n# Persona Mode (bundled servers + pre-built toolsets):\nnpx -y @toolprint/hypertool-mcp mcp run --persona web-dev\n\n# Persona Mode with specific toolset:\nnpx -y @toolprint/hypertool-mcp mcp run --persona web-dev --equip-toolset frontend\n```\n\nüí° **Pro tip**: Personas can be mixed with your existing servers! Add `--mcp-config` to include your custom servers alongside the persona's servers.\n\nüìö **Learn more**: See the complete [Personas Guide](guides/PERSONAS.md) for detailed instructions, creating custom personas, and troubleshooting.\n\n## üìä Context Measurement (NEW!)\n\nSee exactly how much context each tool consumes. Optimize your toolsets with token estimates for every tool.\n\n<div align=\"center\">\n  <img src=\"./assets/toolset_context.png\" alt=\"Toolset Context View\" width=\"700\">\n  <p><em>Active toolset showing token usage per tool</em></p>\n</div>\n\n**Why it matters:**\n- üéØ **Optimize context usage** - Identify heavyweight tools consuming your context window\n- üìâ **Make informed decisions** - See token costs before adding tools to toolsets\n- üîç **Compare alternatives** - Find lighter tools that do the same job\n- üí° **Budget your context** - Understand exactly what you're exposing to your AI\n\n**How to use:**\n\nAsk your AI to use these MCP tools to see context information:\n- `list-available-tools` - Shows token estimates for all available tools\n- `get-active-toolset` - Shows token usage for your currently equipped toolset\n\nEach tool displays estimated tokens and percentage of total context consumed. Perfect for building lean, efficient toolsets!\n\n## üé¨ Demo\n\n### Hotswap toolsets across 100+ tools\n\n_Targeted toolsets across any number of MCPs. Swap to the best toolset for a goal with a tool call. Dynamic tool registration._\n\n<div align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=43fkKOBayCg\">\n    <img src=\"./demos/build_dynamic_toolsets_1080p_25fps.gif\" alt=\"HyperTool Demo - Click for full video\" width=\"800\">\n  </a>\n</div>\n\n## üèóÔ∏è How It Works\n\n```\nBefore: Tool Chaos üòµ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Claude/     ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ 50+ tools from 8 servers   ‚îÇ\n‚îÇ Cursor      ‚îÇ   ‚îÇ ‚ùå Wrong picks             ‚îÇ\n‚îÇ             ‚îÇ   ‚îÇ ‚ùå Slow decisions          ‚îÇ\n‚îÇ             ‚îÇ   ‚îÇ ‚ùå Confused context        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nAfter: Expert Mode üéØ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Claude/     ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ HyperTool    ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ ALL Your Tools  ‚îÇ\n‚îÇ Cursor      ‚îÇ   ‚îÇ (Local)      ‚îÇ   ‚îÇ (Same servers)  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                         ‚îÇ\n                         ‚ñº\n                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                  ‚îÇ Smart Toolsets  ‚îÇ\n                  ‚îÇ üî® coding (5)   ‚îÇ ‚Üê \"I'm coding now\"\n                  ‚îÇ üìù writing (3)  ‚îÇ ‚Üê \"I'm writing now\"\n                  ‚îÇ üìä analysis (4) ‚îÇ ‚Üê \"I'm analyzing now\"\n                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                  ‚úÖ Expert picks every time\n```\n\n### What's a \"Toolset\"? Think Playlists for Your AI\n\nJust like Spotify playlists organize your music, toolsets organize your AI tools:\n\n```\nALL YOUR TOOLS (64 total)              YOUR TOOLSETS\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ üê≥ Docker (19 tools)       ‚îÇ         ‚îÇ üî® \"coding\"      ‚îÇ\n‚îÇ  ‚Ä¢ build_image             ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  ‚Ä¢ git.status    ‚îÇ\n‚îÇ  ‚Ä¢ create_container        ‚îÇ   ‚îÇ     ‚îÇ  ‚Ä¢ git.commit    ‚îÇ\n‚îÇ  ‚Ä¢ run_container           ‚îÇ   ‚îÇ     ‚îÇ  ‚Ä¢ docker.build  ‚îÇ\n‚îÇ  ‚Ä¢ stop_container          ‚îÇ   ‚îÇ     ‚îÇ  ‚Ä¢ docker.run    ‚îÇ\n‚îÇ  ‚Ä¢ [... 15 more]           ‚îÇ   ‚îÇ     ‚îÇ  ‚Ä¢ github.pr     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îÇ üîÄ Git (12 tools)          ‚îÇ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  ‚Ä¢ status                  ‚îÇ   ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  ‚Ä¢ commit                  ‚îÇ   ‚îÇ     ‚îÇ üìù \"writing\"     ‚îÇ\n‚îÇ  ‚Ä¢ push                    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  ‚Ä¢ notion.create ‚îÇ\n‚îÇ  ‚Ä¢ [... 9 more]            ‚îÇ         ‚îÇ  ‚Ä¢ slack.send    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îÇ  ‚Ä¢ grammarly.fix ‚îÇ\n‚îÇ üìù Notion (8 tools)        ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îÇ üí¨ Slack (6 tools)         ‚îÇ     ‚îÇ\n‚îÇ üìä Linear (10 tools)       ‚îÇ     ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ üß™ Testing (9 tools)       ‚îÇ     ‚îî‚îÄ‚ñ∂ ‚îÇ üêõ \"debugging\"   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚Ä¢ logs.search   ‚îÇ\n                                       ‚îÇ  ‚Ä¢ docker.logs   ‚îÇ\nAI sees ALL 64 tools = confused üòµ     ‚îÇ  ‚Ä¢ traces.view   ‚îÇ\n                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n                                       AI sees 3-5 tools = focused üéØ\n```\n\n## üíº Real-World Toolsets\n\nCreate focused toolsets for different workflows:\n\n### üî® Development Mode\n\n```\n\"deep-coding\": git + docker + filesystem (12 tools)\n‚Üí Everything you need for feature development\n\n\"code-review\": git + github + linear (10 tools)\n‚Üí Review PRs, update tickets, merge with confidence\n\n\"debugging\": logs + docker + traces + alerts (8 tools)\n‚Üí Find and fix issues fast\n```\n\n### üìù Content Creation\n\n```\n\"writing\": notion + grammarly + slack (6 tools)\n‚Üí Blog posts, docs, and team updates\n\n\"research\": perplexity + notion + filesystem (7 tools)\n‚Üí Deep dives with organized notes\n```\n\n### üé¨ Real Chat Example\n\n```\nYou: \"I need to debug our API\"\nAI: \"I'll switch to the debugging toolset for better focus\"\n[Now has: logs, traces, curl, docker]\n\nYou: \"Actually, let's write the incident report\"\nAI: \"Switching to writing toolset\"\n[Now has: notion, slack, templates]\n```\n\nüí° **Pro tip**: Start with 3-5 tools per toolset. Your AI will thank you!\n\n## üìã All Features\n\nExplore everything HyperTool can do:\n\n| Feature | Description | Guide |\n|---------|-------------|-------|\n| **üé≠ Personas** | Pre-configured MCP server bundles with curated toolsets. Get started instantly with ready-to-use workflows for web-dev, data science, DevOps, and more. | [Personas Guide](guides/PERSONAS.md) |\n| **üìÅ Server Groups** | Organize MCP servers into logical groups. Launch related servers together, switch between projects, and maintain focused contexts. | [Advanced Guide](guides/ADVANCED.md) |\n| **üìä Context Measurement** | See token estimates for every tool. Optimize your toolsets by understanding exactly how much context each tool consumes. | [Context Measurement](docs/features/context-measurement.md) |\n| **üîß Configuration Mode** | Smart separation of toolset management from operational tools. Keep your AI focused on work, not configuration. | [Configuration Mode Guide](guides/CONFIGURATION_MODE.md) |\n| **üéØ Dynamic Toolsets** | Build, modify, and switch between toolsets on the fly. Adapt your AI's capabilities to match your current task. | [Examples & Recipes](guides/EXAMPLES.md) |\n| **üß† Tool Annotations** | Enhance tools with custom descriptions, examples, and context. Improve your AI's tool selection accuracy by 89%. | [Advanced Guide](guides/ADVANCED.md) |\n| **üöÄ HTTP Mode** | Run HyperTool as a long-lived HTTP server for persistent connections and faster response times. | [Advanced Guide](guides/ADVANCED.md) |\n| **üîå Unlimited Servers** | Connect as many MCP servers as you need. Break free from the 100-tool limit without sacrificing performance. | [Quick Start](#-quick-start) |\n\n## ‚ùì FAQ\n\n### General Questions\n\n**Q: How is this different from just using MCP servers directly?**\nA: HyperTool lets you use unlimited MCP servers without hitting the 100-tool limit, and dynamically switches between focused toolsets for better AI performance.\n\n**Q: What's the difference between Personas and Standard Mode?**\nA: Standard Mode uses your existing MCP server configurations. Personas are pre-packaged bundles that include both MCP servers AND curated toolsets - perfect for getting started quickly or trying new workflows.\n\n**Q: Can I use multiple toolsets at once?**\nA: In stdio mode (default), use `--equip-toolset <name>` when launching. HTTP mode supports one active toolset but you can switch anytime.\n\n**Q: Where are my toolsets and configurations stored?**\nA: Everything is stored locally in `~/.toolprint/hypertool-mcp/`:\n- Personas: `~/.toolprint/hypertool-mcp/personas/`\n- Toolsets: `~/.toolprint/hypertool-mcp/toolsets/`\n- Preferences: `~/.toolprint/hypertool-mcp/config/`\nYou can directly edit these files when HyperTool is not running.\n\n### Setup & Compatibility\n\n**Q: Does this work with Claude Desktop / Cursor / Claude Code?**\nA: Yes! Cursor has full hot-swapping support. Claude Desktop works with restart. Claude Code [hot-swap coming soon](https://github.com/anthropics/claude-code/issues/411).\n\n**Q: What if an MCP server goes down?**\nA: HyperTool monitors health and automatically reconnects when servers come back. Your toolsets stay intact.\n\n**Q: Can I share toolsets with my team?**\nA: Import/export is coming soon! For now, you can copy and share toolset files - they'll work if your team has the same MCP servers configured.\n\n**Q: How accurate are the token estimates in context measurement?**\nA: The estimates use BPE-based approximation for consistent relative comparisons between tools. They're perfect for understanding which tools consume more context, but not exact counts since different LLMs use different tokenizers.\n\n**Q: Does context measurement slow down my toolsets?**\nA: No! Token counts are cached and add less than 10ms overhead. You won't notice any performance impact.\n\n### Technical Questions\n\n**Q: How do I add tools from a new MCP server?**\nA: Just add the server to your `.mcp.hypertool.json` config. It's automatically available for toolsets.\n\n**Q: Can I use this in production?**\nA: Yes! For enterprise support, [contact us](mailto:support@onegrep.dev?subject=HyperTool%20Production%20Use&body=Hi%20team%2C%0A%0AI'm%20interested%20in%20using%20HyperTool%20in%20production.%0A%0ACompany%3A%20%0AUse%20case%3A%20%0AScale%3A%20%0A%0AThanks!).\n\n## üéÆ App Compatibility\n\n**Works with ANY MCP-compatible app!** HyperTool is a standard MCP server, so if your app supports MCP, it supports HyperTool.\n\n### Hot-swap Toolsets Without Restarts\n\n| App | Status | How to Switch Toolsets |\n|-----|---------|------------------------|\n| **Cursor/VSCode** | ‚úÖ Full support | Switch toolsets instantly - no restart needed! |\n| **Claude Code** | ‚è≥ Coming soon | Use `--equip-toolset <name>` flag ([track progress](https://github.com/anthropics/claude-code/issues/4118)) |\n| **Claude Desktop** | ‚è≥ In progress | Restart app after switching toolsets |\n\n---\n\nüìö **Learn More**\n\n- üé≠ [Personas Guide](guides/PERSONAS.md) - Complete guide to using and creating personas\n- üî¨ [Research & Performance](guides/RESEARCH.md) - Why focused toolsets work\n- üöÄ [Advanced Features](guides/ADVANCED.md) - Tool annotations, HTTP mode, CLI\n- üîß [Troubleshooting](guides/TROUBLESHOOTING.md) - Common issues and solutions\n- üìñ [Examples & Recipes](guides/EXAMPLES.md) - Toolset patterns for every workflow\n\n## üõ†Ô∏è Development Setup\n\n### Prerequisites\n\n- Node.js 18+\n- Python 3.8+ (for pre-commit hooks)\n\n### Quick Setup\n\n```bash\n# Clone and install\ngit clone https://github.com/toolprint/hypertool-mcp.git\ncd hypertool-mcp\njust setup-dev  # Installs dependencies and pre-commit hooks\n```\n\n### Pre-commit Hooks\n\nThis project uses pre-commit hooks to ensure code quality:\n\n```bash\n# Install pre-commit hooks (included in setup-dev)\njust setup-pre-commit\n\n# Run hooks manually\njust pre-commit-check        # On staged files\njust pre-commit-check-all    # On all files\n\n# Skip hooks for emergency commits (use sparingly)\nSKIP=eslint,typescript git commit -m \"emergency fix\"\n```\n\n### Available Commands\n\n```bash\njust build          # Build the project\njust test           # Run tests\njust lint           # Run linting\njust format         # Format code\njust typecheck      # Check types\njust pre-publish-checks  # Run all quality checks\n```\n\n### Service command\n\nThe `hypertool-mcp service` subcommand is currently disabled and will exit with a\nnotification when invoked.\n\n## ü§ù Contributing\n\nFound a bug? Have an idea? We'd love your help!\n\n- üêõ [Report issues](https://github.com/toolprint/hypertool-mcp/issues)\n- üí° [Share ideas](https://github.com/toolprint/hypertool-mcp/discussions)\n- üîß [Submit PRs](https://github.com/toolprint/hypertool-mcp/pulls)\n\n## üìÑ License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\n<div align=\"center\">\n\n**Built by developers who got tired of watching AI pick the wrong tools** üéØ\n\n<a href=\"https://toolprint.ai\">\n  <img src=\"./assets/toolprint.png\" alt=\"Toolprint\" width=\"200\">\n</a>\n\n<p>\n  <strong>Built with ‚ù§Ô∏è by <a href=\"https://toolprint.ai\">Toolprint</a></strong><br>\n  <sub>¬© 2025 OneGrep, Inc.</sub>\n</p>\n\n</div>\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect unlimited MCP servers to aggregate tools",
        "Build and switch between task-specific toolsets dynamically",
        "Enhance tools with smart descriptions and usage examples",
        "Use pre-configured personas with curated MCP server bundles and toolsets",
        "Measure and display token context usage per tool for optimization",
        "Organize MCP servers into logical server groups for project management",
        "Run as a long-lived HTTP server for persistent connections and faster responses",
        "Create, modify, and switch toolsets on the fly to adapt AI capabilities"
      ],
      "limitations": [
        "Does not support using multiple toolsets simultaneously (implied by FAQ question left unanswered)",
        "Requires AI to be restarted or reconfigured to switch toolsets",
        "No explicit mention of support for non-MCP tools or protocols"
      ],
      "requirements": [
        "Node.js environment with npm to run npx commands",
        "Existing MCP server configurations or use of provided personas",
        "Access to GitHub or persona repositories for cloning personas",
        "Permission to run local commands and manage configuration files"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed feature descriptions, limitations, and environment requirements, making it highly informative and practical.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./assets/hypertool_darkmode_wordmark_horizontal.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./assets/hypertool_lightmode_wordmark_horizontal.png\">\n  <img alt=\"Shows a darkmode hypertool-mcp Logo in light color mode and a white one in dark color mode.\" src=\"./assets/hypertool_lightmode_wordmark_horizontal.png\"  width=\"full\">\n</picture>\n\n<h1 align=\"center\">Give your AI the best tools from all your MCPs üéØ</h1>\n\n[![Version](https://img.shields.io/npm/v/@toolprint/hypertool-mcp)](https://npmjs.com/package/@toolprint/hypertool-mcp)\n[![Downloads](https://img.shields.io/npm/dm/@toolprint/hypertool-mcp)](https://npmjs.com/package/@toolprint/hypertool-mcp)\n[![Discord](https://img.shields.io/discord/1379961140259459113?logo=discord&logoColor=white&label=Discord&color=5865F2)](https://discord.gg/MbvndnJ45W)\n[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![MCP Compatible](https://img.shields.io/badge/MCP-Compatible-green)](https://modelcontextprotocol.io)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n\n## ‚ö° Features\n\n### üîì **Break Free from Tool Limits**\n\nConnect unlimited MCP servers. Use 10, 50, or 500+ tools total - your AI only sees what it needs.\n\n### üéØ **Task-Specific Toolsets**\n\nBuild \"git-essentials\" with 5 tools instead of drowning in 47 Git commands. Switch contexts instantly.\n\n### üß† **Smart Tool Descriptions**\n\nEnhance tools with examples and context. Watch your AI pick the right tool 89% more often.",
        "start_pos": 0,
        "end_pos": 1618,
        "token_count_estimate": 404,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 1,
        "text": "{\n    \"hypertool\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@toolprint/hypertool-mcp\", \"mcp\", \"run\", \"--mcp-config\", \".mcp.hypertool.json\"]\n    }\n  }\n}\n```\n\n### Step 3: Create Your First Toolset\n\nRestart your AI and try:\n\n```\nYou: \"Create a toolset called 'coding' with git and docker tools\"\nAI: \"Created 'coding' toolset with 15 focused tools\"\n\nYou: \"Switch to coding toolset\"\nAI: \"Equipped! I now have just the tools needed for development\"\n```\n\n**That's it!** Your AI is now focused and effective. üéâ\n\nüí° **Want automated setup?** Try our interactive `setup` command - see [Advanced Guide](guides/ADVANCED.md#setup-command) for details.\n\nüìö **Configuration Mode:** HyperTool uses a smart Configuration Mode to keep toolset management separate from your operational tools. Learn more in the [Configuration Mode Guide](guides/CONFIGURATION_MODE.md).\n\n## üé≠ Personas: Pre-configured Tool Bundles (NEW!)\n\nDon't want to configure from scratch? Use personas - ready-to-use MCP server bundles with pre-built toolsets.\n\n### What are Personas?\n\nThink of personas as \"app bundles\" for your AI - they come with:\n- ‚úÖ Pre-configured MCP servers\n- ‚úÖ Curated toolsets for specific workflows\n- ‚úÖ Everything you need to get started instantly\n\n### Quick Start with Personas\n\n```bash\n# 1. Clone the persona collection\ngit clone https://github.com/toolprint/awesome-mcp-personas\n\n# 2. Add a persona (e.g., web-dev persona)\nhypertool-mcp persona add awesome-mcp-personas/personas/web-dev\n\n# 3. Run with the persona\nnpx -y @toolprint/hypertool-mcp mcp run --persona web-dev\n```\n\nThat's it! No server configuration needed. The persona brings its own servers and toolsets.",
        "start_pos": 1848,
        "end_pos": 3503,
        "token_count_estimate": 413,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 2,
        "text": "---------------|----------|\n| **web-dev** | Git, Docker, Filesystem, Browser, Testing | Full-stack web development |\n| **data-scientist** | Python, Jupyter, Database, Filesystem, Plotting | Data analysis & ML workflows |\n| **devops** | Docker, Kubernetes, AWS, Terraform, Monitoring | Infrastructure & deployment |\n| **content-creator** | Notion, Slack, Grammar, SEO, Social | Writing & content management |\n| **researcher** | Perplexity, Arxiv, Wikipedia, Filesystem | Research & knowledge work |\n\n### Persona vs Standard Mode\n\n```bash\n# Standard Mode (use your existing MCP servers):\nnpx -y @toolprint/hypertool-mcp mcp run --mcp-config .mcp.hypertool.json\n\n# Persona Mode (bundled servers + pre-built toolsets):\nnpx -y @toolprint/hypertool-mcp mcp run --persona web-dev\n\n# Persona Mode with specific toolset:\nnpx -y @toolprint/hypertool-mcp mcp run --persona web-dev --equip-toolset frontend\n```\n\nüí° **Pro tip**: Personas can be mixed with your existing servers! Add `--mcp-config` to include your custom servers alongside the persona's servers.\n\nüìö **Learn more**: See the complete [Personas Guide](guides/PERSONAS.md) for detailed instructions, creating custom personas, and troubleshooting.\n\n## üìä Context Measurement (NEW!)\n\nSee exactly how much context each tool consumes. Optimize your toolsets with token estimates for every tool.",
        "start_pos": 3696,
        "end_pos": 5033,
        "token_count_estimate": 334,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 3,
        "text": "your AI\n\n**How to use:**\n\nAsk your AI to use these MCP tools to see context information:\n- `list-available-tools` - Shows token estimates for all available tools\n- `get-active-toolset` - Shows token usage for your currently equipped toolset\n\nEach tool displays estimated tokens and percentage of total context consumed. Perfect for building lean, efficient toolsets!\n\n## üé¨ Demo\n\n### Hotswap toolsets across 100+ tools\n\n_Targeted toolsets across any number of MCPs. Swap to the best toolset for a goal with a tool call.",
        "start_pos": 5544,
        "end_pos": 6062,
        "token_count_estimate": 129,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 4,
        "text": "organize your AI tools:\n\n```\nALL YOUR TOOLS (64 total)              YOUR TOOLSETS\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ üê≥ Docker (19 tools)       ‚îÇ         ‚îÇ üî® \"coding\"      ‚îÇ\n‚îÇ  ‚Ä¢ build_image             ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  ‚Ä¢ git.status    ‚îÇ\n‚îÇ  ‚Ä¢ create_container        ‚îÇ   ‚îÇ     ‚îÇ  ‚Ä¢ git.commit    ‚îÇ\n‚îÇ  ‚Ä¢ run_container           ‚îÇ   ‚îÇ     ‚îÇ  ‚Ä¢ docker.build  ‚îÇ\n‚îÇ  ‚Ä¢ stop_container          ‚îÇ   ‚îÇ     ‚îÇ  ‚Ä¢ docker.run    ‚îÇ\n‚îÇ  ‚Ä¢ [... 15 more]           ‚îÇ   ‚îÇ     ‚îÇ  ‚Ä¢ github.pr     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îÇ üîÄ Git (12 tools)          ‚îÇ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  ‚Ä¢ status                  ‚îÇ   ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  ‚Ä¢ commit                  ‚îÇ   ‚îÇ     ‚îÇ üìù \"writing\"     ‚îÇ\n‚îÇ  ‚Ä¢ push                    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  ‚Ä¢ notion.create ‚îÇ\n‚îÇ  ‚Ä¢ [...",
        "start_pos": 7392,
        "end_pos": 8174,
        "token_count_estimate": 195,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 5,
        "text": "notion + grammarly + slack (6 tools)\n‚Üí Blog posts, docs, and team updates\n\n\"research\": perplexity + notion + filesystem (7 tools)\n‚Üí Deep dives with organized notes\n```\n\n### üé¨ Real Chat Example\n\n```\nYou: \"I need to debug our API\"\nAI: \"I'll switch to the debugging toolset for better focus\"\n[Now has: logs, traces, curl, docker]\n\nYou: \"Actually, let's write the incident report\"\nAI: \"Switching to writing toolset\"\n[Now has: notion, slack, templates]\n```\n\nüí° **Pro tip**: Start with 3-5 tools per toolset. Your AI will thank you!\n\n## üìã All Features\n\nExplore everything HyperTool can do:\n\n| Feature | Description | Guide |\n|---------|-------------|-------|\n| **üé≠ Personas** | Pre-configured MCP server bundles with curated toolsets. Get started instantly with ready-to-use workflows for web-dev, data science, DevOps, and more. | [Personas Guide](guides/PERSONAS.md) |\n| **üìÅ Server Groups** | Organize MCP servers into logical groups. Launch related servers together, switch between projects, and maintain focused contexts. | [Advanced Guide](guides/ADVANCED.md) |\n| **üìä Context Measurement** | See token estimates for every tool. Optimize your toolsets by understanding exactly how much context each tool consumes. | [Context Measurement](docs/features/context-measurement.md) |\n| **üîß Configuration Mode** | Smart separation of toolset management from operational tools. Keep your AI focused on work, not configuration. | [Configuration Mode Guide](guides/CONFIGURATION_MODE.md) |\n| **üéØ Dynamic Toolsets** | Build, modify, and switch between toolsets on the fly. Adapt your AI's capabilities to match your current task. | [Examples & Recipes](guides/EXAMPLES.md) |\n| **üß† Tool Annotations** | Enhance tools with custom descriptions, examples, and context. Improve your AI's tool selection accuracy by 89%. | [Advanced Guide](guides/ADVANCED.md) |\n| **üöÄ HTTP Mode** | Run HyperTool as a long-lived HTTP server for persistent connections and faster response times.",
        "start_pos": 9240,
        "end_pos": 11198,
        "token_count_estimate": 489,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 6,
        "text": "e your AI's tool selection accuracy by 89%. | [Advanced Guide](guides/ADVANCED.md) |\n| **üöÄ HTTP Mode** | Run HyperTool as a long-lived HTTP server for persistent connections and faster response times. | [Advanced Guide](guides/ADVANCED.md) |\n| **üîå Unlimited Servers** | Connect as many MCP servers as you need. Break free from the 100-tool limit without sacrificing performance. | [Quick Start](#-quick-start) |\n\n## ‚ùì FAQ\n\n### General Questions\n\n**Q: How is this different from just using MCP servers directly?**\nA: HyperTool lets you use unlimited MCP servers without hitting the 100-tool limit, and dynamically switches between focused toolsets for better AI performance.\n\n**Q: What's the difference between Personas and Standard Mode?**\nA: Standard Mode uses your existing MCP server configurations. Personas are pre-packaged bundles that include both MCP servers AND curated toolsets - perfect for getting started quickly or trying new workflows.\n\n**Q: Can I use multiple toolsets at once?**\nA: In stdio mode (default), use `--equip-toolset <name>` when launching. HTTP mode supports one active toolset but you can switch anytime.\n\n**Q: Where are my toolsets and configurations stored?**\nA: Everything is stored locally in `~/.toolprint/hypertool-mcp/`:\n- Personas: `~/.toolprint/hypertool-mcp/personas/`\n- Toolsets: `~/.toolprint/hypertool-mcp/toolsets/`\n- Preferences: `~/.toolprint/hypertool-mcp/config/`\nYou can directly edit these files when HyperTool is not running.\n\n### Setup & Compatibility\n\n**Q: Does this work with Claude Desktop / Cursor / Claude Code?**\nA: Yes! Cursor has full hot-swapping support. Claude Desktop works with restart. Claude Code [hot-swap coming soon](https://github.com/anthropics/claude-code/issues/411).\n\n**Q: What if an MCP server goes down?**\nA: HyperTool monitors health and automatically reconnects when servers come back. Your toolsets stay intact.",
        "start_pos": 10998,
        "end_pos": 12889,
        "token_count_estimate": 472,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 7,
        "text": "s://github.com/anthropics/claude-code/issues/411).\n\n**Q: What if an MCP server goes down?**\nA: HyperTool monitors health and automatically reconnects when servers come back. Your toolsets stay intact.\n\n**Q: Can I share toolsets with my team?**\nA: Import/export is coming soon! For now, you can copy and share toolset files - they'll work if your team has the same MCP servers configured.\n\n**Q: How accurate are the token estimates in context measurement?**\nA: The estimates use BPE-based approximation for consistent relative comparisons between tools. They're perfect for understanding which tools consume more context, but not exact counts since different LLMs use different tokenizers.\n\n**Q: Does context measurement slow down my toolsets?**\nA: No! Token counts are cached and add less than 10ms overhead. You won't notice any performance impact.\n\n### Technical Questions\n\n**Q: How do I add tools from a new MCP server?**\nA: Just add the server to your `.mcp.hypertool.json` config. It's automatically available for toolsets.\n\n**Q: Can I use this in production?**\nA: Yes! For enterprise support, [contact us](mailto:support@onegrep.dev?subject=HyperTool%20Production%20Use&body=Hi%20team%2C%0A%0AI'm%20interested%20in%20using%20HyperTool%20in%20production.%0A%0ACompany%3A%20%0AUse%20case%3A%20%0AScale%3A%20%0A%0AThanks!).\n\n## üéÆ App Compatibility\n\n**Works with ANY MCP-compatible app!** HyperTool is a standard MCP server, so if your app supports MCP, it supports HyperTool.",
        "start_pos": 12689,
        "end_pos": 14167,
        "token_count_estimate": 369,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 8,
        "text": "e Desktop** | ‚è≥ In progress | Restart app after switching toolsets |\n\n---\n\nüìö **Learn More**\n\n- üé≠ [Personas Guide](guides/PERSONAS.md) - Complete guide to using and creating personas\n- üî¨ [Research & Performance](guides/RESEARCH.md) - Why focused toolsets work\n- üöÄ [Advanced Features](guides/ADVANCED.md) - Tool annotations, HTTP mode, CLI\n- üîß [Troubleshooting](guides/TROUBLESHOOTING.md) - Common issues and solutions\n- üìñ [Examples & Recipes](guides/EXAMPLES.md) - Toolset patterns for every workflow\n\n## üõ†Ô∏è Development Setup\n\n### Prerequisites\n\n- Node.js 18+\n- Python 3.8+ (for pre-commit hooks)\n\n### Quick Setup\n\n```bash\n# Clone and install\ngit clone https://github.com/toolprint/hypertool-mcp.git\ncd hypertool-mcp\njust setup-dev  # Installs dependencies and pre-commit hooks\n```\n\n### Pre-commit Hooks\n\nThis project uses pre-commit hooks to ensure code quality:\n\n```bash\n# Install pre-commit hooks (included in setup-dev)\njust setup-pre-commit\n\n# Run hooks manually\njust pre-commit-check        # On staged files\njust pre-commit-check-all    # On all files\n\n# Skip hooks for emergency commits (use sparingly)\nSKIP=eslint,typescript git commit -m \"emergency fix\"\n```\n\n### Available Commands\n\n```bash\njust build          # Build the project\njust test           # Run tests\njust lint           # Run linting\njust format         # Format code\njust typecheck      # Check types\njust pre-publish-checks  # Run all quality checks\n```\n\n### Service command\n\nThe `hypertool-mcp service` subcommand is currently disabled and will exit with a\nnotification when invoked.\n\n## ü§ù Contributing\n\nFound a bug? Have an idea? We'd love your help!\n\n- üêõ [Report issues](https://github.com/toolprint/hypertool-mcp/issues)\n- üí° [Share ideas](https://github.com/toolprint/hypertool-mcp/discussions)\n- üîß [Submit PRs](https://github.com/toolprint/hypertool-mcp/pulls)\n\n## üìÑ License\n\nMIT License - see [LICENSE](LICENSE) file for details.",
        "start_pos": 14537,
        "end_pos": 16446,
        "token_count_estimate": 477,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      },
      {
        "chunk_id": 9,
        "text": "ideas](https://github.com/toolprint/hypertool-mcp/discussions)\n- üîß [Submit PRs](https://github.com/toolprint/hypertool-mcp/pulls)\n\n## üìÑ License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n---\n\n<div align=\"center\">\n\n**Built by developers who got tired of watching AI pick the wrong tools** üéØ\n\n<a href=\"https://toolprint.ai\">\n  <img src=\"./assets/toolprint.png\" alt=\"Toolprint\" width=\"200\">\n</a>\n\n<p>\n  <strong>Built with ‚ù§Ô∏è by <a href=\"https://toolprint.ai\">Toolprint</a></strong><br>\n  <sub>¬© 2025 OneGrep, Inc.</sub>\n</p>\n\n</div>",
        "start_pos": 16246,
        "end_pos": 16791,
        "token_count_estimate": 135,
        "source_type": "readme",
        "agent_id": "ff18cabf8f93ca6d"
      }
    ]
  },
  {
    "agent_id": "dbd0bf70601172c1",
    "name": "ai.urlcheck/urlcheck-mcp",
    "source": "mcp",
    "source_url": "https://github.com/cybrlab-ai/urlcheck-mcp",
    "description": "Analyzes threats and verifies URLs align with the agent's intended goal.",
    "tools": [],
    "detected_capabilities": [
      "analyze"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-10T12:38:33.989319Z",
    "indexed_at": "2026-02-18T04:09:41.422201",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# URLCheck MCP Server\n\n[![smithery badge](https://smithery.ai/badge/cybrlab-ai/urlcheck-mcp)](https://smithery.ai/server/cybrlab-ai/urlcheck-mcp)\n\n> **An MCP-native URL preflight scanning service for autonomous agents. It scans links for threats and confirms they match the intended task before execution. Built for agentic workflows, it provides high-accuracy, context-aware browsing governance with adaptive learning.**\n\n**Publisher:** [CybrLab.ai](https://cybrlab.ai) | **Service:** [URLCheck](https://urlcheck.dev)\n\n**Hosted Trial Tier:** No API key required for up to 100 requests/day. For higher limits and stable quotas, use an API key (contact [contact@cybrlab.ai](mailto:contact@cybrlab.ai)).\n\n---\n\n## Repository Rename Notice\n\nThis repository was renamed from `cybrlab-ai/url-scanner-mcp` to `cybrlab-ai/urlcheck-mcp`.\n\n- Canonical repository URL: `https://github.com/cybrlab-ai/urlcheck-mcp`\n- Canonical Git remote: `git@github.com:cybrlab-ai/urlcheck-mcp.git`\n- GitHub Action consumers must update `uses:` references to `cybrlab-ai/urlcheck-mcp@...` (old action paths are not guaranteed to redirect)\n- Collaborators should update local remotes:\n  - `git remote set-url origin git@github.com:cybrlab-ai/urlcheck-mcp.git`\n  - `git fetch origin --verbose`\n\nDo not create a new repository at the old name (`url-scanner-mcp`) to avoid breaking GitHub redirect behavior.\n\n---\n\n## Overview\n\nURLCheck is an MCP server that enables AI agents and any MCP-compatible client to analyze URLs for malicious content and security threats before navigation.\n\n## Integrations\n\nURLCheck works with any MCP-compatible client. For framework-specific adapters:\n\n| Integration           | Repository                                                             |\n|-----------------------|------------------------------------------------------------------------|\n| LangChain / LangGraph | [langchain-urlcheck](https://github.com/cybrlab-ai/langchain-urlcheck) |\n| OpenClaw plugin       | [urlcheck-openclaw](https://github.com/cybrlab-ai/urlcheck-openclaw)   |\n\nFor manual MCP bridge configuration (any client), see [Quick Start](#quick-start) below.\n\n## Authentication Modes\n\n| Deployment                         | `X-API-Key` Requirement         | Notes                                 |\n|------------------------------------|---------------------------------|---------------------------------------|\n| Hosted (`https://urlcheck.ai/mcp`) | Optional up to 100 requests/day | API key recommended for higher limits |\n| Hosted (`https://urlcheck.ai/mcp`) | Required above trial quota      | Contact support for provisioned keys  |\n\n## Important Notice\n\nThis tool is intended for authorized security assessment only. Use it solely on systems or websites that you own or for which you have got explicit permission to assess. Any unauthorized, unlawful, or malicious use is strictly prohibited. You are responsible for ensuring compliance with all applicable laws, regulations, and contractual obligations.\n\n### Use Cases\n\n- Pre-flight URL validation for AI agents\n- Automated URL security scanning in workflows\n- Malicious link detection in emails/messages\n\n---\n\n## Quick Start\n\n### 1. Configure Your MCP Client\n\nChoose one option:\n\nTrial (hosted, up to 100 requests/day without API key):\n\n```json\n{\n  \"mcpServers\": {\n    \"urlcheck-mcp\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://urlcheck.ai/mcp\"\n    }\n  }\n}\n```\n\nAuthenticated (recommended for stable and higher-volume usage):\n\n```json\n{\n  \"mcpServers\": {\n    \"urlcheck-mcp\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://urlcheck.ai/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### 2. Optional: Initialize Session (stateful mode only)\n\n```bash\n# Only required if the server is running in stateful mode\ncurl -X POST https://urlcheck.ai/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"X-API-Key: YOUR_API_KEY\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"initialize\",\n    \"params\": {\n      \"protocolVersion\": \"2025-06-18\",\n      \"capabilities\": {},\n      \"clientInfo\": {\"name\": \"my-client\", \"version\": \"1.0\"}\n    }\n  }'\n# Response includes Mcp-Session-Id header - save it for subsequent requests\n```\n\n### 3. Start a Scan\n\n`url_scanner_scan` supports two execution modes (the same modes apply to `url_scanner_scan_with_intent`):\n- **Task-augmented (recommended)**: Include the `task` parameter for async execution\n- **Direct**: Omit the `task` parameter for synchronous execution\n\n```bash\ncurl -X POST https://urlcheck.ai/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"MCP-Protocol-Version: 2025-06-18\" \\\n  -H \"X-API-Key: YOUR_API_KEY\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 2,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"url_scanner_scan\",\n      \"arguments\": {\n        \"url\": \"https://example.com\"\n      },\n      \"task\": {\n        \"ttl\": 720000\n      }\n    }\n  }'\n# If stateful mode is enabled, include: -H \"Mcp-Session-Id: YOUR_SESSION_ID\"\n```\n\nResponse (task submitted):\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"result\": {\n    \"task\": {\n      \"taskId\": \"550e8400-e29b-41d4-a716-446655440000\",\n      \"status\": \"working\",\n      \"statusMessage\": \"Queued for processing\",\n      \"createdAt\": \"2026-01-18T12:00:00Z\",\n      \"lastUpdatedAt\": \"2026-01-18T12:00:00Z\",\n      \"ttl\": 720000,\n      \"pollInterval\": 2000\n    }\n  }\n}\n```\n\nOptional: Provide an url visiting intent for additional context (recommended but not required):\n\n```bash\ncurl -X POST https://urlcheck.ai/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"MCP-Protocol-Version: 2025-06-18\" \\\n  -H \"X-API-Key: YOUR_API_KEY\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"url_scanner_scan_with_intent\",\n      \"arguments\": {\n        \"url\": \"https://example.com\",\n        \"intent\": \"Book a hotel room\"\n      },\n      \"task\": {\n        \"ttl\": 720000\n      }\n    }\n  }'\n```\n\nRecommendation: Use `url_scanner_scan_with_intent` when you can state your purpose (login, purchase, booking, payments, file download) so intent/content mismatch can be considered as an additional signal. Otherwise use `url_scanner_scan`.\nMax intent length: 248 characters.\nResult includes `intent_alignment` (`misaligned`, `no_mismatch_detected`, `inconclusive`, or `not_provided`).\n\nDirect-call timeout note: synchronous tool calls use a bounded server wait window (hosted default 100s). If timeout is reached, the server returns JSON-RPC `-32603` with `error.data.taskId` and `error.data.pollInterval` so you can continue via `tasks/get` / `tasks/result`.\n\n### 4. Poll for Results\n\n```bash\ncurl -X POST https://urlcheck.ai/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"MCP-Protocol-Version: 2025-06-18\" \\\n  -H \"X-API-Key: YOUR_API_KEY\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tasks/result\",\n    \"params\": {\n      \"taskId\": \"550e8400-e29b-41d4-a716-446655440000\"\n    }\n  }'\n# If stateful mode is enabled, include: -H \"Mcp-Session-Id: YOUR_SESSION_ID\"\n```\n\nResponse (completed task with agent directive):\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 3,\n  \"result\": {\n    \"contentType\": \"application/json\",\n    \"value\": {\n      \"risk_score\": 0.05,\n      \"confidence\": 0.95,\n      \"analysis_complete\": true,\n      \"agent_access_directive\": \"ALLOW\",\n      \"agent_access_reason\": \"clean\",\n      \"intent_alignment\": \"not_provided\"\n    },\n    \"summary\": \"URL scan completed\"\n  }\n}\n```\n\n---\n\n## Available Tools\n\n| Tool                           | Description                              | Execution Modes             |\n|--------------------------------|------------------------------------------|-----------------------------|\n| `url_scanner_scan`             | Analyze URL for security threats         | Direct (sync), Task (async) |\n| `url_scanner_scan_with_intent` | Analyze URL with optional intent context | Direct (sync), Task (async) |\n\nSee [Full API Documentation](docs/API.md) for detailed schemas and examples.\n\n---\n\n## Authentication\n\nAuthentication requirements depend on deployment mode:\n\n- Hosted endpoint (`https://urlcheck.ai/mcp`): API key is optional for up to 100 requests/day.\n- Hosted endpoint above trial quota: API key required.\n\nSee [Authentication Guide](docs/AUTHENTICATION.md) for details on getting API keys.\n\n---\n\n## Technical Specifications\n\n| Property          | Value                      |\n|-------------------|----------------------------|\n| Registry ID       | `ai.urlcheck/urlcheck-mcp` |\n| MCP Spec          | 2025-06-18                 |\n| Client Protocol   | 2025-06-18                 |\n| Transport         | Streamable HTTP            |\n| Endpoint          | `https://urlcheck.ai/mcp`  |\n| Typical Scan Time | Varies by target           |\n| Supported Schemes | HTTP, HTTPS                |\n| Max URL Length    | Enforced by server         |\n\n---\n\n## Support\n\n- **Publisher**: [CybrLab.ai](https://cybrlab.ai)\n- **Service**: [URLCheck](https://urlcheck.dev)\n- **Email**: contact@cybrlab.ai\n\n---\n\n## License\n\nApache License 2.0 - See [LICENSE](LICENSE) for details.\n\nCopyright CybrLab.ai\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Analyze URLs for malicious content and security threats before navigation",
        "Perform pre-flight URL validation for AI agents",
        "Detect malicious links in emails and messages",
        "Support asynchronous task-based URL scanning with task status polling",
        "Support synchronous direct URL scanning calls",
        "Analyze URLs with optional intent context to assess intent-content alignment",
        "Integrate with any MCP-compatible client and provide framework-specific adapters",
        "Provide adaptive learning for context-aware browsing governance"
      ],
      "limitations": [
        "Trial tier limited to 100 requests per day without API key",
        "API key required for usage above trial quota",
        "Intended only for authorized security assessments on owned or permitted systems",
        "Max intent length limited to 248 characters",
        "Synchronous calls have a bounded server wait timeout (default 100 seconds)",
        "Supports only HTTP and HTTPS URL schemes"
      ],
      "requirements": [
        "Optional API key for hosted endpoint usage up to 100 requests/day",
        "API key required for higher volume or stable quotas",
        "MCP-compatible client supporting streamable HTTP transport",
        "For stateful mode, session initialization and session ID management",
        "Explicit permission to scan target URLs to ensure lawful use"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation and usage instructions, detailed examples, tool descriptions, authentication modes, limitations, and requirements, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# URLCheck MCP Server\n\n[![smithery badge](https://smithery.ai/badge/cybrlab-ai/urlcheck-mcp)](https://smithery.ai/server/cybrlab-ai/urlcheck-mcp)\n\n> **An MCP-native URL preflight scanning service for autonomous agents. It scans links for threats and confirms they match the intended task before execution. Built for agentic workflows, it provides high-accuracy, context-aware browsing governance with adaptive learning.**\n\n**Publisher:** [CybrLab.ai](https://cybrlab.ai) | **Service:** [URLCheck](https://urlcheck.dev)\n\n**Hosted Trial Tier:** No API key required for up to 100 requests/day. For higher limits and stable quotas, use an API key (contact [contact@cybrlab.ai](mailto:contact@cybrlab.ai)).\n\n---\n\n## Repository Rename Notice\n\nThis repository was renamed from `cybrlab-ai/url-scanner-mcp` to `cybrlab-ai/urlcheck-mcp`.\n\n- Canonical repository URL: `https://github.com/cybrlab-ai/urlcheck-mcp`\n- Canonical Git remote: `git@github.com:cybrlab-ai/urlcheck-mcp.git`\n- GitHub Action consumers must update `uses:` references to `cybrlab-ai/urlcheck-mcp@...` (old action paths are not guaranteed to redirect)\n- Collaborators should update local remotes:\n  - `git remote set-url origin git@github.com:cybrlab-ai/urlcheck-mcp.git`\n  - `git fetch origin --verbose`\n\nDo not create a new repository at the old name (`url-scanner-mcp`) to avoid breaking GitHub redirect behavior.\n\n---\n\n## Overview\n\nURLCheck is an MCP server that enables AI agents and any MCP-compatible client to analyze URLs for malicious content and security threats before navigation.\n\n## Integrations\n\nURLCheck works with any MCP-compatible client.",
        "start_pos": 0,
        "end_pos": 1617,
        "token_count_estimate": 404,
        "source_type": "readme",
        "agent_id": "dbd0bf70601172c1"
      },
      {
        "chunk_id": 1,
        "text": "|\n| LangChain / LangGraph | [langchain-urlcheck](https://github.com/cybrlab-ai/langchain-urlcheck) |\n| OpenClaw plugin       | [urlcheck-openclaw](https://github.com/cybrlab-ai/urlcheck-openclaw)   |\n\nFor manual MCP bridge configuration (any client), see [Quick Start](#quick-start) below.\n\n## Authentication Modes\n\n| Deployment                         | `X-API-Key` Requirement         | Notes                                 |\n|------------------------------------|---------------------------------|---------------------------------------|\n| Hosted (`https://urlcheck.ai/mcp`) | Optional up to 100 requests/day | API key recommended for higher limits |\n| Hosted (`https://urlcheck.ai/mcp`) | Required above trial quota      | Contact support for provisioned keys  |\n\n## Important Notice\n\nThis tool is intended for authorized security assessment only. Use it solely on systems or websites that you own or for which you have got explicit permission to assess. Any unauthorized, unlawful, or malicious use is strictly prohibited. You are responsible for ensuring compliance with all applicable laws, regulations, and contractual obligations.\n\n### Use Cases\n\n- Pre-flight URL validation for AI agents\n- Automated URL security scanning in workflows\n- Malicious link detection in emails/messages\n\n---\n\n## Quick Start\n\n### 1. Configure Your MCP Client\n\nChoose one option:\n\nTrial (hosted, up to 100 requests/day without API key):\n\n```json\n{\n  \"mcpServers\": {\n    \"urlcheck-mcp\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://urlcheck.ai/mcp\"\n    }\n  }\n}\n```\n\nAuthenticated (recommended for stable and higher-volume usage):\n\n```json\n{\n  \"mcpServers\": {\n    \"urlcheck-mcp\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://urlcheck.ai/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### 2.",
        "start_pos": 1848,
        "end_pos": 3696,
        "token_count_estimate": 462,
        "source_type": "readme",
        "agent_id": "dbd0bf70601172c1"
      },
      {
        "chunk_id": 2,
        "text": "cpServers\": {\n    \"urlcheck-mcp\": {\n      \"transport\": \"streamable-http\",\n      \"url\": \"https://urlcheck.ai/mcp\",\n      \"headers\": {\n        \"X-API-Key\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### 2. Optional: Initialize Session (stateful mode only)\n\n```bash\n# Only required if the server is running in stateful mode\ncurl -X POST https://urlcheck.ai/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"X-API-Key: YOUR_API_KEY\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"initialize\",\n    \"params\": {\n      \"protocolVersion\": \"2025-06-18\",\n      \"capabilities\": {},\n      \"clientInfo\": {\"name\": \"my-client\", \"version\": \"1.0\"}\n    }\n  }'\n# Response includes Mcp-Session-Id header - save it for subsequent requests\n```\n\n### 3.",
        "start_pos": 3496,
        "end_pos": 4289,
        "token_count_estimate": 198,
        "source_type": "readme",
        "agent_id": "dbd0bf70601172c1"
      },
      {
        "chunk_id": 3,
        "text": "dAt\": \"2026-01-18T12:00:00Z\",\n      \"lastUpdatedAt\": \"2026-01-18T12:00:00Z\",\n      \"ttl\": 720000,\n      \"pollInterval\": 2000\n    }\n  }\n}\n```\n\nOptional: Provide an url visiting intent for additional context (recommended but not required):\n\n```bash\ncurl -X POST https://urlcheck.ai/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: application/json, text/event-stream\" \\\n  -H \"MCP-Protocol-Version: 2025-06-18\" \\\n  -H \"X-API-Key: YOUR_API_KEY\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 3,\n    \"method\": \"tools/call\",\n    \"params\": {\n      \"name\": \"url_scanner_scan_with_intent\",\n      \"arguments\": {\n        \"url\": \"https://example.com\",\n        \"intent\": \"Book a hotel room\"\n      },\n      \"task\": {\n        \"ttl\": 720000\n      }\n    }\n  }'\n```\n\nRecommendation: Use `url_scanner_scan_with_intent` when you can state your purpose (login, purchase, booking, payments, file download) so intent/content mismatch can be considered as an additional signal. Otherwise use `url_scanner_scan`.\nMax intent length: 248 characters.\nResult includes `intent_alignment` (`misaligned`, `no_mismatch_detected`, `inconclusive`, or `not_provided`).\n\nDirect-call timeout note: synchronous tool calls use a bounded server wait window (hosted default 100s). If timeout is reached, the server returns JSON-RPC `-32603` with `error.data.taskId` and `error.data.pollInterval` so you can continue via `tasks/get` / `tasks/result`.\n\n### 4.",
        "start_pos": 5344,
        "end_pos": 6765,
        "token_count_estimate": 355,
        "source_type": "readme",
        "agent_id": "dbd0bf70601172c1"
      },
      {
        "chunk_id": 4,
        "text": ": -H \"Mcp-Session-Id: YOUR_SESSION_ID\"\n```\n\nResponse (completed task with agent directive):\n```json\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 3,\n  \"result\": {\n    \"contentType\": \"application/json\",\n    \"value\": {\n      \"risk_score\": 0.05,\n      \"confidence\": 0.95,\n      \"analysis_complete\": true,\n      \"agent_access_directive\": \"ALLOW\",\n      \"agent_access_reason\": \"clean\",\n      \"intent_alignment\": \"not_provided\"\n    },\n    \"summary\": \"URL scan completed\"\n  }\n}\n```\n\n---\n\n## Available Tools\n\n| Tool                           | Description                              | Execution Modes             |\n|--------------------------------|------------------------------------------|-----------------------------|\n| `url_scanner_scan`             | Analyze URL for security threats         | Direct (sync), Task (async) |\n| `url_scanner_scan_with_intent` | Analyze URL with optional intent context | Direct (sync), Task (async) |\n\nSee [Full API Documentation](docs/API.md) for detailed schemas and examples.\n\n---\n\n## Authentication\n\nAuthentication requirements depend on deployment mode:\n\n- Hosted endpoint (`https://urlcheck.ai/mcp`): API key is optional for up to 100 requests/day.\n- Hosted endpoint above trial quota: API key required.\n\nSee [Authentication Guide](docs/AUTHENTICATION.md) for details on getting API keys.",
        "start_pos": 7192,
        "end_pos": 8502,
        "token_count_estimate": 327,
        "source_type": "readme",
        "agent_id": "dbd0bf70601172c1"
      },
      {
        "chunk_id": 5,
        "text": "|\n\n---\n\n## Support\n\n- **Publisher**: [CybrLab.ai](https://cybrlab.ai)\n- **Service**: [URLCheck](https://urlcheck.dev)\n- **Email**: contact@cybrlab.ai\n\n---\n\n## License\n\nApache License 2.0 - See [LICENSE](LICENSE) for details.\n\nCopyright CybrLab.ai",
        "start_pos": 9040,
        "end_pos": 9293,
        "token_count_estimate": 61,
        "source_type": "readme",
        "agent_id": "dbd0bf70601172c1"
      }
    ]
  },
  {
    "agent_id": "7722ceba698ddc6b",
    "name": "ai.waystation/airtable",
    "source": "mcp",
    "source_url": "https://github.com/waystation-ai/mcp",
    "description": "Access and manage your Airtable bases, tables, and records seamlessly",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-09T14:23:23.086629Z",
    "indexed_at": "2026-02-18T04:09:42.669478",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# What is WayStation\n<img src=\"https://waystation.ai/images/logo.svg\" width=\"50\" align=\"left\"/> [WayStation](https://waystation.ai) connects Claude Desktop, ChatGPT and any MCP host with the productivity tools you use daily such as Notion, Monday, Airtable, Jira etc. through a no-code, secure integration hub. \n\n***The original local WayStation MCP server has been deprecated in favor of the new remote MCP server hosted at https://waystation.ai/mcp. Please refer to the new WayStation MCP server documentation here***\n\n## Overview\nWayStation MCP server is a universal remote MCP server that seamlessly connects Claude (and other clients) to a broad range of productivity tools, including Notion, Monday, AirTable, etc.\n\n- WayStation MCP supports both Streamable HTTPS and SSE transports\n- The default endpoint is https://waystation.ai/mcp. It does transport negotiation and authorization if necessary\n- WayStation also provides preauthenticated individual endpoints like https://waystation.ai/mcp/Iddq66dIdkfARDNb3K. Any registered user can get one in their dashboard at https://waystation.ai/dashboard\n\n## Supported providers\n- WayStation supports the following productivity apps: [Notion](https://waystation.ai/connect/notion), [Monday](https://waystation.ai/connect/monday), [Asana](https://waystation.ai/connect/asana), [Linear](https://waystation.ai/connect/linear), [Atlassian JIRA/Confluence](https://waystation.ai/connect/atlassian), [Slack](https://waystation.ai/connect/slack), [Teams](https://waystation.ai/connect/teams), [Google Drive](https://waystation.ai/connect/gdrive) (including Docs and Sheets), [Office 365](https://waystation.ai/connect/office), [Airtable](https://waystation.ai/connect/airtable), [Miro](https://waystation.ai/connect/miro), [Intercom](https://waystation.ai/connect/intercom), [PayPal](https://waystation.ai/connect/paypal).\n- Users can browse available integrations/providers in the [Integrations Marketplace](https://waystation.ai/marketplace)\n- New integrations are added regularly based on customer requests or community contributions. If you have an integration request, please contact us at support@waystation.ai.\n- Users can connect their apps in the [dashboard](https://waystation.ai/dashboard). The connection process may vary by app but generally involves OAuth2 authentication flow with some additional steps for certain apps.\n\n## Supported AI apps\n- WayStation remote MCP was tested with Claude, Cursor, Cline, WindSurf, and MCP-remote STDIO proxy provider\n- For Claude, user should go into their Settings, then Integrations and click \"Add Integration\". Then enter \"WayStation\" as the Server Name and unique MCP URL from user's dashboard\n- For Cline, user should simply go into the MCP Server screen, switch to the Remote Servers tab, enter \"WayStation\" as the Server Name and unique MCP URL from user's dashboard\n- For Cursor, user should go to the Cursor Settings, MCP tab and click \"Add new global MCP server\". In mcp.json file user should add the entry for WayStation as following:\n```json\n\"WayStation\": {\n      \"url\": \"https://waystation.ai/mcp/<user_unique_id>\"\n}\n```\n\n## Use Cases\nWayStation supports a variety of productivity and automation use cases listed below:\n- [Project Management](https://waystation.ai/ai/project-management)\n- [Task Automation](https://waystation.ai/ai/task-automation)\n- [Meeting Summaries & Action Items](https://waystation.ai/ai/meeting-summaries)\n- [Workflow Automation & Process Optimization](https://waystation.ai/ai/workflow-automation)\n- [Resource & Capacity Planning](https://waystation.ai/ai/resource-capacity-planning)\n- [Risk & Issue Management](https://waystation.ai/ai/risk-issue-management)\n- [Reporting & Insights](https://waystation.ai/ai/reporting-insights)\n- [Portfolio Management](https://waystation.ai/ai/portfolio-management)\n- [Team Collaboration Assistant](https://waystation.ai/ai/team-collaboration-assistant)\n- [Creative Production Management](https://waystation.ai/ai/creative-production-management)\n- [Campaign Management](https://waystation.ai/ai/campaign-management)\n- [Product Management & Roadmapping](https://waystation.ai/ai/product-management-roadmapping)\n- [Product Launch Coordination](https://waystation.ai/ai/product-launch-coordination)\n- [Operations Management](https://waystation.ai/ai/operations-management)\n- [IT Project Coordination](https://waystation.ai/ai/it-project-coordination)\n- [Project Intake & Triage](https://waystation.ai/ai/project-intake-triage)\n- [Knowledge Management Integration](https://waystation.ai/ai/knowledge-management-integration)\n- [Goal Tracking & OKR Alignment](https://waystation.ai/ai/goal-tracking-okr-alignment)\n- [Compliance & Audit Trail Management](https://waystation.ai/ai/compliance-audit-trail)\n- [Timeline & Deadline Optimization](https://waystation.ai/ai/timeline-deadline-optimization)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect Claude and other MCP clients to multiple productivity tools such as Notion, Monday, Airtable, Jira, Slack, Teams, Google Drive, Office 365, Miro, Intercom, and PayPal",
        "Support Streamable HTTPS and Server-Sent Events (SSE) transports for communication",
        "Provide preauthenticated individual MCP endpoints for registered users",
        "Facilitate OAuth2 authentication flow for connecting user apps",
        "Enable integration with various AI applications including Claude, Cursor, Cline, WindSurf, and MCP-remote STDIO proxy",
        "Support a wide range of productivity and automation use cases including project management, task automation, meeting summaries, workflow optimization, and more",
        "Allow users to manage and add integrations via a dashboard and marketplace",
        "Regularly add new integrations based on customer requests and community contributions"
      ],
      "limitations": [
        "The original local WayStation MCP server is deprecated and replaced by the remote server",
        "Connection process and authentication steps may vary depending on the integrated app",
        "No explicit mention of rate limits or performance constraints in the documentation"
      ],
      "requirements": [
        "User registration to obtain unique MCP endpoint URLs from the WayStation dashboard",
        "OAuth2 authentication credentials for connecting third-party productivity apps",
        "Access to supported AI clients configured with the WayStation MCP server URL",
        "Internet access to the remote MCP server at https://waystation.ai/mcp"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides comprehensive installation and usage details, supported tools and AI clients, integration methods, and a broad list of use cases, but lacks explicit limitations and rate limit details.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# What is WayStation\n<img src=\"https://waystation.ai/images/logo.svg\" width=\"50\" align=\"left\"/> [WayStation](https://waystation.ai) connects Claude Desktop, ChatGPT and any MCP host with the productivity tools you use daily such as Notion, Monday, Airtable, Jira etc. through a no-code, secure integration hub. \n\n***The original local WayStation MCP server has been deprecated in favor of the new remote MCP server hosted at https://waystation.ai/mcp. Please refer to the new WayStation MCP server documentation here***\n\n## Overview\nWayStation MCP server is a universal remote MCP server that seamlessly connects Claude (and other clients) to a broad range of productivity tools, including Notion, Monday, AirTable, etc.\n\n- WayStation MCP supports both Streamable HTTPS and SSE transports\n- The default endpoint is https://waystation.ai/mcp. It does transport negotiation and authorization if necessary\n- WayStation also provides preauthenticated individual endpoints like https://waystation.ai/mcp/Iddq66dIdkfARDNb3K. Any registered user can get one in their dashboard at https://waystation.ai/dashboard\n\n## Supported providers\n- WayStation supports the following productivity apps: [Notion](https://waystation.ai/connect/notion), [Monday](https://waystation.ai/connect/monday), [Asana](https://waystation.ai/connect/asana), [Linear](https://waystation.ai/connect/linear), [Atlassian JIRA/Confluence](https://waystation.ai/connect/atlassian), [Slack](https://waystation.ai/connect/slack), [Teams](https://waystation.ai/connect/teams), [Google Drive](https://waystation.ai/connect/gdrive) (including Docs and Sheets), [Office 365](https://waystation.ai/connect/office), [Airtable](https://waystation.ai/connect/airtable), [Miro](https://waystation.ai/connect/miro), [Intercom](https://waystation.ai/connect/intercom), [PayPal](https://waystation.ai/connect/paypal).",
        "start_pos": 0,
        "end_pos": 1865,
        "token_count_estimate": 466,
        "source_type": "readme",
        "agent_id": "7722ceba698ddc6b"
      },
      {
        "chunk_id": 1,
        "text": "ce), [Airtable](https://waystation.ai/connect/airtable), [Miro](https://waystation.ai/connect/miro), [Intercom](https://waystation.ai/connect/intercom), [PayPal](https://waystation.ai/connect/paypal).\n- Users can browse available integrations/providers in the [Integrations Marketplace](https://waystation.ai/marketplace)\n- New integrations are added regularly based on customer requests or community contributions. If you have an integration request, please contact us at support@waystation.ai.\n- Users can connect their apps in the [dashboard](https://waystation.ai/dashboard). The connection process may vary by app but generally involves OAuth2 authentication flow with some additional steps for certain apps.\n\n## Supported AI apps\n- WayStation remote MCP was tested with Claude, Cursor, Cline, WindSurf, and MCP-remote STDIO proxy provider\n- For Claude, user should go into their Settings, then Integrations and click \"Add Integration\". Then enter \"WayStation\" as the Server Name and unique MCP URL from user's dashboard\n- For Cline, user should simply go into the MCP Server screen, switch to the Remote Servers tab, enter \"WayStation\" as the Server Name and unique MCP URL from user's dashboard\n- For Cursor, user should go to the Cursor Settings, MCP tab and click \"Add new global MCP server\".",
        "start_pos": 1665,
        "end_pos": 2966,
        "token_count_estimate": 325,
        "source_type": "readme",
        "agent_id": "7722ceba698ddc6b"
      },
      {
        "chunk_id": 2,
        "text": "flow-automation)\n- [Resource & Capacity Planning](https://waystation.ai/ai/resource-capacity-planning)\n- [Risk & Issue Management](https://waystation.ai/ai/risk-issue-management)\n- [Reporting & Insights](https://waystation.ai/ai/reporting-insights)\n- [Portfolio Management](https://waystation.ai/ai/portfolio-management)\n- [Team Collaboration Assistant](https://waystation.ai/ai/team-collaboration-assistant)\n- [Creative Production Management](https://waystation.ai/ai/creative-production-management)\n- [Campaign Management](https://waystation.ai/ai/campaign-management)\n- [Product Management & Roadmapping](https://waystation.ai/ai/product-management-roadmapping)\n- [Product Launch Coordination](https://waystation.ai/ai/product-launch-coordination)\n- [Operations Management](https://waystation.ai/ai/operations-management)\n- [IT Project Coordination](https://waystation.ai/ai/it-project-coordination)\n- [Project Intake & Triage](https://waystation.ai/ai/project-intake-triage)\n- [Knowledge Management Integration](https://waystation.ai/ai/knowledge-management-integration)\n- [Goal Tracking & OKR Alignment](https://waystation.ai/ai/goal-tracking-okr-alignment)\n- [Compliance & Audit Trail Management](https://waystation.ai/ai/compliance-audit-trail)\n- [Timeline & Deadline Optimization](https://waystation.ai/ai/timeline-deadline-optimization)",
        "start_pos": 3513,
        "end_pos": 4859,
        "token_count_estimate": 336,
        "source_type": "readme",
        "agent_id": "7722ceba698ddc6b"
      }
    ]
  },
  {
    "agent_id": "7de5aac5ba8ab8c4",
    "name": "ai.wild-card/deepcontext",
    "source": "mcp",
    "source_url": "https://github.com/Wildcard-Official/deepcontext",
    "description": "Advanced codebase indexing and semantic search MCP server",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-26T03:44:16.929896Z",
    "indexed_at": "2026-02-18T04:09:44.319669",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Index codebases",
        "Perform semantic search on codebases"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's capabilities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "290bbc28d0570444",
    "name": "ai.xpoz/social-insights",
    "source": "mcp",
    "source_url": "https://www.xpoz.ai",
    "description": "Twitter/X and Instagram data access without API keys. Get profiles, posts, comments, and metrics.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-11-13T17:47:50.690541Z",
    "indexed_at": "2026-02-18T04:09:48.059251",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "MCP Server for Social Intelligence\nSocial Intelligence for AI Agents\nEnable LLMs to search and analyze live social media through natural language.\nNo API keys. No rate limits. Just intelligence.\nGet Started Free\nView Examples\n1.5B+\nPosts Indexed\n2 Min\nSetup Time\nFree Trial\nYour browser does not support the video tag.\nInstallation\nChoose your preferred installation method and start querying in minutes\n√∞¬ü¬å¬ê\nFully Remote\n√¢¬Ä¬¢\nNo Local Installation\n√¢¬Ä¬¢\nNo Self-Hosting Required\n√∞¬ü¬å¬ê\nClaude.ai on the Web\nAdd custom connector\nhttps://mcp.xpoz.ai/mcp\nCopy\nAdd this URL in Claude.ai web connector settings\nView detailed setup guide √¢¬Ü¬í\n√∞¬ü¬í¬ª\nAI Agents\nRun this command from your terminal:\nClaude Code\nGemini CLI\nCodex\nOpenClaw\nclaude mcp add --transport http --scope user xpoz-mcp https://mcp.xpoz.ai/mcp --header \"Authorization: Bearer <your Xpoz API token>\"\nCopy\nRun this command from your system terminal, not within the Agent's CLI.\nView detailed setup guide √¢¬Ü¬í\nTry These Queries\nReal examples you can use immediately after installation\n√∞¬ü¬î¬ç\nSearch Recent Posts\n\"Find tweets about Claude AI from the last 7 days\"\nCopy\nSearch recent discussions and track trending topics\n√∞¬ü¬î¬•\nViral Content Analysis\n\"Show me @elonmusk's most viral tweets this month\"\nCopy\nAnalyze engagement patterns and identify top content\n√∞¬ü¬ì¬ä\nEngagement Filtering\n\"Get tweets mentioning 'MCP' with more than 100 likes\"\nCopy\nFilter by engagement metrics for quality content\n√∞¬ü¬ì¬∑\nInstagram Search\n\"Find Instagram posts from @natgeo about wildlife\"\nCopy\nSearch Instagram by username, keywords, or hashtags\n√∞¬ü¬ó¬£√Ø¬∏¬è\nReddit Discussions\n\"Search Reddit posts about MCP servers in r/ClaudeAI\"\nCopy\nSearch Reddit by subreddit, keywords, or users\n√∞¬ü¬í¬¨\nSentiment Analysis\n\"Analyze sentiment of tweets about AI safety\"\nCopy\nCombine with Claude's analysis for deep insights\n√∞¬ü¬ë¬§\nProfile Information\n\"Get profile info for @AnthropicAI\"\nCopy\nAccess follower counts, verification status, and more\nBuilt For Real Work\nFrom research to marketing, threat detection to competitive analysis\n√∞¬ü¬î¬ç\nSocial Listening\nMonitor brand mentions, track competitors, and analyze sentiment in real-time without expensive tools\n√∞¬ü¬ì¬ä\nResearch & Analysis\nAcademic research, trend analysis, and network mapping with full historical data access\n√∞¬ü¬§¬ñ\nAI Agent Tools\nBuild agents that understand social context and respond to current events automatically\n√∞¬ü¬õ¬°√Ø¬∏¬è\nThreat Detection\nIdentify bot networks, coordinated campaigns, and narrative manipulation at scale\n√∞¬ü¬é¬Ø\nMarketing Intelligence\nTrack campaign performance, influencer analysis, and audience insights without limits\n√∞¬ü¬ì¬à\nCompetitive Analysis\nMonitor competitor activity, benchmark performance, and identify market opportunities\nTransform Claude Into a Social Intelligence Agent\nAccess structured social data through natural language instead of web scraping\nCapability\nClaude\nClaude with Xpoz\nSetup Requirements\nN/A\n√¢¬ú¬ì\nZero installation - fully remote server\nFind Posts by Topic\nCannot search platform-wide\n√¢¬ú¬ì\nSearch millions by keywords, hashtags, mentions\nUser Intelligence\nBasic profile info only\n√¢¬ú¬ì\nComplete history, follower metrics, engagement patterns\nReal-Time Data\nStatic web pages only\n√¢¬ú¬ì\nAccess latest posts, live metrics, current trends\nEngagement Tracking\nSee current displayed numbers\n√¢¬ú¬ì\nTrack engagement over time, identify viral content\nInfluence Mapping\nCannot analyze networks\n√¢¬ú¬ì\nFind influential accounts, analyze conversation networks\nTrend Analysis\nCannot access historical data\n√¢¬ú¬ì\nTrack narratives, sentiment shifts, topic evolution\nScale\nManual URL fetching (slow)\n√¢¬ú¬ì\nQuery thousands of posts instantly\nAdvanced Filters\nNo filtering capability\n√¢¬ú¬ì\nFilter by date, engagement, language, location\nSetup Requirements\nClaude\nN/A\nClaude with Xpoz\n√¢¬ú¬ì\nZero installation - fully remote server\nFind Posts by Topic\nClaude\nCannot search platform-wide\nClaude with Xpoz\n√¢¬ú¬ì\nSearch millions by keywords, hashtags, mentions\nUser Intelligence\nClaude\nBasic profile info only\nClaude with Xpoz\n√¢¬ú¬ì\nComplete history, follower metrics, engagement patterns\nReal-Time Data\nClaude\nStatic web pages only\nClaude with Xpoz\n√¢¬ú¬ì\nAccess latest posts, live metrics, current trends\nEngagement Tracking\nClaude\nSee current displayed numbers\nClaude with Xpoz\n√¢¬ú¬ì\nTrack engagement over time, identify viral content\nInfluence Mapping\nClaude\nCannot analyze networks\nClaude with Xpoz\n√¢¬ú¬ì\nFind influential accounts, analyze conversation networks\nTrend Analysis\nClaude\nCannot access historical data\nClaude with Xpoz\n√¢¬ú¬ì\nTrack narratives, sentiment shifts, topic evolution\nScale\nClaude\nManual URL fetching (slow)\nClaude with Xpoz\n√¢¬ú¬ì\nQuery thousands of posts instantly\nAdvanced Filters\nClaude\nNo filtering capability\nClaude with Xpoz\n√¢¬ú¬ì\nFilter by date, engagement, language, location\nTechnical Excellence\nBuilt on Modern Standards\nMCP 2025 compliant with enterprise-grade features\n√∞¬ü¬ö¬Ä\nStreamable HTTP\nLatest MCP specification with efficient streaming support for real-time data\n√∞¬ü¬î¬ê\nOAuth 2.1 Auth\nSecure authentication with Google OAuth integration and token management\n√∞¬ü¬ì¬°\nEvent Resumption\nResume long-running queries without data loss or starting over\n√∞¬ü¬é¬Ø\nField Selection\nRequest only the data you need for optimal performance and efficiency\n√∞¬ü¬í¬æ\nSmart Caching\nIntelligent caching with automatic invalidation and force-refresh options\n√∞¬ü¬ì¬Ñ\nPagination\nHandle massive datasets with seamless pagination and cursor management\n√∞¬ü¬î¬ç\nAdvanced Filters\nFilter by date, author, language, engagement metrics, and more\n√∞¬ü¬ì¬ä\nRich Metadata\nAccess engagement counts, media URLs, hashtags, mentions, and full context\nFrequently Asked Questions\nEverything you need to know about Xpoz MCP\n1\nDo I need a Twitter API key?\nNo! That's the whole point. Xpoz provides direct access to social media data without requiring expensive API keys, avoiding rate limits and costs entirely.\n2\nHow do I get an access key?\nAfter installing, authenticate via OAuth through your Google account. The server guides you through the simple process on first use.\n3\nWhat's the difference between Claude Desktop and Claude.ai?\nBoth Claude Desktop and Claude.ai web use Web Connectors for remote MCP servers. The setup is the same: add a custom connector via Settings √¢¬Ü¬í Connectors with the remote server URL.\n4\nCan I use this commercially?\nYes! Xpoz MCP is available for both personal and commercial use. Check our licensing terms for specific details about your use case.\n5\nWhat data is available?\nAccess tweets, user profiles, Instagram posts, engagement metrics, hashtags, mentions, media URLs, and full historical data. See docs for complete field lists.\n6\nWhat is MCP (Model Context Protocol)?\nMCP (Model Context Protocol) is an open standard developed by Anthropic that allows AI assistants like Claude to securely connect to external data sources and tools. Think of it as a universal adapter that lets AI models access real-world data without custom API integrations for each service.\n7\nHow does Xpoz compare to Twitter API?\nUnlike Twitter's official API which requires developer approval, rate limits, and costs $100-$5,000/month, Xpoz provides instant access through MCP with no API keys required. You get natural language queries instead of complex API calls, access to 1.5B+ posts, and pricing that starts free.\n8\nWhat social media platforms does Xpoz support?\nXpoz currently supports Twitter/X, Instagram, TikTok, and Reddit. You can search posts, profiles, hashtags, and trends across all four platforms using natural language queries through Claude or other MCP-compatible AI assistants.\n9\nCan I use Xpoz with ChatGPT?\nYes! Xpoz works with both Claude and ChatGPT. For Claude, use the native MCP integration. For ChatGPT, connect via our remote MCP server URL in your ChatGPT settings. Both platforms support natural language queries to access social media data.\n10\nHow much does Twitter API cost vs Xpoz?\nTwitter's Basic API costs $100/month for 10,000 tweets. Their Pro tier is $5,000/month. Xpoz offers a free tier with 100,000 results/month, Pro at $20/month for 1M results, and Max at $200/month for 10M results√¢¬Ä¬îup to 100x more data for a fraction of the cost.\n11\nWhat Reddit data can I access with Xpoz?\nWith Xpoz, you can search Reddit posts and comments by subreddit, keywords, or users. Access post titles, content, upvotes, comments, and user information across any public subreddit√¢¬Ä¬îall through natural language queries with Claude.\nReady to Get Started?\nInstall Xpoz MCP in 2 minutes and unlock social intelligence for Claude\nGet Started Free\nRead Documentation"
    },
    "llm_extracted": {
      "capabilities": [
        "Search and analyze live social media posts using natural language",
        "Access and query over 1.5 billion indexed posts from Twitter/X, Instagram, TikTok, and Reddit",
        "Perform sentiment analysis on social media content",
        "Filter social media posts by date, engagement metrics, language, and location",
        "Retrieve detailed user profile information including follower counts and verification status",
        "Track engagement patterns and identify viral content over time",
        "Analyze influence by finding influential accounts and conversation networks",
        "Monitor brand mentions, competitor activity, and campaign performance in real-time",
        "Support real-time data streaming with efficient pagination and caching",
        "Integrate with AI agents like Claude and ChatGPT via MCP protocol without API keys or rate limits"
      ],
      "limitations": [
        "Cannot perform platform-wide searches with Claude alone without Xpoz MCP",
        "Cannot analyze networks or influence mapping without Xpoz MCP",
        "Cannot access historical data or trend analysis without Xpoz MCP",
        "Manual URL fetching is slow without Xpoz MCP",
        "Claude alone lacks advanced filtering capabilities"
      ],
      "requirements": [
        "Authentication via OAuth 2.1 with Google account to obtain access token",
        "Use of MCP-compatible AI assistants such as Claude or ChatGPT",
        "Adding the Xpoz MCP server URL as a custom connector in AI assistant settings",
        "Authorization header with Bearer token for API access"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "The documentation provides detailed capability descriptions, usage examples, installation instructions, and setup commands but lacks comprehensive limitations and advanced technical details.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "MCP Server for Social Intelligence\nSocial Intelligence for AI Agents\nEnable LLMs to search and analyze live social media through natural language.\nNo API keys. No rate limits. Just intelligence.\nGet Started Free\nView Examples\n1.5B+\nPosts Indexed\n2 Min\nSetup Time\nFree Trial\nYour browser does not support the video tag.\nInstallation\nChoose your preferred installation method and start querying in minutes\n√∞¬ü¬å¬ê\nFully Remote\n√¢¬Ä¬¢\nNo Local Installation\n√¢¬Ä¬¢\nNo Self-Hosting Required\n√∞¬ü¬å¬ê\nClaude.ai on the Web\nAdd custom connector\nhttps://mcp.xpoz.ai/mcp\nCopy\nAdd this URL in Claude.ai web connector settings\nView detailed setup guide √¢¬Ü¬í\n√∞¬ü¬í¬ª\nAI Agents\nRun this command from your terminal:\nClaude Code\nGemini CLI\nCodex\nOpenClaw\nclaude mcp add --transport http --scope user xpoz-mcp https://mcp.xpoz.ai/mcp --header \"Authorization: Bearer <your Xpoz API token>\"\nCopy\nRun this command from your system terminal, not within the Agent's CLI.",
        "start_pos": 0,
        "end_pos": 931,
        "token_count_estimate": 232,
        "source_type": "detail_page",
        "agent_id": "290bbc28d0570444"
      },
      {
        "chunk_id": 1,
        "text": "ation\n\"Get profile info for @AnthropicAI\"\nCopy\nAccess follower counts, verification status, and more\nBuilt For Real Work\nFrom research to marketing, threat detection to competitive analysis\n√∞¬ü¬î¬ç\nSocial Listening\nMonitor brand mentions, track competitors, and analyze sentiment in real-time without expensive tools\n√∞¬ü¬ì¬ä\nResearch & Analysis\nAcademic research, trend analysis, and network mapping with full historical data access\n√∞¬ü¬§¬ñ\nAI Agent Tools\nBuild agents that understand social context and respond to current events automatically\n√∞¬ü¬õ¬°√Ø¬∏¬è\nThreat Detection\nIdentify bot networks, coordinated campaigns, and narrative manipulation at scale\n√∞¬ü¬é¬Ø\nMarketing Intelligence\nTrack campaign performance, influencer analysis, and audience insights without limits\n√∞¬ü¬ì¬à\nCompetitive Analysis\nMonitor competitor activity, benchmark performance, and identify market opportunities\nTransform Claude Into a Social Intelligence Agent\nAccess structured social data through natural language instead of web scraping\nCapability\nClaude\nClaude with Xpoz\nSetup Requirements\nN/A\n√¢¬ú¬ì\nZero installation - fully remote server\nFind Posts by Topic\nCannot search platform-wide\n√¢¬ú¬ì\nSearch millions by keywords, hashtags, mentions\nUser Intelligence\nBasic profile info only\n√¢¬ú¬ì\nComplete history, follower metrics, engagement patterns\nReal-Time Data\nStatic web pages only\n√¢¬ú¬ì\nAccess latest posts, live metrics, current trends\nEngagement Tracking\nSee current displayed numbers\n√¢¬ú¬ì\nTrack engagement over time, identify viral content\nInfluence Mapping\nCannot analyze networks\n√¢¬ú¬ì\nFind influential accounts, analyze conversation networks\nTrend Analysis\nCannot access historical data\n√¢¬ú¬ì\nTrack narratives, sentiment shifts, topic evolution\nScale\nManual URL fetching (slow)\n√¢¬ú¬ì\nQuery thousands of posts instantly\nAdvanced Filters\nNo filtering capability\n√¢¬ú¬ì\nFilter by date, engagement, language, location\nSetup Requirements\nClaude\nN/A\nClaude with Xpoz\n√¢¬ú¬ì\nZero installation - fully remote server\nFind Posts by Topic\nClaude\nCannot search platform-wide\nClaude with Xpoz\n√¢¬ú¬ì\nSearch millions",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 511,
        "source_type": "detail_page",
        "agent_id": "290bbc28d0570444"
      },
      {
        "chunk_id": 2,
        "text": "nguage, location\nSetup Requirements\nClaude\nN/A\nClaude with Xpoz\n√¢¬ú¬ì\nZero installation - fully remote server\nFind Posts by Topic\nClaude\nCannot search platform-wide\nClaude with Xpoz\n√¢¬ú¬ì\nSearch millions by keywords, hashtags, mentions\nUser Intelligence\nClaude\nBasic profile info only\nClaude with Xpoz\n√¢¬ú¬ì\nComplete history, follower metrics, engagement patterns\nReal-Time Data\nClaude\nStatic web pages only\nClaude with Xpoz\n√¢¬ú¬ì\nAccess latest posts, live metrics, current trends\nEngagement Tracking\nClaude\nSee current displayed numbers\nClaude with Xpoz\n√¢¬ú¬ì\nTrack engagement over time, identify viral content\nInfluence Mapping\nClaude\nCannot analyze networks\nClaude with Xpoz\n√¢¬ú¬ì\nFind influential accounts, analyze conversation networks\nTrend Analysis\nClaude\nCannot access historical data\nClaude with Xpoz\n√¢¬ú¬ì\nTrack narratives, sentiment shifts, topic evolution\nScale\nClaude\nManual URL fetching (slow)\nClaude with Xpoz\n√¢¬ú¬ì\nQuery thousands of posts instantly\nAdvanced Filters\nClaude\nNo filtering capability\nClaude with Xpoz\n√¢¬ú¬ì\nFilter by date, engagement, language, location\nTechnical Excellence\nBuilt on Modern Standards\nMCP 2025 compliant with enterprise-grade features\n√∞¬ü¬ö¬Ä\nStreamable HTTP\nLatest MCP specification with efficient streaming support for real-time data\n√∞¬ü¬î¬ê\nOAuth 2.1 Auth\nSecure authentication with Google OAuth integration and token management\n√∞¬ü¬ì¬°\nEvent Resumption\nResume long-running queries without data loss or starting over\n√∞¬ü¬é¬Ø\nField Selection\nRequest only the data you need for optimal performance and efficiency\n√∞¬ü¬í¬æ\nSmart Caching\nIntelligent caching with automatic invalidation and force-refresh options\n√∞¬ü¬ì¬Ñ\nPagination\nHandle massive datasets with seamless pagination and cursor management\n√∞¬ü¬î¬ç\nAdvanced Filters\nFilter by date, author, language, engagement metrics, and more\n√∞¬ü¬ì¬ä\nRich Metadata\nAccess engagement counts, media URLs, hashtags, mentions, and full context\nFrequently Asked Questions\nEverything you need to know about Xpoz MCP\n1\nDo I need a Twitter API key?\nNo! That's the whole point.",
        "start_pos": 3696,
        "end_pos": 5713,
        "token_count_estimate": 504,
        "source_type": "detail_page",
        "agent_id": "290bbc28d0570444"
      },
      {
        "chunk_id": 3,
        "text": "ess engagement counts, media URLs, hashtags, mentions, and full context\nFrequently Asked Questions\nEverything you need to know about Xpoz MCP\n1\nDo I need a Twitter API key?\nNo! That's the whole point. Xpoz provides direct access to social media data without requiring expensive API keys, avoiding rate limits and costs entirely.\n2\nHow do I get an access key?\nAfter installing, authenticate via OAuth through your Google account. The server guides you through the simple process on first use.\n3\nWhat's the difference between Claude Desktop and Claude.ai?\nBoth Claude Desktop and Claude.ai web use Web Connectors for remote MCP servers. The setup is the same: add a custom connector via Settings √¢¬Ü¬í Connectors with the remote server URL.\n4\nCan I use this commercially?\nYes! Xpoz MCP is available for both personal and commercial use. Check our licensing terms for specific details about your use case.\n5\nWhat data is available?\nAccess tweets, user profiles, Instagram posts, engagement metrics, hashtags, mentions, media URLs, and full historical data. See docs for complete field lists.\n6\nWhat is MCP (Model Context Protocol)?\nMCP (Model Context Protocol) is an open standard developed by Anthropic that allows AI assistants like Claude to securely connect to external data sources and tools. Think of it as a universal adapter that lets AI models access real-world data without custom API integrations for each service.\n7\nHow does Xpoz compare to Twitter API?\nUnlike Twitter's official API which requires developer approval, rate limits, and costs $100-$5,000/month, Xpoz provides instant access through MCP with no API keys required. You get natural language queries instead of complex API calls, access to 1.5B+ posts, and pricing that starts free.\n8\nWhat social media platforms does Xpoz support?\nXpoz currently supports Twitter/X, Instagram, TikTok, and Reddit. You can search posts, profiles, hashtags, and trends across all four platforms using natural language queries through Claude or other MCP-compatible AI assistants.",
        "start_pos": 5513,
        "end_pos": 7543,
        "token_count_estimate": 507,
        "source_type": "detail_page",
        "agent_id": "290bbc28d0570444"
      },
      {
        "chunk_id": 4,
        "text": "er/X, Instagram, TikTok, and Reddit. You can search posts, profiles, hashtags, and trends across all four platforms using natural language queries through Claude or other MCP-compatible AI assistants.\n9\nCan I use Xpoz with ChatGPT?\nYes! Xpoz works with both Claude and ChatGPT. For Claude, use the native MCP integration. For ChatGPT, connect via our remote MCP server URL in your ChatGPT settings. Both platforms support natural language queries to access social media data.\n10\nHow much does Twitter API cost vs Xpoz?\nTwitter's Basic API costs $100/month for 10,000 tweets. Their Pro tier is $5,000/month. Xpoz offers a free tier with 100,000 results/month, Pro at $20/month for 1M results, and Max at $200/month for 10M results√¢¬Ä¬îup to 100x more data for a fraction of the cost.\n11\nWhat Reddit data can I access with Xpoz?\nWith Xpoz, you can search Reddit posts and comments by subreddit, keywords, or users. Access post titles, content, upvotes, comments, and user information across any public subreddit√¢¬Ä¬îall through natural language queries with Claude.\nReady to Get Started?\nInstall Xpoz MCP in 2 minutes and unlock social intelligence for Claude\nGet Started Free\nRead Documentation",
        "start_pos": 7343,
        "end_pos": 8532,
        "token_count_estimate": 297,
        "source_type": "detail_page",
        "agent_id": "290bbc28d0570444"
      }
    ]
  },
  {
    "agent_id": "290bbc28d0570444",
    "name": "ai.xpoz/social-insights",
    "source": "mcp",
    "source_url": "https://github.com/nichochar/xpoz-mcp",
    "description": "Twitter/X, Instagram, Reddit & TikTok data for AI agents. 1.5B+ posts. No API keys.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-12T12:21:43.917122Z",
    "indexed_at": "2026-02-18T04:09:48.650253",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to Twitter/X data",
        "Provide access to Instagram data",
        "Provide access to Reddit data",
        "Provide access to TikTok data",
        "Supply a large dataset of over 1.5 billion posts for AI agents"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of supported platforms and data volume but lacks detailed capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "290bbc28d0570444",
    "name": "ai.xpoz/social-insights",
    "source": "mcp",
    "source_url": "https://github.com/xpozpublic/xpoz-mcp",
    "description": "Twitter/X, Instagram, Reddit & TikTok data for AI agents. 1.5B+ posts. No API keys.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2026-02-12T14:49:07.40155Z",
    "indexed_at": "2026-02-18T04:09:52.681537",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Xpoz MCP Server\n\n**Search Twitter, Instagram, Reddit & TikTok from any AI agent.** 1.5B+ posts indexed. Natural language queries. CSV exports up to 500K rows. No API keys needed.\n\n[![MCP Registry](https://img.shields.io/badge/MCP_Registry-listed-blue)](https://registry.modelcontextprotocol.io/?q=xpoz)\n[![Website](https://img.shields.io/badge/xpoz.ai-visit-green)](https://xpoz.ai)\n\n> **Remote MCP server** ‚Äî no local installation required. Connect via Streamable HTTP and authenticate with Google OAuth.\n\n## Quick Start\n\n### Claude Desktop / Cursor / Windsurf\n\nAdd to your MCP config:\n\n```json\n{\n  \"mcpServers\": {\n    \"xpoz\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.xpoz.ai/mcp\"\n    }\n  }\n}\n```\n\n### OpenClaw\n\n```bash\nclawhub install xpoz-social-search\n```\n\nThen authenticate via the OAuth link when prompted.\n\n---\n\n## Platforms & Tools\n\n### Twitter / X (14 tools)\n\n| Tool | Description |\n|------|-------------|\n| `searchTwitterUsers` | Find users by name, bio, or keywords |\n| `getTwitterUser` | Get profile details (followers, bio, metrics) |\n| `getTwitterUsersByKeywords` | Discover users actively posting about topics |\n| `getTwitterUserConnections` | Get followers or following list |\n| `getTwitterPostsByKeywords` | Search tweets by keywords, hashtags, mentions |\n| `getTwitterPostsByAuthor` | Get tweets from a specific user |\n| `getTwitterPostsByIds` | Fetch specific tweets by ID |\n| `getTwitterPostComments` | Get replies to a tweet |\n| `getTwitterPostRetweets` | Get retweets of a tweet |\n| `getTwitterPostQuotes` | Get quote tweets |\n| `getTwitterPostInteractingUsers` | Users who liked/retweeted a post |\n| `countTweets` | Count tweets matching a query (hourly/daily buckets) |\n| `checkOperationStatus` | Poll async export operations |\n| `cancelOperation` | Cancel a running operation |\n\n### Instagram (9 tools)\n\n| Tool | Description |\n|------|-------------|\n| `searchInstagramUsers` | Find users by name or username |\n| `getInstagramUser` | Get profile details and metrics |\n| `getInstagramUsersByKeywords` | Discover users posting about topics |\n| `getInstagramUserConnections` | Get followers or following list |\n| `getInstagramPostsByKeywords` | Search posts by keywords or hashtags |\n| `getInstagramPostsByUser` | Get posts from a specific user |\n| `getInstagramPostsByIds` | Fetch specific posts by ID |\n| `getInstagramPostInteractingUsers` | Users who liked/commented on a post |\n| `getInstagramCommentsByPostId` | Get comments on a post |\n\n### Reddit (6 tools)\n\n| Tool | Description |\n|------|-------------|\n| `searchRedditUsers` | Find users by name |\n| `getRedditUser` | Get user profile and karma |\n| `getRedditUsersByKeywords` | Discover users active in topics |\n| `getRedditPostsByKeywords` | Search posts across subreddits |\n| `getRedditPostWithCommentsById` | Get a post with its comment tree |\n| `getRedditCommentsByKeywords` | Search comments by keywords |\n\n### TikTok (coming soon)\n\n---\n\n## Key Features\n\n- **Natural language queries** ‚Äî describe what you want, Xpoz optimizes the search\n- **CSV exports** ‚Äî up to 500K rows in a single download via async operations\n- **Server-side pagination** ‚Äî handle large result sets efficiently\n- **Field selection** ‚Äî request only the fields you need\n- **Cache control** ‚Äî `forceLatest: true` bypasses cache for real-time data\n- **Async operations** ‚Äî long-running exports with status polling\n- **OAuth 2.1** ‚Äî secure Google OAuth authentication, no API keys to manage\n\n## Use Cases\n\n- **Lead generation** ‚Äî find people discussing problems your product solves\n- **Influencer discovery** ‚Äî identify creators by what they post, not just follower count\n- **Brand monitoring** ‚Äî track mentions, sentiment, and competitor activity\n- **Market research** ‚Äî analyze trends and conversations at scale\n- **Expert finding** ‚Äî discover domain authorities across platforms\n- **Security intelligence** ‚Äî monitor vulnerability discussions before CVEs publish\n\n## Pricing\n\n- **Free tier** available\n- Plans from **$20/mo** ‚Äî includes all platforms and tools\n- See [xpoz.ai/pricing](https://www.xpoz.ai/pricing) for details\n\n## Links\n\n- üåê [xpoz.ai](https://xpoz.ai)\n- üì¶ [MCP Registry](https://registry.modelcontextprotocol.io/?q=xpoz)\n- üõ†Ô∏è [ClawHub Skills](https://clawhub.ai/u/atyachin)\n- üìñ [Documentation](https://help.xpoz.ai)\n\n## License\n\nThis repository contains configuration and documentation only. The Xpoz MCP server is a hosted service ‚Äî see [Terms of Service](https://www.xpoz.ai/terms).\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search Twitter, Instagram, Reddit, and TikTok posts using natural language queries",
        "Find users by name, bio, keywords, or username across multiple social platforms",
        "Retrieve detailed user profiles and metrics from Twitter, Instagram, and Reddit",
        "Fetch posts, comments, retweets, quotes, and interactions by keywords, authors, or IDs",
        "Export large datasets up to 500,000 rows as CSV files via asynchronous operations",
        "Poll and cancel long-running export operations",
        "Handle server-side pagination and field selection for efficient data retrieval",
        "Authenticate securely using Google OAuth 2.1 without requiring API keys"
      ],
      "limitations": [
        "TikTok tools are not yet available (coming soon)",
        "Hosted service only; no local installation of the MCP server",
        "Requires Google OAuth authentication; no alternative auth methods mentioned"
      ],
      "requirements": [
        "Google OAuth 2.1 authentication for access",
        "Internet connection to access the remote MCP server at https://mcp.xpoz.ai/mcp",
        "Compatible MCP client supporting Streamable HTTP protocol",
        "Optional: ClawHub client for installation and OAuth authentication"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions across platforms, usage examples, authentication requirements, pricing info, and known limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Xpoz MCP Server\n\n**Search Twitter, Instagram, Reddit & TikTok from any AI agent.** 1.5B+ posts indexed. Natural language queries. CSV exports up to 500K rows. No API keys needed.\n\n[![MCP Registry](https://img.shields.io/badge/MCP_Registry-listed-blue)](https://registry.modelcontextprotocol.io/?q=xpoz)\n[![Website](https://img.shields.io/badge/xpoz.ai-visit-green)](https://xpoz.ai)\n\n> **Remote MCP server** ‚Äî no local installation required. Connect via Streamable HTTP and authenticate with Google OAuth.\n\n## Quick Start\n\n### Claude Desktop / Cursor / Windsurf\n\nAdd to your MCP config:\n\n```json\n{\n  \"mcpServers\": {\n    \"xpoz\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.xpoz.ai/mcp\"\n    }\n  }\n}\n```\n\n### OpenClaw\n\n```bash\nclawhub install xpoz-social-search\n```\n\nThen authenticate via the OAuth link when prompted.",
        "start_pos": 0,
        "end_pos": 834,
        "token_count_estimate": 208,
        "source_type": "readme",
        "agent_id": "290bbc28d0570444"
      },
      {
        "chunk_id": 1,
        "text": ")\n\n| Tool | Description |\n|------|-------------|\n| `searchInstagramUsers` | Find users by name or username |\n| `getInstagramUser` | Get profile details and metrics |\n| `getInstagramUsersByKeywords` | Discover users posting about topics |\n| `getInstagramUserConnections` | Get followers or following list |\n| `getInstagramPostsByKeywords` | Search posts by keywords or hashtags |\n| `getInstagramPostsByUser` | Get posts from a specific user |\n| `getInstagramPostsByIds` | Fetch specific posts by ID |\n| `getInstagramPostInteractingUsers` | Users who liked/commented on a post |\n| `getInstagramCommentsByPostId` | Get comments on a post |\n\n### Reddit (6 tools)\n\n| Tool | Description |\n|------|-------------|\n| `searchRedditUsers` | Find users by name |\n| `getRedditUser` | Get user profile and karma |\n| `getRedditUsersByKeywords` | Discover users active in topics |\n| `getRedditPostsByKeywords` | Search posts across subreddits |\n| `getRedditPostWithCommentsById` | Get a post with its comment tree |\n| `getRedditCommentsByKeywords` | Search comments by keywords |\n\n### TikTok (coming soon)\n\n---\n\n## Key Features\n\n- **Natural language queries** ‚Äî describe what you want, Xpoz optimizes the search\n- **CSV exports** ‚Äî up to 500K rows in a single download via async operations\n- **Server-side pagination** ‚Äî handle large result sets efficiently\n- **Field selection** ‚Äî request only the fields you need\n- **Cache control** ‚Äî `forceLatest: true` bypasses cache for real-time data\n- **Async operations** ‚Äî long-running exports with status polling\n- **OAuth 2.1** ‚Äî secure Google OAuth authentication, no API keys to manage\n\n## Use Cases\n\n- **Lead generation** ‚Äî find people discussing problems your product solves\n- **Influencer discovery** ‚Äî identify creators by what they post, not just follower count\n- **Brand monitoring** ‚Äî track mentions, sentiment, and competitor activity\n- **Market research** ‚Äî analyze trends and conversations at scale\n- **Expert finding** ‚Äî discover domain authorities across platforms\n- **Security intelligence** ‚Äî monitor vu",
        "start_pos": 1848,
        "end_pos": 3896,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "290bbc28d0570444"
      },
      {
        "chunk_id": 2,
        "text": ", and competitor activity\n- **Market research** ‚Äî analyze trends and conversations at scale\n- **Expert finding** ‚Äî discover domain authorities across platforms\n- **Security intelligence** ‚Äî monitor vulnerability discussions before CVEs publish\n\n## Pricing\n\n- **Free tier** available\n- Plans from **$20/mo** ‚Äî includes all platforms and tools\n- See [xpoz.ai/pricing](https://www.xpoz.ai/pricing) for details\n\n## Links\n\n- üåê [xpoz.ai](https://xpoz.ai)\n- üì¶ [MCP Registry](https://registry.modelcontextprotocol.io/?q=xpoz)\n- üõ†Ô∏è [ClawHub Skills](https://clawhub.ai/u/atyachin)\n- üìñ [Documentation](https://help.xpoz.ai)\n\n## License\n\nThis repository contains configuration and documentation only. The Xpoz MCP server is a hosted service ‚Äî see [Terms of Service](https://www.xpoz.ai/terms).",
        "start_pos": 3696,
        "end_pos": 4478,
        "token_count_estimate": 195,
        "source_type": "readme",
        "agent_id": "290bbc28d0570444"
      }
    ]
  },
  {
    "agent_id": "3d2debacf6381239",
    "name": "ai.zine/mcp",
    "source": "mcp",
    "source_url": "https://github.com/graphlit/graphlit-mcp-server",
    "description": "Your memory, everywhere AI goes. Build knowledge once, access it via MCP anywhere.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-10T18:02:28.779912Z",
    "indexed_at": "2026-02-18T04:09:54.795852",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "[![npm version](https://badge.fury.io/js/graphlit-mcp-server.svg)](https://badge.fury.io/js/graphlit-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@graphlit/graphlit-mcp-server)](https://smithery.ai/server/@graphlit/graphlit-mcp-server)\n\n# Model Context Protocol (MCP) Server for Graphlit Platform\n\n## Overview\n\nThe Model Context Protocol (MCP) Server enables integration between MCP clients and the Graphlit service. This document outlines the setup process and provides a basic example of using the client.\n\nIngest anything from Slack, Discord, websites, Google Drive, email, Jira, Linear or GitHub into a Graphlit project - and then search and retrieve relevant knowledge within an MCP client like Cursor, Windsurf, Goose or Cline.\n\nYour Graphlit project acts as a searchable, and RAG-ready knowledge base across all your developer and product management tools.\n\nDocuments (PDF, DOCX, PPTX, etc.) and HTML web pages will be extracted to Markdown upon ingestion. Audio and video files will be transcribed upon ingestion.\n\nWeb crawling and web search are built-in as MCP tools, with no need to integrate other tools like Firecrawl, Exa, etc. separately.\n\nYou can read more about the MCP Server use cases and features on our [blog](https://www.graphlit.com/blog/graphlit-mcp-server).\n\nWatch our latest [YouTube video](https://www.youtube.com/watch?v=Or-QqonvcAs&t=4s) on using the Graphlit MCP Server with the Goose MCP client.\n\nFor any questions on using the MCP Server, please join our [Discord](https://discord.gg/ygFmfjy3Qx) community and post on the #mcp channel.\n\n<a href=\"https://glama.ai/mcp/servers/fscrivteod\">\n  <img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/fscrivteod/badge\" alt=\"graphlit-mcp-server MCP server\" />\n</a>\n\n## Tools\n\n### Retrieval\n\n- Query Contents\n- Query Collections\n- Query Feeds\n- Query Conversations\n- Retrieve Relevant Sources\n- Retrieve Similar Images\n- Visually Describe Image\n\n### RAG\n\n- Prompt LLM Conversation\n\n### Extraction\n\n- Extract Structured JSON from Text\n\n### Publishing\n\n- Publish as Audio (ElevenLabs Audio)\n- Publish as Image (OpenAI Image Generation)\n\n### Ingestion\n\n- Files\n- Web Pages\n- Messages\n- Posts\n- Emails\n- Issues\n- Text\n- Memory (Short-Term)\n\n### Data Connectors\n\n- Microsoft Outlook email\n- Google Mail\n- Notion\n- Reddit\n- Linear\n- Jira\n- GitHub Issues\n- Google Drive\n- OneDrive\n- SharePoint\n- Dropbox\n- Box\n- GitHub\n- Slack\n- Microsoft Teams\n- Discord\n- Twitter/X\n- Podcasts (RSS)\n\n### Web\n\n- Web Crawling\n- Web Search (including Podcast Search)\n- Web Mapping\n- Screenshot Page\n\n### Notifications\n\n- Slack\n- Email\n- Webhook\n- Twitter/X\n\n### Operations\n\n- Configure Project\n- Create Collection\n- Add Contents to Collection\n- Remove Contents from Collection\n- Delete Collection(s)\n- Delete Feed(s)\n- Delete Content(s)\n- Delete Conversation(s)\n- Is Feed Done?\n- Is Content Done?\n\n### Enumerations\n\n- List Slack Channels\n- List Microsoft Teams Teams\n- List Microsoft Teams Channels\n- List SharePoint Libraries\n- List SharePoint Folders\n- List Linear Projects\n- List Notion Databases\n- List Notion Pages\n- List Dropbox Folders\n- List Box Folders\n- List Discord Guilds\n- List Discord Channels\n- List Google Calendars\n- List Microsoft Calendars\n\n## Resources\n\n- Project\n- Contents\n- Feeds\n- Collections (of Content)\n- Workflows\n- Conversations\n- Specifications\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n- Node.js installed on your system (recommended version 18.x or higher).\n- An active account on the [Graphlit Platform](https://portal.graphlit.dev) with access to the API settings dashboard.\n\n## Configuration\n\nThe Graphlit MCP Server supports environment variables to be set for authentication and configuration:\n\n- `GRAPHLIT_ENVIRONMENT_ID`: Your environment ID.\n- `GRAPHLIT_ORGANIZATION_ID`: Your organization ID.\n- `GRAPHLIT_JWT_SECRET`: Your JWT secret for signing the JWT token.\n\nYou can find these values in the API settings dashboard on the [Graphlit Platform](https://portal.graphlit.dev).\n\n## Installation\n\n### Installing via VS Code\n\nFor quick installation, use one of the one-click install buttons below:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=graphlit&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22organization_id%22%2C%22description%22%3A%22Graphlit%20Organization%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22environment_id%22%2C%22description%22%3A%22Graphlit%20Environment%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22jwt_secret%22%2C%22description%22%3A%22Graphlit%20JWT%20Secret%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22graphlit-mcp-server%22%5D%2C%22env%22%3A%7B%22GRAPHLIT_ORGANIZATION_ID%22%3A%22%24%7Binput%3Aorganization_id%7D%22%2C%22GRAPHLIT_ENVIRONMENT_ID%22%3A%22%24%7Binput%3Aenvironment_id%7D%22%2C%22GRAPHLIT_JWT_SECRET%22%3A%22%24%7Binput%3Ajwt_secret%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=graphlit&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22organization_id%22%2C%22description%22%3A%22Graphlit%20Organization%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22environment_id%22%2C%22description%22%3A%22Graphlit%20Environment%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22jwt_secret%22%2C%22description%22%3A%22Graphlit%20JWT%20Secret%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22graphlit-mcp-server%22%5D%2C%22env%22%3A%7B%22GRAPHLIT_ORGANIZATION_ID%22%3A%22%24%7Binput%3Aorganization_id%7D%22%2C%22GRAPHLIT_ENVIRONMENT_ID%22%3A%22%24%7Binput%3Aenvironment_id%7D%22%2C%22GRAPHLIT_JWT_SECRET%22%3A%22%24%7Binput%3Ajwt_secret%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"organization_id\",\n        \"description\": \"Graphlit Organization ID\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"environment_id\",\n        \"description\": \"Graphlit Environment ID\",\n        \"password\": true\n      },\n      {\n        \"type\": \"promptString\",\n        \"id\": \"jwt_secret\",\n        \"description\": \"Graphlit JWT Secret\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"graphlit\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"graphlit-mcp-server\"],\n        \"env\": {\n          \"GRAPHLIT_ORGANIZATION_ID\": \"${input:organization_id}\",\n          \"GRAPHLIT_ENVIRONMENT_ID\": \"${input:environment_id}\",\n          \"GRAPHLIT_JWT_SECRET\": \"${input:jwt_secret}\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Installing via Windsurf\n\nTo install graphlit-mcp-server in Windsurf IDE application, Cline should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour mcp_config.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Cline\n\nTo install graphlit-mcp-server in Cline IDE application, Cline should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour cline_mcp_settings.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Cursor\n\nTo install graphlit-mcp-server in Cursor IDE application, Cursor should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour mcp.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Smithery\n\nTo install graphlit-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@graphlit/graphlit-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @graphlit/graphlit-mcp-server --client claude\n```\n\n### Installing manually\n\nTo use the Graphlit MCP Server in any MCP client application, use:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\nOptionally, you can configure the credentials for data connectors, such as Slack, Google Email and Notion.\nOnly GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET are required.\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n                \"SLACK_BOT_TOKEN\": \"your-slack-bot-token\",\n                \"DISCORD_BOT_TOKEN\": \"your-discord-bot-token\",\n                \"TWITTER_TOKEN\": \"your-twitter-token\",\n                \"GOOGLE_EMAIL_REFRESH_TOKEN\": \"your-google-refresh-token\",\n                \"GOOGLE_EMAIL_CLIENT_ID\": \"your-google-client-id\",\n                \"GOOGLE_EMAIL_CLIENT_SECRET\": \"your-google-client-secret\",\n                \"LINEAR_API_KEY\": \"your-linear-api-key\",\n                \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"your-github-pat\",\n                \"JIRA_EMAIL\": \"your-jira-email\",\n                \"JIRA_TOKEN\": \"your-jira-token\",\n                \"NOTION_API_KEY\": \"your-notion-api-key\"\n            }\n        }\n    }\n}\n```\n\nNOTE: when running 'npx' on Windows, you may need to explicitly call npx via the command prompt.\n\n```\n\"command\": \"C:\\\\Windows\\\\System32\\\\cmd.exe /c npx\"\n```\n\n## Support\n\nPlease refer to the [Graphlit API Documentation](https://docs.graphlit.dev/).\n\nFor support with the Graphlit MCP Server, please submit a [GitHub Issue](https://github.com/graphlit/graphlit-mcp-server/issues).\n\nFor further support with the Graphlit Platform, please join our [Discord](https://discord.gg/ygFmfjy3Qx) community.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Ingest data from Slack, Discord, websites, Google Drive, email, Jira, Linear, and GitHub into a Graphlit project",
        "Extract text from documents (PDF, DOCX, PPTX) and HTML web pages converting them to Markdown",
        "Transcribe audio and video files upon ingestion",
        "Perform web crawling and web search including podcast search",
        "Query contents, collections, feeds, conversations, and retrieve relevant sources",
        "Retrieve similar images and visually describe images",
        "Publish content as audio using ElevenLabs Audio and as images using OpenAI Image Generation",
        "Configure projects, create and manage collections, feeds, contents, and conversations",
        "Integrate with multiple data connectors including Microsoft Outlook, Google Mail, Notion, Reddit, Linear, Jira, GitHub, Slack, Microsoft Teams, Discord, Twitter/X, and Podcasts via RSS",
        "Send notifications via Slack, Email, Webhook, and Twitter/X"
      ],
      "limitations": [],
      "requirements": [
        "Node.js installed (recommended version 18.x or higher)",
        "Active account on the Graphlit Platform with access to the API settings dashboard",
        "Environment variables for authentication: GRAPHLIT_ENVIRONMENT_ID, GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_JWT_SECRET",
        "Optional credentials for data connectors such as Slack Bot Token, Discord Bot Token, Twitter Token, Google Email OAuth tokens, Linear API Key, GitHub Personal Access Token, Jira credentials, and Notion API Key"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed capability listings, configuration requirements, usage examples for multiple MCP clients, and support resources, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "[![npm version](https://badge.fury.io/js/graphlit-mcp-server.svg)](https://badge.fury.io/js/graphlit-mcp-server)\n[![smithery badge](https://smithery.ai/badge/@graphlit/graphlit-mcp-server)](https://smithery.ai/server/@graphlit/graphlit-mcp-server)\n\n# Model Context Protocol (MCP) Server for Graphlit Platform\n\n## Overview\n\nThe Model Context Protocol (MCP) Server enables integration between MCP clients and the Graphlit service. This document outlines the setup process and provides a basic example of using the client.\n\nIngest anything from Slack, Discord, websites, Google Drive, email, Jira, Linear or GitHub into a Graphlit project - and then search and retrieve relevant knowledge within an MCP client like Cursor, Windsurf, Goose or Cline.\n\nYour Graphlit project acts as a searchable, and RAG-ready knowledge base across all your developer and product management tools.\n\nDocuments (PDF, DOCX, PPTX, etc.) and HTML web pages will be extracted to Markdown upon ingestion. Audio and video files will be transcribed upon ingestion.\n\nWeb crawling and web search are built-in as MCP tools, with no need to integrate other tools like Firecrawl, Exa, etc. separately.\n\nYou can read more about the MCP Server use cases and features on our [blog](https://www.graphlit.com/blog/graphlit-mcp-server).\n\nWatch our latest [YouTube video](https://www.youtube.com/watch?v=Or-QqonvcAs&t=4s) on using the Graphlit MCP Server with the Goose MCP client.\n\nFor any questions on using the MCP Server, please join our [Discord](https://discord.gg/ygFmfjy3Qx) community and post on the #mcp channel.",
        "start_pos": 0,
        "end_pos": 1579,
        "token_count_estimate": 394,
        "source_type": "readme",
        "agent_id": "3d2debacf6381239"
      },
      {
        "chunk_id": 1,
        "text": "nversations\n- Retrieve Relevant Sources\n- Retrieve Similar Images\n- Visually Describe Image\n\n### RAG\n\n- Prompt LLM Conversation\n\n### Extraction\n\n- Extract Structured JSON from Text\n\n### Publishing\n\n- Publish as Audio (ElevenLabs Audio)\n- Publish as Image (OpenAI Image Generation)\n\n### Ingestion\n\n- Files\n- Web Pages\n- Messages\n- Posts\n- Emails\n- Issues\n- Text\n- Memory (Short-Term)\n\n### Data Connectors\n\n- Microsoft Outlook email\n- Google Mail\n- Notion\n- Reddit\n- Linear\n- Jira\n- GitHub Issues\n- Google Drive\n- OneDrive\n- SharePoint\n- Dropbox\n- Box\n- GitHub\n- Slack\n- Microsoft Teams\n- Discord\n- Twitter/X\n- Podcasts (RSS)\n\n### Web\n\n- Web Crawling\n- Web Search (including Podcast Search)\n- Web Mapping\n- Screenshot Page\n\n### Notifications\n\n- Slack\n- Email\n- Webhook\n- Twitter/X\n\n### Operations\n\n- Configure Project\n- Create Collection\n- Add Contents to Collection\n- Remove Contents from Collection\n- Delete Collection(s)\n- Delete Feed(s)\n- Delete Content(s)\n- Delete Conversation(s)\n- Is Feed Done?\n- Is Content Done?\n\n### Enumerations\n\n- List Slack Channels\n- List Microsoft Teams Teams\n- List Microsoft Teams Channels\n- List SharePoint Libraries\n- List SharePoint Folders\n- List Linear Projects\n- List Notion Databases\n- List Notion Pages\n- List Dropbox Folders\n- List Box Folders\n- List Discord Guilds\n- List Discord Channels\n- List Google Calendars\n- List Microsoft Calendars\n\n## Resources\n\n- Project\n- Contents\n- Feeds\n- Collections (of Content)\n- Workflows\n- Conversations\n- Specifications\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n- Node.js installed on your system (recommended version 18.x or higher).\n- An active account on the [Graphlit Platform](https://portal.graphlit.dev) with access to the API settings dashboard.\n\n## Configuration\n\nThe Graphlit MCP Server supports environment variables to be set for authentication and configuration:\n\n- `GRAPHLIT_ENVIRONMENT_ID`: Your environment ID.\n- `GRAPHLIT_ORGANIZATION_ID`: Your organization ID.",
        "start_pos": 1848,
        "end_pos": 3830,
        "token_count_estimate": 495,
        "source_type": "readme",
        "agent_id": "3d2debacf6381239"
      },
      {
        "chunk_id": 2,
        "text": "raphlit MCP Server supports environment variables to be set for authentication and configuration:\n\n- `GRAPHLIT_ENVIRONMENT_ID`: Your environment ID.\n- `GRAPHLIT_ORGANIZATION_ID`: Your organization ID.\n- `GRAPHLIT_JWT_SECRET`: Your JWT secret for signing the JWT token.\n\nYou can find these values in the API settings dashboard on the [Graphlit Platform](https://portal.graphlit.dev).",
        "start_pos": 3630,
        "end_pos": 4012,
        "token_count_estimate": 95,
        "source_type": "readme",
        "agent_id": "3d2debacf6381239"
      },
      {
        "chunk_id": 3,
        "text": "%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22environment_id%22%2C%22description%22%3A%22Graphlit%20Environment%20ID%22%2C%22password%22%3Atrue%7D%2C%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22jwt_secret%22%2C%22description%22%3A%22Graphlit%20JWT%20Secret%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22graphlit-mcp-server%22%5D%2C%22env%22%3A%7B%22GRAPHLIT_ORGANIZATION_ID%22%3A%22%24%7Binput%3Aorganization_id%7D%22%2C%22GRAPHLIT_ENVIRONMENT_ID%22%3A%22%24%7Binput%3Aenvironment_id%7D%22%2C%22GRAPHLIT_JWT_SECRET%22%3A%22%24%7Binput%3Ajwt_secret%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n> Note that the `mcp` key is not needed in the `.vscode/mcp.json` file.",
        "start_pos": 5478,
        "end_pos": 6571,
        "token_count_estimate": 273,
        "source_type": "readme",
        "agent_id": "3d2debacf6381239"
      },
      {
        "chunk_id": 4,
        "text": "nt_id}\",\n          \"GRAPHLIT_JWT_SECRET\": \"${input:jwt_secret}\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Installing via Windsurf\n\nTo install graphlit-mcp-server in Windsurf IDE application, Cline should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour mcp_config.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Cline\n\nTo install graphlit-mcp-server in Cline IDE application, Cline should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour cline_mcp_settings.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Cursor\n\nTo install graphlit-mcp-server in Cursor IDE application, Cursor should use NPX:\n\n```bash\nnpx -y graphlit-mcp-server\n```\n\nYour mcp.json file should be configured similar to:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installin",
        "start_pos": 7326,
        "end_pos": 9374,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "3d2debacf6381239"
      },
      {
        "chunk_id": 5,
        "text": ": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\n### Installing via Smithery\n\nTo install graphlit-mcp-server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@graphlit/graphlit-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @graphlit/graphlit-mcp-server --client claude\n```\n\n### Installing manually\n\nTo use the Graphlit MCP Server in any MCP client application, use:\n\n```\n{\n    \"mcpServers\": {\n        \"graphlit-mcp-server\": {\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"graphlit-mcp-server\"\n            ],\n            \"env\": {\n                \"GRAPHLIT_ORGANIZATION_ID\": \"your-organization-id\",\n                \"GRAPHLIT_ENVIRONMENT_ID\": \"your-environment-id\",\n                \"GRAPHLIT_JWT_SECRET\": \"your-jwt-secret\",\n            }\n        }\n    }\n}\n```\n\nOptionally, you can configure the credentials for data connectors, such as Slack, Google Email and Notion.\nOnly GRAPHLIT_ORGANIZATION_ID, GRAPHLIT_ENVIRONMENT_ID and GRAPHLIT_JWT_SECRET are required.",
        "start_pos": 9174,
        "end_pos": 10345,
        "token_count_estimate": 292,
        "source_type": "readme",
        "agent_id": "3d2debacf6381239"
      },
      {
        "chunk_id": 6,
        "text": "EMAIL_CLIENT_ID\": \"your-google-client-id\",\n                \"GOOGLE_EMAIL_CLIENT_SECRET\": \"your-google-client-secret\",\n                \"LINEAR_API_KEY\": \"your-linear-api-key\",\n                \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"your-github-pat\",\n                \"JIRA_EMAIL\": \"your-jira-email\",\n                \"JIRA_TOKEN\": \"your-jira-token\",\n                \"NOTION_API_KEY\": \"your-notion-api-key\"\n            }\n        }\n    }\n}\n```\n\nNOTE: when running 'npx' on Windows, you may need to explicitly call npx via the command prompt.\n\n```\n\"command\": \"C:\\\\Windows\\\\System32\\\\cmd.exe /c npx\"\n```\n\n## Support\n\nPlease refer to the [Graphlit API Documentation](https://docs.graphlit.dev/).\n\nFor support with the Graphlit MCP Server, please submit a [GitHub Issue](https://github.com/graphlit/graphlit-mcp-server/issues).\n\nFor further support with the Graphlit Platform, please join our [Discord](https://discord.gg/ygFmfjy3Qx) community.",
        "start_pos": 11022,
        "end_pos": 11951,
        "token_count_estimate": 232,
        "source_type": "readme",
        "agent_id": "3d2debacf6381239"
      }
    ]
  },
  {
    "agent_id": "3d2debacf6381239",
    "name": "ai.zine/mcp",
    "source": "mcp",
    "source_url": "https://www.zine.ai/mcp",
    "description": "Your memory, everywhere AI goes. Build knowledge once, access it via MCP anywhere.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-10T18:02:28.773522Z",
    "indexed_at": "2026-02-18T04:09:56.212951",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Store knowledge persistently",
        "Access stored knowledge via MCP protocol"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with a very high-level overview and no detailed information.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "5ccac4e282553fdf",
    "name": "app.certman/server",
    "source": "mcp",
    "source_url": "https://mcp.certman.app/mcp",
    "description": "Create and manage your own Certificate Authority for internal HTTPS.",
    "tools": [],
    "detected_capabilities": [
      "manage",
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-04T01:04:18.39635Z",
    "indexed_at": "2026-02-18T04:09:58.017832",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create a Certificate Authority for internal HTTPS",
        "Manage a Certificate Authority for internal HTTPS"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's purpose but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "5ccac4e282553fdf",
    "name": "app.certman/server",
    "source": "mcp",
    "source_url": "https://mcp.certman.app",
    "description": "Create and manage your own Certificate Authority for internal HTTPS.",
    "tools": [],
    "detected_capabilities": [
      "manage",
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-08T00:10:32.652705Z",
    "indexed_at": "2026-02-18T04:09:58.837579",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create a Certificate Authority for internal HTTPS",
        "Manage a Certificate Authority for internal HTTPS"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's purpose but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "970eceb162401324",
    "name": "app.flaim/mcp",
    "source": "mcp",
    "source_url": "https://github.com/jdguggs10/flaim",
    "description": "Connect ESPN & Yahoo fantasy leagues to Claude, ChatGPT, and Gemini via MCP",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-10T15:36:05.763512Z",
    "indexed_at": "2026-02-18T04:10:00.049688",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Flaim - Fantasy League AI Connector\n\nDoc routing: see `docs/INDEX.md`.\n\nFlaim connects your ESPN fantasy leagues to AI assistants like Claude, ChatGPT, and Gemini CLI. It's an MCP (Model Context Protocol) service that gives AI tools access to your live fantasy data.\n\n## How It Works\n\n1. **Create a Clerk account & sign in** ‚Äî This is where your ESPN credentials and league info are stored\n2. **Sync ESPN credentials** ‚Äî Install the [Chrome extension](https://chromewebstore.google.com/detail/flaim-espn-fantasy-connec/mbnokejgglkfgkeeenolgdpcnfakpbkn) to sync automatically, or enter them manually\n3. **Leagues auto-discovered** ‚Äî Extension finds all your leagues + past seasons and saves them\n4. **Pick a default** ‚Äî Select which league to use by default in AI conversations\n5. **Connect your AI** ‚Äî Add Flaim as a custom MCP connector in Claude, ChatGPT, or Gemini CLI using the MCP URL\n6. **Use MCP tools** ‚Äî Ask about your roster, matchups, standings, etc. directly in your AI\n\nBring your own LLM subscription. Flaim provides the data bridge.\n\n## Automation vs Manual (Quick Clarification)\n\n- **Extension (automatic)**: Auto-pulls ESPN s2/swid and saves to supabase. Runs only when the user clicks **Sync / Re-sync**. It discovers leagues + past seasons and can set a default.\n- **Site (manual)**: `/leagues` is independent. Users can add leagues by ID and manually trigger season discovery.\n\n## What Flaim Is\n\nFlaim is an **authentication and data service** for fantasy sports AI integrations:\n\n- **MCP Server**: Exposes fantasy league data to Claude and ChatGPT via the Model Context Protocol\n- **OAuth Provider**: Handles secure authentication between AI clients and your ESPN data\n- **Credential Manager**: Securely stores and manages ESPN session cookies\n\nFlaim is **not** a chatbot or AI product itself ‚Äî it's the bridge that lets you use your preferred AI tool with your fantasy data.\n\n## Features\n\n- **Chrome Extension (v1.5.1)**: Auto-capture ESPN credentials without manual cookie extraction\n- **Auto-Discovery (v1.2.1+)**: Fan API-based discovery of leagues + past seasons, with granular status messaging and default selection\n- **Claude + ChatGPT + Gemini CLI**: Direct access via MCP protocol (OAuth 2.1)\n- **Live ESPN Data**: espn-client worker with real-time stats\n- **Multi-League + Multi-Season Support**: Store multiple seasons per league and discover past seasons\n\n## About\n\nFlaim is a solo indie project ‚Äî built with care, maintained for the long term. The focus is on reliability, security, and doing one thing well. No VC funding, no growth pressure, just a useful tool for fantasy sports fans who use AI.\n\n## Season Years\n\nSeason year defaults are deterministic and use America/New_York time:\n\n- **Baseball (flb)**: Defaults to the previous year until Feb 1, then switches to the current year\n- **Football (ffl)**: Defaults to the previous year until Jul 1, then switches to the current year\n\n## MCP Tools\n\nThe unified gateway (`https://api.flaim.app/mcp`) exposes these tools:\n\n| Tool | Description |\n|------|-------------|\n| `get_user_session` | User's leagues across all platforms with IDs |\n| `get_ancient_history` | Historical leagues and seasons (2+ years old) |\n| `get_league_info` | League settings and members |\n| `get_roster` | Team roster with player stats |\n| `get_matchups` | Current/upcoming matchups |\n| `get_standings` | League standings |\n| `get_free_agents` | Available free agents |\n\nAll tools take explicit parameters: `platform`, `sport`, `league_id`, `season_year`.\n\n## Architecture\n\n```\nChrome Extension ‚Üí flaim.app ‚Üí Auth Worker ‚Üí Supabase\n                      ‚Üì\nClaude/ChatGPT/Gemini CLI ‚Üí Fantasy MCP Gateway ‚Üí ESPN Client ‚Üí ESPN API\n```\n\n- **Chrome Extension**: Captures ESPN cookies, syncs to Flaim\n- **Web App (Next.js)**: User dashboard, OAuth endpoints, league management\n- **Auth Worker (Cloudflare)**: Token validation, rate limiting, credential storage\n- **Fantasy MCP Gateway (Cloudflare)**: Unified MCP endpoint for all sports\n- **ESPN Client (Cloudflare)**: ESPN API calls (internal, called by gateway)\n- **Supabase**: User data, OAuth tokens, ESPN credentials\n\n---\n\n## For Contributors\n\nSolo developer, hobby project. Keep it simple and stable.\n\n- **Small changes** ‚Äî 1-2 hour tasks, one new concept at a time\n- **Boring tech** ‚Äî Stick to the stack (Next.js, Vercel, Clerk, Cloudflare, Supabase)\n- **Official docs first** ‚Äî Copy from examples before inventing patterns\n\n### Documentation\n\n| Document | Description |\n|----------|-------------|\n| [Architecture](docs/ARCHITECTURE.md) | System design, deployment, troubleshooting |\n| [Current Execution State](docs/dev/CURRENT-EXECUTION-STATE.md) | What is done, in progress, and next |\n| [Changelog](docs/CHANGELOG.md) | Release history |\n| [Web App](web/README.md) | Next.js routes, components, environment |\n| [Workers](workers/README.md) | Cloudflare Workers, MCP tools, ESPN API |\n| [Manual OAuth Runbooks](docs/MANUAL-OAUTH-RUNBOOKS.md) | Claude, ChatGPT, and Gemini verification checklists |\n| [Extension](extension/README.md) | Chrome extension build, Sync Host, CWS |\n\n### Quick Start (Development)\n\n```bash\ngit clone https://github.com/jdguggs10/flaim.git\ncd flaim && npm install\ncp web/.env.example web/.env.local  # add keys\nnpm run dev\n```\n\n---\n\n## Getting Help\n\nThis is a solo indie project with best-effort support. I'll do my best to respond, but it may take time.\n\n- Issues: [GitHub Issues](https://github.com/jdguggs10/flaim/issues)\n- Discussions: [GitHub Discussions](https://github.com/jdguggs10/flaim/discussions)\n\n## License\n\nMIT License - see [LICENSE](LICENSE).\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Authenticate users via OAuth 2.1 for secure ESPN data access",
        "Expose fantasy league data to AI assistants using the Model Context Protocol",
        "Automatically discover and sync ESPN fantasy leagues and past seasons",
        "Provide real-time ESPN fantasy sports data including rosters, matchups, standings, and free agents",
        "Support multiple leagues and multiple seasons per league",
        "Integrate with AI tools such as Claude, ChatGPT, and Gemini CLI via MCP",
        "Manage and securely store ESPN session cookies and credentials",
        "Allow manual addition and management of leagues through a web interface",
        "Offer a Chrome extension to auto-capture ESPN credentials without manual cookie extraction"
      ],
      "limitations": [
        "Does not function as a chatbot or AI product itself",
        "Requires user to bring their own LLM subscription for AI interactions",
        "Chrome extension sync runs only when user manually clicks Sync or Re-sync",
        "Solo indie project with best-effort support and potential response delays",
        "Season year defaults are fixed and based on America/New_York timezone rules"
      ],
      "requirements": [
        "Clerk account for storing ESPN credentials and league information",
        "Chrome extension installation for automatic ESPN credential syncing (optional, manual entry supported)",
        "User ESPN credentials (s2 and swid cookies) for authentication",
        "AI client supporting MCP protocol and OAuth 2.1 (e.g., Claude, ChatGPT, Gemini CLI)",
        "Environment setup for development includes Node.js, Next.js, and environment variables for keys"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, clear descriptions of tools and architecture, explicit limitations, and environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Flaim - Fantasy League AI Connector\n\nDoc routing: see `docs/INDEX.md`.\n\nFlaim connects your ESPN fantasy leagues to AI assistants like Claude, ChatGPT, and Gemini CLI. It's an MCP (Model Context Protocol) service that gives AI tools access to your live fantasy data.\n\n## How It Works\n\n1. **Create a Clerk account & sign in** ‚Äî This is where your ESPN credentials and league info are stored\n2. **Sync ESPN credentials** ‚Äî Install the [Chrome extension](https://chromewebstore.google.com/detail/flaim-espn-fantasy-connec/mbnokejgglkfgkeeenolgdpcnfakpbkn) to sync automatically, or enter them manually\n3. **Leagues auto-discovered** ‚Äî Extension finds all your leagues + past seasons and saves them\n4. **Pick a default** ‚Äî Select which league to use by default in AI conversations\n5. **Connect your AI** ‚Äî Add Flaim as a custom MCP connector in Claude, ChatGPT, or Gemini CLI using the MCP URL\n6. **Use MCP tools** ‚Äî Ask about your roster, matchups, standings, etc. directly in your AI\n\nBring your own LLM subscription. Flaim provides the data bridge.\n\n## Automation vs Manual (Quick Clarification)\n\n- **Extension (automatic)**: Auto-pulls ESPN s2/swid and saves to supabase. Runs only when the user clicks **Sync / Re-sync**. It discovers leagues + past seasons and can set a default.\n- **Site (manual)**: `/leagues` is independent. Users can add leagues by ID and manually trigger season discovery.\n\n## What Flaim Is\n\nFlaim is an **authentication and data service** for fantasy sports AI integrations:\n\n- **MCP Server**: Exposes fantasy league data to Claude and ChatGPT via the Model Context Protocol\n- **OAuth Provider**: Handles secure authentication between AI clients and your ESPN data\n- **Credential Manager**: Securely stores and manages ESPN session cookies\n\nFlaim is **not** a chatbot or AI product itself ‚Äî it's the bridge that lets you use your preferred AI tool with your fantasy data.",
        "start_pos": 0,
        "end_pos": 1898,
        "token_count_estimate": 474,
        "source_type": "readme",
        "agent_id": "970eceb162401324"
      },
      {
        "chunk_id": 1,
        "text": "edential Manager**: Securely stores and manages ESPN session cookies\n\nFlaim is **not** a chatbot or AI product itself ‚Äî it's the bridge that lets you use your preferred AI tool with your fantasy data.\n\n## Features\n\n- **Chrome Extension (v1.5.1)**: Auto-capture ESPN credentials without manual cookie extraction\n- **Auto-Discovery (v1.2.1+)**: Fan API-based discovery of leagues + past seasons, with granular status messaging and default selection\n- **Claude + ChatGPT + Gemini CLI**: Direct access via MCP protocol (OAuth 2.1)\n- **Live ESPN Data**: espn-client worker with real-time stats\n- **Multi-League + Multi-Season Support**: Store multiple seasons per league and discover past seasons\n\n## About\n\nFlaim is a solo indie project ‚Äî built with care, maintained for the long term. The focus is on reliability, security, and doing one thing well. No VC funding, no growth pressure, just a useful tool for fantasy sports fans who use AI.\n\n## Season Years\n\nSeason year defaults are deterministic and use America/New_York time:\n\n- **Baseball (flb)**: Defaults to the previous year until Feb 1, then switches to the current year\n- **Football (ffl)**: Defaults to the previous year until Jul 1, then switches to the current year\n\n## MCP Tools\n\nThe unified gateway (`https://api.flaim.app/mcp`) exposes these tools:\n\n| Tool | Description |\n|------|-------------|\n| `get_user_session` | User's leagues across all platforms with IDs |\n| `get_ancient_history` | Historical leagues and seasons (2+ years old) |\n| `get_league_info` | League settings and members |\n| `get_roster` | Team roster with player stats |\n| `get_matchups` | Current/upcoming matchups |\n| `get_standings` | League standings |\n| `get_free_agents` | Available free agents |\n\nAll tools take explicit parameters: `platform`, `sport`, `league_id`, `season_year`.",
        "start_pos": 1698,
        "end_pos": 3517,
        "token_count_estimate": 454,
        "source_type": "readme",
        "agent_id": "970eceb162401324"
      },
      {
        "chunk_id": 2,
        "text": "Extension ‚Üí flaim.app ‚Üí Auth Worker ‚Üí Supabase\n                      ‚Üì\nClaude/ChatGPT/Gemini CLI ‚Üí Fantasy MCP Gateway ‚Üí ESPN Client ‚Üí ESPN API\n```\n\n- **Chrome Extension**: Captures ESPN cookies, syncs to Flaim\n- **Web App (Next.js)**: User dashboard, OAuth endpoints, league management\n- **Auth Worker (Cloudflare)**: Token validation, rate limiting, credential storage\n- **Fantasy MCP Gateway (Cloudflare)**: Unified MCP endpoint for all sports\n- **ESPN Client (Cloudflare)**: ESPN API calls (internal, called by gateway)\n- **Supabase**: User data, OAuth tokens, ESPN credentials\n\n---\n\n## For Contributors\n\nSolo developer, hobby project. Keep it simple and stable.\n\n- **Small changes** ‚Äî 1-2 hour tasks, one new concept at a time\n- **Boring tech** ‚Äî Stick to the stack (Next.js, Vercel, Clerk, Cloudflare, Supabase)\n- **Official docs first** ‚Äî Copy from examples before inventing patterns\n\n### Documentation\n\n| Document | Description |\n|----------|-------------|\n| [Architecture](docs/ARCHITECTURE.md) | System design, deployment, troubleshooting |\n| [Current Execution State](docs/dev/CURRENT-EXECUTION-STATE.md) | What is done, in progress, and next |\n| [Changelog](docs/CHANGELOG.md) | Release history |\n| [Web App](web/README.md) | Next.js routes, components, environment |\n| [Workers](workers/README.md) | Cloudflare Workers, MCP tools, ESPN API |\n| [Manual OAuth Runbooks](docs/MANUAL-OAUTH-RUNBOOKS.md) | Claude, ChatGPT, and Gemini verification checklists |\n| [Extension](extension/README.md) | Chrome extension build, Sync Host, CWS |\n\n### Quick Start (Development)\n\n```bash\ngit clone https://github.com/jdguggs10/flaim.git\ncd flaim && npm install\ncp web/.env.example web/.env.local  # add keys\nnpm run dev\n```\n\n---\n\n## Getting Help\n\nThis is a solo indie project with best-effort support. I'll do my best to respond, but it may take time.",
        "start_pos": 3546,
        "end_pos": 5396,
        "token_count_estimate": 462,
        "source_type": "readme",
        "agent_id": "970eceb162401324"
      },
      {
        "chunk_id": 3,
        "text": "m install\ncp web/.env.example web/.env.local  # add keys\nnpm run dev\n```\n\n---\n\n## Getting Help\n\nThis is a solo indie project with best-effort support. I'll do my best to respond, but it may take time.\n\n- Issues: [GitHub Issues](https://github.com/jdguggs10/flaim/issues)\n- Discussions: [GitHub Discussions](https://github.com/jdguggs10/flaim/discussions)\n\n## License\n\nMIT License - see [LICENSE](LICENSE).",
        "start_pos": 5196,
        "end_pos": 5602,
        "token_count_estimate": 101,
        "source_type": "readme",
        "agent_id": "970eceb162401324"
      }
    ]
  },
  {
    "agent_id": "156a3b714c73358e",
    "name": "app.getdialer/dialer",
    "source": "mcp",
    "source_url": "https://getdialer.app/mcp",
    "description": "An MCP server that provides your you make outbound phone calls using your own phone number",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-09T00:16:49.16262Z",
    "indexed_at": "2026-02-18T04:10:01.560196",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Make outbound phone calls using your own phone number"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence providing a basic capability but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "3752dab227cec8fb",
    "name": "app.linear/linear",
    "source": "mcp",
    "source_url": "https://mcp.linear.app/sse",
    "description": "MCP server for Linear project management and issue tracking",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-18T15:51:15.598862Z",
    "indexed_at": "2026-02-18T04:10:02.269864",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage Linear project management tasks",
        "Track issues within Linear projects"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's purpose but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "1885a967a6d7e8a3",
    "name": "app.recordo.api/calories-club",
    "source": "mcp",
    "source_url": "https://github.com/AlohaLabs/recordo",
    "description": "AI-powered calorie tracking with photo recognition, barcode scanning, and voice logging",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-03T11:40:29.449394Z",
    "indexed_at": "2026-02-18T04:10:02.746863",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Track calories using AI-powered photo recognition",
        "Scan barcodes to log calorie information",
        "Log calories through voice input"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of features but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "1885a967a6d7e8a3",
    "name": "app.recordo.api/calories-club",
    "source": "mcp",
    "source_url": "https://recordo.app",
    "description": "AI-powered calorie tracking with photo recognition, barcode scanning, and voice logging",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-03T13:02:02.381026Z",
    "indexed_at": "2026-02-18T04:10:07.319992",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "Your AI Second Brain\nRecordo: The\nAI Second Brain\nfor ADHD\nStop losing your best ideas to distraction. Organize your chaos in seconds with\n                    an app that thinks like you."
    },
    "llm_extracted": {
      "capabilities": [
        "Organize ideas quickly",
        "Help manage ADHD-related distractions"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.15,
    "quality_rationale": "The documentation provides only a brief marketing description without details on features, usage, or requirements.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "Your AI Second Brain\nRecordo: The\nAI Second Brain\nfor ADHD\nStop losing your best ideas to distraction. Organize your chaos in seconds with\n                    an app that thinks like you.",
        "start_pos": 0,
        "end_pos": 187,
        "token_count_estimate": 46,
        "source_type": "detail_page",
        "agent_id": "1885a967a6d7e8a3"
      }
    ]
  },
  {
    "agent_id": "56e537012cf65a45",
    "name": "app.thoughtspot/mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/thoughtspot/mcp-server",
    "description": "MCP Server for ThoughtSpot - provides OAuth authentication and tools for querying data",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-17T20:14:22.510543Z",
    "indexed_at": "2026-02-18T04:10:08.694179",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/thoughtspot/visual-embed-sdk/main/static/doc-images/images/ThoughtSpot_appicon.png\" width=120 align=\"center\" alt=\"ThoughtSpot\" />\n</p>\n\n<br/>\n\n# ThoughtSpot MCP Server <br/> ![MCP Server](https://badge.mcpx.dev?type=server 'MCP Server') ![Static Badge](https://img.shields.io/badge/cloudflare%20worker-deployed-green?link=https%3A%2F%2Fdash.cloudflare.com%2F485d90aa3d1ea138ad7ede769fe2c35e%2Fworkers%2Fservices%2Fview%2Fthoughtspot-mcp-server%2Fproduction%2Fmetrics) ![GitHub branch check runs](https://img.shields.io/github/check-runs/thoughtspot/mcp-server/main) [![Coverage Status](https://coveralls.io/repos/github/thoughtspot/mcp-server/badge.svg?branch=main)](https://coveralls.io/github/thoughtspot/mcp-server?branch=main) <a href=\"https://developer.thoughtspot.com/join-discord\" target=\"_blank\"> <img alt=\"Discord: ThoughtSpot\" src=\"https://img.shields.io/discord/1143209406037758065?style=flat-square&label=Chat%20on%20Discord\" /> </a>\n\n\nThe ThoughtSpot MCP Server provides secure OAuth-based authentication and a set of tools for querying and retrieving relevant data from your ThoughtSpot instance. It's a remote server hosted on Cloudflare.\n\nIf you do not have a Thoughtspot account, create one for free [here](https://thoughtspot.com/trial).\n\nLearn more about [ThoughtSpot](https://thoughtspot.com).\n\nJoin our [Discord](https://developers.thoughtspot.com/join-discord) to get support.\n\n## Table of Contents\n\n- [Connect](#connect)\n- [Usage](#usage)\n- [Demo video](#demo)\n- [Usage in APIs](#usage-in-apis)\n  - [OpenAI / ChatGPT](#openai-responses-api)\n  - [Claude](#claude-mcp-connector)\n  - [Gemini](#gemini-api)\n- [Features](#features)\n  - [Supported transports](#supported-transports)\n- [Manual client registration](#manual-client-registration)\n  - [Associating with a ThoughtSpot instance](#associate-with-a-thoughtspot-instance)\n- [Self hosted](#self-hosted)\n- [Stdio support (fallback)](#stdio-support-fallback)\n  - [How to obtain a TS_AUTH_TOKEN](#how-to-obtain-a-ts_auth_token)\n- [Troubleshooting](#troubleshooting)\n- [Contributing](#contributing)\n  - [Local Development](#local-development)\n  - [Endpoints](#endpoints)\n\n\n## Connect\n\nIf using a client which supports remote MCPs natively (Claude.ai etc) then just enter:\n\nMCP Server URL: \n\n```\nhttps://agent.thoughtspot.app/mcp\n```\nPreferred Auth method: Oauth\n\n- For OpenAI ChatGPT Deep Research, add the URL as:\n```js\nhttps://agent.thoughtspot.app/openai/mcp\n```\n\nTo configure this MCP server in your MCP client (such as Claude Desktop, Windsurf, Cursor, etc.) which do not support remote MCPs, add the following configuration to your MCP client settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"ThoughtSpot\": {\n      \"command\": \"npx\",\n      \"args\": [\n         \"mcp-remote\",\n         \"https://agent.thoughtspot.app/mcp\"\n      ]\n    }\n  }\n}\n```\n\nSee the [Troubleshooting](#troubleshooting) section for any errors / more details.\n\n## Usage\n\n1. Once the connection is done, ThoughtSpot datasources would show under the resources section.\n2. Select a datasource (resource), to set the context of your query.\n3. Now you could ask analytical questions, which claude can decide to use the relevant ThoughtSpot tools for.\n\nSee the video below for a complete demo.\n\n## Demo\n\nHere is a demo video using Claude Desktop.\n\nhttps://github.com/user-attachments/assets/72a5383a-7b2a-4987-857a-b6218d7eea22\n\nWatch on [Loom](https://www.loom.com/share/433988d98a7b41fb8df2239da014169a?sid=ef2032a2-6e9b-4902-bef0-57df5623963e)\n\n## Usage in APIs\n\nThoughtSpot's remote MCP server can be used in LLM APIs which support calling MCP tools. \n\nHere are examples with the common LLM providers:\n\n### OpenAI Responses API\n\n```bash\ncurl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"gpt-4.1\",\n  \"tools\": [\n    {\n      \"type\": \"mcp\",\n      \"server_label\": \"thoughtspot\",\n      \"server_url\": \"https://agent.thoughtspot.app/bearer/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer $TS_AUTH_TOKEN\",\n        \"x-ts-host\": \"my-thoughtspot-instance.thoughtspot.cloud\"\n      }\n    }\n  ],\n  \"input\": \"How can I increase my sales ?\"\n}'\n```\n\nMore details on how can you use OpenAI API with MCP tool calling can be found [here](https://platform.openai.com/docs/guides/tools-remote-mcp).\n\n\n### Claude MCP Connector\n\n```bash\ncurl https://api.anthropic.com/v1/messages \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-API-Key: $ANTHROPIC_API_KEY\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"anthropic-beta: mcp-client-2025-04-04\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-20250514\",\n    \"max_tokens\": 1000,\n    \"messages\": [{\n      \"role\": \"user\", \n      \"content\": \"How do I increase my sales ?\"\n    }],\n    \"mcp_servers\": [\n      {\n        \"type\": \"url\",\n        \"url\": \"https://agent.thoughtspot.app/bearer/mcp\",\n        \"name\": \"thoughtspot\",\n        \"authorization_token\": \"$TS_AUTH_TOKEN@my-thoughtspot-instance.thoughtspot.cloud\"\n      }\n    ]\n  }'\n```\n\nNote: In the `authorization_token` field we have suffixed the ThoughtSpot instance host as well with the `@` symbol to the `TS_AUTH_TOKEN`.\n\nMore details on Claude MCP connector [here](https://docs.anthropic.com/en/docs/agents-and-tools/mcp-connector).\n\n\n### Gemini API\n\nMCP tools can be used with the Gemini Python/Typescript SDK. Here is an example using typescript:\n\n```typescript\nimport { GoogleGenAI, FunctionCallingConfigMode , mcpToTool} from '@google/genai';\nimport { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\nimport { StreamableHTTPClientTransport } from \"@modelcontextprotocol/sdk/client/streamableHttp.js\";\n\n// Create server parameters for stdio connection\nconst serverParams = new StreamableHTTPClientTransport(new URL(\"https://agent.thoughtspot.app/bearer/mcp\"), {\n    requestInit: {\n        headers: {\n            \"Authorization\": \"Bearer $TS_AUTH_TOKEN\", // Read below how to get the $TS_AUTH_TOKEN\n            \"x-ts-host\": \"my-thoughtspot-instance.thoughtspot.cloud\"\n        },\n    }\n});\n\nconst client = new Client(\n  {\n    name: \"example-client\",\n    version: \"1.0.0\"\n  }\n);\n\n// Configure the client\nconst ai = new GoogleGenAI({});\n\n// Initialize the connection between client and server\nawait client.connect(serverParams);\n\n// Send request to the model with MCP tools\nconst response = await ai.models.generateContent({\n  model: \"gemini-2.5-flash\",\n  contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,\n  config: {\n    tools: [mcpToTool(client)],  // uses the session, will automatically call the tool\n    // Uncomment if you **don't** want the sdk to automatically call the tool\n    // automaticFunctionCalling: {\n    //   disable: true,\n    // },\n  },\n});\nconsole.log(response.text)\n\n// Close the connection\nawait client.close();\n```\n\nRead [this](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#mcp), for more details on Gemini API MCP tool calling.\n\nAn example using Google ADK + Python can be found [here](https://github.com/thoughtspot/developer-examples/tree/main/mcp/python-google-adk-trusted-auth).\n\n#### Gemini CLI extenstions\n\nThoughtSpot MCP server can also be installed as a Gemini CLI extension.\n\n```bash\ngemini extensions install https://github.com/thoughtspot/mcp-server\n```\n\nRead more about Gemini CLI [here](https://github.com/google-gemini/gemini-cli).\n\n\n### How to get TS_AUTH_TOKEN for APIs ?\n\nFor API usage, you would the token endpoints with a `secret_key` to generate the `API_TOKEN` for a specific user/role, more details [here](https://developers.thoughtspot.com/docs/api-authv2#trusted-auth-v2). \n\n\n## Features\n\n- **OAuth Authentication**: Access your data, as yourself.\n  - Dynamic Client Registration (DCR) support.\n  - Any MCP host is allowed. Let's make the world fact driven.\n- **Tools**:\n  - `ping`: Test connectivity and authentication.\n  - `getRelevantQuestions`: Get relevant data questions from ThoughtSpot analytics based on a user query.\n  - `getAnswer`: Get the answer to a specific question from ThoughtSpot analytics.\n  - `createLiveboard`: Create a liveboard from a list of answers.\n  - `getDataSourceSuggestions`: Get datasource suggestions for a given query.\n- **MCP Resources**:\n   - `datasources`: List of ThoughtSpot Data models the user has access to.\n\n### Supported transports\n\n- SSE [https://agent.thoughtspot.app/sse]()\n- Streamed HTTP [https://agent.thoughtspot.app/mcp]()\n\n\n## Manual client registration\n\nFor MCP hosts which do not(yet) support Dynamic client registration, or they require statically adding Oauth Client Id etc. Go to [this](https://agent.thoughtspot.app/clients) page, to register a new client and copy the details over. The most relevant values are `Oauth Client Id` and `Oauth Client Secret` which should be added when adding ThoughtSpot as an MCP connector in the MCP client (ChatGPT/Claude etc). The generated client details are only available when they are generated and are NOT available later for reference.\n\n### Associate with a ThoughtSpot instance\n\nManual client registration also allows to associate with a specific ThoughtSpot instance, so that your users do not have to enter the Thoughtspot instance URL when doing the authorization flow. While registering the Oauth client add `ThoughtSpot URL` to the appropriate value.\n\n## Self hosted\n\nUse the published docker image to deploy the MCP server in your own environment.\n\nSee [this](deploy/README.md) for details.\n\n## Stdio support (fallback)\n\nIf you are unable to use the remote MCP server due to connectivity restrictions on your Thoughtspot instance. You could use the `stdio` local transport using the `npm` package.\n\nHere is how to configure `stdio` with MCP Client:\n\n```json \n{\n  \"mcpServers\": {\n    \"ThoughtSpot\": {\n      \"command\": \"npx\",\n      \"args\": [\n         \"@thoughtspot/mcp-server\"\n      ],\n      \"env\": {\n         \"TS_INSTANCE\": \"<your Thoughtspot Instance URL>\",\n         \"TS_AUTH_TOKEN\": \"<ThoughtSpot Access Token>\"\n      }\n    }\n  }\n}\n```\n\n### How to obtain a `TS_AUTH_TOKEN` ?\n\n- Go to ThoughtSpot => _Develop_ => _Rest Playground v2.0_\n- _Authentication_ => _Get Full access token_\n- Scroll down and expand the \"body\"\n- Add your \"username\" and \"password\".\n- Put whatever \"validity_time\" you want the token to be.\n- Click on \"Try it out\" on the bottom right.\n- You should get a token in the response, thats the bearer token.\n\n#### Alternative way to get `TS_AUTH_TOKEN`\n- Login to the ThoughtSpot instance as you would normally.\n- Opem in a new tab this URL:\n  - https://your-ts-instance/api/rest/2.0/auth/session/token\n- You will see a JSON response, copy the \"token\" value (without the quotes).\n- This is the token you could use.\n\n### Troubleshooting\n\n> Oauth errors due to CORS/SAML.\n\nMake sure to add the following entries in your ThoughtSpot instance:\n\n*CORS*\n\n- Go to ThoughtSpot => _Develop_ => Security settings\n- Click \"Edit\"\n- Add \"agent.thoughtspot.app\" to the the \"CORS whitelisted domains\". \n\n*SAML* (need to be Admin)\n\n- Go to ThoughtSpot => _Develop_\n- Go to \"All Orgs\" Tab on the left panel if there is one.\n- Click \"Security settings\"\n- Click \"Edit\"\n- Add \"agent.thoughtspot.app\" to the the \"SAML redirect domains\". \n\n> MCP server install error due to node issues\n\n- Make sure node is installed on your machine.\n- Make sure the node version is >=18\n- Check the node version by using the command `node -v`\n\n> 500 error from MCP server\n\n- Make sure the ThoughtSpot cluster the MCP server is connected to is up and running.\n- If the error persists, please collect the logs that you get from the MCP client and the approximate time when the issue occurred.\n- Reach out on [Discord](https://developers.thoughtspot.com/join-discord) to get support.\n- Create a issue on this repository to get help.\n- Submit a [ThoughtSpot support case](https://community.thoughtspot.com/s/article/How-to-submit-a-ThoughtSpot-Support-Case) with all the artifacts.\n\n> Stale MCP auth\n\n- If for some reason the ThoughtSpot MCP server is failing authentication repeatedly, you can do `rm -rf ~/.mcp-auth`.\n- This will remove all stale authentication info, and restart the auth flow again.\n\n## Contributing\n\n### Local Development\n\n1. **Install dependencies**:\n   ```sh\n   npm install\n   ```\n2. **Set up environment variables**:\n   - Copy `.dev.vars` and fill in your ThoughtSpot instance URL and access token.\n3. **Start the development server**:\n   ```sh\n   npm run dev\n   ```\n\n### Endpoints\n\n- `/mcp`: MCP HTTP Streaming endpoint\n- `/sse`: Server-sent events for MCP\n- `/api`: MCP tools exposed as HTTP endpoints\n- `/authorize`, `/token`, `/register`: OAuth endpoints\n- `/bearer/mcp`, `/bearer/sse`: MCP endpoints as bearer auth instead of Oauth, mainly for use in APIs or in cases where Oauth is not working.\n\nMCP Server, ¬© ThoughtSpot, Inc. 2025\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide secure OAuth-based authentication for ThoughtSpot data access",
        "Query and retrieve relevant data from ThoughtSpot instances",
        "Support Dynamic Client Registration (DCR) for OAuth clients",
        "Offer tools such as ping, getRelevantQuestions, getAnswer, createLiveboard, and getDataSourceSuggestions",
        "List ThoughtSpot datasources accessible to the authenticated user",
        "Integrate with LLM APIs including OpenAI, Claude, and Gemini for MCP tool calling",
        "Support multiple transport protocols including SSE and streamed HTTP",
        "Allow manual client registration for OAuth clients and association with specific ThoughtSpot instances",
        "Enable self-hosted deployment via Docker image",
        "Provide stdio local transport fallback for environments with connectivity restrictions"
      ],
      "limitations": [
        "Requires ThoughtSpot instance to be up and running for MCP server connectivity",
        "OAuth errors may occur due to CORS or SAML misconfiguration on ThoughtSpot instance",
        "Manual OAuth client credentials are only available at generation time and cannot be retrieved later",
        "Node.js version must be >=18 for local MCP server installation",
        "Stale MCP authentication tokens may cause access issues"
      ],
      "requirements": [
        "ThoughtSpot account with appropriate permissions",
        "OAuth client credentials (Client ID and Secret) for authentication",
        "TS_AUTH_TOKEN bearer token for API access",
        "Node.js version 18 or higher for local development or self-hosted deployment",
        "Proper CORS and SAML redirect domain configuration on ThoughtSpot instance to allow MCP server access",
        "Access to ThoughtSpot token endpoints or Rest Playground to obtain TS_AUTH_TOKEN"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples across multiple LLM APIs, clear feature descriptions, troubleshooting guidance, and explicit requirements and limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/thoughtspot/visual-embed-sdk/main/static/doc-images/images/ThoughtSpot_appicon.png\" width=120 align=\"center\" alt=\"ThoughtSpot\" />\n</p>\n\n<br/>\n\n# ThoughtSpot MCP Server <br/> ![MCP Server](https://badge.mcpx.dev?type=server 'MCP Server') ![Static Badge](https://img.shields.io/badge/cloudflare%20worker-deployed-green?link=https%3A%2F%2Fdash.cloudflare.com%2F485d90aa3d1ea138ad7ede769fe2c35e%2Fworkers%2Fservices%2Fview%2Fthoughtspot-mcp-server%2Fproduction%2Fmetrics) ![GitHub branch check runs](https://img.shields.io/github/check-runs/thoughtspot/mcp-server/main) [![Coverage Status](https://coveralls.io/repos/github/thoughtspot/mcp-server/badge.svg?branch=main)](https://coveralls.io/github/thoughtspot/mcp-server?branch=main) <a href=\"https://developer.thoughtspot.com/join-discord\" target=\"_blank\"> <img alt=\"Discord: ThoughtSpot\" src=\"https://img.shields.io/discord/1143209406037758065?style=flat-square&label=Chat%20on%20Discord\" /> </a>\n\n\nThe ThoughtSpot MCP Server provides secure OAuth-based authentication and a set of tools for querying and retrieving relevant data from your ThoughtSpot instance. It's a remote server hosted on Cloudflare.\n\nIf you do not have a Thoughtspot account, create one for free [here](https://thoughtspot.com/trial).\n\nLearn more about [ThoughtSpot](https://thoughtspot.com).\n\nJoin our [Discord](https://developers.thoughtspot.com/join-discord) to get support.",
        "start_pos": 0,
        "end_pos": 1465,
        "token_count_estimate": 366,
        "source_type": "readme",
        "agent_id": "56e537012cf65a45"
      },
      {
        "chunk_id": 1,
        "text": "a ThoughtSpot instance](#associate-with-a-thoughtspot-instance)\n- [Self hosted](#self-hosted)\n- [Stdio support (fallback)](#stdio-support-fallback)\n  - [How to obtain a TS_AUTH_TOKEN](#how-to-obtain-a-ts_auth_token)\n- [Troubleshooting](#troubleshooting)\n- [Contributing](#contributing)\n  - [Local Development](#local-development)\n  - [Endpoints](#endpoints)\n\n\n## Connect\n\nIf using a client which supports remote MCPs natively (Claude.ai etc) then just enter:\n\nMCP Server URL: \n\n```\nhttps://agent.thoughtspot.app/mcp\n```\nPreferred Auth method: Oauth\n\n- For OpenAI ChatGPT Deep Research, add the URL as:\n```js\nhttps://agent.thoughtspot.app/openai/mcp\n```\n\nTo configure this MCP server in your MCP client (such as Claude Desktop, Windsurf, Cursor, etc.) which do not support remote MCPs, add the following configuration to your MCP client settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"ThoughtSpot\": {\n      \"command\": \"npx\",\n      \"args\": [\n         \"mcp-remote\",\n         \"https://agent.thoughtspot.app/mcp\"\n      ]\n    }\n  }\n}\n```\n\nSee the [Troubleshooting](#troubleshooting) section for any errors / more details.\n\n## Usage\n\n1. Once the connection is done, ThoughtSpot datasources would show under the resources section.\n2. Select a datasource (resource), to set the context of your query.\n3. Now you could ask analytical questions, which claude can decide to use the relevant ThoughtSpot tools for.\n\nSee the video below for a complete demo.\n\n## Demo\n\nHere is a demo video using Claude Desktop.\n\nhttps://github.com/user-attachments/assets/72a5383a-7b2a-4987-857a-b6218d7eea22\n\nWatch on [Loom](https://www.loom.com/share/433988d98a7b41fb8df2239da014169a?sid=ef2032a2-6e9b-4902-bef0-57df5623963e)\n\n## Usage in APIs\n\nThoughtSpot's remote MCP server can be used in LLM APIs which support calling MCP tools.",
        "start_pos": 1848,
        "end_pos": 3649,
        "token_count_estimate": 450,
        "source_type": "readme",
        "agent_id": "56e537012cf65a45"
      },
      {
        "chunk_id": 2,
        "text": "ers:\n\n### OpenAI Responses API\n\n```bash\ncurl https://api.openai.com/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -d '{\n  \"model\": \"gpt-4.1\",\n  \"tools\": [\n    {\n      \"type\": \"mcp\",\n      \"server_label\": \"thoughtspot\",\n      \"server_url\": \"https://agent.thoughtspot.app/bearer/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer $TS_AUTH_TOKEN\",\n        \"x-ts-host\": \"my-thoughtspot-instance.thoughtspot.cloud\"\n      }\n    }\n  ],\n  \"input\": \"How can I increase my sales ?\"\n}'\n```\n\nMore details on how can you use OpenAI API with MCP tool calling can be found [here](https://platform.openai.com/docs/guides/tools-remote-mcp).\n\n\n### Claude MCP Connector\n\n```bash\ncurl https://api.anthropic.com/v1/messages \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-API-Key: $ANTHROPIC_API_KEY\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -H \"anthropic-beta: mcp-client-2025-04-04\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-20250514\",\n    \"max_tokens\": 1000,\n    \"messages\": [{\n      \"role\": \"user\", \n      \"content\": \"How do I increase my sales ?\"\n    }],\n    \"mcp_servers\": [\n      {\n        \"type\": \"url\",\n        \"url\": \"https://agent.thoughtspot.app/bearer/mcp\",\n        \"name\": \"thoughtspot\",\n        \"authorization_token\": \"$TS_AUTH_TOKEN@my-thoughtspot-instance.thoughtspot.cloud\"\n      }\n    ]\n  }'\n```\n\nNote: In the `authorization_token` field we have suffixed the ThoughtSpot instance host as well with the `@` symbol to the `TS_AUTH_TOKEN`.\n\nMore details on Claude MCP connector [here](https://docs.anthropic.com/en/docs/agents-and-tools/mcp-connector).\n\n\n### Gemini API\n\nMCP tools can be used with the Gemini Python/Typescript SDK.",
        "start_pos": 3696,
        "end_pos": 5389,
        "token_count_estimate": 423,
        "source_type": "readme",
        "agent_id": "56e537012cf65a45"
      },
      {
        "chunk_id": 3,
        "text": "rom \"@modelcontextprotocol/sdk/client/index.js\";\nimport { StreamableHTTPClientTransport } from \"@modelcontextprotocol/sdk/client/streamableHttp.js\";\n\n// Create server parameters for stdio connection\nconst serverParams = new StreamableHTTPClientTransport(new URL(\"https://agent.thoughtspot.app/bearer/mcp\"), {\n    requestInit: {\n        headers: {\n            \"Authorization\": \"Bearer $TS_AUTH_TOKEN\", // Read below how to get the $TS_AUTH_TOKEN\n            \"x-ts-host\": \"my-thoughtspot-instance.thoughtspot.cloud\"\n        },\n    }\n});\n\nconst client = new Client(\n  {\n    name: \"example-client\",\n    version: \"1.0.0\"\n  }\n);\n\n// Configure the client\nconst ai = new GoogleGenAI({});\n\n// Initialize the connection between client and server\nawait client.connect(serverParams);\n\n// Send request to the model with MCP tools\nconst response = await ai.models.generateContent({\n  model: \"gemini-2.5-flash\",\n  contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,\n  config: {\n    tools: [mcpToTool(client)],  // uses the session, will automatically call the tool\n    // Uncomment if you **don't** want the sdk to automatically call the tool\n    // automaticFunctionCalling: {\n    //   disable: true,\n    // },\n  },\n});\nconsole.log(response.text)\n\n// Close the connection\nawait client.close();\n```\n\nRead [this](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#mcp), for more details on Gemini API MCP tool calling.\n\nAn example using Google ADK + Python can be found [here](https://github.com/thoughtspot/developer-examples/tree/main/mcp/python-google-adk-trusted-auth).\n\n#### Gemini CLI extenstions\n\nThoughtSpot MCP server can also be installed as a Gemini CLI extension.\n\n```bash\ngemini extensions install https://github.com/thoughtspot/mcp-server\n```\n\nRead more about Gemini CLI [here](https://github.com/google-gemini/gemini-cli).\n\n\n### How to get TS_AUTH_TOKEN for APIs ?",
        "start_pos": 5544,
        "end_pos": 7457,
        "token_count_estimate": 478,
        "source_type": "readme",
        "agent_id": "56e537012cf65a45"
      },
      {
        "chunk_id": 4,
        "text": "`bash\ngemini extensions install https://github.com/thoughtspot/mcp-server\n```\n\nRead more about Gemini CLI [here](https://github.com/google-gemini/gemini-cli).\n\n\n### How to get TS_AUTH_TOKEN for APIs ?\n\nFor API usage, you would the token endpoints with a `secret_key` to generate the `API_TOKEN` for a specific user/role, more details [here](https://developers.thoughtspot.com/docs/api-authv2#trusted-auth-v2). \n\n\n## Features\n\n- **OAuth Authentication**: Access your data, as yourself.\n  - Dynamic Client Registration (DCR) support.\n  - Any MCP host is allowed. Let's make the world fact driven.\n- **Tools**:\n  - `ping`: Test connectivity and authentication.\n  - `getRelevantQuestions`: Get relevant data questions from ThoughtSpot analytics based on a user query.\n  - `getAnswer`: Get the answer to a specific question from ThoughtSpot analytics.\n  - `createLiveboard`: Create a liveboard from a list of answers.\n  - `getDataSourceSuggestions`: Get datasource suggestions for a given query.\n- **MCP Resources**:\n   - `datasources`: List of ThoughtSpot Data models the user has access to.\n\n### Supported transports\n\n- SSE [https://agent.thoughtspot.app/sse]()\n- Streamed HTTP [https://agent.thoughtspot.app/mcp]()\n\n\n## Manual client registration\n\nFor MCP hosts which do not(yet) support Dynamic client registration, or they require statically adding Oauth Client Id etc. Go to [this](https://agent.thoughtspot.app/clients) page, to register a new client and copy the details over. The most relevant values are `Oauth Client Id` and `Oauth Client Secret` which should be added when adding ThoughtSpot as an MCP connector in the MCP client (ChatGPT/Claude etc). The generated client details are only available when they are generated and are NOT available later for reference.\n\n### Associate with a ThoughtSpot instance\n\nManual client registration also allows to associate with a specific ThoughtSpot instance, so that your users do not have to enter the Thoughtspot instance URL when doing the authorization flow.",
        "start_pos": 7257,
        "end_pos": 9268,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "56e537012cf65a45"
      },
      {
        "chunk_id": 5,
        "text": "tance\n\nManual client registration also allows to associate with a specific ThoughtSpot instance, so that your users do not have to enter the Thoughtspot instance URL when doing the authorization flow. While registering the Oauth client add `ThoughtSpot URL` to the appropriate value.\n\n## Self hosted\n\nUse the published docker image to deploy the MCP server in your own environment.\n\nSee [this](deploy/README.md) for details.\n\n## Stdio support (fallback)\n\nIf you are unable to use the remote MCP server due to connectivity restrictions on your Thoughtspot instance. You could use the `stdio` local transport using the `npm` package.\n\nHere is how to configure `stdio` with MCP Client:\n\n```json \n{\n  \"mcpServers\": {\n    \"ThoughtSpot\": {\n      \"command\": \"npx\",\n      \"args\": [\n         \"@thoughtspot/mcp-server\"\n      ],\n      \"env\": {\n         \"TS_INSTANCE\": \"<your Thoughtspot Instance URL>\",\n         \"TS_AUTH_TOKEN\": \"<ThoughtSpot Access Token>\"\n      }\n    }\n  }\n}\n```\n\n### How to obtain a `TS_AUTH_TOKEN` ?\n\n- Go to ThoughtSpot => _Develop_ => _Rest Playground v2.0_\n- _Authentication_ => _Get Full access token_\n- Scroll down and expand the \"body\"\n- Add your \"username\" and \"password\".\n- Put whatever \"validity_time\" you want the token to be.\n- Click on \"Try it out\" on the bottom right.\n- You should get a token in the response, thats the bearer token.\n\n#### Alternative way to get `TS_AUTH_TOKEN`\n- Login to the ThoughtSpot instance as you would normally.\n- Opem in a new tab this URL:\n  - https://your-ts-instance/api/rest/2.0/auth/session/token\n- You will see a JSON response, copy the \"token\" value (without the quotes).\n- This is the token you could use.\n\n### Troubleshooting\n\n> Oauth errors due to CORS/SAML.\n\nMake sure to add the following entries in your ThoughtSpot instance:\n\n*CORS*\n\n- Go to ThoughtSpot => _Develop_ => Security settings\n- Click \"Edit\"\n- Add \"agent.thoughtspot.app\" to the the \"CORS whitelisted domains\".",
        "start_pos": 9068,
        "end_pos": 11004,
        "token_count_estimate": 484,
        "source_type": "readme",
        "agent_id": "56e537012cf65a45"
      },
      {
        "chunk_id": 6,
        "text": "d the following entries in your ThoughtSpot instance:\n\n*CORS*\n\n- Go to ThoughtSpot => _Develop_ => Security settings\n- Click \"Edit\"\n- Add \"agent.thoughtspot.app\" to the the \"CORS whitelisted domains\". \n\n*SAML* (need to be Admin)\n\n- Go to ThoughtSpot => _Develop_\n- Go to \"All Orgs\" Tab on the left panel if there is one.\n- Click \"Security settings\"\n- Click \"Edit\"\n- Add \"agent.thoughtspot.app\" to the the \"SAML redirect domains\". \n\n> MCP server install error due to node issues\n\n- Make sure node is installed on your machine.\n- Make sure the node version is >=18\n- Check the node version by using the command `node -v`\n\n> 500 error from MCP server\n\n- Make sure the ThoughtSpot cluster the MCP server is connected to is up and running.\n- If the error persists, please collect the logs that you get from the MCP client and the approximate time when the issue occurred.\n- Reach out on [Discord](https://developers.thoughtspot.com/join-discord) to get support.\n- Create a issue on this repository to get help.\n- Submit a [ThoughtSpot support case](https://community.thoughtspot.com/s/article/How-to-submit-a-ThoughtSpot-Support-Case) with all the artifacts.\n\n> Stale MCP auth\n\n- If for some reason the ThoughtSpot MCP server is failing authentication repeatedly, you can do `rm -rf ~/.mcp-auth`.\n- This will remove all stale authentication info, and restart the auth flow again.\n\n## Contributing\n\n### Local Development\n\n1. **Install dependencies**:\n   ```sh\n   npm install\n   ```\n2. **Set up environment variables**:\n   - Copy `.dev.vars` and fill in your ThoughtSpot instance URL and access token.\n3. **Start the development server**:\n   ```sh\n   npm run dev\n   ```\n\n### Endpoints\n\n- `/mcp`: MCP HTTP Streaming endpoint\n- `/sse`: Server-sent events for MCP\n- `/api`: MCP tools exposed as HTTP endpoints\n- `/authorize`, `/token`, `/register`: OAuth endpoints\n- `/bearer/mcp`, `/bearer/sse`: MCP endpoints as bearer auth instead of Oauth, mainly for use in APIs or in cases where Oauth is not working.\n\nMCP Server, ¬© ThoughtSpot, Inc. 2025",
        "start_pos": 10804,
        "end_pos": 12839,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "56e537012cf65a45"
      },
      {
        "chunk_id": 7,
        "text": "r`: OAuth endpoints\n- `/bearer/mcp`, `/bearer/sse`: MCP endpoints as bearer auth instead of Oauth, mainly for use in APIs or in cases where Oauth is not working.\n\nMCP Server, ¬© ThoughtSpot, Inc. 2025",
        "start_pos": 12639,
        "end_pos": 12839,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "56e537012cf65a45"
      }
    ]
  },
  {
    "agent_id": "1bea393f9c592a7f",
    "name": "app.tradeit/mcp",
    "source": "mcp",
    "source_url": "https://github.com/trade-it-inc/trade-it-mcp",
    "description": "Trade stock, crypto, and options on Robinhood, ETrade, Webull, Charles Schwab, Coinbase, or Kraken.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-12-16T04:32:08.51866Z",
    "indexed_at": "2026-02-18T04:10:10.012257",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Trade It MCP Server\n\n## [üëâ Full Documentation Here üëà](https://docs.tradeit.app)\nNow available through the [_Official MCP Registry_](https://registry.modelcontextprotocol.io/?q=app.tradeit%2Fmcp)\n\n\n**Endpoints:**  \n- Streamable HTTP: `https://mcp.tradeit.app/mcp` \n- SSE: `https://mcp.tradeit.app/sse`\n\n## Overview\n\nThe Trade It MCP Server brings stock, crypto, and options trading support to agents. It enables natural-language interaction with stock and crypto brokerages‚Äîexecute trades, query portfolio performance, and surface market insights by sending plain-English requests through the MCP protocol.\n\n**Brokerage Support:**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/robinhood-logo.svg\" alt=\"Robinhood Logo\" /> **[Robinhood](https://robinhood.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/charles_schwab-logo.svg\" alt=\"Charles Scwhab Logo\" /> **[Charles Schwab](https://schwab.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/etrade-logo.svg\" alt=\"ETrade Logo\" /> **[E*Trade](https://etrade.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/webull-logo.svg\" alt=\"Webull Logo\" /> **[Webull](https://webull.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/public-logo.svg\" alt=\"Public Logo\" /> **[Public](https://public.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/tastytrade-logo.svg\" alt=\"Tastytrade Logo\" /> **[Tastytrade](https://tastytrade.com)**\n\n**Crypto Exchange Support:**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/coinbase-logo.svg\" alt=\"Coinbase Logo\" /> **[Coinbase](https://coinbase.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/kraken-logo.svg\" alt=\"Kraken Logo\" /> **[Kraken](https://kraken.com)**\n\nMore to be added soon!\n\nThis server is **remote** so you don't need to run anything locally to connect. Just point your MCP-compatible agent platform to the URL above.\n\n---\n\n## Tools\n\n- üí¨ **Create Trade**\n  Creates a trade order to buy or sell an asset.\n\n  ORDER TYPES:\n  - **market** (default) ‚Üí Executes immediately at current market price. No price fields required.\n  - **limit** ‚Üí Executes only at a specific limit_price or better. Requires `limit_price`.\n  - **stop** ‚Üí Triggers a market order when stop_price is reached. Requires `stop_price`.\n  - **stop_limit** ‚Üí Triggers a limit order when stop_price is reached. Requires BOTH `stop_price` and `limit_price`.\n \n  EXAMPLES:\n  - \"Buy $1000 of Tesla\"\n  - \"Buy $1000 of Tesla, but only if the price drops to $150 or lower\"\n  - \"Sell 10 shares of Apple if the price falls to $140 or lower\"\n  - \"Buy a share of Apple if it hits $200\"\n  - \"Buy 10 shares of Apple if the price rises to $140, but don't pay more than $142 per share\"\n\n  DEFAULTS:\n  - If no amount is given, your default amount is used.\n  - If no account is given, your default account is used. \n  - If no order type is given, the trade is a market order. \n  - If auto-execute is enabled in settings, the trade will execute immediately. Otherwise, it gets created in draft state and requires a call to `Execute Trade` to complete. This allows you to review and confirm trades.\n\n- üí¨ **Create Option Trade (Beta)**\n  Creates a trade order to buy or sell an options contract.\n \n  EXAMPLES:\n  - \"Buy 1 call option on Apple with a $300 strike price expiring next month\"\n  - \"Sell a covered call on my Microsoft shares at $500 strike\"\n  - \"Open a call spread: buy 1 TSLA $475 call and sell 1 TSLA $485 call, both expiring next week\"\n  - \"Buy an ATM straddle on SPY, expiring this Friday\"\n  - \"Buy 2 AMZN 200 1/30 P, limit price $3.50\"\n  - \"Sell AMZN260130P00200000\"\n\n- üí¨ **Execute Trade**\n  Execute the trade on your brokerage.\n\n- üí¨ **Show Account Details**\n  List your linked brokerages along with their current value and cash balance.\n  Example: `\"Show my accounts\"`\n\n- üí¨ **Search Asset**\n  Get current price and metadata for any stock or cryptocurrency.\n  Example: `\"How's Apple doing?\"` or `\"What's the price of TSLA?\"`\n\n---\n\n## Getting Started\n\n1. First, create an account at https://tradeit.app.\n2. Sign up for the Pro plan's free trial.\n3. Connect your brokerage of choice.\n\n## Connecting\n1. Connect your MCP client to `https://mcp.tradeit.app/mcp` or `https://mcp.tradeit.app/sse`.\n2. Authenticate through the browser-based OAuth flow.\n3. You're now ready to start trading!\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Create trade orders to buy or sell stocks, crypto, and options",
        "Execute trades on supported brokerages",
        "Query portfolio performance and account details",
        "Search for current price and metadata of stocks and cryptocurrencies",
        "Support natural-language interaction for trading commands",
        "Connect to multiple brokerages including Robinhood, Charles Schwab, E*Trade, Webull, Public, and Tastytrade",
        "Support crypto exchanges Coinbase and Kraken",
        "Handle various order types including market, limit, stop, and stop-limit",
        "Support options trading with complex orders like spreads and straddles"
      ],
      "limitations": [
        "Options trading is currently in beta",
        "Requires user to have a Trade It account with Pro plan subscription",
        "Trades require OAuth authentication through browser-based flow",
        "Auto-execute trades only if enabled in user settings; otherwise trades remain in draft until executed",
        "Limited to supported brokerages and crypto exchanges listed",
        "No local server deployment; only remote access via provided endpoints"
      ],
      "requirements": [
        "Create an account at https://tradeit.app",
        "Sign up for the Pro plan's free trial",
        "Connect at least one supported brokerage or crypto exchange",
        "Authenticate via browser-based OAuth flow",
        "Use an MCP-compatible agent platform to connect to the server endpoints"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation provides installation steps, detailed tool descriptions with examples, supported brokerages, usage instructions, and limitations, making it comprehensive and well-structured.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Trade It MCP Server\n\n## [üëâ Full Documentation Here üëà](https://docs.tradeit.app)\nNow available through the [_Official MCP Registry_](https://registry.modelcontextprotocol.io/?q=app.tradeit%2Fmcp)\n\n\n**Endpoints:**  \n- Streamable HTTP: `https://mcp.tradeit.app/mcp` \n- SSE: `https://mcp.tradeit.app/sse`\n\n## Overview\n\nThe Trade It MCP Server brings stock, crypto, and options trading support to agents. It enables natural-language interaction with stock and crypto brokerages‚Äîexecute trades, query portfolio performance, and surface market insights by sending plain-English requests through the MCP protocol.\n\n**Brokerage Support:**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/robinhood-logo.svg\" alt=\"Robinhood Logo\" /> **[Robinhood](https://robinhood.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/charles_schwab-logo.svg\" alt=\"Charles Scwhab Logo\" /> **[Charles Schwab](https://schwab.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/etrade-logo.svg\" alt=\"ETrade Logo\" /> **[E*Trade](https://etrade.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/webull-logo.svg\" alt=\"Webull Logo\" /> **[Webull](https://webull.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/public-logo.svg\" alt=\"Public Logo\" /> **[Public](https://public.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/tastytrade-logo.svg\" alt=\"Tastytrade Logo\" /> **[Tastytrade](https://tastytrade.com)**\n\n**Crypto Exchange Support:**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/coinbase-logo.svg\" alt=\"Coinbase Logo\" /> **[Coinbase](https://coinbase.com)**\n- <img height=\"14\" width=\"14\" src=\"https://images.tradeit.app/brokerages/kraken-logo.svg\" alt=\"Kraken Logo\" /> **[Kraken](https://kraken.com)**\n\nMore to be added soon!\n\nThis server is **remote** so you don't need to run anything locally to connect. Just point your MCP-compatible agent platform to the URL above.",
        "start_pos": 0,
        "end_pos": 2043,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "1bea393f9c592a7f"
      },
      {
        "chunk_id": 1,
        "text": "*[Kraken](https://kraken.com)**\n\nMore to be added soon!\n\nThis server is **remote** so you don't need to run anything locally to connect. Just point your MCP-compatible agent platform to the URL above.\n\n---\n\n## Tools\n\n- üí¨ **Create Trade**\n  Creates a trade order to buy or sell an asset.\n\n  ORDER TYPES:\n  - **market** (default) ‚Üí Executes immediately at current market price. No price fields required.\n  - **limit** ‚Üí Executes only at a specific limit_price or better. Requires `limit_price`.\n  - **stop** ‚Üí Triggers a market order when stop_price is reached. Requires `stop_price`.\n  - **stop_limit** ‚Üí Triggers a limit order when stop_price is reached. Requires BOTH `stop_price` and `limit_price`.\n \n  EXAMPLES:\n  - \"Buy $1000 of Tesla\"\n  - \"Buy $1000 of Tesla, but only if the price drops to $150 or lower\"\n  - \"Sell 10 shares of Apple if the price falls to $140 or lower\"\n  - \"Buy a share of Apple if it hits $200\"\n  - \"Buy 10 shares of Apple if the price rises to $140, but don't pay more than $142 per share\"\n\n  DEFAULTS:\n  - If no amount is given, your default amount is used.\n  - If no account is given, your default account is used. \n  - If no order type is given, the trade is a market order. \n  - If auto-execute is enabled in settings, the trade will execute immediately. Otherwise, it gets created in draft state and requires a call to `Execute Trade` to complete. This allows you to review and confirm trades.\n\n- üí¨ **Create Option Trade (Beta)**\n  Creates a trade order to buy or sell an options contract.\n \n  EXAMPLES:\n  - \"Buy 1 call option on Apple with a $300 strike price expiring next month\"\n  - \"Sell a covered call on my Microsoft shares at $500 strike\"\n  - \"Open a call spread: buy 1 TSLA $475 call and sell 1 TSLA $485 call, both expiring next week\"\n  - \"Buy an ATM straddle on SPY, expiring this Friday\"\n  - \"Buy 2 AMZN 200 1/30 P, limit price $3.50\"\n  - \"Sell AMZN260130P00200000\"\n\n- üí¨ **Execute Trade**\n  Execute the trade on your brokerage.",
        "start_pos": 1843,
        "end_pos": 3812,
        "token_count_estimate": 492,
        "source_type": "readme",
        "agent_id": "1bea393f9c592a7f"
      },
      {
        "chunk_id": 2,
        "text": "week\"\n  - \"Buy an ATM straddle on SPY, expiring this Friday\"\n  - \"Buy 2 AMZN 200 1/30 P, limit price $3.50\"\n  - \"Sell AMZN260130P00200000\"\n\n- üí¨ **Execute Trade**\n  Execute the trade on your brokerage.\n\n- üí¨ **Show Account Details**\n  List your linked brokerages along with their current value and cash balance.\n  Example: `\"Show my accounts\"`\n\n- üí¨ **Search Asset**\n  Get current price and metadata for any stock or cryptocurrency.\n  Example: `\"How's Apple doing?\"` or `\"What's the price of TSLA?\"`\n\n---\n\n## Getting Started\n\n1. First, create an account at https://tradeit.app.\n2. Sign up for the Pro plan's free trial.\n3. Connect your brokerage of choice.\n\n## Connecting\n1. Connect your MCP client to `https://mcp.tradeit.app/mcp` or `https://mcp.tradeit.app/sse`.\n2. Authenticate through the browser-based OAuth flow.\n3. You're now ready to start trading!",
        "start_pos": 3612,
        "end_pos": 4467,
        "token_count_estimate": 213,
        "source_type": "readme",
        "agent_id": "1bea393f9c592a7f"
      }
    ]
  },
  {
    "agent_id": "4df50f187714fb1c",
    "name": "app.zenable/zenable",
    "source": "mcp",
    "source_url": "https://mcp.www.zenable.app/",
    "description": "Zenable cleans up sloppy AI code and prevents vulnerabilities with deterministic guardrails",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-28T22:41:40.003484Z",
    "indexed_at": "2026-02-18T04:10:11.216408",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Clean up sloppy AI code",
        "Prevent vulnerabilities in AI code",
        "Apply deterministic guardrails to AI code"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's purpose but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "4df50f187714fb1c",
    "name": "app.zenable/zenable",
    "source": "mcp",
    "source_url": "https://mcp.zenable.app/",
    "description": "Zenable cleans up sloppy AI code and prevents vulnerabilities with deterministic guardrails",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-28T22:41:39.996616Z",
    "indexed_at": "2026-02-18T04:10:11.327268",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Clean up sloppy AI code",
        "Prevent vulnerabilities in AI code",
        "Apply deterministic guardrails to AI code"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's purpose but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "e6a5a24ad7473816",
    "name": "aws.api.us-east-1.ecs-mcp/server",
    "source": "mcp",
    "source_url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-mcp-introduction.html",
    "description": "AI-powered Amazon ECS workload management",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-02-09T23:18:00.777188Z",
    "indexed_at": "2026-02-18T04:10:12.453112",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "Thanks for letting us know we're doing a good job!\nIf you've got a moment, please tell us what we did right so we can do more of it."
    },
    "llm_extracted": {
      "capabilities": [],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.1,
    "quality_rationale": "The documentation provides almost no information beyond a generic thank you message, lacking any details on capabilities, limitations, or requirements.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "Thanks for letting us know we're doing a good job!\nIf you've got a moment, please tell us what we did right so we can do more of it.",
        "start_pos": 0,
        "end_pos": 132,
        "token_count_estimate": 33,
        "source_type": "detail_page",
        "agent_id": "e6a5a24ad7473816"
      }
    ]
  },
  {
    "agent_id": "0c36d208074325d3",
    "name": "aws.api.us-east-1.eks-mcp/server",
    "source": "mcp",
    "source_url": "https://docs.aws.amazon.com/eks/latest/userguide/eks-mcp-getting-started.html",
    "description": "AI-powered Amazon EKS cluster management and troubleshooting",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-29T16:45:51.075699Z",
    "indexed_at": "2026-02-18T04:10:14.444248",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "Thanks for letting us know we're doing a good job!\nIf you've got a moment, please tell us what we did right so we can do more of it."
    },
    "llm_extracted": {
      "capabilities": [],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.1,
    "quality_rationale": "The documentation provides almost no information beyond a generic feedback request, lacking any details on capabilities, limitations, or requirements.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "Thanks for letting us know we're doing a good job!\nIf you've got a moment, please tell us what we did right so we can do more of it.",
        "start_pos": 0,
        "end_pos": 132,
        "token_count_estimate": 33,
        "source_type": "detail_page",
        "agent_id": "0c36d208074325d3"
      }
    ]
  },
  {
    "agent_id": "046105d9c11e3563",
    "name": "biz.icecat/mcp",
    "source": "mcp",
    "source_url": "https://mcp.icecat.biz",
    "description": "Icecat Product Content",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-28T09:28:57.58134Z",
    "indexed_at": "2026-02-18T04:10:16.001285",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to Icecat product content"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single short phrase with minimal information about the server's functionality.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b4e7939034163afa",
    "name": "build.arca.mcp/arca-mcp-server",
    "source": "mcp",
    "source_url": "https://arca.build",
    "description": "Arca is a private data vault where your AI stores your structured data, semantic memory and skills. ",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-12-15T17:05:06.117783Z",
    "indexed_at": "2026-02-18T04:10:54.680657",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Store structured data",
        "Store semantic memory",
        "Store skills"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence providing a very basic overview without details on usage, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "4fee39f7a4b9f59a",
    "name": "capital.hove/read-only-local-mysql-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/hovecapital/read-only-local-mysql-mcp-server",
    "description": "MCP server enabling read-only MySQL queries for Claude Desktop",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-11-03T12:17:18.093271Z",
    "indexed_at": "2026-02-18T04:10:55.770740",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# MySQL MCP Server\n\n[![MCP Registry](https://img.shields.io/badge/MCP_Registry-Available-blue)](https://registry.modelcontextprotocol.io)\n[![npm version](https://img.shields.io/npm/v/@hovecapital/read-only-mysql-mcp-server.svg)](https://www.npmjs.com/package/@hovecapital/read-only-mysql-mcp-server)\n\nA Model Context Protocol (MCP) server that enables Claude Desktop to interact with MySQL databases through natural language queries.\n\n## Features\n\n- Execute read-only SQL queries through Claude Desktop\n- **Dynamic connection configuration** ‚Äî connect to any MySQL database at runtime via connection string\n- Built-in security with query validation (only SELECT statements allowed)\n- Easy integration with Claude Desktop\n- JSON formatted query results\n- Environment-based configuration for database credentials\n\n## Quick Start\n\n### For Claude Code Users (Recommended - Easiest Method)\n\n```bash\nclaude mcp add mysql -s user -- npx -y @hovecapital/read-only-mysql-mcp-server\n```\n\nThen set your database environment variables:\n\n```bash\nexport DB_HOST=localhost\nexport DB_PORT=3306\nexport DB_DATABASE=your_database_name\nexport DB_USERNAME=your_username\nexport DB_PASSWORD=your_password\n```\n\n**Done!** Restart Claude Code and ask: \"What tables are in my database?\"\n\n### For Claude Desktop Users (Manual Configuration)\n\n**1. Open your config file:**\n\n```bash\n# macOS\nopen ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n\n# Windows\nnotepad %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n**2. Add this configuration:**\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@hovecapital/read-only-mysql-mcp-server\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"3306\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n**3. Save, restart Claude Desktop, and test!**\n\n## Prerequisites\n\n- Node.js (v16 or higher) - If using mise, update the command path accordingly\n- MySQL database server\n- Claude Desktop application\n\n## Installation\n\n### Option 1: Install from MCP Registry (Recommended)\n\nThis server is published in the [Model Context Protocol Registry](https://registry.modelcontextprotocol.io) as `capital.hove/read-only-local-mysql-mcp-server`.\n\n#### Method A: Claude Code CLI (Easiest!)\n\n```bash\nclaude mcp add mysql -s user -- npx -y @hovecapital/read-only-mysql-mcp-server\n```\n\nThen configure your database credentials using environment variables. Restart Claude Code and you're done!\n\n**Benefits:**\n\n- One command installation\n- No manual JSON editing\n- Automatic configuration\n\n#### Method B: Manual JSON Configuration\n\n**For Claude Desktop:**\n\nEdit `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@hovecapital/read-only-mysql-mcp-server\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"3306\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n**For Claude Code:**\n\nEdit `~/.config/claude-code/settings.json` (macOS/Linux) or `%APPDATA%\\claude-code\\settings.json` (Windows):\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"mysql\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@hovecapital/read-only-mysql-mcp-server\"],\n        \"env\": {\n          \"DB_HOST\": \"localhost\",\n          \"DB_PORT\": \"3306\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Option 2: Install from npm\n\n```bash\nnpm install -g @hovecapital/read-only-mysql-mcp-server\n```\n\n### Option 3: Installation with Claude Code\n\nIf you're using Claude Code, you can easily install this MCP server:\n\n```bash\n# Clone the repository\ngit clone https://github.com/hovecapital/read-only-local-mysql-mcp-server.git\ncd read-only-local-mysql-mcp-server\n\n# Install dependencies and build\nnpm install\nnpm run build\n```\n\nThen configure Claude Code by adding to your MCP settings.\n\n### Option 4: Manual Installation\n\n#### 1. Clone or Download\n\nSave the repository to a directory on your system:\n\n```bash\nmkdir ~/mcp-servers/mysql\ncd ~/mcp-servers/mysql\ngit clone https://github.com/hovecapital/read-only-local-mysql-mcp-server.git .\n```\n\n#### 2. Install Dependencies\n\n```bash\nnpm install\nnpm run build\n```\n\n## Configuration\n\n> **Note:** If you installed via Option 1 (MCP Registry with npx), you've already configured everything! This section is for users who chose Options 2, 3, or 4 (npm or manual installation).\n\n### Claude Code Configuration\n\nIf you're using Claude Code with a manual installation, add the MySQL server to your MCP settings:\n\n1. Open your Claude Code settings (typically in `~/.config/claude-code/settings.json` on macOS/Linux or `%APPDATA%\\claude-code\\settings.json` on Windows)\n\n2. Add the MySQL MCP server configuration:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"mysql\": {\n        \"command\": \"node\",\n        \"args\": [\"/absolute/path/to/read-only-local-mysql-mcp-server/dist/index.js\"],\n        \"env\": {\n          \"DB_HOST\": \"localhost\",\n          \"DB_PORT\": \"3306\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n1. Restart Claude Code for the changes to take effect.\n\n### Claude Desktop Configuration\n\nIf you're using Claude Desktop with a manual installation, open your Claude Desktop configuration file:\n\n**macOS:**\n\n```bash\n~/Library/Application Support/Claude/claude_desktop_config.json\n```\n\n**Windows:**\n\n```bash\n%APPDATA%\\Claude\\claude_desktop_config.json\n```\n\nAdd the MySQL server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/read-only-local-mysql-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"3306\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n### Using mise for Node.js\n\nIf you're using [mise](https://mise.jdx.dev/) for Node.js version management, make sure to use the full path to the Node.js executable in your configuration.\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `DB_HOST` | MySQL server hostname | `mysql` |\n| `DB_PORT` | MySQL server port | `3306` |\n| `DB_DATABASE` | Database name | `database` |\n| `DB_USERNAME` | MySQL username | `root` |\n| `DB_PASSWORD` | MySQL password | (empty) |\n\n## Usage\n\n1. **Restart Claude Desktop** after updating the configuration\n2. **Start chatting** with Claude about your database\n\n### Example Queries\n\n```bash\n\"Show me all tables in my database\"\n\"What's the structure of the users table?\"\n\"Get the first 10 records from the products table\"\n\"How many orders were placed last month?\"\n\"Show me users with email addresses ending in @gmail.com\"\n```\n\nClaude will automatically convert your natural language requests into appropriate SQL queries and execute them against your database.\n\n## Tools\n\n### `connect`\n\nConnect to a MySQL database using a connection string. The connection persists for subsequent queries until changed or disconnected.\n\n**Parameters:**\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `connectionString` | Yes | MySQL connection string (e.g., `mysql://user:password@host:port/database`) |\n\n### `disconnect`\n\nDisconnect from the current runtime database and revert to the default environment-configured connection. Takes no parameters.\n\n### `query`\n\nRun a read-only SQL query against the currently connected database.\n\n**Parameters:**\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `sql` | Yes | SQL query to execute (read-only) |\n| `connectionString` | No | MySQL connection string to override the current connection for this query only |\n\n## Dynamic Connection\n\nThe server supports switching databases at runtime without restarting. Connection priority:\n\n1. **Per-query `connectionString`** ‚Äî overrides everything for a single query\n2. **Runtime connection** (from `connect` tool) ‚Äî persists until `disconnect` is called\n3. **Environment variables** ‚Äî the default fallback\n\n**Connection string format:**\n\n```\nmysql://username:password@hostname:port/database_name\n```\n\n**Example workflow:**\n\n```\n1. connect ‚Üí mysql://analyst:pass@prod-db:3306/analytics\n2. query   ‚Üí SELECT COUNT(*) FROM events\n3. query   ‚Üí SELECT * FROM users LIMIT 5 (uses the same analytics connection)\n4. query   ‚Üí SELECT * FROM orders LIMIT 5, connectionString: mysql://analyst:pass@prod-db:3306/sales\n              (one-off override ‚Äî does not change the stored connection)\n5. disconnect ‚Üí reverts to env var defaults\n```\n\n## Security Features\n\n### Read-Only Operations\n\nThe server only allows SELECT queries. The following operations are blocked:\n\n- `INSERT` - Adding new records\n- `UPDATE` - Modifying existing records  \n- `DELETE` - Removing records\n- `DROP` - Removing tables/databases\n- `ALTER` - Modifying table structure\n- `CREATE` - Creating new tables/databases\n\n### Recommended Database Setup\n\nFor enhanced security, create a dedicated read-only user for the MCP server:\n\n```sql\n-- Create a read-only user\nCREATE USER 'claude_readonly'@'localhost' IDENTIFIED BY 'secure_password';\n\n-- Grant only SELECT permissions on your specific database\nGRANT SELECT ON your_database_name.* TO 'claude_readonly'@'localhost';\n\n-- Apply the changes\nFLUSH PRIVILEGES;\n```\n\n## Troubleshooting\n\n### Connection Issues\n\n1. **Verify MySQL is running**: Check if your MySQL server is active\n2. **Check credentials**: Ensure username/password are correct\n3. **Network connectivity**: Confirm Claude Desktop can reach your MySQL server\n\n### Configuration Issues\n\n1. **Restart required**: Always restart Claude Desktop after configuration changes\n2. **Path accuracy**: Ensure the absolute path to `dist/index.js` is correct\n3. **JSON syntax**: Validate your `claude_desktop_config.json` format\n\n### Debug Mode\n\nTo see server logs, you can run the server manually:\n\n```bash\nnode dist/index.js\n```\n\n## File Structure\n\n```bash\n~/mcp-servers/mysql/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ dist/\n‚îÇ   ‚îú‚îÄ‚îÄ index.js\n‚îÇ   ‚îî‚îÄ‚îÄ index.d.ts\n‚îú‚îÄ‚îÄ package.json\n‚îú‚îÄ‚îÄ tsconfig.json\n‚îî‚îÄ‚îÄ node_modules/\n```\n\n## Dependencies\n\n- **@modelcontextprotocol/sdk**: MCP protocol implementation\n- **mysql2**: Modern MySQL client for Node.js with Promise support\n\n## Contributing\n\nFeel free to submit issues and enhancement requests!\n\n## License\n\nThis project is open source and available under the [MIT License](LICENSE).\n\n## Support\n\nIf you encounter issues:\n\n1. Check the troubleshooting section above\n2. Verify your MySQL connection independently\n3. Ensure Claude Desktop is updated to the latest version\n4. Review the Claude Desktop MCP documentation\n\n---\n\n**Note**: This server is designed for development and analysis purposes. For production use, consider additional security measures and monitoring.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Execute read-only SQL SELECT queries on MySQL databases",
        "Dynamically connect to any MySQL database at runtime via connection string",
        "Persist database connections across multiple queries until disconnected",
        "Override connection per query using a connection string",
        "Return query results formatted as JSON",
        "Integrate seamlessly with Claude Desktop and Claude Code",
        "Validate queries to block non-SELECT operations for security",
        "Allow configuration via environment variables or JSON config files",
        "Provide tools to connect, disconnect, and query the database"
      ],
      "limitations": [
        "Cannot execute write operations such as INSERT, UPDATE, DELETE, DROP, ALTER, or CREATE",
        "Requires a MySQL database accessible to the client environment",
        "Does not support production-grade security out of the box; recommends read-only user setup",
        "Requires restart of Claude Desktop or Claude Code after configuration changes",
        "Limited to MySQL databases only"
      ],
      "requirements": [
        "Node.js version 16 or higher installed",
        "Access to a running MySQL database server",
        "Claude Desktop or Claude Code application installed",
        "Environment variables or configuration JSON specifying DB_HOST, DB_PORT, DB_DATABASE, DB_USERNAME, and DB_PASSWORD",
        "Optional: mise for Node.js version management if used"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, configuration options, security considerations, troubleshooting tips, and environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# MySQL MCP Server\n\n[![MCP Registry](https://img.shields.io/badge/MCP_Registry-Available-blue)](https://registry.modelcontextprotocol.io)\n[![npm version](https://img.shields.io/npm/v/@hovecapital/read-only-mysql-mcp-server.svg)](https://www.npmjs.com/package/@hovecapital/read-only-mysql-mcp-server)\n\nA Model Context Protocol (MCP) server that enables Claude Desktop to interact with MySQL databases through natural language queries.\n\n## Features\n\n- Execute read-only SQL queries through Claude Desktop\n- **Dynamic connection configuration** ‚Äî connect to any MySQL database at runtime via connection string\n- Built-in security with query validation (only SELECT statements allowed)\n- Easy integration with Claude Desktop\n- JSON formatted query results\n- Environment-based configuration for database credentials\n\n## Quick Start\n\n### For Claude Code Users (Recommended - Easiest Method)\n\n```bash\nclaude mcp add mysql -s user -- npx -y @hovecapital/read-only-mysql-mcp-server\n```\n\nThen set your database environment variables:\n\n```bash\nexport DB_HOST=localhost\nexport DB_PORT=3306\nexport DB_DATABASE=your_database_name\nexport DB_USERNAME=your_username\nexport DB_PASSWORD=your_password\n```\n\n**Done!** Restart Claude Code and ask: \"What tables are in my database?\"\n\n### For Claude Desktop Users (Manual Configuration)\n\n**1. Open your config file:**\n\n```bash\n# macOS\nopen ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n\n# Windows\nnotepad %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n**2. Add this configuration:**\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@hovecapital/read-only-mysql-mcp-server\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"3306\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n**3.",
        "start_pos": 0,
        "end_pos": 1890,
        "token_count_estimate": 472,
        "source_type": "readme",
        "agent_id": "4fee39f7a4b9f59a"
      },
      {
        "chunk_id": 1,
        "text": "HOST\": \"localhost\",\n        \"DB_PORT\": \"3306\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n**3. Save, restart Claude Desktop, and test!**\n\n## Prerequisites\n\n- Node.js (v16 or higher) - If using mise, update the command path accordingly\n- MySQL database server\n- Claude Desktop application\n\n## Installation\n\n### Option 1: Install from MCP Registry (Recommended)\n\nThis server is published in the [Model Context Protocol Registry](https://registry.modelcontextprotocol.io) as `capital.hove/read-only-local-mysql-mcp-server`.\n\n#### Method A: Claude Code CLI (Easiest!)\n\n```bash\nclaude mcp add mysql -s user -- npx -y @hovecapital/read-only-mysql-mcp-server\n```\n\nThen configure your database credentials using environment variables. Restart Claude Code and you're done!",
        "start_pos": 1690,
        "end_pos": 2559,
        "token_count_estimate": 217,
        "source_type": "readme",
        "agent_id": "4fee39f7a4b9f59a"
      },
      {
        "chunk_id": 2,
        "text": "calhost\",\n          \"DB_PORT\": \"3306\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Option 2: Install from npm\n\n```bash\nnpm install -g @hovecapital/read-only-mysql-mcp-server\n```\n\n### Option 3: Installation with Claude Code\n\nIf you're using Claude Code, you can easily install this MCP server:\n\n```bash\n# Clone the repository\ngit clone https://github.com/hovecapital/read-only-local-mysql-mcp-server.git\ncd read-only-local-mysql-mcp-server\n\n# Install dependencies and build\nnpm install\nnpm run build\n```\n\nThen configure Claude Code by adding to your MCP settings.\n\n### Option 4: Manual Installation\n\n#### 1. Clone or Download\n\nSave the repository to a directory on your system:\n\n```bash\nmkdir ~/mcp-servers/mysql\ncd ~/mcp-servers/mysql\ngit clone https://github.com/hovecapital/read-only-local-mysql-mcp-server.git .\n```\n\n#### 2. Install Dependencies\n\n```bash\nnpm install\nnpm run build\n```\n\n## Configuration\n\n> **Note:** If you installed via Option 1 (MCP Registry with npx), you've already configured everything! This section is for users who chose Options 2, 3, or 4 (npm or manual installation).\n\n### Claude Code Configuration\n\nIf you're using Claude Code with a manual installation, add the MySQL server to your MCP settings:\n\n1. Open your Claude Code settings (typically in `~/.config/claude-code/settings.json` on macOS/Linux or `%APPDATA%\\claude-code\\settings.json` on Windows)\n\n2. Add the MySQL MCP server configuration:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"mysql\": {\n        \"command\": \"node\",\n        \"args\": [\"/absolute/path/to/read-only-local-mysql-mcp-server/dist/index.js\"],\n        \"env\": {\n          \"DB_HOST\": \"localhost\",\n          \"DB_PORT\": \"3306\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n1. Restart Claude Code for the changes to take effect.",
        "start_pos": 3538,
        "end_pos": 5558,
        "token_count_estimate": 505,
        "source_type": "readme",
        "agent_id": "4fee39f7a4b9f59a"
      },
      {
        "chunk_id": 3,
        "text": "ASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n1. Restart Claude Code for the changes to take effect.\n\n### Claude Desktop Configuration\n\nIf you're using Claude Desktop with a manual installation, open your Claude Desktop configuration file:\n\n**macOS:**\n\n```bash\n~/Library/Application Support/Claude/claude_desktop_config.json\n```\n\n**Windows:**\n\n```bash\n%APPDATA%\\Claude\\claude_desktop_config.json\n```\n\nAdd the MySQL server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"mysql\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/read-only-local-mysql-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"3306\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n### Using mise for Node.js\n\nIf you're using [mise](https://mise.jdx.dev/) for Node.js version management, make sure to use the full path to the Node.js executable in your configuration.\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `DB_HOST` | MySQL server hostname | `mysql` |\n| `DB_PORT` | MySQL server port | `3306` |\n| `DB_DATABASE` | Database name | `database` |\n| `DB_USERNAME` | MySQL username | `root` |\n| `DB_PASSWORD` | MySQL password | (empty) |\n\n## Usage\n\n1. **Restart Claude Desktop** after updating the configuration\n2. **Start chatting** with Claude about your database\n\n### Example Queries\n\n```bash\n\"Show me all tables in my database\"\n\"What's the structure of the users table?\"\n\"Get the first 10 records from the products table\"\n\"How many orders were placed last month?\"\n\"Show me users with email addresses ending in @gmail.com\"\n```\n\nClaude will automatically convert your natural language requests into appropriate SQL queries and execute them against your database.\n\n## Tools\n\n### `connect`\n\nConnect to a MySQL database using a connection string.",
        "start_pos": 5358,
        "end_pos": 7393,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "4fee39f7a4b9f59a"
      },
      {
        "chunk_id": 4,
        "text": "tomatically convert your natural language requests into appropriate SQL queries and execute them against your database.\n\n## Tools\n\n### `connect`\n\nConnect to a MySQL database using a connection string. The connection persists for subsequent queries until changed or disconnected.\n\n**Parameters:**\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `connectionString` | Yes | MySQL connection string (e.g., `mysql://user:password@host:port/database`) |\n\n### `disconnect`\n\nDisconnect from the current runtime database and revert to the default environment-configured connection. Takes no parameters.\n\n### `query`\n\nRun a read-only SQL query against the currently connected database.\n\n**Parameters:**\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `sql` | Yes | SQL query to execute (read-only) |\n| `connectionString` | No | MySQL connection string to override the current connection for this query only |\n\n## Dynamic Connection\n\nThe server supports switching databases at runtime without restarting. Connection priority:\n\n1. **Per-query `connectionString`** ‚Äî overrides everything for a single query\n2. **Runtime connection** (from `connect` tool) ‚Äî persists until `disconnect` is called\n3. **Environment variables** ‚Äî the default fallback\n\n**Connection string format:**\n\n```\nmysql://username:password@hostname:port/database_name\n```\n\n**Example workflow:**\n\n```\n1. connect ‚Üí mysql://analyst:pass@prod-db:3306/analytics\n2. query   ‚Üí SELECT COUNT(*) FROM events\n3. query   ‚Üí SELECT * FROM users LIMIT 5 (uses the same analytics connection)\n4. query   ‚Üí SELECT * FROM orders LIMIT 5, connectionString: mysql://analyst:pass@prod-db:3306/sales\n              (one-off override ‚Äî does not change the stored connection)\n5. disconnect ‚Üí reverts to env var defaults\n```\n\n## Security Features\n\n### Read-Only Operations\n\nThe server only allows SELECT queries.",
        "start_pos": 7193,
        "end_pos": 9100,
        "token_count_estimate": 476,
        "source_type": "readme",
        "agent_id": "4fee39f7a4b9f59a"
      },
      {
        "chunk_id": 5,
        "text": "(one-off override ‚Äî does not change the stored connection)\n5. disconnect ‚Üí reverts to env var defaults\n```\n\n## Security Features\n\n### Read-Only Operations\n\nThe server only allows SELECT queries. The following operations are blocked:\n\n- `INSERT` - Adding new records\n- `UPDATE` - Modifying existing records  \n- `DELETE` - Removing records\n- `DROP` - Removing tables/databases\n- `ALTER` - Modifying table structure\n- `CREATE` - Creating new tables/databases\n\n### Recommended Database Setup\n\nFor enhanced security, create a dedicated read-only user for the MCP server:\n\n```sql\n-- Create a read-only user\nCREATE USER 'claude_readonly'@'localhost' IDENTIFIED BY 'secure_password';\n\n-- Grant only SELECT permissions on your specific database\nGRANT SELECT ON your_database_name.* TO 'claude_readonly'@'localhost';\n\n-- Apply the changes\nFLUSH PRIVILEGES;\n```\n\n## Troubleshooting\n\n### Connection Issues\n\n1. **Verify MySQL is running**: Check if your MySQL server is active\n2. **Check credentials**: Ensure username/password are correct\n3. **Network connectivity**: Confirm Claude Desktop can reach your MySQL server\n\n### Configuration Issues\n\n1. **Restart required**: Always restart Claude Desktop after configuration changes\n2. **Path accuracy**: Ensure the absolute path to `dist/index.js` is correct\n3. **JSON syntax**: Validate your `claude_desktop_config.json` format\n\n### Debug Mode\n\nTo see server logs, you can run the server manually:\n\n```bash\nnode dist/index.js\n```\n\n## File Structure\n\n```bash\n~/mcp-servers/mysql/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ dist/\n‚îÇ   ‚îú‚îÄ‚îÄ index.js\n‚îÇ   ‚îî‚îÄ‚îÄ index.d.ts\n‚îú‚îÄ‚îÄ package.json\n‚îú‚îÄ‚îÄ tsconfig.json\n‚îî‚îÄ‚îÄ node_modules/\n```\n\n## Dependencies\n\n- **@modelcontextprotocol/sdk**: MCP protocol implementation\n- **mysql2**: Modern MySQL client for Node.js with Promise support\n\n## Contributing\n\nFeel free to submit issues and enhancement requests!\n\n## License\n\nThis project is open source and available under the [MIT License](LICENSE).\n\n## Support\n\nIf you encounter issues:\n\n1. Check the troubleshooting section above\n2.",
        "start_pos": 8900,
        "end_pos": 10940,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "4fee39f7a4b9f59a"
      },
      {
        "chunk_id": 6,
        "text": "and enhancement requests!\n\n## License\n\nThis project is open source and available under the [MIT License](LICENSE).\n\n## Support\n\nIf you encounter issues:\n\n1. Check the troubleshooting section above\n2. Verify your MySQL connection independently\n3. Ensure Claude Desktop is updated to the latest version\n4. Review the Claude Desktop MCP documentation\n\n---\n\n**Note**: This server is designed for development and analysis purposes. For production use, consider additional security measures and monitoring.",
        "start_pos": 10740,
        "end_pos": 11242,
        "token_count_estimate": 125,
        "source_type": "readme",
        "agent_id": "4fee39f7a4b9f59a"
      }
    ]
  },
  {
    "agent_id": "1621666ba4fa295e",
    "name": "capital.hove/read-only-local-postgres-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/hovecapital/read-only-local-postgres-mcp-server",
    "description": "MCP server for read-only PostgreSQL database queries in Claude Desktop",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-01-29T08:50:16.035232Z",
    "indexed_at": "2026-02-18T04:10:57.483394",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# PostgreSQL MCP Server\n\n[![MCP Registry](https://img.shields.io/badge/MCP_Registry-Available-blue)](https://registry.modelcontextprotocol.io)\n[![npm version](https://img.shields.io/npm/v/@hovecapital/read-only-postgres-mcp-server.svg)](https://www.npmjs.com/package/@hovecapital/read-only-postgres-mcp-server)\n\nA Model Context Protocol (MCP) server that enables Claude Desktop to interact with PostgreSQL databases through natural language queries.\n\n## Features\n\n- Execute read-only SQL queries through Claude Desktop or Claude Code\n- **Dynamic database connections** - connect to any PostgreSQL database at runtime\n- Built-in security with query validation (only SELECT statements allowed)\n- Easy integration with Claude Desktop and Claude Code\n- JSON formatted query results\n- Environment-based default configuration with runtime override support\n\n## Quick Start\n\n### For Claude Code Users (Recommended - Easiest Method)\n\n```bash\nclaude mcp add postgres -s user -- npx -y @hovecapital/read-only-postgres-mcp-server\n```\n\nThen set your database environment variables:\n\n```bash\nexport DB_HOST=localhost\nexport DB_PORT=5432\nexport DB_DATABASE=your_database_name\nexport DB_USERNAME=your_username\nexport DB_PASSWORD=your_password\n```\n\n**Done!** Restart Claude Code and ask: \"What tables are in my database?\"\n\n### For Claude Desktop Users (Manual Configuration)\n\n**1. Open your config file:**\n\n```bash\n# macOS\nopen ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n\n# Windows\nnotepad %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n**2. Add this configuration:**\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@hovecapital/read-only-postgres-mcp-server\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"5432\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n**3. Save, restart Claude Desktop, and test!**\n\n## Prerequisites\n\n- Node.js (v16 or higher) - If using mise, update the command path accordingly\n- PostgreSQL database server\n- Claude Desktop application\n\n## Installation\n\n### Option 1: Install from MCP Registry (Recommended)\n\nThis server is published in the [Model Context Protocol Registry](https://registry.modelcontextprotocol.io) as `capital.hove/read-only-local-postgres-mcp-server`.\n\n#### Method A: Claude Code CLI (Easiest!)\n\n```bash\nclaude mcp add postgres -s user -- npx -y @hovecapital/read-only-postgres-mcp-server\n```\n\nThen configure your database credentials using environment variables. Restart Claude Code and you're done!\n\n**Benefits:**\n\n- One command installation\n- No manual JSON editing\n- Automatic configuration\n\n#### Method B: Manual JSON Configuration\n\n**For Claude Desktop:**\n\nEdit `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows):\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@hovecapital/read-only-postgres-mcp-server\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"5432\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n**For Claude Code:**\n\nEdit `~/.config/claude-code/settings.json` (macOS/Linux) or `%APPDATA%\\claude-code\\settings.json` (Windows):\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"postgres\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@hovecapital/read-only-postgres-mcp-server\"],\n        \"env\": {\n          \"DB_HOST\": \"localhost\",\n          \"DB_PORT\": \"5432\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Option 2: Install from npm\n\n```bash\nnpm install -g @hovecapital/read-only-postgres-mcp-server\n```\n\n### Option 3: Installation with Claude Code\n\nIf you're using Claude Code, you can easily install this MCP server:\n\n```bash\n# Clone the repository\ngit clone https://github.com/hovecapital/read-only-local-postgres-mcp-server.git\ncd read-only-local-postgres-mcp-server\n\n# Install dependencies and build\nnpm install\nnpm run build\n```\n\nThen configure Claude Code by adding to your MCP settings.\n\n### Option 4: Manual Installation\n\n#### 1. Clone or Download\n\nSave the repository to a directory on your system:\n\n```bash\nmkdir ~/mcp-servers/postgres\ncd ~/mcp-servers/postgres\ngit clone https://github.com/hovecapital/read-only-local-postgres-mcp-server.git .\n```\n\n#### 2. Install Dependencies\n\n```bash\nnpm install\nnpm run build\n```\n\n## Configuration\n\n> **Note:** If you installed via Option 1 (MCP Registry with npx), you've already configured everything! This section is for users who chose Options 2, 3, or 4 (npm or manual installation).\n\n### Claude Code Configuration\n\nIf you're using Claude Code with a manual installation, add the PostgreSQL server to your MCP settings:\n\n1. Open your Claude Code settings (typically in `~/.config/claude-code/settings.json` on macOS/Linux or `%APPDATA%\\claude-code\\settings.json` on Windows)\n\n2. Add the PostgreSQL MCP server configuration:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"postgres\": {\n        \"command\": \"node\",\n        \"args\": [\"/absolute/path/to/read-only-local-postgres-mcp-server/dist/index.js\"],\n        \"env\": {\n          \"DB_HOST\": \"localhost\",\n          \"DB_PORT\": \"5432\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n1. Restart Claude Code for the changes to take effect.\n\n### Claude Desktop Configuration\n\nIf you're using Claude Desktop with a manual installation, open your Claude Desktop configuration file:\n\n**macOS:**\n\n```bash\n~/Library/Application Support/Claude/claude_desktop_config.json\n```\n\n**Windows:**\n\n```bash\n%APPDATA%\\Claude\\claude_desktop_config.json\n```\n\nAdd the PostgreSQL server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/read-only-local-postgres-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"5432\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n### Using mise for Node.js\n\nIf you're using [mise](https://mise.jdx.dev/) for Node.js version management, make sure to use the full path to the Node.js executable in your configuration.\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `DB_HOST` | PostgreSQL server hostname | `localhost` |\n| `DB_PORT` | PostgreSQL server port | `5432` |\n| `DB_DATABASE` | Database name | `postgres` |\n| `DB_USERNAME` | PostgreSQL username | `postgres` |\n| `DB_PASSWORD` | PostgreSQL password | (empty) |\n| `DB_SSL` | Enable SSL connection | `false` |\n\n## Tools\n\nThis MCP server exposes three tools that Claude can use to interact with PostgreSQL databases.\n\n### `connect`\n\nConnect to a PostgreSQL database using a connection string. The connection persists for subsequent queries until changed or disconnected.\n\n**Parameters:**\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `connectionString` | string | Yes | PostgreSQL connection string |\n\n**Connection String Format:**\n\n```\npostgres://username:password@host:port/database?sslmode=require\npostgresql://username:password@host:port/database\n```\n\n**SSL Modes Supported:**\n\n- `sslmode=require` - Require SSL (recommended for remote connections)\n- `sslmode=verify-full` - Require SSL with certificate verification\n- No sslmode parameter - No SSL (for local connections)\n\n**Example Usage (natural language):**\n\n```\n\"Connect to postgres://myuser:mypass@db.example.com:5432/production\"\n\"Connect to this database: postgres://admin:secret@localhost/analytics\"\n```\n\n**Response:**\n\n```json\n{\n  \"status\": \"connected\",\n  \"host\": \"db.example.com\",\n  \"port\": 5432,\n  \"database\": \"production\",\n  \"user\": \"myuser\",\n  \"ssl\": true\n}\n```\n\n---\n\n### `disconnect`\n\nDisconnect from the current runtime database and revert to the default environment-configured connection.\n\n**Parameters:** None\n\n**Example Usage (natural language):**\n\n```\n\"Disconnect from the current database\"\n\"Go back to the default database\"\n```\n\n**Response:**\n\n```json\n{\n  \"status\": \"disconnected\",\n  \"message\": \"Reverted to default environment connection\",\n  \"host\": \"localhost\",\n  \"database\": \"postgres\"\n}\n```\n\n---\n\n### `query`\n\nRun a read-only SQL query against the currently connected database. Optionally override the connection for a single query.\n\n**Parameters:**\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `sql` | string | Yes | SQL query to execute (SELECT only) |\n| `connectionString` | string | No | Override connection for this query only |\n\n**Example Usage (natural language):**\n\n```\n\"Show me all tables in the database\"\n\"SELECT * FROM users LIMIT 10\"\n\"Run this query on postgres://other:pass@host/db: SELECT count(*) FROM orders\"\n```\n\n**Response:**\n\n```json\n[\n  { \"id\": 1, \"name\": \"Alice\", \"email\": \"alice@example.com\" },\n  { \"id\": 2, \"name\": \"Bob\", \"email\": \"bob@example.com\" }\n]\n```\n\n---\n\n### Tool Reference for LLMs\n\nWhen using this MCP server, Claude can:\n\n1. **Query the default database** (configured via environment variables):\n\n   ```\n   User: \"What tables are in my database?\"\n   Claude: [Uses query tool with SQL: \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\"]\n   ```\n\n2. **Connect to a different database dynamically**:\n\n   ```\n   User: \"Connect to postgres://user:pass@newhost/newdb and show me the users table\"\n   Claude: [Uses connect tool first, then query tool]\n   ```\n\n3. **One-off query to a different database** (without switching active connection):\n\n   ```\n   User: \"How many records are in the orders table on postgres://user:pass@analytics/warehouse?\"\n   Claude: [Uses query tool with connectionString parameter]\n   ```\n\n4. **Revert to default connection**:\n\n   ```\n   User: \"Go back to my local database\"\n   Claude: [Uses disconnect tool]\n   ```\n\n## Usage\n\n1. **Restart Claude Desktop/Code** after updating the configuration\n2. **Start chatting** with Claude about your database\n\n### Example Queries\n\n**Basic queries (uses default/active connection):**\n\n```\n\"Show me all tables in my database\"\n\"What's the structure of the users table?\"\n\"Get the first 10 records from the products table\"\n\"How many orders were placed last month?\"\n\"Show me users with email addresses ending in @gmail.com\"\n```\n\n**Dynamic connection examples:**\n\n```\n\"Connect to postgres://analyst:password@analytics.example.com:5432/warehouse\"\n\"Now show me all the tables\"\n\"What's the total revenue in the sales table?\"\n\"Disconnect and go back to my local database\"\n```\n\n**One-off queries to different databases:**\n\n```\n\"Run SELECT count(*) FROM users on postgres://admin:secret@prod.example.com/app\"\n\"Check the orders table on my staging database: postgres://dev:dev@staging/app\"\n```\n\nClaude will automatically convert your natural language requests into appropriate SQL queries and execute them against your database.\n\n## Security Features\n\n### Read-Only Operations\n\nThe server enforces read-only access on **all connections** (both environment-configured and runtime dynamic connections). The following operations are blocked:\n\n- `INSERT` - Adding new records\n- `UPDATE` - Modifying existing records\n- `DELETE` - Removing records\n- `DROP` - Removing tables/databases\n- `ALTER` - Modifying table structure\n- `CREATE` - Creating new tables/databases\n- `TRUNCATE` - Removing all records from a table\n- `GRANT` - Modifying permissions\n- `REVOKE` - Removing permissions\n\n### Dynamic Connection Security\n\nWhen using the `connect` tool or `connectionString` parameter:\n\n- **Read-only enforcement still applies** - All queries are validated regardless of connection source\n- **Credentials are not logged** - Connection strings with passwords are never written to logs\n- **Sanitized responses** - The `connect` tool response excludes passwords\n- **Session-based** - Runtime connections only persist for the current MCP session\n\n### Recommended Database Setup\n\nFor enhanced security, create a dedicated read-only user for the MCP server:\n\n```sql\n-- Create a read-only user\nCREATE USER claude_readonly WITH PASSWORD 'secure_password';\n\n-- Grant only SELECT permissions on your specific schema\nGRANT USAGE ON SCHEMA public TO claude_readonly;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO claude_readonly;\n\n-- Grant permissions for future tables (optional)\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO claude_readonly;\n```\n\n## Troubleshooting\n\n### Connection Issues\n\n1. **Verify PostgreSQL is running**: Check if your PostgreSQL server is active\n2. **Check credentials**: Ensure username/password are correct\n3. **Network connectivity**: Confirm Claude Desktop can reach your PostgreSQL server\n\n### Configuration Issues\n\n1. **Restart required**: Always restart Claude Desktop after configuration changes\n2. **Path accuracy**: Ensure the absolute path to `dist/index.js` is correct\n3. **JSON syntax**: Validate your `claude_desktop_config.json` format\n\n### Debug Mode\n\nTo see server logs, you can run the server manually:\n\n```bash\nnode dist/index.js\n```\n\n## File Structure\n\n```bash\n~/mcp-servers/postgres/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ dist/\n‚îÇ   ‚îú‚îÄ‚îÄ index.js\n‚îÇ   ‚îî‚îÄ‚îÄ index.d.ts\n‚îú‚îÄ‚îÄ package.json\n‚îú‚îÄ‚îÄ tsconfig.json\n‚îî‚îÄ‚îÄ node_modules/\n```\n\n## Dependencies\n\n- **@modelcontextprotocol/sdk**: MCP protocol implementation\n- **pg**: PostgreSQL client for Node.js\n\n## Contributing\n\nFeel free to submit issues and enhancement requests!\n\n## License\n\nThis project is open source and available under the [MIT License](LICENSE).\n\n## Support\n\nIf you encounter issues:\n\n1. Check the troubleshooting section above\n2. Verify your PostgreSQL connection independently\n3. Ensure Claude Desktop is updated to the latest version\n4. Review the Claude Desktop MCP documentation\n\n---\n\n**Note**: This server is designed for development and analysis purposes. For production use, consider additional security measures and monitoring.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Execute read-only SQL SELECT queries on PostgreSQL databases",
        "Dynamically connect to any PostgreSQL database at runtime using connection strings",
        "Disconnect from the current database and revert to default environment-configured connection",
        "Return query results in JSON format",
        "Integrate seamlessly with Claude Desktop and Claude Code",
        "Validate queries to enforce read-only access and block any data modification commands",
        "Support SSL connections with multiple sslmode options",
        "Allow one-off queries to override the active connection without changing it"
      ],
      "limitations": [
        "Cannot execute any SQL commands other than SELECT (no INSERT, UPDATE, DELETE, DROP, ALTER, CREATE, TRUNCATE, GRANT, REVOKE)",
        "Requires PostgreSQL database server; does not support other database types",
        "Read-only access enforced on all connections, including dynamic ones",
        "No mention of support for complex query features like stored procedures or functions",
        "No built-in support for query rate limiting or concurrency controls"
      ],
      "requirements": [
        "Node.js version 16 or higher",
        "PostgreSQL database server accessible with valid credentials",
        "Claude Desktop or Claude Code application for integration",
        "Environment variables or configuration file setup for DB_HOST, DB_PORT, DB_DATABASE, DB_USERNAME, DB_PASSWORD",
        "Optional: SSL configuration via DB_SSL environment variable"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, explicit tool descriptions, configuration guidance, security limitations, and environment requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# PostgreSQL MCP Server\n\n[![MCP Registry](https://img.shields.io/badge/MCP_Registry-Available-blue)](https://registry.modelcontextprotocol.io)\n[![npm version](https://img.shields.io/npm/v/@hovecapital/read-only-postgres-mcp-server.svg)](https://www.npmjs.com/package/@hovecapital/read-only-postgres-mcp-server)\n\nA Model Context Protocol (MCP) server that enables Claude Desktop to interact with PostgreSQL databases through natural language queries.\n\n## Features\n\n- Execute read-only SQL queries through Claude Desktop or Claude Code\n- **Dynamic database connections** - connect to any PostgreSQL database at runtime\n- Built-in security with query validation (only SELECT statements allowed)\n- Easy integration with Claude Desktop and Claude Code\n- JSON formatted query results\n- Environment-based default configuration with runtime override support\n\n## Quick Start\n\n### For Claude Code Users (Recommended - Easiest Method)\n\n```bash\nclaude mcp add postgres -s user -- npx -y @hovecapital/read-only-postgres-mcp-server\n```\n\nThen set your database environment variables:\n\n```bash\nexport DB_HOST=localhost\nexport DB_PORT=5432\nexport DB_DATABASE=your_database_name\nexport DB_USERNAME=your_username\nexport DB_PASSWORD=your_password\n```\n\n**Done!** Restart Claude Code and ask: \"What tables are in my database?\"\n\n### For Claude Desktop Users (Manual Configuration)\n\n**1. Open your config file:**\n\n```bash\n# macOS\nopen ~/Library/Application\\ Support/Claude/claude_desktop_config.json\n\n# Windows\nnotepad %APPDATA%\\Claude\\claude_desktop_config.json\n```\n\n**2. Add this configuration:**\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@hovecapital/read-only-postgres-mcp-server\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"5432\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n**3.",
        "start_pos": 0,
        "end_pos": 1941,
        "token_count_estimate": 485,
        "source_type": "readme",
        "agent_id": "1621666ba4fa295e"
      },
      {
        "chunk_id": 1,
        "text": "HOST\": \"localhost\",\n        \"DB_PORT\": \"5432\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n**3. Save, restart Claude Desktop, and test!**\n\n## Prerequisites\n\n- Node.js (v16 or higher) - If using mise, update the command path accordingly\n- PostgreSQL database server\n- Claude Desktop application\n\n## Installation\n\n### Option 1: Install from MCP Registry (Recommended)\n\nThis server is published in the [Model Context Protocol Registry](https://registry.modelcontextprotocol.io) as `capital.hove/read-only-local-postgres-mcp-server`.\n\n#### Method A: Claude Code CLI (Easiest!)\n\n```bash\nclaude mcp add postgres -s user -- npx -y @hovecapital/read-only-postgres-mcp-server\n```\n\nThen configure your database credentials using environment variables. Restart Claude Code and you're done!",
        "start_pos": 1741,
        "end_pos": 2624,
        "token_count_estimate": 220,
        "source_type": "readme",
        "agent_id": "1621666ba4fa295e"
      },
      {
        "chunk_id": 2,
        "text": "{\n          \"DB_HOST\": \"localhost\",\n          \"DB_PORT\": \"5432\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Option 2: Install from npm\n\n```bash\nnpm install -g @hovecapital/read-only-postgres-mcp-server\n```\n\n### Option 3: Installation with Claude Code\n\nIf you're using Claude Code, you can easily install this MCP server:\n\n```bash\n# Clone the repository\ngit clone https://github.com/hovecapital/read-only-local-postgres-mcp-server.git\ncd read-only-local-postgres-mcp-server\n\n# Install dependencies and build\nnpm install\nnpm run build\n```\n\nThen configure Claude Code by adding to your MCP settings.\n\n### Option 4: Manual Installation\n\n#### 1. Clone or Download\n\nSave the repository to a directory on your system:\n\n```bash\nmkdir ~/mcp-servers/postgres\ncd ~/mcp-servers/postgres\ngit clone https://github.com/hovecapital/read-only-local-postgres-mcp-server.git .\n```\n\n#### 2. Install Dependencies\n\n```bash\nnpm install\nnpm run build\n```\n\n## Configuration\n\n> **Note:** If you installed via Option 1 (MCP Registry with npx), you've already configured everything! This section is for users who chose Options 2, 3, or 4 (npm or manual installation).\n\n### Claude Code Configuration\n\nIf you're using Claude Code with a manual installation, add the PostgreSQL server to your MCP settings:\n\n1. Open your Claude Code settings (typically in `~/.config/claude-code/settings.json` on macOS/Linux or `%APPDATA%\\claude-code\\settings.json` on Windows)\n\n2. Add the PostgreSQL MCP server configuration:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"postgres\": {\n        \"command\": \"node\",\n        \"args\": [\"/absolute/path/to/read-only-local-postgres-mcp-server/dist/index.js\"],\n        \"env\": {\n          \"DB_HOST\": \"localhost\",\n          \"DB_PORT\": \"5432\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n1.",
        "start_pos": 3589,
        "end_pos": 5617,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "1621666ba4fa295e"
      },
      {
        "chunk_id": 3,
        "text": "t\",\n          \"DB_PORT\": \"5432\",\n          \"DB_DATABASE\": \"your_database_name\",\n          \"DB_USERNAME\": \"your_username\",\n          \"DB_PASSWORD\": \"your_password\"\n        }\n      }\n    }\n  }\n}\n```\n\n1. Restart Claude Code for the changes to take effect.\n\n### Claude Desktop Configuration\n\nIf you're using Claude Desktop with a manual installation, open your Claude Desktop configuration file:\n\n**macOS:**\n\n```bash\n~/Library/Application Support/Claude/claude_desktop_config.json\n```\n\n**Windows:**\n\n```bash\n%APPDATA%\\Claude\\claude_desktop_config.json\n```\n\nAdd the PostgreSQL server configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"postgres\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/read-only-local-postgres-mcp-server/dist/index.js\"],\n      \"env\": {\n        \"DB_HOST\": \"localhost\",\n        \"DB_PORT\": \"5432\",\n        \"DB_DATABASE\": \"your_database_name\",\n        \"DB_USERNAME\": \"your_username\",\n        \"DB_PASSWORD\": \"your_password\"\n      }\n    }\n  }\n}\n```\n\n### Using mise for Node.js\n\nIf you're using [mise](https://mise.jdx.dev/) for Node.js version management, make sure to use the full path to the Node.js executable in your configuration.\n\n### Environment Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `DB_HOST` | PostgreSQL server hostname | `localhost` |\n| `DB_PORT` | PostgreSQL server port | `5432` |\n| `DB_DATABASE` | Database name | `postgres` |\n| `DB_USERNAME` | PostgreSQL username | `postgres` |\n| `DB_PASSWORD` | PostgreSQL password | (empty) |\n| `DB_SSL` | Enable SSL connection | `false` |\n\n## Tools\n\nThis MCP server exposes three tools that Claude can use to interact with PostgreSQL databases.\n\n### `connect`\n\nConnect to a PostgreSQL database using a connection string. The connection persists for subsequent queries until changed or disconnected.",
        "start_pos": 5417,
        "end_pos": 7242,
        "token_count_estimate": 456,
        "source_type": "readme",
        "agent_id": "1621666ba4fa295e"
      },
      {
        "chunk_id": 4,
        "text": "rameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| `connectionString` | string | Yes | PostgreSQL connection string |\n\n**Connection String Format:**\n\n```\npostgres://username:password@host:port/database?sslmode=require\npostgresql://username:password@host:port/database\n```\n\n**SSL Modes Supported:**\n\n- `sslmode=require` - Require SSL (recommended for remote connections)\n- `sslmode=verify-full` - Require SSL with certificate verification\n- No sslmode parameter - No SSL (for local connections)\n\n**Example Usage (natural language):**\n\n```\n\"Connect to postgres://myuser:mypass@db.example.com:5432/production\"\n\"Connect to this database: postgres://admin:secret@localhost/analytics\"\n```\n\n**Response:**\n\n```json\n{\n  \"status\": \"connected\",\n  \"host\": \"db.example.com\",\n  \"port\": 5432,\n  \"database\": \"production\",\n  \"user\": \"myuser\",\n  \"ssl\": true\n}\n```\n\n---\n\n### `disconnect`\n\nDisconnect from the current runtime database and revert to the default environment-configured connection.\n\n**Parameters:** None\n\n**Example Usage (natural language):**\n\n```\n\"Disconnect from the current database\"\n\"Go back to the default database\"\n```\n\n**Response:**\n\n```json\n{\n  \"status\": \"disconnected\",\n  \"message\": \"Reverted to default environment connection\",\n  \"host\": \"localhost\",\n  \"database\": \"postgres\"\n}\n```\n\n---\n\n### `query`\n\nRun a read-only SQL query against the currently connected database. Optionally override the connection for a single query.",
        "start_pos": 7265,
        "end_pos": 8731,
        "token_count_estimate": 366,
        "source_type": "readme",
        "agent_id": "1621666ba4fa295e"
      },
      {
        "chunk_id": 5,
        "text": "postgres://other:pass@host/db: SELECT count(*) FROM orders\"\n```\n\n**Response:**\n\n```json\n[\n  { \"id\": 1, \"name\": \"Alice\", \"email\": \"alice@example.com\" },\n  { \"id\": 2, \"name\": \"Bob\", \"email\": \"bob@example.com\" }\n]\n```\n\n---\n\n### Tool Reference for LLMs\n\nWhen using this MCP server, Claude can:\n\n1. **Query the default database** (configured via environment variables):\n\n   ```\n   User: \"What tables are in my database?\"\n   Claude: [Uses query tool with SQL: \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\"]\n   ```\n\n2. **Connect to a different database dynamically**:\n\n   ```\n   User: \"Connect to postgres://user:pass@newhost/newdb and show me the users table\"\n   Claude: [Uses connect tool first, then query tool]\n   ```\n\n3. **One-off query to a different database** (without switching active connection):\n\n   ```\n   User: \"How many records are in the orders table on postgres://user:pass@analytics/warehouse?\"\n   Claude: [Uses query tool with connectionString parameter]\n   ```\n\n4. **Revert to default connection**:\n\n   ```\n   User: \"Go back to my local database\"\n   Claude: [Uses disconnect tool]\n   ```\n\n## Usage\n\n1. **Restart Claude Desktop/Code** after updating the configuration\n2.",
        "start_pos": 9113,
        "end_pos": 10330,
        "token_count_estimate": 304,
        "source_type": "readme",
        "agent_id": "1621666ba4fa295e"
      },
      {
        "chunk_id": 6,
        "text": "ifferent databases:**\n\n```\n\"Run SELECT count(*) FROM users on postgres://admin:secret@prod.example.com/app\"\n\"Check the orders table on my staging database: postgres://dev:dev@staging/app\"\n```\n\nClaude will automatically convert your natural language requests into appropriate SQL queries and execute them against your database.\n\n## Security Features\n\n### Read-Only Operations\n\nThe server enforces read-only access on **all connections** (both environment-configured and runtime dynamic connections). The following operations are blocked:\n\n- `INSERT` - Adding new records\n- `UPDATE` - Modifying existing records\n- `DELETE` - Removing records\n- `DROP` - Removing tables/databases\n- `ALTER` - Modifying table structure\n- `CREATE` - Creating new tables/databases\n- `TRUNCATE` - Removing all records from a table\n- `GRANT` - Modifying permissions\n- `REVOKE` - Removing permissions\n\n### Dynamic Connection Security\n\nWhen using the `connect` tool or `connectionString` parameter:\n\n- **Read-only enforcement still applies** - All queries are validated regardless of connection source\n- **Credentials are not logged** - Connection strings with passwords are never written to logs\n- **Sanitized responses** - The `connect` tool response excludes passwords\n- **Session-based** - Runtime connections only persist for the current MCP session\n\n### Recommended Database Setup\n\nFor enhanced security, create a dedicated read-only user for the MCP server:\n\n```sql\n-- Create a read-only user\nCREATE USER claude_readonly WITH PASSWORD 'secure_password';\n\n-- Grant only SELECT permissions on your specific schema\nGRANT USAGE ON SCHEMA public TO claude_readonly;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO claude_readonly;\n\n-- Grant permissions for future tables (optional)\nALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO claude_readonly;\n```\n\n## Troubleshooting\n\n### Connection Issues\n\n1. **Verify PostgreSQL is running**: Check if your PostgreSQL server is active\n2. **Check credentials**: Ensure username/password are correct\n3.",
        "start_pos": 10961,
        "end_pos": 12994,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "1621666ba4fa295e"
      },
      {
        "chunk_id": 7,
        "text": "adonly;\n```\n\n## Troubleshooting\n\n### Connection Issues\n\n1. **Verify PostgreSQL is running**: Check if your PostgreSQL server is active\n2. **Check credentials**: Ensure username/password are correct\n3. **Network connectivity**: Confirm Claude Desktop can reach your PostgreSQL server\n\n### Configuration Issues\n\n1. **Restart required**: Always restart Claude Desktop after configuration changes\n2. **Path accuracy**: Ensure the absolute path to `dist/index.js` is correct\n3. **JSON syntax**: Validate your `claude_desktop_config.json` format\n\n### Debug Mode\n\nTo see server logs, you can run the server manually:\n\n```bash\nnode dist/index.js\n```\n\n## File Structure\n\n```bash\n~/mcp-servers/postgres/\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îî‚îÄ‚îÄ index.ts\n‚îú‚îÄ‚îÄ dist/\n‚îÇ   ‚îú‚îÄ‚îÄ index.js\n‚îÇ   ‚îî‚îÄ‚îÄ index.d.ts\n‚îú‚îÄ‚îÄ package.json\n‚îú‚îÄ‚îÄ tsconfig.json\n‚îî‚îÄ‚îÄ node_modules/\n```\n\n## Dependencies\n\n- **@modelcontextprotocol/sdk**: MCP protocol implementation\n- **pg**: PostgreSQL client for Node.js\n\n## Contributing\n\nFeel free to submit issues and enhancement requests!\n\n## License\n\nThis project is open source and available under the [MIT License](LICENSE).\n\n## Support\n\nIf you encounter issues:\n\n1. Check the troubleshooting section above\n2. Verify your PostgreSQL connection independently\n3. Ensure Claude Desktop is updated to the latest version\n4. Review the Claude Desktop MCP documentation\n\n---\n\n**Note**: This server is designed for development and analysis purposes. For production use, consider additional security measures and monitoring.",
        "start_pos": 12794,
        "end_pos": 14287,
        "token_count_estimate": 373,
        "source_type": "readme",
        "agent_id": "1621666ba4fa295e"
      }
    ]
  },
  {
    "agent_id": "1953e78e57a2fb93",
    "name": "capital.hove/read-only-mysql-mcp-server",
    "source": "mcp",
    "source_url": "",
    "description": "MCP server for read-only MySQL database queries in Claude Desktop",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-11-03T10:20:41.823143Z",
    "indexed_at": "2026-02-18T04:10:58.839714",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Execute read-only MySQL database queries",
        "Integrate with Claude Desktop for database access"
      ],
      "limitations": [
        "Cannot perform write or update operations on the database"
      ],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic understanding of the server's purpose and scope but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "0de65f801191a774",
    "name": "casa.find-me.sub1/send-email-mcp",
    "source": "mcp",
    "source_url": "https://sub1.find-me.casa",
    "description": "Non functional server (yet)",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-11-26T10:28:28.126971Z",
    "indexed_at": "2026-02-18T04:10:58.860368",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.0,
    "quality_rationale": "The documentation consists of a single non-informative sentence, providing no details on capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "0de65f801191a774",
    "name": "casa.find-me.sub1/send-email-mcp",
    "source": "mcp",
    "source_url": "https://alpic.ai/",
    "description": "Non functional server (yet)",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-11-28T13:54:25.882327Z",
    "indexed_at": "2026-02-18T04:11:13.709132",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "detail_page": "Or get inspired by the teams who love us.\nKiwi.com rethought flight booking for a world where AI agents are the primary interface. By building an MCP-native experience, they made live travel search usable, reliable, and agent-friendly. With Alpic, they shipped one of the first production-ready flight booking apps for ChatGPT.\nRead more\nTwelveLabs worked with Alpic to build and deploy its MCP Server, a standardized interface that lets AI agents access its video understanding capabilities. This enables applications to search, summarize, and reason over video content without custom integrations, bringing video intelligence into AI workflows.\nRead more\nBitmovin builds video streaming infrastructure for teams that care about quality, observability, and scale. As AI copilots become part of daily workflows, Bitmovin aimed to make its encoding, playback, and analytics available directly within those tools, without separate dashboards or custom integrations.\nRead more\nClarifeye needed a scalable way to deploy its AI teammates into customers√¢¬Ä¬ô AI environments without rebuilding custom frontends or duplicating infrastructure. By leveraging MCP with Alpic, Clarifeye delivers secure, auditable, and explainable AI behavior directly where users already work.\nRead more\nKiwi.com rethought flight booking for a world where AI agents are the primary interface. By building an MCP-native experience, they made live travel search usable, reliable, and agent-friendly. With Alpic, they shipped one of the first production-ready flight booking apps for ChatGPT.\nRead more\nTwelveLabs worked with Alpic to build and deploy its MCP Server, a standardized interface that lets AI agents access its video understanding capabilities. This enables applications to search, summarize, and reason over video content without custom integrations, bringing video intelligence into AI workflows.\nRead more\nBitmovin builds video streaming infrastructure for teams that care about quality, observability, and scale. As AI copilots become part of daily workflows, Bitmovin aimed to make its encoding, playback, and analytics available directly within those tools, without separate dashboards or custom integrations.\nRead more\nClarifeye needed a scalable way to deploy its AI teammates into customers√¢¬Ä¬ô AI environments without rebuilding custom frontends or duplicating infrastructure. By leveraging MCP with Alpic, Clarifeye delivers secure, auditable, and explainable AI behavior directly where users already work.\nRead more"
    },
    "llm_extracted": {
      "capabilities": [],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The documentation provides only high-level marketing descriptions without specific details, examples, or technical information about the server's capabilities, limitations, or requirements.",
    "llm_text_source": "detail_page",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "Or get inspired by the teams who love us.\nKiwi.com rethought flight booking for a world where AI agents are the primary interface. By building an MCP-native experience, they made live travel search usable, reliable, and agent-friendly. With Alpic, they shipped one of the first production-ready flight booking apps for ChatGPT.\nRead more\nTwelveLabs worked with Alpic to build and deploy its MCP Server, a standardized interface that lets AI agents access its video understanding capabilities. This enables applications to search, summarize, and reason over video content without custom integrations, bringing video intelligence into AI workflows.\nRead more\nBitmovin builds video streaming infrastructure for teams that care about quality, observability, and scale. As AI copilots become part of daily workflows, Bitmovin aimed to make its encoding, playback, and analytics available directly within those tools, without separate dashboards or custom integrations.\nRead more\nClarifeye needed a scalable way to deploy its AI teammates into customers√¢¬Ä¬ô AI environments without rebuilding custom frontends or duplicating infrastructure. By leveraging MCP with Alpic, Clarifeye delivers secure, auditable, and explainable AI behavior directly where users already work.\nRead more\nKiwi.com rethought flight booking for a world where AI agents are the primary interface. By building an MCP-native experience, they made live travel search usable, reliable, and agent-friendly. With Alpic, they shipped one of the first production-ready flight booking apps for ChatGPT.\nRead more\nTwelveLabs worked with Alpic to build and deploy its MCP Server, a standardized interface that lets AI agents access its video understanding capabilities. This enables applications to search, summarize, and reason over video content without custom integrations, bringing video intelligence into AI workflows.\nRead more\nBitmovin builds video streaming infrastructure for teams that care about quality, observability, and scale.",
        "start_pos": 0,
        "end_pos": 1997,
        "token_count_estimate": 499,
        "source_type": "detail_page",
        "agent_id": "0de65f801191a774"
      },
      {
        "chunk_id": 1,
        "text": "ontent without custom integrations, bringing video intelligence into AI workflows.\nRead more\nBitmovin builds video streaming infrastructure for teams that care about quality, observability, and scale. As AI copilots become part of daily workflows, Bitmovin aimed to make its encoding, playback, and analytics available directly within those tools, without separate dashboards or custom integrations.\nRead more\nClarifeye needed a scalable way to deploy its AI teammates into customers√¢¬Ä¬ô AI environments without rebuilding custom frontends or duplicating infrastructure. By leveraging MCP with Alpic, Clarifeye delivers secure, auditable, and explainable AI behavior directly where users already work.\nRead more",
        "start_pos": 1797,
        "end_pos": 2507,
        "token_count_estimate": 177,
        "source_type": "detail_page",
        "agent_id": "0de65f801191a774"
      }
    ]
  },
  {
    "agent_id": "24990660104d2224",
    "name": "ch.martinelli/jooq-mcp",
    "source": "mcp",
    "source_url": "https://github.com/martinellich/jooq-mcp",
    "description": "An MCP server that provides access to the jOOQ documentation",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-12T13:41:08.407071Z",
    "indexed_at": "2026-02-18T04:11:14.909738",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# jOOQ MCP Server\n\nA Model Context Protocol (MCP) server that provides AI models with access to jOOQ documentation. This Spring Boot application uses Spring AI to expose jOOQ documentation as MCP tools, allowing AI systems to query and retrieve information about jOOQ features, SQL examples, and best practices.\n\n## Features\n\nThe MCP server provides the following tools:\n\n- **searchDocumentation**: Search jOOQ documentation for specific topics\n- **getSqlExamples**: Get SQL query building examples for specific operations\n- **getCodeGenerationGuide**: Retrieve jOOQ code generation documentation\n- **getDatabaseSupport**: Get database-specific support information\n- **getQueryDslReference**: Get Query DSL reference for specific statement types\n- **getAdvancedFeatures**: Access documentation for advanced jOOQ features\n\n## Getting Started\n\n### Prerequisites\n\n- Java 21 or higher\n- Maven 3.6+\n\n### Running the Application\n\n1. **Clone the repository:**\n   ```bash\n   git clone <repository-url>\n   cd jooq-mcp\n   ```\n\n2. **Build the project:**\n   ```bash\n   ./mvnw clean install\n   ```\n\n3. **Run the application:**\n   ```bash\n   ./mvnw spring-boot:run\n   ```\n\nThe MCP server will start and be available for connections from MCP clients.\n\n### Docker Deployment\n\n1. **Build the JAR file:**\n   ```bash\n   ./mvnw clean package\n   ```\n\n2. **Build the Docker image:**\n   ```bash\n   docker build -t jooq-mcp .\n   ```\n\n3. **Run the Docker container:**\n   ```bash\n   docker run -p 8080:8080 jooq-mcp\n   ```\n\n### Fly.io Deployment\n\nThis application is configured for deployment on Fly.io:\n\n1. **Install Fly CLI and authenticate:**\n   ```bash\n   brew install flyctl  # or your preferred installation method\n   fly auth login\n   ```\n\n2. **Deploy the application:**\n   ```bash\n   ./mvnw clean package\n   fly deploy\n   ```\n\nThe application includes health checks at `/actuator/health` and is configured with auto-scaling.\n\n### Using with MCP Clients\n\nThis server can be used with any MCP-compatible AI client. The server exposes tools that allow AI models to:\n\n- Search jOOQ documentation by keyword\n- Retrieve specific SQL examples and code snippets\n- Access database-specific configuration information\n- Get guidance on code generation setup\n- Find information about advanced jOOQ features\n\n### Configuration\n\nThe application can be configured via `application.properties`:\n\n```properties\n# MCP Server Configuration\nspring.ai.mcp.server.name=jooq-documentation-mcp\nspring.ai.mcp.server.version=1.0.0\nspring.ai.mcp.server.type=SYNC\nspring.ai.mcp.server.capabilities.tool=true\n\n# Cache Configuration\nspring.cache.type=caffeine\nspring.cache.caffeine.spec=maximumSize=100,expireAfterWrite=1h\n\n# jOOQ Documentation Crawler Configuration\njooq.documentation.crawler.max-depth=4\njooq.documentation.crawler.max-urls-per-section=100\njooq.documentation.crawler.timeout-ms=10000\njooq.documentation.crawler.cache-duration-hours=24\n\n# Server Configuration - SSE Buffer Settings\nserver.tomcat.max-http-response-header-size=64KB\nserver.tomcat.max-swallow-size=10MB\n```\n\n### Example Usage\n\nWhen connected to an MCP client, you can ask questions like:\n\n- \"How do I create a SELECT statement in jOOQ?\"\n- \"Show me examples of jOOQ INSERT operations\"\n- \"What databases does jOOQ support?\"\n- \"How do I configure jOOQ code generation?\"\n- \"How do I use transactions in jOOQ?\"\n\nThe server will fetch the relevant documentation and provide detailed answers with code examples.\n\n## Architecture\n\nThe application consists of:\n\n- **JooqDocumentationService**: Main service class with @Tool annotated methods for MCP integration\n- **LocalJooqDocumentationService**: Provides local documentation indexing and full-text search with TF-IDF scoring\n- **InvertedIndex**: Implements advanced full-text search capabilities with relevance scoring\n- **JooqDocumentationCrawler**: Crawls and fetches jOOQ documentation for local storage\n- **JooqDocumentationFetcher**: Handles parsing and extraction of documentation content\n- **McpConfiguration**: Spring configuration for MCP tool registration\n- **Caching**: Caffeine-based caching to improve performance\n\n### Key Features\n\n- **Local Documentation Storage**: Documentation is stored locally in `src/main/resources/docs/` for faster access\n- **Full-Text Search**: Advanced search using TF-IDF scoring for better relevance\n- **Efficient Indexing**: In-memory inverted index for fast document retrieval\n- **Code Example Extraction**: Automatic extraction and categorization of code examples\n\n## Testing\n\nRun the test suite:\n\n```bash\n./mvnw test\n```\n\n## Development\n\nThe application uses:\n- Spring Boot 3.5.4\n- Spring AI 1.0.0 with MCP Server support\n- JSoup for HTML parsing\n- Caffeine for caching\n\nTo add new tools, create methods annotated with `@Tool` in the `JooqDocumentationService` class."
    },
    "llm_extracted": {
      "capabilities": [
        "Search jOOQ documentation for specific topics",
        "Retrieve SQL query building examples for specific operations",
        "Provide jOOQ code generation documentation",
        "Deliver database-specific support information",
        "Offer Query DSL reference for specific statement types",
        "Access documentation for advanced jOOQ features"
      ],
      "limitations": [],
      "requirements": [
        "Java 21 or higher",
        "Maven 3.6+",
        "MCP-compatible AI client for interaction"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation includes comprehensive installation instructions, usage examples, detailed tool descriptions, configuration options, architecture overview, and testing guidance, but does not explicitly mention limitations.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# jOOQ MCP Server\n\nA Model Context Protocol (MCP) server that provides AI models with access to jOOQ documentation. This Spring Boot application uses Spring AI to expose jOOQ documentation as MCP tools, allowing AI systems to query and retrieve information about jOOQ features, SQL examples, and best practices.\n\n## Features\n\nThe MCP server provides the following tools:\n\n- **searchDocumentation**: Search jOOQ documentation for specific topics\n- **getSqlExamples**: Get SQL query building examples for specific operations\n- **getCodeGenerationGuide**: Retrieve jOOQ code generation documentation\n- **getDatabaseSupport**: Get database-specific support information\n- **getQueryDslReference**: Get Query DSL reference for specific statement types\n- **getAdvancedFeatures**: Access documentation for advanced jOOQ features\n\n## Getting Started\n\n### Prerequisites\n\n- Java 21 or higher\n- Maven 3.6+\n\n### Running the Application\n\n1. **Clone the repository:**\n   ```bash\n   git clone <repository-url>\n   cd jooq-mcp\n   ```\n\n2. **Build the project:**\n   ```bash\n   ./mvnw clean install\n   ```\n\n3. **Run the application:**\n   ```bash\n   ./mvnw spring-boot:run\n   ```\n\nThe MCP server will start and be available for connections from MCP clients.\n\n### Docker Deployment\n\n1. **Build the JAR file:**\n   ```bash\n   ./mvnw clean package\n   ```\n\n2. **Build the Docker image:**\n   ```bash\n   docker build -t jooq-mcp .\n   ```\n\n3. **Run the Docker container:**\n   ```bash\n   docker run -p 8080:8080 jooq-mcp\n   ```\n\n### Fly.io Deployment\n\nThis application is configured for deployment on Fly.io:\n\n1. **Install Fly CLI and authenticate:**\n   ```bash\n   brew install flyctl  # or your preferred installation method\n   fly auth login\n   ```\n\n2. **Deploy the application:**\n   ```bash\n   ./mvnw clean package\n   fly deploy\n   ```\n\nThe application includes health checks at `/actuator/health` and is configured with auto-scaling.\n\n### Using with MCP Clients\n\nThis server can be used with any MCP-compatible AI client.",
        "start_pos": 0,
        "end_pos": 1994,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "24990660104d2224"
      },
      {
        "chunk_id": 1,
        "text": "deploy\n   ```\n\nThe application includes health checks at `/actuator/health` and is configured with auto-scaling.\n\n### Using with MCP Clients\n\nThis server can be used with any MCP-compatible AI client. The server exposes tools that allow AI models to:\n\n- Search jOOQ documentation by keyword\n- Retrieve specific SQL examples and code snippets\n- Access database-specific configuration information\n- Get guidance on code generation setup\n- Find information about advanced jOOQ features\n\n### Configuration\n\nThe application can be configured via `application.properties`:\n\n```properties\n# MCP Server Configuration\nspring.ai.mcp.server.name=jooq-documentation-mcp\nspring.ai.mcp.server.version=1.0.0\nspring.ai.mcp.server.type=SYNC\nspring.ai.mcp.server.capabilities.tool=true\n\n# Cache Configuration\nspring.cache.type=caffeine\nspring.cache.caffeine.spec=maximumSize=100,expireAfterWrite=1h\n\n# jOOQ Documentation Crawler Configuration\njooq.documentation.crawler.max-depth=4\njooq.documentation.crawler.max-urls-per-section=100\njooq.documentation.crawler.timeout-ms=10000\njooq.documentation.crawler.cache-duration-hours=24\n\n# Server Configuration - SSE Buffer Settings\nserver.tomcat.max-http-response-header-size=64KB\nserver.tomcat.max-swallow-size=10MB\n```\n\n### Example Usage\n\nWhen connected to an MCP client, you can ask questions like:\n\n- \"How do I create a SELECT statement in jOOQ?\"\n- \"Show me examples of jOOQ INSERT operations\"\n- \"What databases does jOOQ support?\"\n- \"How do I configure jOOQ code generation?\"\n- \"How do I use transactions in jOOQ?\"\n\nThe server will fetch the relevant documentation and provide detailed answers with code examples.",
        "start_pos": 1794,
        "end_pos": 3437,
        "token_count_estimate": 410,
        "source_type": "readme",
        "agent_id": "24990660104d2224"
      },
      {
        "chunk_id": 2,
        "text": "mentation indexing and full-text search with TF-IDF scoring\n- **InvertedIndex**: Implements advanced full-text search capabilities with relevance scoring\n- **JooqDocumentationCrawler**: Crawls and fetches jOOQ documentation for local storage\n- **JooqDocumentationFetcher**: Handles parsing and extraction of documentation content\n- **McpConfiguration**: Spring configuration for MCP tool registration\n- **Caching**: Caffeine-based caching to improve performance\n\n### Key Features\n\n- **Local Documentation Storage**: Documentation is stored locally in `src/main/resources/docs/` for faster access\n- **Full-Text Search**: Advanced search using TF-IDF scoring for better relevance\n- **Efficient Indexing**: In-memory inverted index for fast document retrieval\n- **Code Example Extraction**: Automatic extraction and categorization of code examples\n\n## Testing\n\nRun the test suite:\n\n```bash\n./mvnw test\n```\n\n## Development\n\nThe application uses:\n- Spring Boot 3.5.4\n- Spring AI 1.0.0 with MCP Server support\n- JSoup for HTML parsing\n- Caffeine for caching\n\nTo add new tools, create methods annotated with `@Tool` in the `JooqDocumentationService` class.",
        "start_pos": 3642,
        "end_pos": 4791,
        "token_count_estimate": 287,
        "source_type": "readme",
        "agent_id": "24990660104d2224"
      }
    ]
  },
  {
    "agent_id": "a7e771958e1ae2e9",
    "name": "ch.pfx/mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/pitwch/pfx-mcp-server",
    "description": "MCP Server f√ºr Forterro Proffix Px5 ERP",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-11-08T09:40:29.683436Z",
    "indexed_at": "2026-02-18T04:11:16.078878",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# pfx MCP Server f√ºr Forterro Proffix Px5\n\n**üá©üá™ Deutsch | [üá¨üáß English](README.en.md)**\n\n**Model Context Protocol Server f√ºr Forterro Proffix Px5 ERP**\n\nDie universelle Schnittstelle f√ºr AI/KI-Integration mit deinem Proffix Px5 ERP\n\nVerbinde AI-Assistenten (Claude, ChatGPT, Gemini) mit deinem Proffix Px5 √ºber standardisiertes MCP-Protokoll.\n\n**JSON-RPC 2.0 Transport ‚Ä¢ Parameterbasierte Auth ‚Ä¢ Aufbereitete Proffix Endpoints**\n\n## üåü Was ist das Model Context Protocol (MCP)?\n\nModel Context Protocol (MCP) ist ein offener Standard von Anthropic f√ºr sichere AI-Integration. KI-Assistenten greifen direkt auf deine Systeme zu - ohne manuelle Datenkopien oder Screenshots.\n\n**Ohne MCP:** \"Zeig mir alle offenen Rechnungen\" ‚Üí Du musst Proffix √∂ffnen, Daten exportieren, in die KI kopieren\n\n**Mit MCP:** \"Zeig mir alle offenen Rechnungen\" ‚Üí Die KI greift direkt auf Proffix zu und liefert die Antwort\n\n- **Echtzeit-Zugriff:** KI arbeitet mit aktuellen Daten aus deinen Systemen\n- **Sicherheit:** Keine Daten werden in der KI gespeichert - nur tempor√§rer Zugriff\n- **Automatisierung:** KI kann komplexe Aufgaben √ºber mehrere Systeme hinweg ausf√ºhren\n- **Nat√ºrliche Sprache:** Keine SQL oder API-Kenntnisse erforderlich\n- **Standardisiert:** Funktioniert mit allen MCP-kompatiblen KI-Assistenten\n\n## üìã Was ist pfx MCP?\n\npfx MCP ist der erste MCP Server f√ºr Forterro Proffix Px5. Verbinde AI-Assistenten wie Claude, ChatGPT und Gemini mit deinem ERP. Greife auf Daten zu, erstelle Berichte und automatisiere Workflows - direkt √ºber nat√ºrliche Sprache.\n\n- **Proffix Funktionen** direkt via MCP Tools\n- **JSON-RPC 2.0 Transport** f√ºr alle MCP-Clients\n- **Kostenloser API Key** (Beta)\n- **Claude, ChatGPT, Gemini Ready**\n\n**Offiziell gelistet im MCP Registry:**\n- üì¶ Package: `ch.pfx/mcp-server`\n- üîó Registry: [registry.modelcontextprotocol.io](https://registry.modelcontextprotocol.io)\n- üíª GitHub: [github.com/pitwch/pfx-mcp-server](https://github.com/pitwch/pfx-mcp-server)\n- ‚úÖ Status: Active ‚Ä¢ Version 1.0.0 ‚Ä¢ Published 2025-11-08\n\n## üí° Praktische Anwendungsf√§lle\n\n### üìä Intelligente Datenabfragen\n**Beispiel:** \"Zeige mir alle offenen Rechnungen\"\n\nDie KI greift direkt auf deine Proffix-Daten zu und liefert strukturierte Ergebnisse - ohne SQL oder API-Kenntnisse.\n\n### üîç Komplexe Suchen\n**Beispiel:** \"Suche Artikel mit 'Laptop' im Namen und Preis unter 1000 CHF\"\n\nNat√ºrliche Sprachabfragen werden automatisch in pr√§zise API-Calls umgewandelt.\n\n### üìà Automatische Berichte\n**Beispiel:** \"Erstelle einen Bericht √ºber die Top 10 Kunden nach Umsatz\"\n\nDie KI aggregiert Daten, erstellt Analysen und formatiert Ergebnisse professionell.\n\n### üîî √Ñnderungsverfolgung\n**Beispiel:** \"Welche Adressen haben sich diese Woche ge√§ndert?\"\n\nZeitbasierte Abfragen und √Ñnderungsanalysen in Echtzeit.\n\n### üíº Weitere Anwendungsf√§lle\n- **Lagerverwaltung:** \"Zeige mir alle Artikel mit Lagerbestand unter 10\"\n- **Kundenanalysen:** \"Analysiere die Umsatzentwicklung von Kunde 1001\"\n- **Workflow-Automatisierung:** \"Erstelle eine neue Adresse f√ºr Firma XY\"\n- **Endpoint-Discovery:** \"Welche API-Endpoints gibt es f√ºr Auftr√§ge?\"\n- **Multi-System-Abfragen:** \"Vergleiche Proffix-Daten mit unserem CRM\"\n\n## üöÄ Quick Start\n\n### Option 1: One-Click Installation (Empfohlen) üéØ\n\n**F√ºr Claude Desktop Benutzer - Einfachste Installation!**\n\nDie schnellste Methode - keine manuelle Konfiguration n√∂tig!\n\n1. **API Key holen**  \n   Besuche [request-api-key.html](https://mcp.pfx.ch/request-api-key.html) und fordere deinen kostenlosen Key an (per E-Mail)\n\n2. **MCPB Bundle laden**  \n   [pfx-mcp-server.mcpb herunterladen](https://github.com/pitwch/pfx-mcp-server/releases/latest/download/pfx-mcp-server.mcpb)\n\n3. **Installieren**  \n   In Claude Desktop: Einstellungen ‚Üí Erweiterungen ‚Üí Erweiterte Einstellungen (Bereich Extension Developer) ‚Üí Erweiterung installieren‚Ä¶ ‚Üí die `.mcpb` Datei ausw√§hlen und den Anweisungen folgen\n\n4. **Zugangsdaten eingeben**  \n   API Key + deine Proffix Px5 Credentials (Username, Passwort, URL, Port, Datenbank)\n\n5. **Fertig!**  \n   Claude neu starten und testen: *\"Zeige mir alle Adressen aus Z√ºrich aus Proffix Px5\"*\n\n---\n\n### Option 2: Manuelle Installation (Fortgeschritten) ‚öôÔ∏è\n\n**F√ºr andere MCP-Clients oder erweiterte Konfiguration**\n\nF√ºr andere MCP-Clients (Cursor, Windsurf, Gemini CLI, etc.) oder wenn du die Konfiguration selbst verwalten m√∂chtest:\n\n1. **API Key anfordern**  \n   [request-api-key.html](https://mcp.pfx.ch/request-api-key.html)\n\n2. **Bridge-Script laden**  \n   [mcp-http-bridge.txt](https://mcp.pfx.ch/bridge/mcp-http-bridge.txt) herunterladen und zu `mcp-http-bridge.js` umbenennen\n\n3. **Config-Datei √∂ffnen**  \n   Je nach Client:\n   - Claude: `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows) oder `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)\n   - Cursor/Windsurf: Siehe [AI Client Setup](https://mcp.pfx.ch/ai-clients.html)\n\n4. **Server hinzuf√ºgen**  \n   Siehe Beispiel-Config unten:\n\n```json\n{\n  \"mcpServers\": {\n    \"pfx-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"C:\\\\mcp\\\\mcp-http-bridge.js\", \"https://mcp.pfx.ch/api/server\"],\n      \"env\": {\n        \"HTTP_AUTHORIZATION\": \"Bearer DEIN_API_KEY\",\n        \"PROFFIX_USERNAME\": \"dein-username\",\n        \"PROFFIX_PASSWORD\": \"dein-passwort\",\n        \"PROFFIX_URL\": \"https://dein-proffix.com\",\n        \"PROFFIX_PORT\": \"11011\",\n        \"PROFFIX_DATABASE\": \"deine-db\",\n        \"RESPONSE_FORMAT\": \"json\"\n      }\n    }\n  }\n}\n```\n\n5. **Client neu starten und testen**  \n   Frage deinen AI-Assistenten: *\"Zeige mir alle Adressen aus Proffix\"*\n\n## üìö Dokumentation\n\nVollst√§ndige Setup-Anleitungen f√ºr alle AI-Clients:\n- **[Hauptdokumentation](https://mcp.pfx.ch)**\n- **[AI Client Setup](https://mcp.pfx.ch/ai-clients.html)** (Claude, ChatGPT, Gemini, Cursor, Windsurf, Continue.dev)\n- **[Testing & Debug](https://mcp.pfx.ch/debug.html)**\n\n## üîß Unterst√ºtzte MCP-Clients\n\n- ‚úÖ Claude Desktop\n- ‚úÖ Cursor IDE\n- ‚úÖ Windsurf IDE\n- ‚úÖ Continue.dev (VS Code/JetBrains)\n- ‚úÖ Gemini CLI\n- ‚ö° ChatGPT (experimentell)\n- ‚úÖ Custom Clients (via MCP SDK)\n\n## üîí Sicherheit\n\n### Authentifizierung\n- **API Key** f√ºr Zugriffskontrolle (kostenlos w√§hrend Beta)\n- **Proffix Credentials** als HTTP Headers (verschl√ºsselt √ºbertragen)\n- **Keine Datenspeicherung** auf MCP Server\n- **Parameterbasierte Auth** ohne Sessions\n- Zugangsdaten werden bei jeder Anfrage √ºbertragen und nicht gespeichert\n\n### Best Practices\n- Verwende immer HTTPS f√ºr die Kommunikation\n- Speichere Zugangsdaten niemals im Client-Code\n- Implementiere Rate-Limiting auf Client-Seite\n- √úberwache API-Zugriffe regelm√§√üig\n- Verwende starke Passw√∂rter f√ºr Proffix API Benutzer\n\n### Server-Sicherheit\n- Umfassende .htaccess Sicherheitsregeln\n- Schutz sensibler Dateien und Konfigurationen\n- HTTPS-Verschl√ºsselung wird empfohlen\n\n## üåê Remote Server\n\nDer pfx MCP Server l√§uft als **hosted service** unter:\n```\nhttps://mcp.pfx.ch/api/server\n```\n\n**Transport:** JSON-RPC 2.0 via HTTP  \n**Status:** [https://mcp.pfx.ch/api/version](https://mcp.pfx.ch/api/version)\n\n## üîå Model Context Protocol API\n\nDer pfx MCP Server implementiert das standardisierte Model Context Protocol √ºber JSON-RPC 2.0:\n\n### MCP Methoden\n- **initialize** - Handshake zwischen Client und Server. Tauscht Capabilities und Protokollversion aus.\n- **tools/list** - Listet alle verf√ºgbaren Proffix-Operationen auf. Erfordert Authentifizierung.\n- **tools/call** - F√ºhrt eine Proffix-Operation aus. Parameter werden in `arguments` √ºbergeben.\n\n### Verf√ºgbare Proffix Tools\n- `proffix_search_endpoints` - Fuzzy-Search √ºber 120+ Endpoints\n- `proffix_call_endpoint` - Direkter Endpoint-Aufruf\n- `proffix_describe_endpoint` - Endpoint-Dokumentation\n- Alle spezifischen Proffix API Endpoints (Adressen, Artikel, Auftr√§ge, etc.)\n\n### Response Formate\n\nDer Server unterst√ºtzt zwei Antwortformate:\n\n- **JSON** (Standard) - Strukturierte JSON-Antworten direkt von der Proffix API\n- **TOON** (AI-optimiert) - Angereicherte Antworten mit nat√ºrlichsprachigen Beschreibungen, optimiert f√ºr AI-Verarbeitung\n\n**Format-Aktivierung:**\n\n1. **Global** (empfohlen) - Gilt f√ºr alle Aufrufe:\n```json\n{\n  \"name\": \"proffix_call_endpoint\",\n  \"arguments\": {\n    \"endpointId\": 9,\n    \"format\": \"toon\",\n    \"params\": {\n      \"limit\": 10\n    }\n  }\n}\n```\n\n2. **Per-Call** - √úberschreibt globale Einstellung:\n```json\n{\n  \"name\": \"proffix_call_endpoint\",\n  \"arguments\": {\n    \"endpointId\": 9,\n    \"params\": {\n      \"limit\": 10,\n      \"format\": \"toon\"\n    }\n  }\n}\n```\n\n**Priorit√§t:** `params.format` > `arguments.format` > `\"json\"` (Standard)\n\n**Server URL:** `https://mcp.pfx.ch/api/server`\n\n[üîß Test-Beispiele & Debugging](https://mcp.pfx.ch/debug.html)\n\n## üí° Beispiel-Abfragen\n\n### Allgemeine Abfragen\n```\n\"Zeige mir alle offenen Rechnungen\"\n\"Suche Artikel mit 'Laptop' im Namen\"\n\"Welche Adressen haben sich diese Woche ge√§ndert?\"\n\"Erstelle einen Bericht √ºber Ums√§tze nach Kunde\"\n```\n\n### Abteilungsspezifische Beispiele\n- **Rechnungswesen:** \"Zeige alle unbezahlten Rechnungen √§lter als 30 Tage\"\n- **Vertrieb:** \"Liste alle Angebote aus Q4 2024 mit Status 'Offen'\"\n- **Einkauf:** \"Welche Bestellungen sind √ºberf√§llig?\"\n- **Controlling:** \"Erstelle eine Umsatz√ºbersicht nach Produktgruppen\"\n- **Support:** \"Finde alle Servicef√§lle von Kunde XY\"\n- **Entwicklung:** \"Dokumentiere alle verf√ºgbaren Proffix-Endpoints\"\n\n## üîó Links\n\n- **Website:** [https://mcp.pfx.ch](https://mcp.pfx.ch)\n- **API Key:** [https://mcp.pfx.ch/request-api-key.html](https://mcp.pfx.ch/request-api-key.html)\n- **Status:** [https://mcp.pfx.ch/api/version](https://mcp.pfx.ch/api/version)\n\n## üìÑ License\n\nMIT License - See LICENSE file for details\n\n## üÜò Support\n\nBei Fragen: [https://mcp.pfx.ch/#kontakt](https://mcp.pfx.ch/#kontakt)\n\n---\n\n**Hinweis:** Erfordert Zugang zur Forterro Proffix Px5 REST API.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect AI assistants (Claude, ChatGPT, Gemini) to Forterro Proffix Px5 ERP via MCP protocol",
        "Provide real-time access to Proffix Px5 data for AI queries",
        "Execute Proffix API operations through JSON-RPC 2.0 transport",
        "Convert natural language queries into precise API calls for data retrieval and automation",
        "Generate automated reports and data analyses from Proffix Px5",
        "Support multiple MCP clients including Claude Desktop, Cursor, Windsurf, Continue.dev, Gemini CLI, and experimental ChatGPT",
        "Offer endpoint discovery and detailed endpoint documentation",
        "Provide secure parameter-based authentication with API keys and Proffix credentials",
        "Deliver responses in JSON or AI-optimized TOON format"
      ],
      "limitations": [
        "Requires access to Forterro Proffix Px5 REST API",
        "ChatGPT client support is experimental",
        "No data storage on MCP server; all credentials must be provided with each request",
        "Rate limiting must be implemented on client side",
        "Only supports Forterro Proffix Px5 ERP system"
      ],
      "requirements": [
        "API key obtained from https://mcp.pfx.ch/request-api-key.html",
        "Valid Forterro Proffix Px5 credentials (username, password, URL, port, database)",
        "HTTPS communication recommended for security",
        "Compatible MCP client software (e.g., Claude Desktop, Cursor, Windsurf, Gemini CLI)",
        "Node.js environment for manual bridge script usage (optional)"
      ]
    },
    "documentation_quality": 0.95,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, security considerations, supported clients, limitations, and requirements, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# pfx MCP Server f√ºr Forterro Proffix Px5\n\n**üá©üá™ Deutsch | [üá¨üáß English](README.en.md)**\n\n**Model Context Protocol Server f√ºr Forterro Proffix Px5 ERP**\n\nDie universelle Schnittstelle f√ºr AI/KI-Integration mit deinem Proffix Px5 ERP\n\nVerbinde AI-Assistenten (Claude, ChatGPT, Gemini) mit deinem Proffix Px5 √ºber standardisiertes MCP-Protokoll.\n\n**JSON-RPC 2.0 Transport ‚Ä¢ Parameterbasierte Auth ‚Ä¢ Aufbereitete Proffix Endpoints**\n\n## üåü Was ist das Model Context Protocol (MCP)?\n\nModel Context Protocol (MCP) ist ein offener Standard von Anthropic f√ºr sichere AI-Integration. KI-Assistenten greifen direkt auf deine Systeme zu - ohne manuelle Datenkopien oder Screenshots.\n\n**Ohne MCP:** \"Zeig mir alle offenen Rechnungen\" ‚Üí Du musst Proffix √∂ffnen, Daten exportieren, in die KI kopieren\n\n**Mit MCP:** \"Zeig mir alle offenen Rechnungen\" ‚Üí Die KI greift direkt auf Proffix zu und liefert die Antwort\n\n- **Echtzeit-Zugriff:** KI arbeitet mit aktuellen Daten aus deinen Systemen\n- **Sicherheit:** Keine Daten werden in der KI gespeichert - nur tempor√§rer Zugriff\n- **Automatisierung:** KI kann komplexe Aufgaben √ºber mehrere Systeme hinweg ausf√ºhren\n- **Nat√ºrliche Sprache:** Keine SQL oder API-Kenntnisse erforderlich\n- **Standardisiert:** Funktioniert mit allen MCP-kompatiblen KI-Assistenten\n\n## üìã Was ist pfx MCP?\n\npfx MCP ist der erste MCP Server f√ºr Forterro Proffix Px5. Verbinde AI-Assistenten wie Claude, ChatGPT und Gemini mit deinem ERP. Greife auf Daten zu, erstelle Berichte und automatisiere Workflows - direkt √ºber nat√ºrliche Sprache.",
        "start_pos": 0,
        "end_pos": 1543,
        "token_count_estimate": 385,
        "source_type": "readme",
        "agent_id": "a7e771958e1ae2e9"
      },
      {
        "chunk_id": 1,
        "text": "y.modelcontextprotocol.io)\n- üíª GitHub: [github.com/pitwch/pfx-mcp-server](https://github.com/pitwch/pfx-mcp-server)\n- ‚úÖ Status: Active ‚Ä¢ Version 1.0.0 ‚Ä¢ Published 2025-11-08\n\n## üí° Praktische Anwendungsf√§lle\n\n### üìä Intelligente Datenabfragen\n**Beispiel:** \"Zeige mir alle offenen Rechnungen\"\n\nDie KI greift direkt auf deine Proffix-Daten zu und liefert strukturierte Ergebnisse - ohne SQL oder API-Kenntnisse.\n\n### üîç Komplexe Suchen\n**Beispiel:** \"Suche Artikel mit 'Laptop' im Namen und Preis unter 1000 CHF\"\n\nNat√ºrliche Sprachabfragen werden automatisch in pr√§zise API-Calls umgewandelt.\n\n### üìà Automatische Berichte\n**Beispiel:** \"Erstelle einen Bericht √ºber die Top 10 Kunden nach Umsatz\"\n\nDie KI aggregiert Daten, erstellt Analysen und formatiert Ergebnisse professionell.\n\n### üîî √Ñnderungsverfolgung\n**Beispiel:** \"Welche Adressen haben sich diese Woche ge√§ndert?\"\n\nZeitbasierte Abfragen und √Ñnderungsanalysen in Echtzeit.\n\n### üíº Weitere Anwendungsf√§lle\n- **Lagerverwaltung:** \"Zeige mir alle Artikel mit Lagerbestand unter 10\"\n- **Kundenanalysen:** \"Analysiere die Umsatzentwicklung von Kunde 1001\"\n- **Workflow-Automatisierung:** \"Erstelle eine neue Adresse f√ºr Firma XY\"\n- **Endpoint-Discovery:** \"Welche API-Endpoints gibt es f√ºr Auftr√§ge?\"\n- **Multi-System-Abfragen:** \"Vergleiche Proffix-Daten mit unserem CRM\"\n\n## üöÄ Quick Start\n\n### Option 1: One-Click Installation (Empfohlen) üéØ\n\n**F√ºr Claude Desktop Benutzer - Einfachste Installation!**\n\nDie schnellste Methode - keine manuelle Konfiguration n√∂tig!\n\n1. **API Key holen**  \n   Besuche [request-api-key.html](https://mcp.pfx.ch/request-api-key.html) und fordere deinen kostenlosen Key an (per E-Mail)\n\n2. **MCPB Bundle laden**  \n   [pfx-mcp-server.mcpb herunterladen](https://github.com/pitwch/pfx-mcp-server/releases/latest/download/pfx-mcp-server.mcpb)\n\n3. **Installieren**  \n   In Claude Desktop: Einstellungen ‚Üí Erweiterungen ‚Üí Erweiterte Einstellungen (Bereich Extension Developer) ‚Üí Erweiterung installieren‚Ä¶ ‚Üí die `.mcpb` Datei ausw√§hlen und den Anweisungen folgen\n\n4.",
        "start_pos": 1848,
        "end_pos": 3885,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "a7e771958e1ae2e9"
      },
      {
        "chunk_id": 2,
        "text": "In Claude Desktop: Einstellungen ‚Üí Erweiterungen ‚Üí Erweiterte Einstellungen (Bereich Extension Developer) ‚Üí Erweiterung installieren‚Ä¶ ‚Üí die `.mcpb` Datei ausw√§hlen und den Anweisungen folgen\n\n4. **Zugangsdaten eingeben**  \n   API Key + deine Proffix Px5 Credentials (Username, Passwort, URL, Port, Datenbank)\n\n5. **Fertig!**  \n   Claude neu starten und testen: *\"Zeige mir alle Adressen aus Z√ºrich aus Proffix Px5\"*\n\n---\n\n### Option 2: Manuelle Installation (Fortgeschritten) ‚öôÔ∏è\n\n**F√ºr andere MCP-Clients oder erweiterte Konfiguration**\n\nF√ºr andere MCP-Clients (Cursor, Windsurf, Gemini CLI, etc.) oder wenn du die Konfiguration selbst verwalten m√∂chtest:\n\n1. **API Key anfordern**  \n   [request-api-key.html](https://mcp.pfx.ch/request-api-key.html)\n\n2. **Bridge-Script laden**  \n   [mcp-http-bridge.txt](https://mcp.pfx.ch/bridge/mcp-http-bridge.txt) herunterladen und zu `mcp-http-bridge.js` umbenennen\n\n3. **Config-Datei √∂ffnen**  \n   Je nach Client:\n   - Claude: `%APPDATA%\\Claude\\claude_desktop_config.json` (Windows) oder `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS)\n   - Cursor/Windsurf: Siehe [AI Client Setup](https://mcp.pfx.ch/ai-clients.html)\n\n4. **Server hinzuf√ºgen**  \n   Siehe Beispiel-Config unten:\n\n```json\n{\n  \"mcpServers\": {\n    \"pfx-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\"C:\\\\mcp\\\\mcp-http-bridge.js\", \"https://mcp.pfx.ch/api/server\"],\n      \"env\": {\n        \"HTTP_AUTHORIZATION\": \"Bearer DEIN_API_KEY\",\n        \"PROFFIX_USERNAME\": \"dein-username\",\n        \"PROFFIX_PASSWORD\": \"dein-passwort\",\n        \"PROFFIX_URL\": \"https://dein-proffix.com\",\n        \"PROFFIX_PORT\": \"11011\",\n        \"PROFFIX_DATABASE\": \"deine-db\",\n        \"RESPONSE_FORMAT\": \"json\"\n      }\n    }\n  }\n}\n```\n\n5.",
        "start_pos": 3685,
        "end_pos": 5430,
        "token_count_estimate": 434,
        "source_type": "readme",
        "agent_id": "a7e771958e1ae2e9"
      },
      {
        "chunk_id": 3,
        "text": "fix\"*\n\n## üìö Dokumentation\n\nVollst√§ndige Setup-Anleitungen f√ºr alle AI-Clients:\n- **[Hauptdokumentation](https://mcp.pfx.ch)**\n- **[AI Client Setup](https://mcp.pfx.ch/ai-clients.html)** (Claude, ChatGPT, Gemini, Cursor, Windsurf, Continue.dev)\n- **[Testing & Debug](https://mcp.pfx.ch/debug.html)**\n\n## üîß Unterst√ºtzte MCP-Clients\n\n- ‚úÖ Claude Desktop\n- ‚úÖ Cursor IDE\n- ‚úÖ Windsurf IDE\n- ‚úÖ Continue.dev (VS Code/JetBrains)\n- ‚úÖ Gemini CLI\n- ‚ö° ChatGPT (experimentell)\n- ‚úÖ Custom Clients (via MCP SDK)\n\n## üîí Sicherheit\n\n### Authentifizierung\n- **API Key** f√ºr Zugriffskontrolle (kostenlos w√§hrend Beta)\n- **Proffix Credentials** als HTTP Headers (verschl√ºsselt √ºbertragen)\n- **Keine Datenspeicherung** auf MCP Server\n- **Parameterbasierte Auth** ohne Sessions\n- Zugangsdaten werden bei jeder Anfrage √ºbertragen und nicht gespeichert\n\n### Best Practices\n- Verwende immer HTTPS f√ºr die Kommunikation\n- Speichere Zugangsdaten niemals im Client-Code\n- Implementiere Rate-Limiting auf Client-Seite\n- √úberwache API-Zugriffe regelm√§√üig\n- Verwende starke Passw√∂rter f√ºr Proffix API Benutzer\n\n### Server-Sicherheit\n- Umfassende .htaccess Sicherheitsregeln\n- Schutz sensibler Dateien und Konfigurationen\n- HTTPS-Verschl√ºsselung wird empfohlen\n\n## üåê Remote Server\n\nDer pfx MCP Server l√§uft als **hosted service** unter:\n```\nhttps://mcp.pfx.ch/api/server\n```\n\n**Transport:** JSON-RPC 2.0 via HTTP  \n**Status:** [https://mcp.pfx.ch/api/version](https://mcp.pfx.ch/api/version)\n\n## üîå Model Context Protocol API\n\nDer pfx MCP Server implementiert das standardisierte Model Context Protocol √ºber JSON-RPC 2.0:\n\n### MCP Methoden\n- **initialize** - Handshake zwischen Client und Server. Tauscht Capabilities und Protokollversion aus.\n- **tools/list** - Listet alle verf√ºgbaren Proffix-Operationen auf. Erfordert Authentifizierung.\n- **tools/call** - F√ºhrt eine Proffix-Operation aus. Parameter werden in `arguments` √ºbergeben.",
        "start_pos": 5533,
        "end_pos": 7433,
        "token_count_estimate": 475,
        "source_type": "readme",
        "agent_id": "a7e771958e1ae2e9"
      },
      {
        "chunk_id": 4,
        "text": "on aus.\n- **tools/list** - Listet alle verf√ºgbaren Proffix-Operationen auf. Erfordert Authentifizierung.\n- **tools/call** - F√ºhrt eine Proffix-Operation aus. Parameter werden in `arguments` √ºbergeben.\n\n### Verf√ºgbare Proffix Tools\n- `proffix_search_endpoints` - Fuzzy-Search √ºber 120+ Endpoints\n- `proffix_call_endpoint` - Direkter Endpoint-Aufruf\n- `proffix_describe_endpoint` - Endpoint-Dokumentation\n- Alle spezifischen Proffix API Endpoints (Adressen, Artikel, Auftr√§ge, etc.)\n\n### Response Formate\n\nDer Server unterst√ºtzt zwei Antwortformate:\n\n- **JSON** (Standard) - Strukturierte JSON-Antworten direkt von der Proffix API\n- **TOON** (AI-optimiert) - Angereicherte Antworten mit nat√ºrlichsprachigen Beschreibungen, optimiert f√ºr AI-Verarbeitung\n\n**Format-Aktivierung:**\n\n1. **Global** (empfohlen) - Gilt f√ºr alle Aufrufe:\n```json\n{\n  \"name\": \"proffix_call_endpoint\",\n  \"arguments\": {\n    \"endpointId\": 9,\n    \"format\": \"toon\",\n    \"params\": {\n      \"limit\": 10\n    }\n  }\n}\n```\n\n2.",
        "start_pos": 7233,
        "end_pos": 8219,
        "token_count_estimate": 246,
        "source_type": "readme",
        "agent_id": "a7e771958e1ae2e9"
      },
      {
        "chunk_id": 5,
        "text": "**Controlling:** \"Erstelle eine Umsatz√ºbersicht nach Produktgruppen\"\n- **Support:** \"Finde alle Servicef√§lle von Kunde XY\"\n- **Entwicklung:** \"Dokumentiere alle verf√ºgbaren Proffix-Endpoints\"\n\n## üîó Links\n\n- **Website:** [https://mcp.pfx.ch](https://mcp.pfx.ch)\n- **API Key:** [https://mcp.pfx.ch/request-api-key.html](https://mcp.pfx.ch/request-api-key.html)\n- **Status:** [https://mcp.pfx.ch/api/version](https://mcp.pfx.ch/api/version)\n\n## üìÑ License\n\nMIT License - See LICENSE file for details\n\n## üÜò Support\n\nBei Fragen: [https://mcp.pfx.ch/#kontakt](https://mcp.pfx.ch/#kontakt)\n\n---\n\n**Hinweis:** Erfordert Zugang zur Forterro Proffix Px5 REST API.",
        "start_pos": 9081,
        "end_pos": 9735,
        "token_count_estimate": 163,
        "source_type": "readme",
        "agent_id": "a7e771958e1ae2e9"
      }
    ]
  },
  {
    "agent_id": "6677febc2dad8b3d",
    "name": "ci.git/mymlh-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/wei/mymlh-mcp-server",
    "description": "OAuth-enabled MyMLH MCP server for accessing MyMLH data.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-18T01:20:01.35206Z",
    "indexed_at": "2026-02-18T04:11:18.381287",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "![mymlh-mcp-server](https://socialify.git.ci/wei/mymlh-mcp-server/image?description=1&font=Bitter&logo=https%3A%2F%2Fstatic.mlh.io%2Fbrand-assets%2Flogo%2Fofficial%2Fmlh-logo-color.svg&name=1&theme=Light)\n\n[![Remote HTTP MCP Server](https://img.shields.io/badge/MCP-Streamable_HTTP-000000?logo=modelcontextprotocol&logoColor=white)](https://modelcontextprotocol.io/docs/concepts/transports#http-streaming)\n[![Built for MyMLH](https://img.shields.io/badge/Built_for-MyMLH-E73427?logo=data:image/svg%2bxml;base64,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)](https://my.mlh.io/)\n[![Cloudflare Workers](https://img.shields.io/badge/Cloudflare-Workers-F38020?logo=cloudflare&logoColor=white)](https://developers.cloudflare.com/workers/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-3178C6?logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Built with Hono](https://img.shields.io/badge/Built_with-Hono-E36002?logo=hono&logoColor=white)](https://hono.dev/)\n[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](https://wei.mit-license.org)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that provides secure, OAuth-authenticated access to [MyMLH](https://my.mlh.io/). This server enables AI assistants and MCP clients to interact with the MyMLH API on behalf of users.\n\n## Features\n\n- **Secure Authentication**: Implements [MyMLH API v4 with OAuth](https://my.mlh.io/developers/docs) for robust and secure user authentication.\n- **User Data Access**: Provides tools to fetch a user's MyMLH profile, education, employment history, and more.\n- **Automatic Token Management**: Handles token refresh and secure storage automatically.\n- **Cloudflare Workers**: Built to run on the edge for low-latency, scalable performance.\n- **Easy Deployment**: Can be deployed to your own Cloudflare account in minutes.\n\n## Quick Start\n\nYou can connect to our publicly hosted instance using any MCP client that supports the [Streamable HTTP transport with OAuth](https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization).\n\n**Endpoint**: `https://mymlh-mcp.git.ci/mcp`\n\n### Add MCP Server\n\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_MCP-0098FF)](https://insiders.vscode.dev/redirect/mcp/install?name=mymlh&config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fmymlh-mcp.git.ci%2Fmcp%22%7D)\n[![Install in Cursor](https://img.shields.io/badge/Cursor-Install_MCP-000000)](https://cursor.com/en/install-mcp?name=mymlh&config=eyJ1cmwiOiJodHRwczovL215bWxoLW1jcC5naXQuY2kvbWNwIn0%3D)\n\nExample configuration snippets for common MCP clients:\n\n**VS Code:**\n\n```json\n{\n  \"servers\": {\n    \"mymlh\": {\n      \"type\": \"http\",\n      \"url\": \"https://mymlh-mcp.git.ci/mcp\"\n    }\n  }\n}\n```\n\n**Cursor and many clients:**\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"url\": \"https://mymlh-mcp.git.ci/mcp\"\n    }\n  }\n}\n```\n\n**Windsurf and many clients:**\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"serverUrl\": \"https://mymlh-mcp.git.ci/mcp\"\n    }\n  }\n}\n```\n\n**Augment Code:**\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"url\": \"https://mymlh-mcp.git.ci/mcp\",\n      \"type\": \"http\"\n    }\n  }\n}\n```\n\n**Claude Code:**\n\n```sh\nclaude mcp add --transport http mymlh https://mymlh-mcp.git.ci/mcp\n```\n\n**Gemini CLI:**\n\n_Gemini currently only supports the deprecated SSE protocol._\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"url\": \"https://mymlh-mcp.git.ci/sse\"\n    }\n  }\n}\n```\n\n**Roo Code, Cline, KiloCode:**\n\nAlthough these clients support Streamable HTTP transport, they do not yet support the OAuth authentication flow. Please use the fallback option below. See open feature requests for [Roo Code](https://github.com/RooCodeInc/Roo-Code/issues/7296), [Cline](https://github.com/cline/cline/issues/4523).\n\nFor other clients, please consult their documentation for connecting to an MCP server. If you see 401 errors, the client likely does not support [Streamable HTTP with OAuth](https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization) and you will need to use the fallback option below.\n\n### Fallback Option\n\nFor environments where Streamable HTTP with OAuth is not supported, you may fall back to stdio transport with¬†[`mcp-remote`](https://www.npmjs.com/package/mcp-remote). This wraps the HTTP MCP server into a local stdio interface, forwarding requests over HTTP behind the scenes to ensure compatibility.\n\nExample¬†`mcp-remote`¬†configuration snippet:\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mymlh-mcp.git.ci/mcp\"\n      ]\n    }\n  }\n}\n```\n\nSee [`mcp-remote` documentation](https://www.npmjs.com/package/mcp-remote#usage) for more details on usage.\n\n## Available Tools\n\nOnce connected and authenticated, you can use the following tools:\n\n| Tool                      | Description                                                        |\n| ------------------------- | ------------------------------------------------------------------ |\n| `mymlh_get_user`          | Fetch current MyMLH user profile                                   |\n| `mymlh_get_token`         | Return current MyMLH access token details                          |\n| `mymlh_refresh_token`     | Exchange MyMLH refresh_token for a new access token and persist it |\n\n### Test with MCP Inspector\n\nYou can test the remote MCP server using the [Model Context Protocol Inspector](https://modelcontextprotocol.io/docs/tools/inspector).\n\n1.  Run the Inspector from your terminal:\n    ```bash\n    npx @modelcontextprotocol/inspector@latest\n    ```\n2.  Enter the server URL: `https://mymlh-mcp.git.ci/mcp` and click \"Connect\".\n3.  Follow the authentication flow to connect and test the tools.\n\n### Testing with Cloudflare AI Playground\n\nYou can also test the server directly using the [Cloudflare Workers AI LLM Playground](https://playground.ai.cloudflare.com/).\n\n1.  Go to the playground link.\n2.  Enter the server URL: `https://mymlh-mcp.git.ci/mcp`\n3.  Follow the authentication flow to connect and test the tools.\n\n### Example Usage\n\nYou can interact with the MyMLH MCP server using natural language in your AI assistant:\n\n- \"Get my MyMLH user info.\"\n- \"Show me my MyMLH profile.\"\n- \"Generate a resume using my MyMLH profile.\"\n- \"Create a GitHub profile README using my MyMLH data.\"\n\n## Deploying Your Own Instance\n\nFor full control, you can deploy your own instance to Cloudflare. See the [Deployment Guide](DEPLOYMENT.md) for detailed instructions.\n\n## Contributing\n\nWe welcome contributions! Whether you're fixing a bug, adding a feature, or improving documentation, your help is appreciated.\n\nFor development setup, project structure, how to add tools, and contributing guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## License\n\n[MIT](https://wei.mit-license.org)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Provide secure OAuth-authenticated access to MyMLH API v4",
        "Fetch a user's MyMLH profile, education, and employment history",
        "Automatically manage and refresh OAuth tokens",
        "Serve MCP requests via Streamable HTTP transport with OAuth",
        "Support integration with multiple MCP clients including VS Code, Cursor, Windsurf, Augment Code, and Claude Code",
        "Allow fallback to stdio transport via mcp-remote for clients lacking OAuth support",
        "Deploy on Cloudflare Workers for low-latency, scalable edge performance",
        "Enable AI assistants to interact with MyMLH data using natural language commands"
      ],
      "limitations": [
        "Does not support clients that lack Streamable HTTP with OAuth authentication without fallback",
        "Fallback option requires use of mcp-remote wrapper for compatibility",
        "Some MCP clients (e.g., Roo Code, Cline, KiloCode) do not yet support OAuth flow",
        "Gemini CLI only supports deprecated SSE protocol, not Streamable HTTP"
      ],
      "requirements": [
        "MyMLH API v4 OAuth credentials for authentication",
        "MCP client supporting Streamable HTTP transport with OAuth for direct connection",
        "Cloudflare account for deploying own instance",
        "Node.js environment for using mcp-remote fallback wrapper"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed tool descriptions, fallback options, deployment guidance, and explicit limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "![mymlh-mcp-server](https://socialify.git.ci/wei/mymlh-mcp-server/image?description=1&font=Bitter&logo=https%3A%2F%2Fstatic.mlh.io%2Fbrand-assets%2Flogo%2Fofficial%2Fmlh-logo-color.svg&name=1&theme=Light)\n\n[![Remote HTTP MCP Server](https://img.shields.io/badge/MCP-Streamable_HTTP-000000?logo=modelcontextprotocol&logoColor=white)](https://modelcontextprotocol.io/docs/concepts/transports#http-streaming)\n[![Built for MyMLH](https://img.shields.io/badge/Built_for-MyMLH-E73427?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbDpzcGFjZT0icHJlc2VydmUiIGlkPSJMYXllcl8xIiB4PSIwIiB5PSIwIiBzdHlsZT0iZW5hYmxlLWJhY2tncm91bmQ6bmV3IDAgMCAxMjgwIDQ4MC43NCIgdmVyc2lvbj0iMS4xIiB2aWV3Qm94PSIwIDAgMTI4MCA0ODAuNzQiPjxzdHlsZT4uc3Qwe2ZpbGwtcnVsZTpldmVub2RkO2NsaXAtcnVsZTpldmVub2RkO2ZpbGw6I2ZmZn08L3N0eWxlPjxwYXRoIGlkPSJTaGFwZV8wMDAwMDAzMzMzOTA1MDQ4NDMzMDU1ODk5MDAwMDAwNjAxOTE2MzA0MDkwNTA5Nzg4NF8iIGQ9Ik00MTYuNDUgMzIuOThjOC4yOCAzLjYgMTUuNDggOC42NCAyMS42IDE0Ljc2IDYuMTIgNi4xMiAxMC44IDEzLjMyIDE0Ljc2IDIxLjYgMy42IDguMjggNS40IDE3LjI4IDUuNCAyNi42NHYzNDEuMjdjMCAzLjI0LTEuMDggNS43Ni0zLjI0IDcuOTJzLTQuNjggMy4yNC03LjkyIDMuMjRoLTgxLjcyYy0zLjI0IDAtNS43Ni0xLjA4LTcuNTYtMy4yNHMtMi44OC00LjY4LTIuODgtNy45MlYxMjYuOTNoLTY1Ljg4djMxMC4zMWMwIDMuMjQtMS4wOCA1Ljc2LTIuODggNy45MnMtNC42OCAzLjI0LTcuNTYgMy4yNGgtNzQuMTZjLTMuMjQgMC01Ljc2LTEuMDgtNy45Mi0zLjI0cy0zLjI0LTQuNjgtMy4yNC03LjkyVjEyNi45M2gtNjUuODh2MzEwLjMxYzAgNy41Ni0zLjYgMTEuMTYtMTEuMTYgMTEuMTZoLTgxYy03LjU2IDAtMTEuMTYtMy42LTExLjE2LTExLjE2VjM4Ljc0YzAtNy41NiAzLjYtMTEuMTYgMTEuMTYtMTEuMTZoMzU0LjIzYzkuNzMgMCAxOC4zNyAxLjggMjcuMDEgNS40eiIgY2xhc3M9InN0MCIvPjxwYXRoIGlkPSJTaGFwZV8wMDAwMDE2NTkzNzc4NTYzNDc0MjEzMDgxMDAwMDAwMzU1ODM2MzY0NzQ2NjQ1OTU0M18iIGQ9Ik04MTQuMjMgMzQ2LjE3YzMuMjQgMCA2LjEyIDEuMDggNy45MiAzLjI0IDIuMTYgMi4xNiAzLjI0IDUuMDQgMy4yNCA4LjI4djc5LjU2YzAgMy4yNC0xLjA4IDUuNzYtMy4yNCA4LjI4LTIuMTYgMi4xNi00LjY4IDMuMjQtNy45MiAzLjI0SDUzNC41MmMtNy45MiAwLTExLjg4LTMuOTYtMTEuODgtMTEuNTJWMzkuMWMwLTcuNTYgMy45Ni0xMS41MiAxMS44OC0xMS41Mmg4NS42OGM3LjkyIDAgMTEuODggMy45NiAxMS44OCAxMS41MnYyOTYuNjNjMCAzLjI0IDEuM",
        "start_pos": 0,
        "end_pos": 2048,
        "token_count_estimate": 512,
        "source_type": "readme",
        "agent_id": "6677febc2dad8b3d"
      },
      {
        "chunk_id": 1,
        "text": "i4xNi00LjY4IDMuMjQtNy45MiAzLjI0SDUzNC41MmMtNy45MiAwLTExLjg4LTMuOTYtMTEuODgtMTEuNTJWMzkuMWMwLTcuNTYgMy45Ni0xMS41MiAxMS44OC0xMS41Mmg4NS42OGM3LjkyIDAgMTEuODggMy45NiAxMS44OCAxMS41MnYyOTYuNjNjMCAzLjI0IDEuMDggNS43NiAzLjYgNy45MiAyLjE2IDIuMTYgNS4wNCAyLjg4IDguMjggMi44OGgxNzAuMjd2LS4zNnoiIGNsYXNzPSJzdDAiLz48cGF0aCBpZD0iU2hhcGVfMDAwMDAxMDM5NTIxOTc1MzE2MzEzMjUyOTAwMDAwMDUxOTk2NzQ2NzgxNDM5NDUxNDRfIiBkPSJNMTI0NC4wNSAyNy41OGMzLjI0IDAgNi4xMiAxLjA4IDguMjggMy4yNCAyLjE2IDIuMTYgMy42IDUuMDQgMy42IDguMjh2Mzk5LjIyYzAgMy4yNC0xLjA4IDUuNzYtMy42IDguMjgtMi4xNiAyLjE2LTUuMDQgMy4yNC04LjI4IDMuMjRoLTg3LjEyYy0zLjI0IDAtNi4xMi0xLjA4LTguMjgtMy4yNC0yLjE2LTIuMTYtMy42LTUuMDQtMy42LTguMjhWMjk5LjAxYzAtMy4yNC0xLjA4LTUuNzYtMy4yNC03Ljkycy00LjY4LTIuODgtOC4yOC0yLjg4SDEwMTEuNWMtMy4yNCAwLTYuMTIgMS4wOC04LjI4IDIuODgtMi4xNiAyLjE2LTMuNiA0LjY4LTMuNiA3LjkydjEzOS4zMWMwIDcuNTYtMy45NiAxMS41Mi0xMS44OCAxMS41MmgtODYuNGMtNy45MiAwLTExLjg4LTMuOTYtMTEuODgtMTEuNTJWMzkuMWMwLTcuNTYgMy45Ni0xMS41MiAxMS44OC0xMS41Mmg4Ni40YzcuOTIgMCAxMS44OCAzLjk2IDExLjg4IDExLjUydjEzNC45OWMwIDcuNTYgMy45NiAxMS41MiAxMS44OCAxMS41MmgxMjIuMDNjMy4yNCAwIDYuMTItMS4wOCA4LjI4LTMuMjQgMi4xNi0yLjE2IDMuMjQtNS4wNCAzLjI0LTguMjhWMzkuMWMwLTMuMjQgMS4wOC01Ljc2IDMuNi04LjI4IDIuMTYtMi4xNiA1LjA0LTMuMjQgOC4yOC0zLjI0aDg3LjEyeiIgY2xhc3M9InN0MCIvPjwvc3ZnPg==)](https://my.mlh.io/)\n[![Cloudflare Workers](https://img.shields.io/badge/Cloudflare-Workers-F38020?logo=cloudflare&logoColor=white)](https://developers.cloudflare.com/workers/)\n[![TypeScript](https://img.shields.io/badge/TypeScript-3178C6?logo=typescript&logoColor=white)](https://www.typescriptlang.org/)\n[![Built with Hono](https://img.shields.io/badge/Built_with-Hono-E36002?logo=hono&logoColor=white)](https://hono.dev/)\n[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](https://wei.mit-license.org)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that provides secure, OAuth-authenticated access to [MyMLH](https://my.mlh.io/).",
        "start_pos": 1848,
        "end_pos": 3798,
        "token_count_estimate": 487,
        "source_type": "readme",
        "agent_id": "6677febc2dad8b3d"
      },
      {
        "chunk_id": 2,
        "text": "g)](https://wei.mit-license.org)\n\nA [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that provides secure, OAuth-authenticated access to [MyMLH](https://my.mlh.io/). This server enables AI assistants and MCP clients to interact with the MyMLH API on behalf of users.\n\n## Features\n\n- **Secure Authentication**: Implements [MyMLH API v4 with OAuth](https://my.mlh.io/developers/docs) for robust and secure user authentication.\n- **User Data Access**: Provides tools to fetch a user's MyMLH profile, education, employment history, and more.\n- **Automatic Token Management**: Handles token refresh and secure storage automatically.\n- **Cloudflare Workers**: Built to run on the edge for low-latency, scalable performance.\n- **Easy Deployment**: Can be deployed to your own Cloudflare account in minutes.\n\n## Quick Start\n\nYou can connect to our publicly hosted instance using any MCP client that supports the [Streamable HTTP transport with OAuth](https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization).",
        "start_pos": 3598,
        "end_pos": 4654,
        "token_count_estimate": 264,
        "source_type": "readme",
        "agent_id": "6677febc2dad8b3d"
      },
      {
        "chunk_id": 3,
        "text": "t.ci/mcp\"\n    }\n  }\n}\n```\n\n**Windsurf and many clients:**\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"serverUrl\": \"https://mymlh-mcp.git.ci/mcp\"\n    }\n  }\n}\n```\n\n**Augment Code:**\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"url\": \"https://mymlh-mcp.git.ci/mcp\",\n      \"type\": \"http\"\n    }\n  }\n}\n```\n\n**Claude Code:**\n\n```sh\nclaude mcp add --transport http mymlh https://mymlh-mcp.git.ci/mcp\n```\n\n**Gemini CLI:**\n\n_Gemini currently only supports the deprecated SSE protocol._\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"url\": \"https://mymlh-mcp.git.ci/sse\"\n    }\n  }\n}\n```\n\n**Roo Code, Cline, KiloCode:**\n\nAlthough these clients support Streamable HTTP transport, they do not yet support the OAuth authentication flow. Please use the fallback option below. See open feature requests for [Roo Code](https://github.com/RooCodeInc/Roo-Code/issues/7296), [Cline](https://github.com/cline/cline/issues/4523).\n\nFor other clients, please consult their documentation for connecting to an MCP server. If you see 401 errors, the client likely does not support [Streamable HTTP with OAuth](https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization) and you will need to use the fallback option below.\n\n### Fallback Option\n\nFor environments where Streamable HTTP with OAuth is not supported, you may fall back to stdio transport with¬†[`mcp-remote`](https://www.npmjs.com/package/mcp-remote). This wraps the HTTP MCP server into a local stdio interface, forwarding requests over HTTP behind the scenes to ensure compatibility.\n\nExample¬†`mcp-remote`¬†configuration snippet:\n\n```json\n{\n  \"mcpServers\": {\n    \"mymlh\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mymlh-mcp.git.ci/mcp\"\n      ]\n    }\n  }\n}\n```\n\nSee [`mcp-remote` documentation](https://www.npmjs.com/package/mcp-remote#usage) for more details on usage.",
        "start_pos": 5446,
        "end_pos": 7321,
        "token_count_estimate": 468,
        "source_type": "readme",
        "agent_id": "6677febc2dad8b3d"
      },
      {
        "chunk_id": 4,
        "text": "gs\": [\n        \"mcp-remote\",\n        \"https://mymlh-mcp.git.ci/mcp\"\n      ]\n    }\n  }\n}\n```\n\nSee [`mcp-remote` documentation](https://www.npmjs.com/package/mcp-remote#usage) for more details on usage.\n\n## Available Tools\n\nOnce connected and authenticated, you can use the following tools:\n\n| Tool                      | Description                                                        |\n| ------------------------- | ------------------------------------------------------------------ |\n| `mymlh_get_user`          | Fetch current MyMLH user profile                                   |\n| `mymlh_get_token`         | Return current MyMLH access token details                          |\n| `mymlh_refresh_token`     | Exchange MyMLH refresh_token for a new access token and persist it |\n\n### Test with MCP Inspector\n\nYou can test the remote MCP server using the [Model Context Protocol Inspector](https://modelcontextprotocol.io/docs/tools/inspector).\n\n1.  Run the Inspector from your terminal:\n    ```bash\n    npx @modelcontextprotocol/inspector@latest\n    ```\n2.  Enter the server URL: `https://mymlh-mcp.git.ci/mcp` and click \"Connect\".\n3.  Follow the authentication flow to connect and test the tools.\n\n### Testing with Cloudflare AI Playground\n\nYou can also test the server directly using the [Cloudflare Workers AI LLM Playground](https://playground.ai.cloudflare.com/).\n\n1.  Go to the playground link.\n2.  Enter the server URL: `https://mymlh-mcp.git.ci/mcp`\n3.  Follow the authentication flow to connect and test the tools.\n\n### Example Usage\n\nYou can interact with the MyMLH MCP server using natural language in your AI assistant:\n\n- \"Get my MyMLH user info.\"\n- \"Show me my MyMLH profile.\"\n- \"Generate a resume using my MyMLH profile.\"\n- \"Create a GitHub profile README using my MyMLH data.\"\n\n## Deploying Your Own Instance\n\nFor full control, you can deploy your own instance to Cloudflare. See the [Deployment Guide](DEPLOYMENT.md) for detailed instructions.",
        "start_pos": 7121,
        "end_pos": 9087,
        "token_count_estimate": 491,
        "source_type": "readme",
        "agent_id": "6677febc2dad8b3d"
      },
      {
        "chunk_id": 5,
        "text": "ile README using my MyMLH data.\"\n\n## Deploying Your Own Instance\n\nFor full control, you can deploy your own instance to Cloudflare. See the [Deployment Guide](DEPLOYMENT.md) for detailed instructions.\n\n## Contributing\n\nWe welcome contributions! Whether you're fixing a bug, adding a feature, or improving documentation, your help is appreciated.\n\nFor development setup, project structure, how to add tools, and contributing guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## License\n\n[MIT](https://wei.mit-license.org)",
        "start_pos": 8887,
        "end_pos": 9411,
        "token_count_estimate": 130,
        "source_type": "readme",
        "agent_id": "6677febc2dad8b3d"
      }
    ]
  },
  {
    "agent_id": "d63e3946f1dfd8c6",
    "name": "co.axiom/mcp",
    "source": "mcp",
    "source_url": "https://github.com/axiomhq/mcp",
    "description": "List datasets, schemas, run APL queries, and use prompts for exploration, anomalies, and monitoring.",
    "tools": [],
    "detected_capabilities": [
      "monitor"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-09-30T18:09:02.670593Z",
    "indexed_at": "2026-02-18T04:11:19.982818",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "```\n\n                  ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó\n                  ‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó\n                  ‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù\n                  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù\n                  ‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë\n                  ‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nModel Context Protocol Server ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Axiom, Inc.\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n```\n\n# axiom-mcp\nThe Axiom MCP Server connects AI assistants to your Axiom observability data using the Model Context Protocol (MCP). This repository contains:\n\n- A Cloudflare Workers app that hosts the MCP server (`apps/mcp`).\n- A TypeScript package with core MCP utilities and tools (`packages/mcp`).\n\nFor installation, setup, supported tools, authentication, and client-specific instructions (Claude, Cursor, VS Code, etc.), please see the official documentation:\n\nhttps://axiom.co/docs/console/intelligence/mcp-server#axiom-mcp-server\n\nIssues and contributions are welcome. See AGENTS.md for contributor guidelines.\n  - CSV formatting for tabular data instead of verbose JSON\n  - Automatic `maxBinAutoGroups` for time-series aggregations\n  - Intelligent result shaping that prioritizes important fields\n  - Adaptive truncation based on data volume\n\n## Runtime URL Parameters\n\nWhen connecting to the hosted server endpoints (`/sse` or `/mcp`), you can pass runtime tuning parameters via query string:\n\n- `max-age`: Integer. Caps total table cells rendered by tools. Example: `?max-age=500`.\n- `with-otel`: Boolean (`1`/`true`). Enables OpenTelemetry tool family if your org has OTel integrations. Example: `?with-otel=1`.\n\nExample connection URL (MCP Inspector):\n\n```\nhttp://localhost:8788/sse?org-id=<ORG_ID>&max-age=500&with-otel=1\n```\n\n\n## Troubleshooting\n\n### Connection Issues\n\nRemote MCP connections are still early. If you experience issues:\n1. Try restarting your client\n2. Disable and re-enable the Axiom MCP server\n3. Check your authentication credentials\n4. Try clearing and re-authenticating your client\n\n### Authentication Errors\n\n- **OAuth**: Ensure you're logged into Axiom in your browser\n\nOR\n- **Personal Token**: Verify your token starts with `xapt-` and hasn't expired\n- **Organization ID**: Must match the organization that issued the token\n\n## Support\n\n- [Documentation](https://axiom.co/docs)\n- [GitHub Issues](https://github.com/axiomhq/mcp/issues)\n- [Community Discord](https://axiom.co/discord)\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect AI assistants to Axiom observability data using the Model Context Protocol",
        "Host an MCP server via a Cloudflare Workers app",
        "Provide core MCP utilities and tools through a TypeScript package",
        "Format tabular data as CSV instead of verbose JSON",
        "Automatically determine maxBinAutoGroups for time-series aggregations",
        "Shape query results intelligently to prioritize important fields",
        "Adaptively truncate data based on volume",
        "Allow runtime tuning of server behavior via URL query parameters",
        "Enable OpenTelemetry tool family integration when requested"
      ],
      "limitations": [
        "Remote MCP connections are still early and may experience connection issues",
        "Authentication requires valid OAuth login or personal token with correct organization ID",
        "Personal tokens must start with 'xapt-' and not be expired"
      ],
      "requirements": [
        "Valid authentication via OAuth login or personal token starting with 'xapt-'",
        "Organization ID matching the token issuer",
        "Access to Axiom observability data and integrations",
        "Client support for MCP protocol and ability to pass runtime URL parameters"
      ]
    },
    "documentation_quality": 0.75,
    "quality_rationale": "The documentation provides installation pointers, usage examples, tool descriptions, authentication details, and troubleshooting guidance but refers to external docs for full setup and client-specific instructions.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "```\n\n                  ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó\n                  ‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó\n                  ‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù\n                  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù\n                  ‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë\n                  ‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nModel Context Protocol Server ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ Axiom, Inc.\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n```\n\n# axiom-mcp\nThe Axiom MCP Server connects AI assistants to your Axiom observability data using the Model Context Protocol (MCP). This repository contains:\n\n- A Cloudflare Workers app that hosts the MCP server (`apps/mcp`).\n- A TypeScript package with core MCP utilities and tools (`packages/mcp`).\n\nFor installation, setup, supported tools, authentication, and client-specific instructions (Claude, Cursor, VS Code, etc.), please see the official documentation:\n\nhttps://axiom.co/docs/console/intelligence/mcp-server#axiom-mcp-server\n\nIssues and contributions are welcome. See AGENTS.md for contributor guidelines.\n  - CSV formatting for tabular data instead of verbose JSON\n  - Automatic `maxBinAutoGroups` for time-series aggregations\n  - Intelligent result shaping that prioritizes important fields\n  - Adaptive truncation based on data volume\n\n## Runtime URL Parameters\n\nWhen connecting to the hosted server endpoints (`/sse` or `/mcp`), you can pass runtime tuning parameters via query string:\n\n- `max-age`: Integer. Caps total table cells rendered by tools. Example: `?max-age=500`.\n- `with-otel`: Boolean (`1`/`true`). Enables OpenTelemetry tool family if your org has OTel integrations. Example: `?with-otel=1`.\n\nExample connection URL (MCP Inspector):\n\n```\nhttp://localhost:8788/sse?org-id=<ORG_ID>&max-age=500&with-otel=1\n```\n\n\n## Troubleshooting\n\n### Connection Issues\n\nRemote MCP connections are still early. If you experience issues:\n1.",
        "start_pos": 0,
        "end_pos": 2038,
        "token_count_estimate": 509,
        "source_type": "readme",
        "agent_id": "d63e3946f1dfd8c6"
      },
      {
        "chunk_id": 1,
        "text": "Inspector):\n\n```\nhttp://localhost:8788/sse?org-id=<ORG_ID>&max-age=500&with-otel=1\n```\n\n\n## Troubleshooting\n\n### Connection Issues\n\nRemote MCP connections are still early. If you experience issues:\n1. Try restarting your client\n2. Disable and re-enable the Axiom MCP server\n3. Check your authentication credentials\n4. Try clearing and re-authenticating your client\n\n### Authentication Errors\n\n- **OAuth**: Ensure you're logged into Axiom in your browser\n\nOR\n- **Personal Token**: Verify your token starts with `xapt-` and hasn't expired\n- **Organization ID**: Must match the organization that issued the token\n\n## Support\n\n- [Documentation](https://axiom.co/docs)\n- [GitHub Issues](https://github.com/axiomhq/mcp/issues)\n- [Community Discord](https://axiom.co/discord)\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details.",
        "start_pos": 1838,
        "end_pos": 2675,
        "token_count_estimate": 209,
        "source_type": "readme",
        "agent_id": "d63e3946f1dfd8c6"
      }
    ]
  },
  {
    "agent_id": "803b55f1ff178154",
    "name": "co.contraption/mcp",
    "source": "mcp",
    "source_url": "https://github.com/contraptionco/mcp",
    "description": "An MCP server that provides [describe what your server does]",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-02T15:38:37.916768Z",
    "indexed_at": "2026-02-18T04:11:21.364142",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Contraption Company MCP\n\nAn MCP (Model Context Protocol) server for [Contraption Company](https://contraption.co) essay, built on [Chroma Cloud](https://trychroma.com).\n\n## How to Install\n\nContraption Company MCP is available as a hosted MCP server with no authentication.\n\n| Field      | Value                        |\n| ---------- | ---------------------------- |\n| Server URL | `https://mcp.contraption.co`  |\n\n### How to configure in common clients\n\n<details>\n<summary><b>Cursor</b></summary>\n\nUse the deep link to install directly in Cursor: [Install Contraption Company MCP](cursor://anysphere.cursor-deeplink/mcp/install?name=contraption-company&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250cmFwdGlvbi5jbyJ9).\n\nOr, create or edit `~/.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"contraption-company\": {\n      \"url\": \"https://mcp.contraption.co\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>ChatGPT</b></summary>\n\n1. Open **Settings ‚Üí Connectors**.\n2. Click **Create new connector**.\n3. Set **MCP Server URL** to `https://mcp.contraption.co`.\n4. Leave authentication blank and save.\n\n</details>\n\n<details>\n<summary><b>VS Code (Copilot Chat MCP)</b></summary>\n\nCreate or edit `.vscode/mcp.json`:\n\n```json\n{\n  \"servers\": {\n    \"contraption-company\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.contraption.co\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>Codex</b></summary>\n\nAdd to `~/.codex/config.toml`:\n\n```toml\n[mcp_servers.contraption-company]\ncommand = \"npx\"\nargs = [\"mcp-remote\", \"--transport\", \"http\", \"https://mcp.contraption.co\"]\n```\n\n</details>\n\n<details>\n<summary><b>Claude Code</b></summary>\n\nRun in your terminal:\n\n```bash\nclaude mcp add --transport http contraption-company https://mcp.contraption.co\n```\n\n</details>\n\n<details>\n<summary><b>OpenAI SDK (Python)</b></summary>\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"List the newest Contraption Company blog posts.\",\n    tools=[\n        {\n            \"type\": \"mcp\",\n            \"server_label\": \"contraption-company\",\n            \"server_url\": \"https://mcp.contraption.co\",\n            \"require_approval\": \"never\",\n        }\n    ],\n)\nprint(response)\n```\n\n</details>\n\n## Features\n\n- Semantic Search: Hybrid search with Voyage contextualized embeddings + sparse Splade vectors\n- Automatic Indexing: Syncs with the blog API on startup and via scheduled polling\n- Full Content Access: Uses Ghost Admin API to index all published posts and pages, including members-only posts\n- Fast Performance: Powered by FastAPI and Chroma Cloud\n- Background Updates: Polls Ghost every few minutes for new, updated, or deleted posts and pages\n- Query Logging: Records searches in a dedicated Chroma collection for analysis\n- Docker Ready: Includes Dockerfile for easy deployment\n- Well Tested: Comprehensive test suite with pytest\n\n## Run Locally\n\n1. Clone and install:\n\n```bash\ngit clone <repository>\ncd mcp\nuv sync --all-extras\n```\n\n2. Configure environment:\n\n```bash\ncp .env.example .env\n# Edit .env with your credentials\n```\n\n3. Run the server:\n\n```bash\n./run.sh\n# Or: uv run python -m src.main\n```\n\n### Docker\n\n```bash\n# Build\ndocker build -t contraption-mcp .\n\n# Run\ndocker run -p 8000:8000 --env-file .env contraption-mcp\n\n# Or use docker-compose\ndocker-compose up\n```\n\n## Configuration\n\nRunning locally requires credentials for external services:\n\n- **Ghost Admin API Key**: From your Ghost Admin panel (Settings > Integrations)\n- **Chroma Cloud Credentials**: Tenant ID, Database, and API key from Chroma Cloud\n- **Chroma Query Collection (optional)**: Set `CHROMA_QUERY_COLLECTION` to override the default `queries` collection\n- **Voyage API Key**: Required to generate contextualized embeddings\n- **Ghost Blog URL**: Your Ghost blog's URL\n- **Polling Interval (optional)**: Set `POLL_INTERVAL_SECONDS` to override the default 5 minute sync cadence\n\n## MCP Tools\n\n- `fetch(id=None, url=None, method=\"GET\", headers=None, body=None)`: Fetch a single post or page via the MCP fetch contract using the canonical URL as the identifier. Provide either the `id` returned by `list_posts`/`search` (which is the canonical URL) or a `url`; Ghost slugs and shorthand schemes are also accepted but responses always resolve to full URLs.\n- `list_posts(sort_by, page, limit)`: List posts with pagination, returning canonical URLs as identifiers\n- `search(query, limit)`: Semantic search across posts and pages that emits canonical URLs for result IDs\n\n## API Endpoints\n\n- `GET /`: Server info (redirects to GitHub repo for non-MCP requests)\n- `GET /health`: Health check\n- `GET /debug/search`: Debug search endpoint (see `/debug/docs` for Swagger UI)\n- `/mcp/*`: MCP protocol endpoints\n\n### Background Sync\n\nThe server polls the Ghost Admin API every 5 minutes to detect new, updated, or deleted posts and pages. Adjust the cadence by setting the `POLL_INTERVAL_SECONDS` environment variable.\n\n## Development\n\n```bash\n# Install dev dependencies\nmake dev\n\n# Run tests\nmake test\n\n# Lint and format\nmake format lint\n\n# Run all checks\nmake check\n```\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Perform semantic search across posts and pages using hybrid embeddings",
        "Automatically index blog content via Ghost Admin API on startup and scheduled polling",
        "Fetch individual posts or pages by canonical URL or ID",
        "List posts with pagination and sorting",
        "Provide fast API responses powered by FastAPI and Chroma Cloud",
        "Log queries for analysis in a dedicated Chroma collection",
        "Support integration with multiple clients including Cursor, ChatGPT, VS Code, Codex, Claude Code, and OpenAI SDK",
        "Run as a hosted MCP server or locally via Docker or direct installation",
        "Poll Ghost API periodically to sync new, updated, or deleted content"
      ],
      "limitations": [
        "Requires valid API keys and credentials for Ghost Admin API, Chroma Cloud, and Voyage API",
        "No authentication on the hosted MCP server",
        "Polling interval defaults to 5 minutes and may be adjusted only via environment variable",
        "Only indexes published posts and pages including members-only posts from Ghost",
        "Responses always resolve to full canonical URLs, no partial or shorthand URLs returned"
      ],
      "requirements": [
        "Ghost Admin API key from Ghost Admin panel",
        "Chroma Cloud credentials including Tenant ID, Database, and API key",
        "Voyage API key for generating contextualized embeddings",
        "Ghost blog URL",
        "Optional environment variables for polling interval and Chroma query collection override"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples across multiple clients, detailed feature descriptions, configuration requirements, API endpoint details, and development/testing guidance.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Contraption Company MCP\n\nAn MCP (Model Context Protocol) server for [Contraption Company](https://contraption.co) essay, built on [Chroma Cloud](https://trychroma.com).\n\n## How to Install\n\nContraption Company MCP is available as a hosted MCP server with no authentication.\n\n| Field      | Value                        |\n| ---------- | ---------------------------- |\n| Server URL | `https://mcp.contraption.co`  |\n\n### How to configure in common clients\n\n<details>\n<summary><b>Cursor</b></summary>\n\nUse the deep link to install directly in Cursor: [Install Contraption Company MCP](cursor://anysphere.cursor-deeplink/mcp/install?name=contraption-company&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250cmFwdGlvbi5jbyJ9).\n\nOr, create or edit `~/.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"contraption-company\": {\n      \"url\": \"https://mcp.contraption.co\"\n    }\n  }\n}\n```\n\n</details>\n\n<details>\n<summary><b>ChatGPT</b></summary>\n\n1. Open **Settings ‚Üí Connectors**.\n2. Click **Create new connector**.\n3. Set **MCP Server URL** to `https://mcp.contraption.co`.\n4. Leave authentication blank and save.",
        "start_pos": 0,
        "end_pos": 1097,
        "token_count_estimate": 274,
        "source_type": "readme",
        "agent_id": "803b55f1ff178154"
      },
      {
        "chunk_id": 1,
        "text": "nAI\n\nclient = OpenAI()\n\nresponse = client.responses.create(\n    model=\"gpt-5\",\n    input=\"List the newest Contraption Company blog posts.\",\n    tools=[\n        {\n            \"type\": \"mcp\",\n            \"server_label\": \"contraption-company\",\n            \"server_url\": \"https://mcp.contraption.co\",\n            \"require_approval\": \"never\",\n        }\n    ],\n)\nprint(response)\n```\n\n</details>\n\n## Features\n\n- Semantic Search: Hybrid search with Voyage contextualized embeddings + sparse Splade vectors\n- Automatic Indexing: Syncs with the blog API on startup and via scheduled polling\n- Full Content Access: Uses Ghost Admin API to index all published posts and pages, including members-only posts\n- Fast Performance: Powered by FastAPI and Chroma Cloud\n- Background Updates: Polls Ghost every few minutes for new, updated, or deleted posts and pages\n- Query Logging: Records searches in a dedicated Chroma collection for analysis\n- Docker Ready: Includes Dockerfile for easy deployment\n- Well Tested: Comprehensive test suite with pytest\n\n## Run Locally\n\n1. Clone and install:\n\n```bash\ngit clone <repository>\ncd mcp\nuv sync --all-extras\n```\n\n2. Configure environment:\n\n```bash\ncp .env.example .env\n# Edit .env with your credentials\n```\n\n3. Run the server:\n\n```bash\n./run.sh\n# Or: uv run python -m src.main\n```\n\n### Docker\n\n```bash\n# Build\ndocker build -t contraption-mcp .",
        "start_pos": 1848,
        "end_pos": 3216,
        "token_count_estimate": 342,
        "source_type": "readme",
        "agent_id": "803b55f1ff178154"
      },
      {
        "chunk_id": 2,
        "text": "ge API Key**: Required to generate contextualized embeddings\n- **Ghost Blog URL**: Your Ghost blog's URL\n- **Polling Interval (optional)**: Set `POLL_INTERVAL_SECONDS` to override the default 5 minute sync cadence\n\n## MCP Tools\n\n- `fetch(id=None, url=None, method=\"GET\", headers=None, body=None)`: Fetch a single post or page via the MCP fetch contract using the canonical URL as the identifier. Provide either the `id` returned by `list_posts`/`search` (which is the canonical URL) or a `url`; Ghost slugs and shorthand schemes are also accepted but responses always resolve to full URLs.\n- `list_posts(sort_by, page, limit)`: List posts with pagination, returning canonical URLs as identifiers\n- `search(query, limit)`: Semantic search across posts and pages that emits canonical URLs for result IDs\n\n## API Endpoints\n\n- `GET /`: Server info (redirects to GitHub repo for non-MCP requests)\n- `GET /health`: Health check\n- `GET /debug/search`: Debug search endpoint (see `/debug/docs` for Swagger UI)\n- `/mcp/*`: MCP protocol endpoints\n\n### Background Sync\n\nThe server polls the Ghost Admin API every 5 minutes to detect new, updated, or deleted posts and pages. Adjust the cadence by setting the `POLL_INTERVAL_SECONDS` environment variable.\n\n## Development\n\n```bash\n# Install dev dependencies\nmake dev\n\n# Run tests\nmake test\n\n# Lint and format\nmake format lint\n\n# Run all checks\nmake check\n```\n\n## License\n\nMIT",
        "start_pos": 3696,
        "end_pos": 5110,
        "token_count_estimate": 353,
        "source_type": "readme",
        "agent_id": "803b55f1ff178154"
      }
    ]
  },
  {
    "agent_id": "f85ef182b80ffd1b",
    "name": "co.dockai/mcp",
    "source": "mcp",
    "source_url": "https://github.com/dock-ai/mcp",
    "description": "Discover MCP endpoints for real-world entities by resolving business domains.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-01-05T11:36:22.049326Z",
    "indexed_at": "2026-02-18T04:11:22.608953",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Dock AI MCP\n\nMCP server for [Dock AI](https://dockai.co) - discover MCP endpoints for real-world entities.\n\n## What is this?\n\nDock AI is a registry that maps businesses to their MCP connectors. This MCP server allows AI agents to discover which MCP servers can interact with a given entity (restaurant, hotel, salon, etc.) by querying the Dock AI registry.\n\n## Hosted Version\n\nUse the hosted version at `https://connect.dockai.co/mcp` - no installation required.\n\n```json\n{\n  \"mcpServers\": {\n    \"dock-ai\": {\n      \"url\": \"https://connect.dockai.co/mcp\"\n    }\n  }\n}\n```\n\n## Self-Hosting\n\n### Deploy to Vercel\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/dock-ai/mcp)\n\n### Run locally\n\n```bash\n# Using uvx\nuvx dock-ai-mcp\n\n# Or install and run\npip install dock-ai-mcp\ndock-ai-mcp\n```\n\nThe server starts on `http://0.0.0.0:8080/mcp`.\n\n## Tools\n\n### `resolve_domain`\n\nCheck if an MCP connector exists for a business domain.\n\n**Input:**\n\n- `domain` (string): The business domain to resolve (e.g., \"example-restaurant.com\")\n\n**Output:**\n\n```json\n{\n  \"domain\": \"example-restaurant.com\",\n  \"entities\": [\n    {\n      \"name\": \"Example Restaurant\",\n      \"path\": null,\n      \"location\": { \"city\": \"Paris\", \"country\": \"FR\" },\n      \"mcps\": [\n        {\n          \"provider\": \"booking-provider\",\n          \"endpoint\": \"https://mcp.booking-provider.com\",\n          \"entity_id\": \"entity-123\",\n          \"capabilities\": [\"reservations\", \"availability\"],\n          \"verification\": { \"level\": 2, \"method\": \"dual_attestation\" }\n        }\n      ]\n    }\n  ],\n  \"claude_desktop_config\": {\n    \"mcpServers\": {\n      \"booking-provider\": { \"url\": \"https://mcp.booking-provider.com/mcp\" }\n    }\n  }\n}\n```\n\n## Examples\n\n### Example 1: Restaurant Reservation\n\n```\nUser: \"Book a table at Gloria Osteria Paris\"\n\nAgent: [searches web for \"Gloria Osteria Paris official website\"]\n       -> Finds domain: gloria-osteria.com\n       [calls resolve_domain(\"gloria-osteria.com\")]\n       -> Gets MCP endpoint for SevenRooms\n       -> Connects to the MCP server\n       -> Books the table\n```\n\n### Example 2: Hotel Booking\n\n```\nUser: \"I need a room at The Hoxton in London\"\n\nAgent: [searches web for \"The Hoxton London website\"]\n       -> Finds domain: thehoxton.com\n       [calls resolve_domain(\"thehoxton.com\")]\n       -> Gets MCP endpoints for available booking providers\n       -> Uses the MCP to check availability and book\n```\n\n### Example 3: Business with No MCP Yet\n\n```\nUser: \"Book at Le Paris Paris restaurant\"\n\nAgent: [calls resolve_domain(\"leparisparis.fr\")]\n       -> Response shows pending_providers: [{ \"provider\": \"thefork\", ... }]\n       -> Informs user: \"This restaurant uses TheFork for reservations,\n          but TheFork hasn't published an MCP connector yet.\n          You can book directly on TheFork's website.\"\n```\n\n## Support\n\n- **Documentation**: [dockai.co/docs](https://dockai.co/docs)\n- **Issues**: [GitHub Issues](https://github.com/dock-ai/mcp/issues)\n- **Email**: support@dockai.co\n\n## Privacy\n\nThis MCP server queries the Dock AI registry API to resolve domains. No user data is collected or stored. See our [Privacy Policy](https://dockai.co/privacy).\n\n## License\n\nMIT\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Discover MCP endpoints for real-world business entities",
        "Resolve if an MCP connector exists for a given business domain",
        "Provide detailed MCP server information including provider, endpoint, entity ID, and capabilities",
        "Support verification metadata for MCP connectors",
        "Enable AI agents to find MCP servers to interact with restaurants, hotels, salons, and similar entities",
        "Allow self-hosting via Vercel deployment or local installation",
        "Provide a hosted MCP server endpoint for immediate use"
      ],
      "limitations": [
        "Cannot interact directly with businesses without MCP connectors published",
        "Dependent on Dock AI registry API for domain resolution",
        "No user data collection or storage, limiting personalized features",
        "Some businesses may have pending MCP providers without published connectors"
      ],
      "requirements": [
        "Access to Dock AI registry API",
        "Python environment for local installation (pip install dock-ai-mcp)",
        "Optional: Vercel account for deployment",
        "Network access to hosted or self-hosted MCP server endpoints"
      ]
    },
    "documentation_quality": 0.85,
    "quality_rationale": "The documentation includes installation instructions, usage examples, detailed tool descriptions, limitations, and requirements, providing a comprehensive overview.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Dock AI MCP\n\nMCP server for [Dock AI](https://dockai.co) - discover MCP endpoints for real-world entities.\n\n## What is this?\n\nDock AI is a registry that maps businesses to their MCP connectors. This MCP server allows AI agents to discover which MCP servers can interact with a given entity (restaurant, hotel, salon, etc.) by querying the Dock AI registry.\n\n## Hosted Version\n\nUse the hosted version at `https://connect.dockai.co/mcp` - no installation required.\n\n```json\n{\n  \"mcpServers\": {\n    \"dock-ai\": {\n      \"url\": \"https://connect.dockai.co/mcp\"\n    }\n  }\n}\n```\n\n## Self-Hosting\n\n### Deploy to Vercel\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/dock-ai/mcp)\n\n### Run locally\n\n```bash\n# Using uvx\nuvx dock-ai-mcp\n\n# Or install and run\npip install dock-ai-mcp\ndock-ai-mcp\n```\n\nThe server starts on `http://0.0.0.0:8080/mcp`.\n\n## Tools\n\n### `resolve_domain`\n\nCheck if an MCP connector exists for a business domain.",
        "start_pos": 0,
        "end_pos": 992,
        "token_count_estimate": 248,
        "source_type": "readme",
        "agent_id": "f85ef182b80ffd1b"
      },
      {
        "chunk_id": 1,
        "text": "ris\"\n\nAgent: [searches web for \"Gloria Osteria Paris official website\"]\n       -> Finds domain: gloria-osteria.com\n       [calls resolve_domain(\"gloria-osteria.com\")]\n       -> Gets MCP endpoint for SevenRooms\n       -> Connects to the MCP server\n       -> Books the table\n```\n\n### Example 2: Hotel Booking\n\n```\nUser: \"I need a room at The Hoxton in London\"\n\nAgent: [searches web for \"The Hoxton London website\"]\n       -> Finds domain: thehoxton.com\n       [calls resolve_domain(\"thehoxton.com\")]\n       -> Gets MCP endpoints for available booking providers\n       -> Uses the MCP to check availability and book\n```\n\n### Example 3: Business with No MCP Yet\n\n```\nUser: \"Book at Le Paris Paris restaurant\"\n\nAgent: [calls resolve_domain(\"leparisparis.fr\")]\n       -> Response shows pending_providers: [{ \"provider\": \"thefork\", ... }]\n       -> Informs user: \"This restaurant uses TheFork for reservations,\n          but TheFork hasn't published an MCP connector yet.\n          You can book directly on TheFork's website.\"\n```\n\n## Support\n\n- **Documentation**: [dockai.co/docs](https://dockai.co/docs)\n- **Issues**: [GitHub Issues](https://github.com/dock-ai/mcp/issues)\n- **Email**: support@dockai.co\n\n## Privacy\n\nThis MCP server queries the Dock AI registry API to resolve domains. No user data is collected or stored. See our [Privacy Policy](https://dockai.co/privacy).\n\n## License\n\nMIT",
        "start_pos": 1848,
        "end_pos": 3236,
        "token_count_estimate": 346,
        "source_type": "readme",
        "agent_id": "f85ef182b80ffd1b"
      }
    ]
  },
  {
    "agent_id": "15640b0afc236565",
    "name": "co.flyweel/mcp-server",
    "source": "mcp",
    "source_url": "https://api.flyweel.co/functions/v1/mcp-server/mcp",
    "description": "Access Google & Meta Ads data via AI. Analyse campaign performance in seconds.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2026-01-15T02:02:08.72561Z",
    "indexed_at": "2026-02-18T04:11:24.716705",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Access Google Ads data",
        "Access Meta Ads data",
        "Analyse campaign performance quickly"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "1664a594fddbaf16",
    "name": "co.heyspark.mcp/server",
    "source": "mcp",
    "source_url": "https://github.com/jhibird/HeySpark-MCP",
    "description": "Search and discover local businesses. 30+ categories with verified contact info, hours, and reviews.",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-16T13:12:52.113124Z",
    "indexed_at": "2026-02-18T04:11:28.491550",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search local businesses",
        "Discover local businesses",
        "Provide verified contact information for businesses",
        "Provide business hours",
        "Provide business reviews",
        "Support search across 30+ business categories"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functionality and scope but lacks detailed examples, usage instructions, or limitations.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "7de659d6c29d1d56",
    "name": "co.huggingface/hf-mcp-server",
    "source": "mcp",
    "source_url": "https://huggingface.co/mcp?login",
    "description": "Connect to Hugging Face Hub and thousands of Gradio AI Applications",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-22T10:55:52.98995Z",
    "indexed_at": "2026-02-18T04:11:30.279095",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Connect to Hugging Face Hub",
        "Access thousands of Gradio AI Applications"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of connectivity and access capabilities but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "228c0629c179c8fc",
    "name": "co.okahu.mcp-registry/okahu",
    "source": "mcp",
    "source_url": "https://mcp.okahu.co",
    "description": "Cloud hosted Okahu MCP server that helps you manage genAI trace data",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-12-29T21:02:43.660564Z",
    "indexed_at": "2026-02-18T04:11:30.581387",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage genAI trace data"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with minimal detail, providing only a basic idea of the server's purpose.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "228c0629c179c8fc",
    "name": "co.okahu.mcp-registry/okahu",
    "source": "mcp",
    "source_url": "https://mcp.okahu.co/mcp",
    "description": "Cloud hosted Okahu MCP server that helps you manage genAI trace data",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-12-29T21:18:47.587908Z",
    "indexed_at": "2026-02-18T04:11:32.179713",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Manage genAI trace data"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence providing only a basic overview without details on features, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b795f13997b07274",
    "name": "co.pipeboard/meta-ads-mcp",
    "source": "mcp",
    "source_url": "https://mcp.pipeboard.co/meta-ads-mcp",
    "description": "Facebook / Meta Ads automation with AI: analyze performance, test creatives, optimize spend.",
    "tools": [],
    "detected_capabilities": [
      "analyze"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-24T16:14:42.455797Z",
    "indexed_at": "2026-02-18T04:11:33.233943",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Analyze Facebook/Meta Ads performance",
        "Test ad creatives",
        "Optimize ad spend"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "Provides a basic description of core functionalities but lacks detailed examples, structure, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "855766f4e8fe60ab",
    "name": "co.thisdot.docusign-navigator/mcp",
    "source": "mcp",
    "source_url": "https://github.com/thisdot/docusign-navigator-mcp",
    "description": "Secure Docusign Navigator integration for AI assistants to access and analyze agreement data.",
    "tools": [],
    "detected_capabilities": [
      "analyze"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-24T16:26:39.380072Z",
    "indexed_at": "2026-02-18T04:11:35.042305",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Docusign Navigator MCP Server\n\n<div align=\"center\">\n  <img src=\"public/thisdot-labs-logo.png\" alt=\"This Dot Labs\" width=\"200\"/>\n\n**By This Dot Labs**\n\n</div>\n\n<br/>\n\nA Model Context Protocol (MCP) server that connects your AI assistant to Docusign Navigator. Query and analyze your Docusign agreements using natural language - no complex APIs or manual searches required.\n\n## Why Use This?\n\nTransform how you work with Docusign agreements:\n\n- **Natural Language Access**: Ask questions like \"Show me my pending contracts\" or \"Find agreements with XYZ Corp\"\n- **AI-Powered Insights**: Let your AI assistant analyze agreement details, statuses, and metadata\n- **Secure Connection**: OAuth 2.0 authentication keeps your Docusign data safe\n- **No Code Required**: Works directly with compatible AI tools like Claude Desktop and VS Code\n\n## What You Need\n\n- Active Docusign account with Navigator access\n- Compatible AI client (Claude Desktop, VS Code with MCP extension, etc.)\n- Internet connection\n\n## Getting Started\n\n### 1. Add to Your AI Client\n\nThe server is deployed and ready to use at: **`https://docusign-navigator.thisdot.co/mcp`**\n\nChoose your AI client below:\n\n#### Claude Desktop\n\nAdd this to your Claude Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"docusign-navigator\": {\n      \"command\": \"mcp-server-fetch\",\n      \"args\": [\"https://docusign-navigator.thisdot.co/mcp\"]\n    }\n  }\n}\n```\n\n#### Visual Studio Code\n\n1. Open Command Palette: `Ctrl+Shift+P` / `Cmd+Shift+P`\n2. Type: `mcp: add server`\n3. Select `HTTP (HTTP or Server-Sent Events)`\n4. Enter: `https://docusign-navigator.thisdot.co/mcp`\n\n#### Other MCP Clients\n\nAdd to your configuration:\n\n```json\n{\n  \"servers\": {\n    \"docusign-navigator\": {\n      \"url\": \"https://docusign-navigator.thisdot.co/mcp\",\n      \"type\": \"http\"\n    }\n  }\n}\n```\n\n### 2. Connect Your Docusign Account\n\nWhen you first use a Docusign command, you'll be prompted to authenticate:\n\n1. Your AI client will provide an authorization link\n2. Click the link to sign in to Docusign\n3. Authorize the connection\n4. Return to your AI client - you're ready to go!\n\n### 3. Start Using Natural Language\n\nTry these example queries with your AI assistant:\n\n```\n\"Show me my Docusign agreements\"\n\"Tell me about agreement [ID]\"\n\"Find contracts with ABC Company\"\n\"What agreements are pending signature?\"\n```\n\n## What You Can Do\n\nYour AI assistant will have access to these capabilities:\n\n### Check Authentication\n\n\"Am I connected to Docusign?\"\n\"Check my authentication status\"\n\n### List Agreements\n\n\"Show me all my agreements\"\n\"What contracts do I have?\"\n\"List my Docusign documents\"\n\n### Get Agreement Details\n\n\"Tell me about agreement [ID]\"\n\"Show me details for contract [ID]\"\n\n### Search Agreements\n\n\"Find service agreements\"\n\"Search for contracts with ABC Company\"\n\"Show me expired agreements\"\n\n## Example Conversation\n\n```\nYou: \"Show me my Docusign agreements\"\nAI: \"You have 3 agreements:\n     ‚Ä¢ Service Agreement with XYZ Corp (pending signature)\n     ‚Ä¢ NDA with ABC Inc (completed)\n     ‚Ä¢ Consulting Contract (in review)\"\n\nYou: \"Tell me more about the Service Agreement\"\nAI: \"The Service Agreement with XYZ Corp was created on January 15th\n     and is currently pending signature. It includes standard service\n     terms and payment schedules.\"\n```\n\n## Security & Privacy\n\nYour data stays secure:\n\n- **OAuth 2.0 Authentication**: Industry-standard secure authentication\n- **No Data Storage**: Your agreements are never stored on our servers\n- **Direct API Access**: Real-time connection to your Docusign account\n- **Revocable Access**: Disconnect anytime through your Docusign settings\n\n## Troubleshooting\n\n### Can't Connect?\n\n1. Verify your Docusign account has Navigator access\n2. Check that you completed the OAuth authorization\n3. Try the \"Check authentication status\" command\n4. Ensure your AI client supports MCP HTTP transport\n\n### No Agreements Showing?\n\n1. Confirm you have agreements in Docusign Navigator\n2. Check that Navigator is enabled for your account\n3. Try authenticating again\n\n### Still Need Help?\n\n- [Report an Issue](https://github.com/thisdot/docusign-navigator-mcp/issues)\n- [Learn About MCP](https://modelcontextprotocol.io/)\n\n## Contributing\n\nWant to contribute or run your own instance? See our [Contributing Guide](CONTRIBUTING.md) for development setup, architecture details, and deployment instructions.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Connect AI assistants to Docusign Navigator via MCP",
        "Authenticate users securely using OAuth 2.0",
        "Query and list Docusign agreements using natural language",
        "Retrieve detailed information about specific agreements",
        "Search agreements by criteria such as company name or status",
        "Check authentication status with Docusign",
        "Provide real-time access to Docusign data without storing it"
      ],
      "limitations": [
        "Requires active Docusign account with Navigator access",
        "Depends on AI clients that support MCP HTTP transport",
        "No data storage on server side; all data accessed live",
        "Limited to agreements accessible via Docusign Navigator",
        "OAuth authorization must be completed before use"
      ],
      "requirements": [
        "Active Docusign account with Navigator access",
        "Compatible AI client supporting MCP protocol (e.g., Claude Desktop, VS Code with MCP extension)",
        "Internet connection",
        "OAuth 2.0 authentication flow completion"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed capability descriptions, security considerations, troubleshooting tips, and requirements, making it highly complete and user-friendly.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Docusign Navigator MCP Server\n\n<div align=\"center\">\n  <img src=\"public/thisdot-labs-logo.png\" alt=\"This Dot Labs\" width=\"200\"/>\n\n**By This Dot Labs**\n\n</div>\n\n<br/>\n\nA Model Context Protocol (MCP) server that connects your AI assistant to Docusign Navigator. Query and analyze your Docusign agreements using natural language - no complex APIs or manual searches required.\n\n## Why Use This?\n\nTransform how you work with Docusign agreements:\n\n- **Natural Language Access**: Ask questions like \"Show me my pending contracts\" or \"Find agreements with XYZ Corp\"\n- **AI-Powered Insights**: Let your AI assistant analyze agreement details, statuses, and metadata\n- **Secure Connection**: OAuth 2.0 authentication keeps your Docusign data safe\n- **No Code Required**: Works directly with compatible AI tools like Claude Desktop and VS Code\n\n## What You Need\n\n- Active Docusign account with Navigator access\n- Compatible AI client (Claude Desktop, VS Code with MCP extension, etc.)\n- Internet connection\n\n## Getting Started\n\n### 1. Add to Your AI Client\n\nThe server is deployed and ready to use at: **`https://docusign-navigator.thisdot.co/mcp`**\n\nChoose your AI client below:\n\n#### Claude Desktop\n\nAdd this to your Claude Desktop configuration file:\n\n```json\n{\n  \"mcpServers\": {\n    \"docusign-navigator\": {\n      \"command\": \"mcp-server-fetch\",\n      \"args\": [\"https://docusign-navigator.thisdot.co/mcp\"]\n    }\n  }\n}\n```\n\n#### Visual Studio Code\n\n1. Open Command Palette: `Ctrl+Shift+P` / `Cmd+Shift+P`\n2. Type: `mcp: add server`\n3. Select `HTTP (HTTP or Server-Sent Events)`\n4. Enter: `https://docusign-navigator.thisdot.co/mcp`\n\n#### Other MCP Clients\n\nAdd to your configuration:\n\n```json\n{\n  \"servers\": {\n    \"docusign-navigator\": {\n      \"url\": \"https://docusign-navigator.thisdot.co/mcp\",\n      \"type\": \"http\"\n    }\n  }\n}\n```\n\n### 2. Connect Your Docusign Account\n\nWhen you first use a Docusign command, you'll be prompted to authenticate:\n\n1. Your AI client will provide an authorization link\n2. Click the link to sign in to Docusign\n3.",
        "start_pos": 0,
        "end_pos": 2035,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "855766f4e8fe60ab"
      },
      {
        "chunk_id": 1,
        "text": "nect Your Docusign Account\n\nWhen you first use a Docusign command, you'll be prompted to authenticate:\n\n1. Your AI client will provide an authorization link\n2. Click the link to sign in to Docusign\n3. Authorize the connection\n4. Return to your AI client - you're ready to go!\n\n### 3. Start Using Natural Language\n\nTry these example queries with your AI assistant:\n\n```\n\"Show me my Docusign agreements\"\n\"Tell me about agreement [ID]\"\n\"Find contracts with ABC Company\"\n\"What agreements are pending signature?\"\n```\n\n## What You Can Do\n\nYour AI assistant will have access to these capabilities:\n\n### Check Authentication\n\n\"Am I connected to Docusign?\"\n\"Check my authentication status\"\n\n### List Agreements\n\n\"Show me all my agreements\"\n\"What contracts do I have?\"\n\"List my Docusign documents\"\n\n### Get Agreement Details\n\n\"Tell me about agreement [ID]\"\n\"Show me details for contract [ID]\"\n\n### Search Agreements\n\n\"Find service agreements\"\n\"Search for contracts with ABC Company\"\n\"Show me expired agreements\"\n\n## Example Conversation\n\n```\nYou: \"Show me my Docusign agreements\"\nAI: \"You have 3 agreements:\n     ‚Ä¢ Service Agreement with XYZ Corp (pending signature)\n     ‚Ä¢ NDA with ABC Inc (completed)\n     ‚Ä¢ Consulting Contract (in review)\"\n\nYou: \"Tell me more about the Service Agreement\"\nAI: \"The Service Agreement with XYZ Corp was created on January 15th\n     and is currently pending signature. It includes standard service\n     terms and payment schedules.\"\n```\n\n## Security & Privacy\n\nYour data stays secure:\n\n- **OAuth 2.0 Authentication**: Industry-standard secure authentication\n- **No Data Storage**: Your agreements are never stored on our servers\n- **Direct API Access**: Real-time connection to your Docusign account\n- **Revocable Access**: Disconnect anytime through your Docusign settings\n\n## Troubleshooting\n\n### Can't Connect?\n\n1. Verify your Docusign account has Navigator access\n2. Check that you completed the OAuth authorization\n3. Try the \"Check authentication status\" command\n4.",
        "start_pos": 1835,
        "end_pos": 3829,
        "token_count_estimate": 498,
        "source_type": "readme",
        "agent_id": "855766f4e8fe60ab"
      },
      {
        "chunk_id": 2,
        "text": "gs\n\n## Troubleshooting\n\n### Can't Connect?\n\n1. Verify your Docusign account has Navigator access\n2. Check that you completed the OAuth authorization\n3. Try the \"Check authentication status\" command\n4. Ensure your AI client supports MCP HTTP transport\n\n### No Agreements Showing?\n\n1. Confirm you have agreements in Docusign Navigator\n2. Check that Navigator is enabled for your account\n3. Try authenticating again\n\n### Still Need Help?\n\n- [Report an Issue](https://github.com/thisdot/docusign-navigator-mcp/issues)\n- [Learn About MCP](https://modelcontextprotocol.io/)\n\n## Contributing\n\nWant to contribute or run your own instance? See our [Contributing Guide](CONTRIBUTING.md) for development setup, architecture details, and deployment instructions.",
        "start_pos": 3629,
        "end_pos": 4380,
        "token_count_estimate": 187,
        "source_type": "readme",
        "agent_id": "855766f4e8fe60ab"
      }
    ]
  },
  {
    "agent_id": "e796150128b6519c",
    "name": "com.1stdibs/1stDibs",
    "source": "mcp",
    "source_url": "https://www.1stdibs.com/soa/mcp/",
    "description": "MCP server for browsing and searching items on 1stDibs marketplace.",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-16T23:01:22.451132Z",
    "indexed_at": "2026-02-18T04:11:36.617348",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Browse items on 1stDibs marketplace",
        "Search items on 1stDibs marketplace"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of browsing and searching capabilities but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "86b2663e5103a2d1",
    "name": "com.adspirer/ads",
    "source": "mcp",
    "source_url": "https://github.com/amekala/ads-mcp",
    "description": "Remote MCP for cross-platform ad creation (Google Ads, TikTok). OAuth 2.1 with progress streaming.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-28T02:00:45.758066Z",
    "indexed_at": "2026-02-18T04:11:37.667694",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Ads MCP\n\nRemote Model Context Protocol (MCP) server for cross-platform ad management. Create, analyze, and optimize campaigns across **Google Ads, Meta Ads, TikTok Ads, and LinkedIn Ads** from any MCP-compatible AI assistant.\n\n## Quick Links\n\n- **MCP Remote URL:** `https://mcp.adspirer.com/mcp`\n- **Transport:** Streamable HTTP\n- **Registry ID:** [`com.adspirer/ads`](https://registry.modelcontextprotocol.io)\n- **Authentication:** OAuth 2.1 with PKCE (dynamic client registration supported)\n- **Website:** https://www.adspirer.com\n- **Support:** abhi@adspirer.com\n\n## What It Does\n\n- **91 tools** across 4 ad platforms for campaign creation, performance analysis, and optimization\n- Plan and validate campaigns using structured prompts\n- Research keywords with real CPC data and competitive analysis\n- Create Google Ads Search and Performance Max campaigns end-to-end\n- Launch Meta, TikTok, and LinkedIn ad campaigns\n- Analyze performance with actionable optimization recommendations\n- Manage multiple ad accounts across platforms\n\n## Platforms & Tools\n\n| Platform | Tools | Capabilities |\n|----------|-------|-------------|\n| Google Ads | 39 | Search campaigns, Performance Max, keyword research, performance analysis, asset management, ad extensions |\n| LinkedIn Ads | 28 | Sponsored content, lead gen forms, audience targeting, campaign analytics |\n| Meta Ads | 20 | Image campaigns, carousel campaigns, audience targeting, performance tracking |\n| TikTok Ads | 4 | In-feed video/image campaigns, asset validation |\n| **Total** | **91** | Plus 2 resources and 6 prompts |\n\n## How to Connect\n\nSee [CONNECTING.md](CONNECTING.md) for detailed setup instructions for each platform.\n\n### Claude (Recommended)\n\n1. Open **Settings > Connectors > Add custom connector**\n2. Name: **Ads MCP**\n3. URL: `https://mcp.adspirer.com/mcp`\n4. Complete OAuth 2.1 sign-in\n5. Link your ad accounts on first use\n\n### Claude Code\n\n```bash\nclaude mcp add --transport http adspirer https://mcp.adspirer.com/mcp\n```\n\n### ChatGPT\n\n1. Open **Settings > Connectors > Add custom connector**\n2. Name: **Ads MCP**\n3. URL: `https://mcp.adspirer.com/mcp`\n4. Follow OAuth 2.1 sign-in flow\n\n### Cursor\n\nAdd to `~/.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"adspirer\": {\n      \"url\": \"https://mcp.adspirer.com/mcp\"\n    }\n  }\n}\n```\n\n### OpenAI Codex\n\nAdd to `~/.codex/config.toml`:\n\n```toml\n[mcp_servers.adspirer]\nurl = \"https://mcp.adspirer.com/mcp\"\n```\n\n## Example Prompts\n\n**Keyword Research:**\n```\nResearch keywords for my emergency plumbing business in Chicago.\nShow me high-intent keywords with real CPC data and budget recommendations.\n```\n\n**Performance Analysis:**\n```\nShow me campaign performance for the last 30 days across all platforms.\nWhich campaigns are converting best and what should I optimize?\n```\n\n**Campaign Creation:**\n```\nCreate a Google Performance Max campaign for luxury watches targeting\nNew York with a $50/day budget.\n```\n\n**Multi-Platform Strategy:**\n```\nI want to advertise my handmade jewelry business across Google and LinkedIn.\nResearch keywords for Google Ads and create a LinkedIn sponsored content campaign\ntargeting small business owners.\n```\n\n## Technical Details\n\n- **Protocol:** MCP 2025-03-26 (with fallback to 2024-11-05)\n- **Transport:** Streamable HTTP\n- **OAuth:** RFC 8252 (Authorization Code + PKCE) with RFC 7591 (Dynamic Client Registration) and RFC 9728 (Protected Resource Metadata)\n- **Tool Annotations:** All tools include MCP safety metadata (`readOnlyHint`, `destructiveHint`)\n\n## Security\n\n- HTTPS/TLS for all data transmission\n- OAuth 2.1 with PKCE for authentication\n- Dynamic client registration for CLI tools (Claude Code, Cursor, Codex)\n- Encrypted token storage\n- No conversation logging -- only tool requests are processed\n\nSee [SECURITY.md](SECURITY.md) for vulnerability reporting.\n\n## Documentation\n\n- [Connecting Guide](CONNECTING.md)\n- [Privacy Policy](PRIVACY.md)\n- [Terms of Service](TERMS.md)\n- [Security](SECURITY.md)\n- [Support](SUPPORT.md)\n\n## Support\n\n- **Email:** abhi@adspirer.com\n- **Issues:** https://github.com/amekala/ads-mcp/issues\n- **Website:** https://www.adspirer.com\n- **Server Status:** https://mcp.adspirer.com/health\n\n## License\n\nProprietary -- See [Terms of Service](TERMS.md) for usage terms.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Create and manage ad campaigns across Google Ads, Meta Ads, TikTok Ads, and LinkedIn Ads",
        "Analyze campaign performance with actionable optimization recommendations",
        "Research keywords with real CPC data and competitive analysis",
        "Plan and validate ad campaigns using structured prompts",
        "Manage multiple ad accounts across supported platforms",
        "Launch various campaign types including Google Search, Performance Max, Meta image and carousel campaigns, TikTok in-feed campaigns, and LinkedIn sponsored content",
        "Provide cross-platform advertising strategies combining multiple ad networks",
        "Support OAuth 2.1 with PKCE authentication and dynamic client registration for secure access",
        "Expose 91 specialized tools for detailed ad management and 6 prompts for common workflows"
      ],
      "limitations": [
        "Limited TikTok Ads support with only 4 tools focused on in-feed video/image campaigns and asset validation",
        "Proprietary license restricting usage as per Terms of Service",
        "No conversation logging; only tool requests are processed, which may limit contextual interactions",
        "Requires OAuth 2.1 authentication and linked ad accounts to operate",
        "No explicit mention of support for other ad platforms beyond Google, Meta, TikTok, and LinkedIn"
      ],
      "requirements": [
        "OAuth 2.1 authentication with PKCE and dynamic client registration",
        "Linked ad accounts on supported platforms (Google Ads, Meta Ads, TikTok Ads, LinkedIn Ads)",
        "MCP-compatible AI assistant or client supporting Streamable HTTP transport",
        "Access to MCP Remote URL at https://mcp.adspirer.com/mcp",
        "Client setup as per platform instructions (Claude, ChatGPT, Cursor, OpenAI Codex)"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed usage examples, platform and tool descriptions, security and authentication details, and clearly states limitations and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Ads MCP\n\nRemote Model Context Protocol (MCP) server for cross-platform ad management. Create, analyze, and optimize campaigns across **Google Ads, Meta Ads, TikTok Ads, and LinkedIn Ads** from any MCP-compatible AI assistant.\n\n## Quick Links\n\n- **MCP Remote URL:** `https://mcp.adspirer.com/mcp`\n- **Transport:** Streamable HTTP\n- **Registry ID:** [`com.adspirer/ads`](https://registry.modelcontextprotocol.io)\n- **Authentication:** OAuth 2.1 with PKCE (dynamic client registration supported)\n- **Website:** https://www.adspirer.com\n- **Support:** abhi@adspirer.com\n\n## What It Does\n\n- **91 tools** across 4 ad platforms for campaign creation, performance analysis, and optimization\n- Plan and validate campaigns using structured prompts\n- Research keywords with real CPC data and competitive analysis\n- Create Google Ads Search and Performance Max campaigns end-to-end\n- Launch Meta, TikTok, and LinkedIn ad campaigns\n- Analyze performance with actionable optimization recommendations\n- Manage multiple ad accounts across platforms\n\n## Platforms & Tools\n\n| Platform | Tools | Capabilities |\n|----------|-------|-------------|\n| Google Ads | 39 | Search campaigns, Performance Max, keyword research, performance analysis, asset management, ad extensions |\n| LinkedIn Ads | 28 | Sponsored content, lead gen forms, audience targeting, campaign analytics |\n| Meta Ads | 20 | Image campaigns, carousel campaigns, audience targeting, performance tracking |\n| TikTok Ads | 4 | In-feed video/image campaigns, asset validation |\n| **Total** | **91** | Plus 2 resources and 6 prompts |\n\n## How to Connect\n\nSee [CONNECTING.md](CONNECTING.md) for detailed setup instructions for each platform.\n\n### Claude (Recommended)\n\n1. Open **Settings > Connectors > Add custom connector**\n2. Name: **Ads MCP**\n3. URL: `https://mcp.adspirer.com/mcp`\n4. Complete OAuth 2.1 sign-in\n5. Link your ad accounts on first use\n\n### Claude Code\n\n```bash\nclaude mcp add --transport http adspirer https://mcp.adspirer.com/mcp\n```\n\n### ChatGPT\n\n1.",
        "start_pos": 0,
        "end_pos": 2013,
        "token_count_estimate": 503,
        "source_type": "readme",
        "agent_id": "86b2663e5103a2d1"
      },
      {
        "chunk_id": 1,
        "text": "spirer.com/mcp`\n4. Complete OAuth 2.1 sign-in\n5. Link your ad accounts on first use\n\n### Claude Code\n\n```bash\nclaude mcp add --transport http adspirer https://mcp.adspirer.com/mcp\n```\n\n### ChatGPT\n\n1. Open **Settings > Connectors > Add custom connector**\n2. Name: **Ads MCP**\n3. URL: `https://mcp.adspirer.com/mcp`\n4. Follow OAuth 2.1 sign-in flow\n\n### Cursor\n\nAdd to `~/.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"adspirer\": {\n      \"url\": \"https://mcp.adspirer.com/mcp\"\n    }\n  }\n}\n```\n\n### OpenAI Codex\n\nAdd to `~/.codex/config.toml`:\n\n```toml\n[mcp_servers.adspirer]\nurl = \"https://mcp.adspirer.com/mcp\"\n```\n\n## Example Prompts\n\n**Keyword Research:**\n```\nResearch keywords for my emergency plumbing business in Chicago.\nShow me high-intent keywords with real CPC data and budget recommendations.\n```\n\n**Performance Analysis:**\n```\nShow me campaign performance for the last 30 days across all platforms.\nWhich campaigns are converting best and what should I optimize?\n```\n\n**Campaign Creation:**\n```\nCreate a Google Performance Max campaign for luxury watches targeting\nNew York with a $50/day budget.\n```\n\n**Multi-Platform Strategy:**\n```\nI want to advertise my handmade jewelry business across Google and LinkedIn.\nResearch keywords for Google Ads and create a LinkedIn sponsored content campaign\ntargeting small business owners.\n```\n\n## Technical Details\n\n- **Protocol:** MCP 2025-03-26 (with fallback to 2024-11-05)\n- **Transport:** Streamable HTTP\n- **OAuth:** RFC 8252 (Authorization Code + PKCE) with RFC 7591 (Dynamic Client Registration) and RFC 9728 (Protected Resource Metadata)\n- **Tool Annotations:** All tools include MCP safety metadata (`readOnlyHint`, `destructiveHint`)\n\n## Security\n\n- HTTPS/TLS for all data transmission\n- OAuth 2.1 with PKCE for authentication\n- Dynamic client registration for CLI tools (Claude Code, Cursor, Codex)\n- Encrypted token storage\n- No conversation logging -- only tool requests are processed\n\nSee [SECURITY.md](SECURITY.md) for vulnerability reporting.",
        "start_pos": 1813,
        "end_pos": 3826,
        "token_count_estimate": 503,
        "source_type": "readme",
        "agent_id": "86b2663e5103a2d1"
      },
      {
        "chunk_id": 2,
        "text": "stration for CLI tools (Claude Code, Cursor, Codex)\n- Encrypted token storage\n- No conversation logging -- only tool requests are processed\n\nSee [SECURITY.md](SECURITY.md) for vulnerability reporting.\n\n## Documentation\n\n- [Connecting Guide](CONNECTING.md)\n- [Privacy Policy](PRIVACY.md)\n- [Terms of Service](TERMS.md)\n- [Security](SECURITY.md)\n- [Support](SUPPORT.md)\n\n## Support\n\n- **Email:** abhi@adspirer.com\n- **Issues:** https://github.com/amekala/ads-mcp/issues\n- **Website:** https://www.adspirer.com\n- **Server Status:** https://mcp.adspirer.com/health\n\n## License\n\nProprietary -- See [Terms of Service](TERMS.md) for usage terms.",
        "start_pos": 3626,
        "end_pos": 4265,
        "token_count_estimate": 159,
        "source_type": "readme",
        "agent_id": "86b2663e5103a2d1"
      }
    ]
  },
  {
    "agent_id": "b8f27365d5c5a526",
    "name": "com.agentpmt/agentpmt",
    "source": "mcp",
    "source_url": "https://github.com/Apoth3osis-ai/agentPMTwebapp",
    "description": "AI agent marketplace for automated employees, workflows, skills, and tool orchestration.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-13T23:13:35.013727Z",
    "indexed_at": "2026-02-18T04:11:39.059518",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide an AI agent marketplace",
        "Offer automated employees",
        "Support workflows",
        "Enable skills management",
        "Facilitate tool orchestration"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence providing a basic overview without details, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "b8f27365d5c5a526",
    "name": "com.agentpmt/agentpmt",
    "source": "mcp",
    "source_url": "https://github.com/Apoth3osis-ai/agentpmt-mcp-public",
    "description": "AI agent marketplace for automated employees, workflows, skills, and tool orchestration.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-13T23:35:34.261774Z",
    "indexed_at": "2026-02-18T04:11:41.146846",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# AgentPMT MCP Server\n\nAgentPMT is an AI agent marketplace for creating automated employees and assistants. This MCP server connects your AI agents to the AgentPMT ecosystem, enabling them to discover, purchase, and orchestrate tools, workflows, and skills.\n\n## Features\n\n- **Automated Employees & Assistants** - Build AI agents that work autonomously around the clock\n- **Dynamic Tool Discovery** - Agents browse and discover tools from the marketplace in real-time\n- **Autonomous Purchasing** - Agents can purchase and use tools with budget-controlled spending\n- **Workflow Orchestration** - Chain multiple tools together into complex workflows\n- **Enterprise Budget Controls** - Set spending limits, approve purchases, and monitor usage\n- **Web3 Payments** - USDC payments on Base network for transparent, autonomous transactions\n\n## Quick Start\n\n### Connect via MCP\n\n**Endpoint:** `https://api.agentpmt.com/mcp`\n**Transport:** Streamable HTTP\n**Protocol Version:** 2025-03-26\n\n### Authentication\n\nAgentPMT uses API key + budget key authentication. You need:\n\n1. An **API Key** - identifies your account\n2. A **Budget Key** - identifies which budget to charge\n\nGet your keys at [agentpmt.com](https://agentpmt.com) by creating an account and setting up a budget.\n\n### Configuration\n\n#### Claude Desktop / Claude Code\n\nAdd to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"agentpmt\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://api.agentpmt.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <base64-encoded-credentials>\"\n      }\n    }\n  }\n}\n```\n\n#### VS Code / GitHub Copilot\n\nAgentPMT is available in the MCP server gallery. Search for `com.agentpmt/agentpmt` in the Extensions view (`@mcp` filter).\n\nOr add manually to `.vscode/mcp.json`:\n\n```json\n{\n  \"servers\": {\n    \"agentpmt\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://api.agentpmt.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <base64-encoded-credentials>\"\n      }\n    }\n  }\n}\n```\n\n#### <a id=\"cursor\"></a>Cursor\n\n**One-click install:** [Install AgentPMT in Cursor](cursor://anysphere.cursor-deeplink/mcp/install?name=agentpmt&config=eyJ1cmwiOiAiaHR0cHM6Ly9hcGkuYWdlbnRwbXQuY29tL21jcCIsICJoZWFkZXJzIjogeyJBdXRob3JpemF0aW9uIjogIkJlYXJlciBZT1VSX0FQSV9LRVkifX0%3D)\n\nOr add manually to your Cursor MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"agentpmt\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://api.agentpmt.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <base64-encoded-credentials>\"\n      }\n    }\n  }\n}\n```\n\nAfter installing, replace `YOUR_API_KEY` with your base64-encoded credentials (see [Encoding Credentials](#encoding-credentials) below).\n\n#### Cline\n\nAdd to your Cline MCP settings with the same configuration format as above.\n\n### <a id=\"encoding-credentials\"></a>Encoding Credentials\n\nEncode your API key and budget key as a base64 bearer token:\n\n```bash\n# Format: api_key:budget_key\necho -n \"your-api-key:your-budget-key\" | base64\n```\n\nOr use JSON format:\n\n```bash\necho -n '{\"api_key\":\"your-api-key\",\"budget_key\":\"your-budget-key\"}' | base64\n```\n\nUse the resulting base64 string as the Bearer token in the Authorization header.\n\n### OAuth Authentication\n\nAgentPMT also supports full OAuth 2.0 authentication. The OAuth metadata endpoint is available at:\n\n```\nhttps://api.agentpmt.com/.well-known/oauth-protected-resource\n```\n\nMCP clients that support OAuth will automatically discover the authorization server and initiate the flow.\n\n## Available Tools\n\nOnce connected, your AI agent can access:\n\n### Built-in Tools\n\n| Tool | Description |\n|------|-------------|\n| `AgentPMT-Refresh-Tools` | Refresh the available tool list from the marketplace |\n| `AgentPMT-Report-Tool-Issue` | Report issues with any tool to the AgentPMT team |\n\n### Marketplace Tools\n\nThe full tool list is dynamically loaded based on your budget configuration. Tools span categories including:\n\n- **Communication** - Email, messaging, notifications\n- **Data & Analytics** - Data processing, visualization, reporting\n- **Development** - Code generation, testing, deployment\n- **File Management** - Storage, conversion, processing\n- **Search & Research** - Web search, knowledge retrieval\n- **Workflow Automation** - Task scheduling, orchestration\n- And many more\n\nUse `AgentPMT-Refresh-Tools` to see the latest available tools.\n\n## API Reference\n\n### MCP Protocol\n\nAgentPMT implements the Model Context Protocol (MCP) specification version 2025-03-26.\n\n**Supported Methods:**\n\n| Method | Description |\n|--------|-------------|\n| `initialize` | Initialize connection and exchange capabilities |\n| `tools/list` | List all available tools for your budget |\n| `tools/call` | Execute a tool (may trigger a purchase) |\n| `ping` | Health check |\n\n### Session Management\n\n- Sessions are automatically created on first authenticated request\n- Sessions expire after 2 hours of inactivity\n- Session IDs are returned in the `Mcp-Session-Id` response header\n\n## Pricing\n\nTools on the AgentPMT marketplace have individual pricing set by vendors. Your budget controls how much your agents can spend. Visit [agentpmt.com](https://agentpmt.com) to:\n\n- Browse available tools and pricing\n- Set up budgets with spending limits\n- Monitor agent spending in real-time\n- Review purchase history and usage\n\n## Support\n\n- **Email:** [admin@agentpmt.com](mailto:admin@agentpmt.com)\n- **Website:** [agentpmt.com](https://agentpmt.com)\n- **Issues:** [GitHub Issues](https://github.com/Apoth3osis-ai/agentpmt-mcp-public/issues)\n\n## Registry Listings\n\nAgentPMT is registered on the [Official MCP Registry](https://registry.modelcontextprotocol.io) as `com.agentpmt/agentpmt`.\n\n## License\n\nProprietary - All rights reserved.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Build AI agents that work autonomously around the clock",
        "Enable agents to discover tools dynamically from the marketplace in real-time",
        "Allow agents to autonomously purchase and use tools with budget-controlled spending",
        "Orchestrate complex workflows by chaining multiple tools together",
        "Set and enforce enterprise budget controls including spending limits and purchase approvals",
        "Process USDC payments on the Base network for transparent, autonomous transactions",
        "Provide built-in tools for refreshing tool lists and reporting tool issues",
        "Support OAuth 2.0 authentication and API key plus budget key authentication",
        "Implement MCP protocol methods including initialize, tools/list, tools/call, and ping",
        "Automatically manage sessions with expiration after inactivity"
      ],
      "limitations": [
        "Sessions expire after 2 hours of inactivity",
        "Tool availability and pricing depend on vendor settings and budget configuration",
        "Requires budget setup and spending limits to control agent purchases",
        "Proprietary license restricts usage and redistribution"
      ],
      "requirements": [
        "API Key identifying the user account",
        "Budget Key identifying the budget to charge",
        "Base64-encoded credentials for Authorization header",
        "Account creation and budget setup at agentpmt.com",
        "MCP client supporting streamable HTTP transport and MCP protocol version 2025-03-26",
        "Optional OAuth 2.0 support for authentication"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, usage examples, detailed feature descriptions, authentication methods, API references, limitations, and requirements.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# AgentPMT MCP Server\n\nAgentPMT is an AI agent marketplace for creating automated employees and assistants. This MCP server connects your AI agents to the AgentPMT ecosystem, enabling them to discover, purchase, and orchestrate tools, workflows, and skills.\n\n## Features\n\n- **Automated Employees & Assistants** - Build AI agents that work autonomously around the clock\n- **Dynamic Tool Discovery** - Agents browse and discover tools from the marketplace in real-time\n- **Autonomous Purchasing** - Agents can purchase and use tools with budget-controlled spending\n- **Workflow Orchestration** - Chain multiple tools together into complex workflows\n- **Enterprise Budget Controls** - Set spending limits, approve purchases, and monitor usage\n- **Web3 Payments** - USDC payments on Base network for transparent, autonomous transactions\n\n## Quick Start\n\n### Connect via MCP\n\n**Endpoint:** `https://api.agentpmt.com/mcp`\n**Transport:** Streamable HTTP\n**Protocol Version:** 2025-03-26\n\n### Authentication\n\nAgentPMT uses API key + budget key authentication. You need:\n\n1. An **API Key** - identifies your account\n2. A **Budget Key** - identifies which budget to charge\n\nGet your keys at [agentpmt.com](https://agentpmt.com) by creating an account and setting up a budget.\n\n### Configuration\n\n#### Claude Desktop / Claude Code\n\nAdd to your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"agentpmt\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://api.agentpmt.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <base64-encoded-credentials>\"\n      }\n    }\n  }\n}\n```\n\n#### VS Code / GitHub Copilot\n\nAgentPMT is available in the MCP server gallery. Search for `com.agentpmt/agentpmt` in the Extensions view (`@mcp` filter).",
        "start_pos": 0,
        "end_pos": 1738,
        "token_count_estimate": 434,
        "source_type": "readme",
        "agent_id": "b8f27365d5c5a526"
      },
      {
        "chunk_id": 1,
        "text": "-http\",\n      \"url\": \"https://api.agentpmt.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <base64-encoded-credentials>\"\n      }\n    }\n  }\n}\n```\n\n#### <a id=\"cursor\"></a>Cursor\n\n**One-click install:** [Install AgentPMT in Cursor](cursor://anysphere.cursor-deeplink/mcp/install?name=agentpmt&config=eyJ1cmwiOiAiaHR0cHM6Ly9hcGkuYWdlbnRwbXQuY29tL21jcCIsICJoZWFkZXJzIjogeyJBdXRob3JpemF0aW9uIjogIkJlYXJlciBZT1VSX0FQSV9LRVkifX0%3D)\n\nOr add manually to your Cursor MCP settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"agentpmt\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://api.agentpmt.com/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer <base64-encoded-credentials>\"\n      }\n    }\n  }\n}\n```\n\nAfter installing, replace `YOUR_API_KEY` with your base64-encoded credentials (see [Encoding Credentials](#encoding-credentials) below).\n\n#### Cline\n\nAdd to your Cline MCP settings with the same configuration format as above.\n\n### <a id=\"encoding-credentials\"></a>Encoding Credentials\n\nEncode your API key and budget key as a base64 bearer token:\n\n```bash\n# Format: api_key:budget_key\necho -n \"your-api-key:your-budget-key\" | base64\n```\n\nOr use JSON format:\n\n```bash\necho -n '{\"api_key\":\"your-api-key\",\"budget_key\":\"your-budget-key\"}' | base64\n```\n\nUse the resulting base64 string as the Bearer token in the Authorization header.\n\n### OAuth Authentication\n\nAgentPMT also supports full OAuth 2.0 authentication. The OAuth metadata endpoint is available at:\n\n```\nhttps://api.agentpmt.com/.well-known/oauth-protected-resource\n```\n\nMCP clients that support OAuth will automatically discover the authorization server and initiate the flow.",
        "start_pos": 1848,
        "end_pos": 3500,
        "token_count_estimate": 413,
        "source_type": "readme",
        "agent_id": "b8f27365d5c5a526"
      },
      {
        "chunk_id": 2,
        "text": "the marketplace |\n| `AgentPMT-Report-Tool-Issue` | Report issues with any tool to the AgentPMT team |\n\n### Marketplace Tools\n\nThe full tool list is dynamically loaded based on your budget configuration. Tools span categories including:\n\n- **Communication** - Email, messaging, notifications\n- **Data & Analytics** - Data processing, visualization, reporting\n- **Development** - Code generation, testing, deployment\n- **File Management** - Storage, conversion, processing\n- **Search & Research** - Web search, knowledge retrieval\n- **Workflow Automation** - Task scheduling, orchestration\n- And many more\n\nUse `AgentPMT-Refresh-Tools` to see the latest available tools.\n\n## API Reference\n\n### MCP Protocol\n\nAgentPMT implements the Model Context Protocol (MCP) specification version 2025-03-26.\n\n**Supported Methods:**\n\n| Method | Description |\n|--------|-------------|\n| `initialize` | Initialize connection and exchange capabilities |\n| `tools/list` | List all available tools for your budget |\n| `tools/call` | Execute a tool (may trigger a purchase) |\n| `ping` | Health check |\n\n### Session Management\n\n- Sessions are automatically created on first authenticated request\n- Sessions expire after 2 hours of inactivity\n- Session IDs are returned in the `Mcp-Session-Id` response header\n\n## Pricing\n\nTools on the AgentPMT marketplace have individual pricing set by vendors. Your budget controls how much your agents can spend. Visit [agentpmt.com](https://agentpmt.com) to:\n\n- Browse available tools and pricing\n- Set up budgets with spending limits\n- Monitor agent spending in real-time\n- Review purchase history and usage\n\n## Support\n\n- **Email:** [admin@agentpmt.com](mailto:admin@agentpmt.com)\n- **Website:** [agentpmt.com](https://agentpmt.com)\n- **Issues:** [GitHub Issues](https://github.com/Apoth3osis-ai/agentpmt-mcp-public/issues)\n\n## Registry Listings\n\nAgentPMT is registered on the [Official MCP Registry](https://registry.modelcontextprotocol.io) as `com.agentpmt/agentpmt`.\n\n## License\n\nProprietary - All rights reserved.",
        "start_pos": 3696,
        "end_pos": 5732,
        "token_count_estimate": 508,
        "source_type": "readme",
        "agent_id": "b8f27365d5c5a526"
      },
      {
        "chunk_id": 3,
        "text": "ues)\n\n## Registry Listings\n\nAgentPMT is registered on the [Official MCP Registry](https://registry.modelcontextprotocol.io) as `com.agentpmt/agentpmt`.\n\n## License\n\nProprietary - All rights reserved.",
        "start_pos": 5532,
        "end_pos": 5732,
        "token_count_estimate": 49,
        "source_type": "readme",
        "agent_id": "b8f27365d5c5a526"
      }
    ]
  },
  {
    "agent_id": "70e4ffd205c3144f",
    "name": "com.agilitycms/mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/agility/agility-mcp-server",
    "description": "An MCP server that provides access to Agility CMS.  See https://mcp.agilitycms.com for more details.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-09-15T21:00:43.34878Z",
    "indexed_at": "2026-02-18T04:11:43.102953",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to Agility CMS"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.2,
    "quality_rationale": "The description is a single sentence with minimal information and no details on features, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "198837fa6378277f",
    "name": "com.ai-erd/ai-erd",
    "source": "mcp",
    "source_url": "https://ai-erd.com",
    "description": "AI-powered ERD design tool. Create and manage database schemas using DBML with real-time canvas.",
    "tools": [],
    "detected_capabilities": [
      "manage",
      "create"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-12-29T10:30:10.376598Z",
    "indexed_at": "2026-02-18T04:11:47.097684",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Create database schemas using DBML",
        "Manage database schemas",
        "Provide a real-time canvas for ERD design"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functionality but lacks detailed information, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "54f2f1e876624f19",
    "name": "com.aiqbee/brain",
    "source": "mcp",
    "source_url": "https://github.com/AIQBee/aiqbee-ai",
    "description": "AI knowledge graph for architecture, portfolio, and digital strategy management.",
    "tools": [],
    "detected_capabilities": [
      "manage"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2026-02-15T05:08:34.124567Z",
    "indexed_at": "2026-02-18T04:11:48.853772",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "# Aiqbee AI\n\nConnect AI assistants to your [Aiqbee](https://aiqbee.com) brains via the [Model Context Protocol](https://modelcontextprotocol.io/).\n\nSearch, create, and link knowledge across your architecture, portfolio, and digital strategy - all through natural conversation with your AI assistant.\n\n---\n\n## Supported Clients\n\n| AI Tool | Integration | Setup |\n|---------|-------------|-------|\n| **Claude Code** | Plugin | `claude plugin install AIQBee/aiqbee-ai` |\n| **Claude Desktop** | MCP Config | [JSON config below](#claude-desktop) |\n| **Cursor** | MCP Config | [JSON config below](#cursor) |\n| **VS Code / Copilot** | MCP Config | [JSON config below](#vs-code) |\n| **Gemini CLI** | Extension | `gemini extensions install https://github.com/AIQBee/aiqbee-ai` |\n| **ChatGPT** | MCP Config | [JSON config below](#other-mcp-clients) |\n| **Windsurf** | MCP Config | [JSON config below](#other-mcp-clients) |\n\n---\n\n## Quick Start\n\n### Claude Code\n\n```bash\nclaude plugin install AIQBee/aiqbee-ai\n```\n\nRestart Claude Code after installation.\n\n### Gemini CLI\n\n```bash\ngemini extensions install https://github.com/AIQBee/aiqbee-ai\n```\n\nRestart Gemini CLI and authenticate when prompted.\n\n### Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"aiqbee\": {\n      \"url\": \"https://mcp.aiqbee.com/mcp\"\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd to `.cursor/mcp.json` in your project:\n\n```json\n{\n  \"mcpServers\": {\n    \"aiqbee\": {\n      \"url\": \"https://mcp.aiqbee.com/mcp\"\n    }\n  }\n}\n```\n\n### VS Code\n\nAdd to your `settings.json`:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"aiqbee\": {\n        \"type\": \"sse\",\n        \"url\": \"https://mcp.aiqbee.com/mcp\"\n      }\n    }\n  }\n}\n```\n\n### Other MCP Clients\n\nFor any MCP-compatible client, point it at:\n\n```\nhttps://mcp.aiqbee.com/mcp\n```\n\n---\n\n## Authentication\n\nAiqbee uses **OAuth 2.0** with secure authorization. When you first connect, your MCP client will prompt you to sign in. The server supports:\n\n- **Public clients** (Claude Desktop, Cursor): Standard PKCE flow\n- **Confidential clients** (ChatGPT): Server-side PKCE with callback\n\nNo API keys needed - just sign in with your existing Aiqbee account.\n\n---\n\n## Available Tools\n\n### Read\n\n| Tool | Description |\n|------|-------------|\n| `aiqbee_search` | Search neurons in your knowledge graph |\n| `aiqbee_fetch` | Get full neuron content, relationships, and files |\n| `aiqbee_get_brain_info` | Get brain metadata and statistics |\n| `aiqbee_get_neuron_types` | List all neuron types with counts |\n| `aiqbee_list_neurons` | Paginated neuron listing with filtering |\n| `aiqbee_get_relationships` | Get incoming/outgoing relationships for a neuron |\n\n### Write\n\n| Tool | Description |\n|------|-------------|\n| `aiqbee_create_neuron` | Create a new neuron in your brain |\n| `aiqbee_update_neuron` | Update an existing neuron |\n| `aiqbee_delete_neuron` | Delete a neuron |\n| `aiqbee_create_relationship` | Create a link between two neurons |\n| `aiqbee_update_relationship` | Update an existing relationship |\n| `aiqbee_delete_relationship` | Remove a relationship |\n\n---\n\n## Example Prompts\n\nOnce connected, try asking your AI assistant:\n\n- *\"Search my brain for anything related to cloud migration\"*\n- *\"Show me all the architecture decisions we've made\"*\n- *\"Create a new neuron about our API gateway strategy\"*\n- *\"Link the microservices neuron to the deployment pipeline neuron\"*\n- *\"What are the relationships for the data platform neuron?\"*\n- *\"Summarize the key concepts in my digital strategy brain\"*\n\n---\n\n## What is Aiqbee?\n\n[Aiqbee](https://aiqbee.com) is a web-based architecture, portfolio, and digital strategy management platform.\n\n- **Knowledge Graphs** - Organize ideas as \"neurons\" connected by \"synapses\"\n- **Architecture Management** - Document and manage enterprise architecture\n- **Portfolio Management** - Track products, projects, and digital assets\n- **AI-Powered Search** - Find anything across your knowledge base\n- **Collaboration** - Team workspaces with role-based access\n- **Microsoft 365 Integration** - Works with your existing tools\n\n---\n\n## Links\n\n- **Platform**: https://app.aiqbee.com\n- **Documentation**: https://app.aiqbee.com/help\n- **MCP Server**: https://mcp.aiqbee.com/mcp\n\n---\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Search neurons in the knowledge graph",
        "Retrieve full neuron content, relationships, and files",
        "Get brain metadata and statistics",
        "List neuron types with counts",
        "List neurons with pagination and filtering",
        "Get incoming and outgoing relationships for a neuron",
        "Create, update, and delete neurons",
        "Create, update, and delete relationships between neurons",
        "Integrate with multiple AI clients via Model Context Protocol",
        "Authenticate users via OAuth 2.0 with PKCE flow"
      ],
      "limitations": [
        "No API keys supported; requires OAuth 2.0 authentication",
        "Rate limits or usage quotas not specified",
        "Limited to managing knowledge within Aiqbee brains",
        "No direct support for non-MCP clients"
      ],
      "requirements": [
        "An existing Aiqbee account for OAuth 2.0 sign-in",
        "MCP-compatible AI client configured to connect to https://mcp.aiqbee.com/mcp",
        "OAuth 2.0 support in client for authentication (PKCE flow for public clients, server-side PKCE for confidential clients)"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation instructions, detailed tool descriptions, usage examples, supported clients, authentication methods, and links, covering all key aspects for effective use.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "# Aiqbee AI\n\nConnect AI assistants to your [Aiqbee](https://aiqbee.com) brains via the [Model Context Protocol](https://modelcontextprotocol.io/).\n\nSearch, create, and link knowledge across your architecture, portfolio, and digital strategy - all through natural conversation with your AI assistant.\n\n---\n\n## Supported Clients\n\n| AI Tool | Integration | Setup |\n|---------|-------------|-------|\n| **Claude Code** | Plugin | `claude plugin install AIQBee/aiqbee-ai` |\n| **Claude Desktop** | MCP Config | [JSON config below](#claude-desktop) |\n| **Cursor** | MCP Config | [JSON config below](#cursor) |\n| **VS Code / Copilot** | MCP Config | [JSON config below](#vs-code) |\n| **Gemini CLI** | Extension | `gemini extensions install https://github.com/AIQBee/aiqbee-ai` |\n| **ChatGPT** | MCP Config | [JSON config below](#other-mcp-clients) |\n| **Windsurf** | MCP Config | [JSON config below](#other-mcp-clients) |\n\n---\n\n## Quick Start\n\n### Claude Code\n\n```bash\nclaude plugin install AIQBee/aiqbee-ai\n```\n\nRestart Claude Code after installation.\n\n### Gemini CLI\n\n```bash\ngemini extensions install https://github.com/AIQBee/aiqbee-ai\n```\n\nRestart Gemini CLI and authenticate when prompted.\n\n### Claude Desktop\n\nAdd to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"aiqbee\": {\n      \"url\": \"https://mcp.aiqbee.com/mcp\"\n    }\n  }\n}\n```\n\n### Cursor\n\nAdd to `.cursor/mcp.json` in your project:\n\n```json\n{\n  \"mcpServers\": {\n    \"aiqbee\": {\n      \"url\": \"https://mcp.aiqbee.com/mcp\"\n    }\n  }\n}\n```\n\n### VS Code\n\nAdd to your `settings.json`:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"aiqbee\": {\n        \"type\": \"sse\",\n        \"url\": \"https://mcp.aiqbee.com/mcp\"\n      }\n    }\n  }\n}\n```\n\n### Other MCP Clients\n\nFor any MCP-compatible client, point it at:\n\n```\nhttps://mcp.aiqbee.com/mcp\n```\n\n---\n\n## Authentication\n\nAiqbee uses **OAuth 2.0** with secure authorization. When you first connect, your MCP client will prompt you to sign in.",
        "start_pos": 0,
        "end_pos": 1952,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "54f2f1e876624f19"
      },
      {
        "chunk_id": 1,
        "text": "ient, point it at:\n\n```\nhttps://mcp.aiqbee.com/mcp\n```\n\n---\n\n## Authentication\n\nAiqbee uses **OAuth 2.0** with secure authorization. When you first connect, your MCP client will prompt you to sign in. The server supports:\n\n- **Public clients** (Claude Desktop, Cursor): Standard PKCE flow\n- **Confidential clients** (ChatGPT): Server-side PKCE with callback\n\nNo API keys needed - just sign in with your existing Aiqbee account.\n\n---\n\n## Available Tools\n\n### Read\n\n| Tool | Description |\n|------|-------------|\n| `aiqbee_search` | Search neurons in your knowledge graph |\n| `aiqbee_fetch` | Get full neuron content, relationships, and files |\n| `aiqbee_get_brain_info` | Get brain metadata and statistics |\n| `aiqbee_get_neuron_types` | List all neuron types with counts |\n| `aiqbee_list_neurons` | Paginated neuron listing with filtering |\n| `aiqbee_get_relationships` | Get incoming/outgoing relationships for a neuron |\n\n### Write\n\n| Tool | Description |\n|------|-------------|\n| `aiqbee_create_neuron` | Create a new neuron in your brain |\n| `aiqbee_update_neuron` | Update an existing neuron |\n| `aiqbee_delete_neuron` | Delete a neuron |\n| `aiqbee_create_relationship` | Create a link between two neurons |\n| `aiqbee_update_relationship` | Update an existing relationship |\n| `aiqbee_delete_relationship` | Remove a relationship |\n\n---\n\n## Example Prompts\n\nOnce connected, try asking your AI assistant:\n\n- *\"Search my brain for anything related to cloud migration\"*\n- *\"Show me all the architecture decisions we've made\"*\n- *\"Create a new neuron about our API gateway strategy\"*\n- *\"Link the microservices neuron to the deployment pipeline neuron\"*\n- *\"What are the relationships for the data platform neuron?\"*\n- *\"Summarize the key concepts in my digital strategy brain\"*\n\n---\n\n## What is Aiqbee?\n\n[Aiqbee](https://aiqbee.com) is a web-based architecture, portfolio, and digital strategy management platform.",
        "start_pos": 1752,
        "end_pos": 3667,
        "token_count_estimate": 478,
        "source_type": "readme",
        "agent_id": "54f2f1e876624f19"
      },
      {
        "chunk_id": 2,
        "text": "*\n- *\"Summarize the key concepts in my digital strategy brain\"*\n\n---\n\n## What is Aiqbee?\n\n[Aiqbee](https://aiqbee.com) is a web-based architecture, portfolio, and digital strategy management platform.\n\n- **Knowledge Graphs** - Organize ideas as \"neurons\" connected by \"synapses\"\n- **Architecture Management** - Document and manage enterprise architecture\n- **Portfolio Management** - Track products, projects, and digital assets\n- **AI-Powered Search** - Find anything across your knowledge base\n- **Collaboration** - Team workspaces with role-based access\n- **Microsoft 365 Integration** - Works with your existing tools\n\n---\n\n## Links\n\n- **Platform**: https://app.aiqbee.com\n- **Documentation**: https://app.aiqbee.com/help\n- **MCP Server**: https://mcp.aiqbee.com/mcp\n\n---\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.",
        "start_pos": 3467,
        "end_pos": 4306,
        "token_count_estimate": 209,
        "source_type": "readme",
        "agent_id": "54f2f1e876624f19"
      }
    ]
  },
  {
    "agent_id": "59a11a6811ceb2a8",
    "name": "com.allstacks/allstacks-mcp",
    "source": "mcp",
    "source_url": "",
    "description": "Provides 208+ tools for AI-native engineering intelligence via Allstacks API.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-27T16:43:42.575778Z",
    "indexed_at": "2026-02-18T04:11:50.702380",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to over 208 tools for AI-native engineering intelligence",
        "Integrate with the Allstacks API for engineering data insights"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the number of tools and the API integration but lacks detailed capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "59a11a6811ceb2a8",
    "name": "com.allstacks/allstacks-mcp",
    "source": "mcp",
    "source_url": "",
    "description": "Provides 208+ tools for AI-native engineering intelligence via Allstacks API.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-27T16:50:37.013013Z",
    "indexed_at": "2026-02-18T04:11:50.718240",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to over 208 tools for AI-native engineering intelligence",
        "Integrate with the Allstacks API for engineering data insights"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence that mentions the number of tools and the API but lacks detail on usage, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "59a11a6811ceb2a8",
    "name": "com.allstacks/allstacks-mcp",
    "source": "mcp",
    "source_url": "",
    "description": "Provides 208+ tools for AI-native engineering intelligence via Allstacks API.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-27T19:33:41.052283Z",
    "indexed_at": "2026-02-18T04:11:50.728030",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to 208+ tools for AI-native engineering intelligence",
        "Integrate with the Allstacks API for engineering data insights"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence that lists the number of tools and the API used but lacks detailed capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "59a11a6811ceb2a8",
    "name": "com.allstacks/allstacks-mcp",
    "source": "mcp",
    "source_url": "",
    "description": "Provides 208+ tools for AI-native engineering intelligence via Allstacks API.",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "unknown",
    "last_updated": "2025-10-27T19:42:56.515336Z",
    "indexed_at": "2026-02-18T04:11:50.759216",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Provide access to over 208 tools for AI-native engineering intelligence",
        "Integrate with the Allstacks API for engineering data insights"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description is a single sentence that mentions the number of tools and the API but lacks detail on specific capabilities, limitations, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "7ccca7062ba1ad84",
    "name": "com.amplitude/mcp-server",
    "source": "mcp",
    "source_url": "https://mcp.amplitude.com/mcp",
    "description": "Search, access, and get insights on your Amplitude data",
    "tools": [],
    "detected_capabilities": [
      "search"
    ],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "open_source",
    "last_updated": "2025-10-16T20:22:46.742037Z",
    "indexed_at": "2026-02-18T04:11:50.783469",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {},
    "llm_extracted": {
      "capabilities": [
        "Search Amplitude data",
        "Access Amplitude data",
        "Get insights on Amplitude data"
      ],
      "limitations": [],
      "requirements": []
    },
    "documentation_quality": 0.3,
    "quality_rationale": "The description provides a basic overview of the server's functions but lacks detail, examples, or requirements.",
    "llm_text_source": "description_only",
    "documentation_chunks": []
  },
  {
    "agent_id": "f5297cd861cece8e",
    "name": "com.apify/apify-mcp-server",
    "source": "mcp",
    "source_url": "https://github.com/apify/apify-mcp-server",
    "description": "Apify MCP Server providing access to thousands of web scraping and automation tools from Apify Store",
    "tools": [],
    "detected_capabilities": [],
    "llm_backbone": "Unknown",
    "arena_elo": null,
    "arena_battles": null,
    "community_rating": null,
    "rating_count": 0,
    "pricing": "freemium",
    "last_updated": "2025-09-19T13:48:15.372766Z",
    "indexed_at": "2026-02-18T04:11:52.762984",
    "description_embedding": null,
    "testability_tier": "n/a",
    "documentation": {
      "readme": "<h1 align=\"center\">\n    <a href=\"https://mcp.apify.com\">\n        <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_dark_background.png\">\n            <img alt=\"Apify MCP Server\" src=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_white_background.png\" width=\"500\">\n        </picture>\n    </a>\n    <br>\n    <small><a href=\"https://mcp.apify.com\">mcp.apify.com</a></small>\n</h1>\n\n<p align=center>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" alt=\"NPM latest version\" data-canonical-src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml\"><img src=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://smithery.ai/server/@apify/mcp\"><img src=\"https://smithery.ai/badge/@apify/mcp\" alt=\"smithery badge\"></a>\n</p>\n\n\nThe Apify Model Context Protocol (MCP) server at [**mcp.apify.com**](https://mcp.apify.com) enables your AI agents to extract data from social media, search engines, maps, e-commerce sites, and any other website using thousands of ready-made scrapers, crawlers, and automation tools from the [Apify Store](https://apify.com/store). It supports OAuth, allowing you to connect from clients like Claude.ai or Visual Studio Code using just the URL.\n\n> **üöÄ Use the hosted Apify MCP Server!**\n>\n> For the best experience, connect your AI assistant to our hosted server at **[`https://mcp.apify.com`](https://mcp.apify.com)**. The hosted server supports the latest features - including output schema inference for structured Actor results - that are not available when running locally via stdio.\n\nüí∞ The server also supports [Skyfire agentic payments](#-skyfire-agentic-payments), allowing AI agents to pay for Actor runs without an API token.\n\nApify MCP Server is compatible with `Claude Code, Claude.ai, Cursor, VS Code` and any client that adheres to the Model Context Protocol.\nCheck out the [MCP clients section](#-mcp-clients) for more details or visit the [MCP configuration page](https://mcp.apify.com).\n\n![Apify-MCP-server](https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify-mcp-server.png)\n\n## Table of Contents\n- [üåê Introducing the Apify MCP server](#-introducing-the-apify-mcp-server)\n- [üöÄ Quickstart](#-quickstart)\n- [‚ö†Ô∏è SSE transport deprecation](#%EF%B8%8F-sse-transport-deprecation)\n- [ü§ñ MCP clients](#-mcp-clients)\n- [ü™Ñ Try Apify MCP instantly](#-try-apify-mcp-instantly)\n- [üí∞ Skyfire agentic payments](#-skyfire-agentic-payments)\n- [üõ†Ô∏è Tools, resources, and prompts](#%EF%B8%8F-tools-resources-and-prompts)\n- [üìä Telemetry](#-telemetry)\n- [üêõ Troubleshooting (local MCP server)](#-troubleshooting-local-mcp-server)\n- [‚öôÔ∏è Development](#%EF%B8%8F-development)\n- [ü§ù Contributing](#-contributing)\n- [üìö Learn more](#-learn-more)\n\n# üåê Introducing the Apify MCP server\n\nThe Apify MCP Server allows an AI assistant to use any [Apify Actor](https://apify.com/store) as a tool to perform a specific task.\nFor example, it can:\n- Use [Facebook Posts Scraper](https://apify.com/apify/facebook-posts-scraper) to extract data from Facebook posts from multiple pages/profiles.\n- Use [Google Maps Email Extractor](https://apify.com/lukaskrivka/google-maps-with-contact-details) to extract contact details from Google Maps.\n- Use [Google Search Results Scraper](https://apify.com/apify/google-search-scraper) to scrape Google Search Engine Results Pages (SERPs).\n- Use [Instagram Scraper](https://apify.com/apify/instagram-scraper) to scrape Instagram posts, profiles, places, photos, and comments.\n- Use [RAG Web Browser](https://apify.com/apify/web-scraper) to search the web, scrape the top N URLs, and return their content.\n\n**Video tutorial: Integrate 8,000+ Apify Actors and Agents with Claude**\n\n[![Apify MCP Server Tutorial: Integrate 5,000+ Apify Actors and Agents with Claude](https://img.youtube.com/vi/BKu8H91uCTg/hqdefault.jpg)](https://www.youtube.com/watch?v=BKu8H91uCTg)\n\n# üöÄ Quickstart\n\nYou can use the Apify MCP Server in two ways:\n\n**HTTPS Endpoint (mcp.apify.com)**: Connect from your MCP client via OAuth or by including the `Authorization: Bearer <APIFY_TOKEN>` header in your requests. This is the recommended method for most use cases. Because it supports OAuth, you can connect from clients like [Claude.ai](https://claude.ai) or [Visual Studio Code](https://code.visualstudio.com/) using just the URL: `https://mcp.apify.com`.\n- `https://mcp.apify.com` streamable transport\n\n**Standard Input/Output (stdio)**: Ideal for local integrations and command-line tools like the Claude for Desktop client.\n- Set the MCP client server command to `npx @apify/actors-mcp-server` and the `APIFY_TOKEN` environment variable to your Apify API token.\n- See `npx @apify/actors-mcp-server --help` for more options.\n\nYou can find detailed instructions for setting up the MCP server in the [Apify documentation](https://docs.apify.com/platform/integrations/mcp).\n\n# ‚ö†Ô∏è SSE transport deprecation on April 1, 2026\n\nUpdate your MCP client config before April 1, 2026.\nThe Apify MCP server is dropping Server-Sent Events (SSE) transport in favor of Streamable HTTP, in line with the official MCP spec.\n\nGo to [mcp.apify.com](https://mcp.apify.com/) to update the installation for your client of choice, with a valid endpoint.\n\n# ü§ñ MCP clients\n\nApify MCP Server is compatible with any MCP client that adheres to the [Model Context Protocol](https://modelcontextprotocol.org/), but the level of support for dynamic tool discovery and other features may vary between clients.\n<!--Therefore, the server uses [mcp-client-capabilities](https://github.com/apify/mcp-client-capabilities) to detect client capabilities and adjust its behavior accordingly.-->\n\nTo interact with the Apify MCP server, you can use clients such as: [Claude Desktop](https://claude.ai/download), [Visual Studio Code](https://code.visualstudio.com/), or [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client).\n\nVisit [mcp.apify.com](https://mcp.apify.com) to configure the server for your preferred client.\n\n![Apify-MCP-configuration-clients](https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/mcp-clients.png)\n\n### Supported clients matrix\n\nThe following table outlines the tested MCP clients and their level of support for key features.\n\n| Client                      | Dynamic Tool Discovery | Notes                                                |\n|-----------------------------|------------------------|------------------------------------------------------|\n| **Claude.ai (web)**         | üü° Partial             | Tools mey need to be reloaded manually in the client |\n| **Claude Desktop**          | üü° Partial             | Tools may need to be reloaded manually in the client |\n| **VS Code (Genie)**         | ‚úÖ Full                 |                                                      |\n| **Cursor**                  | ‚úÖ Full                 |                                                      |\n| **Apify Tester MCP Client** | ‚úÖ Full                 | Designed for testing Apify MCP servers               |\n| **OpenCode**                | ‚úÖ Full                 |                                                      |\n\n\n**Smart tool selection based on client capabilities:**\n\nWhen the `actors` tool category is requested, the server intelligently selects the most appropriate Actor-related tools based on the client's capabilities:\n\n- **Clients with dynamic tool support** (e.g., Claude.ai web, VS Code Genie): The server provides the `add-actor` tool instead of `call-actor`. This allows for a better user experience where users can dynamically discover and add new Actors as tools during their conversation.\n\n- **Clients with limited dynamic tool support** (e.g., Claude Desktop): The server provides the standard `call-actor` tool along with other Actor category tools, ensuring compatibility while maintaining functionality.\n\n# ü™Ñ Try Apify MCP instantly\n\nWant to try Apify MCP without any setup?\n\nCheck out [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n\nThis interactive, chat-like interface provides an easy way to explore the capabilities of Apify MCP without any local setup.\nJust sign in with your Apify account and start experimenting with web scraping, data extraction, and automation tools!\n\nOr use the MCP bundle file (formerly known as Anthropic Desktop extension file, or DXT) for one-click installation: [Apify MCP server MCPB file](https://github.com/apify/apify-mcp-server/releases/latest/download/apify-mcp-server.mcpb)\n\n# üí∞ Skyfire agentic payments\n\nThe Apify MCP Server integrates with [Skyfire](https://www.skyfire.xyz/) to enable agentic payments - AI agents can autonomously pay for Actor runs without requiring an Apify API token. Instead of authenticating with `APIFY_TOKEN`, the agent uses Skyfire PAY tokens to cover billing for each tool call.\n\n**Prerequisites:**\n- A [Skyfire account](https://www.skyfire.xyz/) with a funded wallet\n- An MCP client that supports multiple servers (e.g., Claude Desktop, OpenCode, VS Code)\n\n**Setup:**\n\nConfigure both the Skyfire MCP server and the Apify MCP server in your MCP client. Enable payment mode by adding the `payment=skyfire` query parameter to the Apify server URL:\n\n```json\n{\n  \"mcpServers\": {\n    \"skyfire\": {\n      \"url\": \"https://api.skyfire.xyz/mcp/sse\",\n      \"headers\": {\n        \"skyfire-api-key\": \"<YOUR_SKYFIRE_API_KEY>\"\n      }\n    },\n    \"apify\": {\n      \"url\": \"https://mcp.apify.com?payment=skyfire\"\n    }\n  }\n}\n```\n\n**How it works:**\n\nWhen Skyfire mode is enabled, the agent handles the full payment flow autonomously:\n\n1. The agent discovers relevant Actors via `search-actors` or `fetch-actor-details` (these remain free).\n2. Before executing an Actor, the agent creates a PAY token using the `create-pay-token` tool from the Skyfire MCP server (minimum $5.00 USD).\n3. The agent passes the PAY token in the `skyfire-pay-id` input property when calling the Actor tool.\n4. Results are returned as usual. Unused funds on the token remain available for future runs or are returned upon expiration.\n\nTo learn more, see the [Skyfire integration documentation](https://docs.apify.com/platform/integrations/skyfire) and the [Agentic Payments with Skyfire](https://blog.apify.com/agentic-payments-skyfire/) blog post.\n\n# üõ†Ô∏è Tools, resources, and prompts\n\nThe MCP server provides a set of tools for interacting with Apify Actors.\nSince the Apify Store is large and growing rapidly, the MCP server provides a way to dynamically discover and use new Actors.\n\n### Actors\n\nAny [Apify Actor](https://apify.com/store) can be used as a tool.\nBy default, the server is pre-configured with one Actor, `apify/rag-web-browser`, and several helper tools.\nThe MCP server loads an Actor's input schema and creates a corresponding MCP tool.\nThis allows the AI agent to know exactly what arguments to pass to the Actor and what to expect in return.\n\n\nFor example, for the `apify/rag-web-browser` Actor, the input parameters are:\n\n```json\n{\n  \"query\": \"restaurants in San Francisco\",\n  \"maxResults\": 3\n}\n```\nYou don't need to manually specify which Actor to call or its input parameters; the LLM handles this automatically.\nWhen a tool is called, the arguments are automatically passed to the Actor by the LLM.\nYou can refer to the specific Actor's documentation for a list of available arguments.\n\n### Helper tools\n\nOne of the most powerful features of using MCP with Apify is dynamic tool discovery.\nIt allows an AI agent to find new tools (Actors) as needed and incorporate them.\nHere are some special MCP operations and how the Apify MCP Server supports them:\n\n- **Apify Actors**: Search for Actors, view their details, and use them as tools for the AI.\n- **Apify documentation**: Search the Apify documentation and fetch specific documents to provide context to the AI.\n- **Actor runs**: Get lists of your Actor runs, inspect their details, and retrieve logs.\n- **Apify storage**: Access data from your datasets and key-value stores.\n\n### Overview of available tools\n\nHere is an overview list of all the tools provided by the Apify MCP Server.\n\n| Tool name | Category | Description | Enabled by default |\n| :--- | :--- | :--- | :---: |\n| `search-actors` | actors | Search for Actors in the Apify Store. | ‚úÖ |\n| `fetch-actor-details` | actors | Retrieve detailed information about a specific Actor, including its input schema, README, pricing, and Actor output schema. | ‚úÖ |\n| `call-actor`* | actors | Call an Actor and get its run results. Use fetch-actor-details first to get the Actor's input schema. | ‚ùî |\n| `get-actor-run` | runs | Get detailed information about a specific Actor run. |  |\n| `get-actor-output`* | - | Retrieve the output from an Actor call which is not included in the output preview of the Actor tool. | ‚úÖ |\n| `search-apify-docs` | docs | Search the Apify documentation for relevant pages. | ‚úÖ |\n| `fetch-apify-docs` | docs | Fetch the full content of an Apify documentation page by its URL. | ‚úÖ |\n| [`apify-slash-rag-web-browser`](https://apify.com/apify/rag-web-browser) | Actor (see [tool configuration](#tools-configuration)) | An Actor tool to browse the web. | ‚úÖ |\n| `get-actor-run-list` | runs | Get a list of an Actor's runs, filterable by status. |  |\n| `get-actor-log` | runs | Retrieve the logs for a specific Actor run. |  |\n| `get-dataset` | storage | Get metadata about a specific dataset. |  |\n| `get-dataset-items` | storage | Retrieve items from a dataset with support for filtering and pagination. |  |\n| `get-dataset-schema` | storage | Generate a JSON schema from dataset items. |  |\n| `get-key-value-store` | storage | Get metadata about a specific key-value store. |  |\n| `get-key-value-store-keys`| storage | List the keys within a specific key-value store. |  |\n| `get-key-value-store-record`| storage | Get the value associated with a specific key in a key-value store. |  |\n| `get-dataset-list` | storage | List all available datasets for the user. |  |\n| `get-key-value-store-list`| storage | List all available key-value stores for the user. |  |\n| `add-actor`* | experimental | Add an Actor as a new tool for the user to call. | ‚ùî |\n\n> **Note:**\n>\n> When using the `actors` tool category, clients that support dynamic tool discovery (like Claude.ai web and VS Code) automatically receive the `add-actor` tool instead of `call-actor` for enhanced Actor discovery capabilities.\n>\n> The `get-actor-output` tool is automatically included with any Actor-related tool, such as `call-actor`, `add-actor`, or any specific Actor tool like `apify-slash-rag-web-browser`. When you call an Actor - either through the `call-actor` tool or directly via an Actor tool (e.g., `apify-slash-rag-web-browser`) - you receive a preview of the output. The preview depends on the Actor's output format and length; for some Actors and runs, it may include the entire output, while for others, only a limited version is returned to avoid overwhelming the LLM. To retrieve the full output of an Actor run, use the `get-actor-output` tool (supports limit, offset, and field filtering) with the `datasetId` provided by the Actor call.\n\n### Tool annotations\n\nAll tools include metadata annotations to help MCP clients and LLMs understand tool behavior:\n\n- **`title`**: Short display name for the tool (e.g., \"Search Actors\", \"Call Actor\", \"apify/rag-web-browser\")\n- **`readOnlyHint`**: `true` for tools that only read data without modifying state (e.g., `get-dataset`, `fetch-actor-details`)\n- **`openWorldHint`**: `true` for tools that access external resources outside the Apify platform (e.g., `call-actor` executes external Actors, `get-html-skeleton` scrapes external websites). Tools that interact only with the Apify platform (like `search-actors` or `fetch-apify-docs`) do not have this hint.\n\n### Tools configuration\n\nThe `tools` configuration parameter is used to specify loaded tools - either categories or specific tools directly, and Apify Actors. For example, `tools=storage,runs` loads two categories; `tools=add-actor` loads just one tool.\n\nWhen no query parameters are provided, the MCP server loads the following `tools` by default:\n\n- `actors`\n- `docs`\n- `apify/rag-web-browser`\n\nIf the tools parameter is specified, only the listed tools or categories will be enabled - no default tools will be included.\n\n> **Easy configuration:**\n>\n> Use the [UI configurator](https://mcp.apify.com/) to configure your server, then copy the configuration to your client.\n\n**Configuring the hosted server:**\n\nThe hosted server can be configured using query parameters in the URL. For example, to load the default tools, use:\n\n```\nhttps://mcp.apify.com?tools=actors,docs,apify/rag-web-browser\n```\n\n\nFor minimal configuration, if you want to use only a single Actor tool - without any discovery or generic calling tools, the server can be configured as follows:\n\n```\nhttps://mcp.apify.com?tools=apify/my-actor\n```\n\nThis setup exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n**Configuring the CLI:**\n\nThe CLI can be configured using command-line flags. For example, to load the same tools as in the hosted server configuration, use:\n\n```bash\nnpx @apify/actors-mcp-server --tools actors,docs,apify/rag-web-browser\n```\n\nThe minimal configuration is similar to the hosted server configuration:\n\n```bash\nnpx @apify/actors-mcp-server --tools apify/my-actor\n```\n\nAs above, this exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n> **‚ö†Ô∏è Important recommendation**\n>\n> **The default tools configuration may change in future versions.** When no `tools` parameter is specified, the server currently loads default tools, but this behavior is subject to change.\n>\n> **For production use and stable interfaces, always explicitly specify the `tools` parameter** to ensure your configuration remains consistent across updates.\n\n### UI mode configuration\n\nThe `uiMode` parameter enables OpenAI-specific widget rendering in tool responses. When enabled, tools like `search-actors` return interactive widget responses optimized for OpenAI clients.\n\n**Configuring the hosted server:**\n\nEnable UI mode using the `ui` query parameter:\n\n```\nhttps://mcp.apify.com?ui=openai\n```\n\nYou can combine it with other parameters:\n\n```\nhttps://mcp.apify.com?tools=actors,docs&ui=openai\n```\n\n**Configuring the CLI:**\n\nThe CLI can be configured using command-line flags. For example, to enable UI mode:\n\n```bash\nnpx @apify/actors-mcp-server --ui openai\n```\n\nYou can also set it via the `UI_MODE` environment variable:\n\n```bash\nexport UI_MODE=openai\nnpx @apify/actors-mcp-server\n```\n\n### Backward compatibility\n\nThe v2 configuration preserves backward compatibility with v1 usage. Notes:\n\n- `actors` param (URL) and `--actors` flag (CLI) are still supported.\n  - Internally they are merged into `tools` selectors.\n  - Examples: `?actors=apify/rag-web-browser` ‚â° `?tools=apify/rag-web-browser`; `--actors apify/rag-web-browser` ‚â° `--tools apify/rag-web-browser`.\n- `enable-adding-actors` (CLI) and `enableAddingActors` (URL) are supported but deprecated.\n  - Prefer `tools=experimental` or including the specific tool `tools=add-actor`.\n  - Behavior remains: when enabled with no `tools` specified, the server exposes only `add-actor`; when categories/tools are selected, `add-actor` is also included.\n- `enableActorAutoLoading` remains as a legacy alias for `enableAddingActors` and is mapped automatically.\n- Defaults remain compatible: when no `tools` are specified, the server loads `actors`, `docs`, and `apify/rag-web-browser`.\n  - If any `tools` are specified, the defaults are not added (same as v1 intent for explicit selection).\n- `call-actor` is now included by default via the `actors` category (additive change). To exclude it, specify an explicit `tools` list without `actors`.\n- `preview` category is deprecated and removed. Use specific tool names instead.\n\nExisting URLs and commands using `?actors=...` or `--actors` continue to work unchanged.\n\n### Prompts\n\nThe server provides a set of predefined example prompts to help you get started interacting with Apify through MCP. For example, there is a `GetLatestNewsOnTopic` prompt that allows you to easily retrieve the latest news on a specific topic using the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor.\n\n### Resources\n\nThe server does not yet provide any resources.\n\n## üì° Telemetry\n\nThe Apify MCP Server collects telemetry data about tool calls to help Apify understand usage patterns and improve the service.\nBy default, telemetry is **enabled** for all tool calls.\n\nThe stdio transport also uses [Sentry](https://sentry.io) for error tracking, which helps us identify and fix issues faster.\nSentry is automatically disabled when telemetry is opted out.\n\n### Opting out of telemetry\n\nYou can opt out of telemetry (including Sentry error tracking) by setting the `--telemetry-enabled` CLI flag to `false` or the `TELEMETRY_ENABLED` environment variable to `false`.\nCLI flags take precedence over environment variables.\n\n#### Examples\n\n**For the remote server (mcp.apify.com)**:\n```text\n# Disable via URL parameter\nhttps://mcp.apify.com?telemetry-enabled=false\n```\n\n**For the local stdio server**:\n```bash\n# Disable via CLI flag\nnpx @apify/actors-mcp-server --telemetry-enabled=false\n\n# Or set environment variable\nexport TELEMETRY_ENABLED=false\nnpx @apify/actors-mcp-server\n```\n\n# ‚öôÔ∏è Development\n\nPlease see the [CONTRIBUTING.md](./CONTRIBUTING.md) guide for contribution guidelines and commit message conventions.\n\nFor detailed development setup, project structure, and local testing instructions, see the [DEVELOPMENT.md](./DEVELOPMENT.md) guide.\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/en) (v18 or higher)\n\nCreate an environment file, `.env`, with the following content:\n```text\nAPIFY_TOKEN=\"your-apify-token\"\n```\n\nBuild the `actor-mcp-server` package:\n\n```bash\nnpm run build\n```\n\n## Start HTTP streamable MCP server\n\nRun using Apify CLI:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nexport APIFY_META_ORIGIN=STANDBY\napify run -p\n```\n\nOnce the server is running, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to debug the server exposed at `http://localhost:3001`.\n\n## Start standard input/output (stdio) MCP server\n\nYou can launch the MCP Inspector with this command:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector node ./dist/stdio.js\n```\n\nUpon launching, the Inspector will display a URL that you can open in your browser to begin debugging.\n\n## Unauthenticated access\n\nWhen the `tools` query parameter includes only tools explicitly enabled for unauthenticated use, the hosted server allows access without an API token.\nCurrently allowed tools: `search-actors`, `search-apify-docs`, `fetch-apify-docs`.\nExample: `https://mcp.apify.com?tools=search-actors`.\n\n## üê¶ Canary PR releases\n\nApify MCP is split across two repositories: this one for core MCP logic and the private `apify-mcp-server-internal` for the hosted server.\nChanges must be synchronized between both.\n\nTo create a canary release, add the `beta` tag to your PR branch.\nThis publishes the package to [pkg.pr.new](https://pkg.pr.new/) for staging and testing before merging.\nSee [the workflow file](.github/workflows/pre_release.yaml) for details.\n\n## üêã Docker Hub integration\nThe Apify MCP Server is also available on [Docker Hub](https://hub.docker.com/mcp/server/apify-mcp-server/overview), registered via the [mcp-registry](https://github.com/docker/mcp-registry) repository. The entry in `servers/apify-mcp-server/server.yaml` should be deployed automatically by the Docker Hub MCP registry (deployment frequency is unknown). **Before making major changes to the `stdio` server version, be sure to test it locally to ensure the Docker build passes.** To test, change the `source.branch` to your PR branch and run `task build -- apify-mcp-server`. For more details, see [CONTRIBUTING.md](https://github.com/docker/mcp-registry/blob/main/CONTRIBUTING.md).\n\n# üêõ Troubleshooting (local MCP server)\n\n- Make sure you have `node` installed by running `node -v`.\n- Make sure the `APIFY_TOKEN` environment variable is set.\n- Always use the latest version of the MCP server by using `@apify/actors-mcp-server@latest`.\n\n### Debugging the NPM package\n\nTo debug the server, use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) tool:\n\n```shell\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector npx -y @apify/actors-mcp-server\n```\n\n## üí° Limitations\n\nThe Actor input schema is processed to be compatible with most MCP clients while adhering to [JSON Schema](https://json-schema.org/) standards. The processing includes:\n- **Descriptions** are truncated to 500 characters (as defined in `MAX_DESCRIPTION_LENGTH`).\n- **Enum fields** are truncated to a maximum combined length of 2000 characters for all elements (as defined in `ACTOR_ENUM_MAX_LENGTH`).\n- **Required fields** are explicitly marked with a `REQUIRED` prefix in their descriptions for compatibility with frameworks that may not handle the JSON schema properly.\n- **Nested properties** are built for special cases like proxy configuration and request list sources to ensure the correct input structure.\n- **Array item types** are inferred when not explicitly defined in the schema, using a priority order: explicit type in items > prefill type > default value type > editor type.\n- **Enum values and examples** are added to property descriptions to ensure visibility, even if the client doesn't fully support the JSON schema.\n- **Rental Actors** are only available for use with the hosted MCP server at https://mcp.apify.com. When running the server locally via stdio, you can only access Actors that are already added to your local toolset. To dynamically search for and use any Actor from the Apify Store‚Äîincluding rental Actors‚Äîconnect to the hosted endpoint.\n\n# ü§ù Contributing\n\nWe welcome contributions to improve the Apify MCP Server! Here's how you can help:\n\n- **üêõ Report issues**: Find a bug or have a feature request? [Open an issue](https://github.com/apify/apify-mcp-server/issues).\n- **üîß Submit pull requests**: Fork the repo and submit pull requests with enhancements or fixes.\n- **üìö Documentation**: Improvements to docs and examples are always welcome.\n- **üí° Share use cases**: Contribute examples to help other users.\n\nFor major changes, please open an issue first to discuss your proposal and ensure it aligns with the project's goals.\n\n# üìö Learn more\n\n- [Model Context Protocol](https://modelcontextprotocol.org/)\n- [What are AI Agents?](https://blog.apify.com/what-are-ai-agents/)\n- [What is MCP and why does it matter?](https://blog.apify.com/what-is-model-context-protocol/)\n- [How to use MCP with Apify Actors](https://blog.apify.com/how-to-use-mcp/)\n- [Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- [Webinar: Building and Monetizing MCP Servers on Apify](https://www.youtube.com/watch?v=w3AH3jIrXXo)\n- [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)\n"
    },
    "llm_extracted": {
      "capabilities": [
        "Extract data from social media platforms using ready-made scrapers",
        "Scrape search engine results pages (SERPs) from Google",
        "Extract contact details from Google Maps",
        "Scrape Instagram posts, profiles, places, photos, and comments",
        "Search the web and scrape content from top URLs using the RAG Web Browser Actor",
        "Dynamically discover and use new Apify Actors as tools with input schema inference",
        "Support OAuth authentication for connecting from clients like Claude.ai and Visual Studio Code",
        "Enable agentic payments via Skyfire allowing AI agents to autonomously pay for Actor runs without API tokens",
        "Provide compatibility with multiple MCP clients including Claude Desktop, VS Code Genie, Cursor, and Apify Tester MCP Client"
      ],
      "limitations": [
        "Server-Sent Events (SSE) transport will be deprecated and unsupported after April 1, 2026",
        "Dynamic tool discovery support varies by client; some clients require manual tool reload",
        "Skyfire agentic payments require a funded Skyfire account and compatible MCP clients",
        "Local stdio usage requires manual setup and environment variable configuration",
        "Some clients have only partial support for dynamic tool discovery and may have limited user experience"
      ],
      "requirements": [
        "Apify API token for authentication when connecting via HTTPS endpoint without OAuth",
        "OAuth support in MCP clients for seamless connection to hosted server",
        "Skyfire account with funded wallet for agentic payments",
        "MCP clients that adhere to the Model Context Protocol",
        "Environment variable APIFY_TOKEN set for local stdio server usage",
        "Compatible MCP clients such as Claude.ai, Claude Desktop, VS Code Genie, Cursor, or Apify Tester MCP Client"
      ]
    },
    "documentation_quality": 0.9,
    "quality_rationale": "The documentation provides comprehensive installation and usage instructions, detailed examples of capabilities, client compatibility matrix, payment integration details, and explicit limitations including deprecation notices.",
    "llm_text_source": "readme",
    "documentation_chunks": [
      {
        "chunk_id": 0,
        "text": "<h1 align=\"center\">\n    <a href=\"https://mcp.apify.com\">\n        <picture>\n            <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_dark_background.png\">\n            <img alt=\"Apify MCP Server\" src=\"https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify_mcp_server_white_background.png\" width=\"500\">\n        </picture>\n    </a>\n    <br>\n    <small><a href=\"https://mcp.apify.com\">mcp.apify.com</a></small>\n</h1>\n\n<p align=center>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" alt=\"NPM latest version\" data-canonical-src=\"https://img.shields.io/npm/v/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://www.npmjs.com/package/@apify/actors-mcp-server\" rel=\"nofollow\"><img src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/npm/dm/@apify/actors-mcp-server.svg\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml\"><img src=\"https://github.com/apify/actors-mcp-server/actions/workflows/check.yaml/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width: 100%;\"></a>\n    <a href=\"https://smithery.ai/server/@apify/mcp\"><img src=\"https://smithery.ai/badge/@apify/mcp\" alt=\"smithery badge\"></a>\n</p>\n\n\nThe Apify Model Context Protocol (MCP) server at [**mcp.apify.com**](https://mcp.apify.com) enables your AI agents to extract data from social media, search engines, maps, e-commerce sites, and any other website using thousands of ready-made scrapers, crawlers, and automation tools from the [Apify Store](https://apify.com/store). It supports OAuth, allowing you to connect from clients like Claude.ai or Visual Studio Code using just the URL.",
        "start_pos": 0,
        "end_pos": 1952,
        "token_count_estimate": 488,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 1,
        "text": "rapers, crawlers, and automation tools from the [Apify Store](https://apify.com/store). It supports OAuth, allowing you to connect from clients like Claude.ai or Visual Studio Code using just the URL.\n\n> **üöÄ Use the hosted Apify MCP Server!**\n>\n> For the best experience, connect your AI assistant to our hosted server at **[`https://mcp.apify.com`](https://mcp.apify.com)**. The hosted server supports the latest features - including output schema inference for structured Actor results - that are not available when running locally via stdio.\n\nüí∞ The server also supports [Skyfire agentic payments](#-skyfire-agentic-payments), allowing AI agents to pay for Actor runs without an API token.\n\nApify MCP Server is compatible with `Claude Code, Claude.ai, Cursor, VS Code` and any client that adheres to the Model Context Protocol.\nCheck out the [MCP clients section](#-mcp-clients) for more details or visit the [MCP configuration page](https://mcp.apify.com).\n\n![Apify-MCP-server](https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/apify-mcp-server.png)\n\n## Table of Contents\n- [üåê Introducing the Apify MCP server](#-introducing-the-apify-mcp-server)\n- [üöÄ Quickstart](#-quickstart)\n- [‚ö†Ô∏è SSE transport deprecation](#%EF%B8%8F-sse-transport-deprecation)\n- [ü§ñ MCP clients](#-mcp-clients)\n- [ü™Ñ Try Apify MCP instantly](#-try-apify-mcp-instantly)\n- [üí∞ Skyfire agentic payments](#-skyfire-agentic-payments)\n- [üõ†Ô∏è Tools, resources, and prompts](#%EF%B8%8F-tools-resources-and-prompts)\n- [üìä Telemetry](#-telemetry)\n- [üêõ Troubleshooting (local MCP server)](#-troubleshooting-local-mcp-server)\n- [‚öôÔ∏è Development](#%EF%B8%8F-development)\n- [ü§ù Contributing](#-contributing)\n- [üìö Learn more](#-learn-more)\n\n# üåê Introducing the Apify MCP server\n\nThe Apify MCP Server allows an AI assistant to use any [Apify Actor](https://apify.com/store) as a tool to perform a specific task.",
        "start_pos": 1752,
        "end_pos": 3645,
        "token_count_estimate": 473,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 2,
        "text": "[üìö Learn more](#-learn-more)\n\n# üåê Introducing the Apify MCP server\n\nThe Apify MCP Server allows an AI assistant to use any [Apify Actor](https://apify.com/store) as a tool to perform a specific task.\nFor example, it can:\n- Use [Facebook Posts Scraper](https://apify.com/apify/facebook-posts-scraper) to extract data from Facebook posts from multiple pages/profiles.\n- Use [Google Maps Email Extractor](https://apify.com/lukaskrivka/google-maps-with-contact-details) to extract contact details from Google Maps.\n- Use [Google Search Results Scraper](https://apify.com/apify/google-search-scraper) to scrape Google Search Engine Results Pages (SERPs).\n- Use [Instagram Scraper](https://apify.com/apify/instagram-scraper) to scrape Instagram posts, profiles, places, photos, and comments.\n- Use [RAG Web Browser](https://apify.com/apify/web-scraper) to search the web, scrape the top N URLs, and return their content.\n\n**Video tutorial: Integrate 8,000+ Apify Actors and Agents with Claude**\n\n[![Apify MCP Server Tutorial: Integrate 5,000+ Apify Actors and Agents with Claude](https://img.youtube.com/vi/BKu8H91uCTg/hqdefault.jpg)](https://www.youtube.com/watch?v=BKu8H91uCTg)\n\n# üöÄ Quickstart\n\nYou can use the Apify MCP Server in two ways:\n\n**HTTPS Endpoint (mcp.apify.com)**: Connect from your MCP client via OAuth or by including the `Authorization: Bearer <APIFY_TOKEN>` header in your requests. This is the recommended method for most use cases. Because it supports OAuth, you can connect from clients like [Claude.ai](https://claude.ai) or [Visual Studio Code](https://code.visualstudio.com/) using just the URL: `https://mcp.apify.com`.\n- `https://mcp.apify.com` streamable transport\n\n**Standard Input/Output (stdio)**: Ideal for local integrations and command-line tools like the Claude for Desktop client.\n- Set the MCP client server command to `npx @apify/actors-mcp-server` and the `APIFY_TOKEN` environment variable to your Apify API token.\n- See `npx @apify/actors-mcp-server --help` for more options.",
        "start_pos": 3445,
        "end_pos": 5456,
        "token_count_estimate": 502,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 3,
        "text": "- Set the MCP client server command to `npx @apify/actors-mcp-server` and the `APIFY_TOKEN` environment variable to your Apify API token.\n- See `npx @apify/actors-mcp-server --help` for more options.\n\nYou can find detailed instructions for setting up the MCP server in the [Apify documentation](https://docs.apify.com/platform/integrations/mcp).\n\n# ‚ö†Ô∏è SSE transport deprecation on April 1, 2026\n\nUpdate your MCP client config before April 1, 2026.\nThe Apify MCP server is dropping Server-Sent Events (SSE) transport in favor of Streamable HTTP, in line with the official MCP spec.\n\nGo to [mcp.apify.com](https://mcp.apify.com/) to update the installation for your client of choice, with a valid endpoint.\n\n# ü§ñ MCP clients\n\nApify MCP Server is compatible with any MCP client that adheres to the [Model Context Protocol](https://modelcontextprotocol.org/), but the level of support for dynamic tool discovery and other features may vary between clients.\n<!--Therefore, the server uses [mcp-client-capabilities](https://github.com/apify/mcp-client-capabilities) to detect client capabilities and adjust its behavior accordingly.-->\n\nTo interact with the Apify MCP server, you can use clients such as: [Claude Desktop](https://claude.ai/download), [Visual Studio Code](https://code.visualstudio.com/), or [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client).\n\nVisit [mcp.apify.com](https://mcp.apify.com) to configure the server for your preferred client.\n\n![Apify-MCP-configuration-clients](https://raw.githubusercontent.com/apify/apify-mcp-server/refs/heads/master/docs/mcp-clients.png)\n\n### Supported clients matrix\n\nThe following table outlines the tested MCP clients and their level of support for key features.",
        "start_pos": 5256,
        "end_pos": 6991,
        "token_count_estimate": 433,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 4,
        "text": "|-----------------------------|------------------------|------------------------------------------------------|\n| **Claude.ai (web)**         | üü° Partial             | Tools mey need to be reloaded manually in the client |\n| **Claude Desktop**          | üü° Partial             | Tools may need to be reloaded manually in the client |\n| **VS Code (Genie)**         | ‚úÖ Full                 |                                                      |\n| **Cursor**                  | ‚úÖ Full                 |                                                      |\n| **Apify Tester MCP Client** | ‚úÖ Full                 | Designed for testing Apify MCP servers               |\n| **OpenCode**                | ‚úÖ Full                 |                                                      |\n\n\n**Smart tool selection based on client capabilities:**\n\nWhen the `actors` tool category is requested, the server intelligently selects the most appropriate Actor-related tools based on the client's capabilities:\n\n- **Clients with dynamic tool support** (e.g., Claude.ai web, VS Code Genie): The server provides the `add-actor` tool instead of `call-actor`. This allows for a better user experience where users can dynamically discover and add new Actors as tools during their conversation.\n\n- **Clients with limited dynamic tool support** (e.g., Claude Desktop): The server provides the standard `call-actor` tool along with other Actor category tools, ensuring compatibility while maintaining functionality.\n\n# ü™Ñ Try Apify MCP instantly\n\nWant to try Apify MCP without any setup?\n\nCheck out [Apify Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n\nThis interactive, chat-like interface provides an easy way to explore the capabilities of Apify MCP without any local setup.\nJust sign in with your Apify account and start experimenting with web scraping, data extraction, and automation tools!",
        "start_pos": 7104,
        "end_pos": 8999,
        "token_count_estimate": 473,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 5,
        "text": "des an easy way to explore the capabilities of Apify MCP without any local setup.\nJust sign in with your Apify account and start experimenting with web scraping, data extraction, and automation tools!\n\nOr use the MCP bundle file (formerly known as Anthropic Desktop extension file, or DXT) for one-click installation: [Apify MCP server MCPB file](https://github.com/apify/apify-mcp-server/releases/latest/download/apify-mcp-server.mcpb)\n\n# üí∞ Skyfire agentic payments\n\nThe Apify MCP Server integrates with [Skyfire](https://www.skyfire.xyz/) to enable agentic payments - AI agents can autonomously pay for Actor runs without requiring an Apify API token. Instead of authenticating with `APIFY_TOKEN`, the agent uses Skyfire PAY tokens to cover billing for each tool call.\n\n**Prerequisites:**\n- A [Skyfire account](https://www.skyfire.xyz/) with a funded wallet\n- An MCP client that supports multiple servers (e.g., Claude Desktop, OpenCode, VS Code)\n\n**Setup:**\n\nConfigure both the Skyfire MCP server and the Apify MCP server in your MCP client. Enable payment mode by adding the `payment=skyfire` query parameter to the Apify server URL:\n\n```json\n{\n  \"mcpServers\": {\n    \"skyfire\": {\n      \"url\": \"https://api.skyfire.xyz/mcp/sse\",\n      \"headers\": {\n        \"skyfire-api-key\": \"<YOUR_SKYFIRE_API_KEY>\"\n      }\n    },\n    \"apify\": {\n      \"url\": \"https://mcp.apify.com?payment=skyfire\"\n    }\n  }\n}\n```\n\n**How it works:**\n\nWhen Skyfire mode is enabled, the agent handles the full payment flow autonomously:\n\n1. The agent discovers relevant Actors via `search-actors` or `fetch-actor-details` (these remain free).\n2. Before executing an Actor, the agent creates a PAY token using the `create-pay-token` tool from the Skyfire MCP server (minimum $5.00 USD).\n3. The agent passes the PAY token in the `skyfire-pay-id` input property when calling the Actor tool.\n4. Results are returned as usual. Unused funds on the token remain available for future runs or are returned upon expiration.",
        "start_pos": 8799,
        "end_pos": 10781,
        "token_count_estimate": 495,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 6,
        "text": "token in the `skyfire-pay-id` input property when calling the Actor tool.\n4. Results are returned as usual. Unused funds on the token remain available for future runs or are returned upon expiration.\n\nTo learn more, see the [Skyfire integration documentation](https://docs.apify.com/platform/integrations/skyfire) and the [Agentic Payments with Skyfire](https://blog.apify.com/agentic-payments-skyfire/) blog post.\n\n# üõ†Ô∏è Tools, resources, and prompts\n\nThe MCP server provides a set of tools for interacting with Apify Actors.\nSince the Apify Store is large and growing rapidly, the MCP server provides a way to dynamically discover and use new Actors.\n\n### Actors\n\nAny [Apify Actor](https://apify.com/store) can be used as a tool.\nBy default, the server is pre-configured with one Actor, `apify/rag-web-browser`, and several helper tools.\nThe MCP server loads an Actor's input schema and creates a corresponding MCP tool.\nThis allows the AI agent to know exactly what arguments to pass to the Actor and what to expect in return.\n\n\nFor example, for the `apify/rag-web-browser` Actor, the input parameters are:\n\n```json\n{\n  \"query\": \"restaurants in San Francisco\",\n  \"maxResults\": 3\n}\n```\nYou don't need to manually specify which Actor to call or its input parameters; the LLM handles this automatically.\nWhen a tool is called, the arguments are automatically passed to the Actor by the LLM.\nYou can refer to the specific Actor's documentation for a list of available arguments.\n\n### Helper tools\n\nOne of the most powerful features of using MCP with Apify is dynamic tool discovery.\nIt allows an AI agent to find new tools (Actors) as needed and incorporate them.\nHere are some special MCP operations and how the Apify MCP Server supports them:\n\n- **Apify Actors**: Search for Actors, view their details, and use them as tools for the AI.\n- **Apify documentation**: Search the Apify documentation and fetch specific documents to provide context to the AI.\n- **Actor runs**: Get lists of your Actor runs, inspect their details, and retrieve logs.",
        "start_pos": 10581,
        "end_pos": 12625,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 7,
        "text": "ify documentation**: Search the Apify documentation and fetch specific documents to provide context to the AI.\n- **Actor runs**: Get lists of your Actor runs, inspect their details, and retrieve logs.\n- **Apify storage**: Access data from your datasets and key-value stores.\n\n### Overview of available tools\n\nHere is an overview list of all the tools provided by the Apify MCP Server.\n\n| Tool name | Category | Description | Enabled by default |\n| :--- | :--- | :--- | :---: |\n| `search-actors` | actors | Search for Actors in the Apify Store. | ‚úÖ |\n| `fetch-actor-details` | actors | Retrieve detailed information about a specific Actor, including its input schema, README, pricing, and Actor output schema. | ‚úÖ |\n| `call-actor`* | actors | Call an Actor and get its run results. Use fetch-actor-details first to get the Actor's input schema. | ‚ùî |\n| `get-actor-run` | runs | Get detailed information about a specific Actor run. |  |\n| `get-actor-output`* | - | Retrieve the output from an Actor call which is not included in the output preview of the Actor tool. | ‚úÖ |\n| `search-apify-docs` | docs | Search the Apify documentation for relevant pages. | ‚úÖ |\n| `fetch-apify-docs` | docs | Fetch the full content of an Apify documentation page by its URL. | ‚úÖ |\n| [`apify-slash-rag-web-browser`](https://apify.com/apify/rag-web-browser) | Actor (see [tool configuration](#tools-configuration)) | An Actor tool to browse the web. | ‚úÖ |\n| `get-actor-run-list` | runs | Get a list of an Actor's runs, filterable by status. |  |\n| `get-actor-log` | runs | Retrieve the logs for a specific Actor run. |  |\n| `get-dataset` | storage | Get metadata about a specific dataset. |  |\n| `get-dataset-items` | storage | Retrieve items from a dataset with support for filtering and pagination. |  |\n| `get-dataset-schema` | storage | Generate a JSON schema from dataset items. |  |\n| `get-key-value-store` | storage | Get metadata about a specific key-value store. |  |\n| `get-key-value-store-keys`| storage | List the keys within a specific key-value store.",
        "start_pos": 12425,
        "end_pos": 14468,
        "token_count_estimate": 510,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 8,
        "text": "rom dataset items. |  |\n| `get-key-value-store` | storage | Get metadata about a specific key-value store. |  |\n| `get-key-value-store-keys`| storage | List the keys within a specific key-value store. |  |\n| `get-key-value-store-record`| storage | Get the value associated with a specific key in a key-value store. |  |\n| `get-dataset-list` | storage | List all available datasets for the user. |  |\n| `get-key-value-store-list`| storage | List all available key-value stores for the user. |  |\n| `add-actor`* | experimental | Add an Actor as a new tool for the user to call. | ‚ùî |\n\n> **Note:**\n>\n> When using the `actors` tool category, clients that support dynamic tool discovery (like Claude.ai web and VS Code) automatically receive the `add-actor` tool instead of `call-actor` for enhanced Actor discovery capabilities.\n>\n> The `get-actor-output` tool is automatically included with any Actor-related tool, such as `call-actor`, `add-actor`, or any specific Actor tool like `apify-slash-rag-web-browser`. When you call an Actor - either through the `call-actor` tool or directly via an Actor tool (e.g., `apify-slash-rag-web-browser`) - you receive a preview of the output. The preview depends on the Actor's output format and length; for some Actors and runs, it may include the entire output, while for others, only a limited version is returned to avoid overwhelming the LLM. To retrieve the full output of an Actor run, use the `get-actor-output` tool (supports limit, offset, and field filtering) with the `datasetId` provided by the Actor call.",
        "start_pos": 14268,
        "end_pos": 15823,
        "token_count_estimate": 388,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 9,
        "text": "ut modifying state (e.g., `get-dataset`, `fetch-actor-details`)\n- **`openWorldHint`**: `true` for tools that access external resources outside the Apify platform (e.g., `call-actor` executes external Actors, `get-html-skeleton` scrapes external websites). Tools that interact only with the Apify platform (like `search-actors` or `fetch-apify-docs`) do not have this hint.\n\n### Tools configuration\n\nThe `tools` configuration parameter is used to specify loaded tools - either categories or specific tools directly, and Apify Actors. For example, `tools=storage,runs` loads two categories; `tools=add-actor` loads just one tool.\n\nWhen no query parameters are provided, the MCP server loads the following `tools` by default:\n\n- `actors`\n- `docs`\n- `apify/rag-web-browser`\n\nIf the tools parameter is specified, only the listed tools or categories will be enabled - no default tools will be included.\n\n> **Easy configuration:**\n>\n> Use the [UI configurator](https://mcp.apify.com/) to configure your server, then copy the configuration to your client.\n\n**Configuring the hosted server:**\n\nThe hosted server can be configured using query parameters in the URL. For example, to load the default tools, use:\n\n```\nhttps://mcp.apify.com?tools=actors,docs,apify/rag-web-browser\n```\n\n\nFor minimal configuration, if you want to use only a single Actor tool - without any discovery or generic calling tools, the server can be configured as follows:\n\n```\nhttps://mcp.apify.com?tools=apify/my-actor\n```\n\nThis setup exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n**Configuring the CLI:**\n\nThe CLI can be configured using command-line flags.",
        "start_pos": 16116,
        "end_pos": 17791,
        "token_count_estimate": 418,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 10,
        "text": "imal configuration is similar to the hosted server configuration:\n\n```bash\nnpx @apify/actors-mcp-server --tools apify/my-actor\n```\n\nAs above, this exposes only the specified Actor (`apify/my-actor`) as a tool. No other tools will be available.\n\n> **‚ö†Ô∏è Important recommendation**\n>\n> **The default tools configuration may change in future versions.** When no `tools` parameter is specified, the server currently loads default tools, but this behavior is subject to change.\n>\n> **For production use and stable interfaces, always explicitly specify the `tools` parameter** to ensure your configuration remains consistent across updates.\n\n### UI mode configuration\n\nThe `uiMode` parameter enables OpenAI-specific widget rendering in tool responses. When enabled, tools like `search-actors` return interactive widget responses optimized for OpenAI clients.\n\n**Configuring the hosted server:**\n\nEnable UI mode using the `ui` query parameter:\n\n```\nhttps://mcp.apify.com?ui=openai\n```\n\nYou can combine it with other parameters:\n\n```\nhttps://mcp.apify.com?tools=actors,docs&ui=openai\n```\n\n**Configuring the CLI:**\n\nThe CLI can be configured using command-line flags. For example, to enable UI mode:\n\n```bash\nnpx @apify/actors-mcp-server --ui openai\n```\n\nYou can also set it via the `UI_MODE` environment variable:\n\n```bash\nexport UI_MODE=openai\nnpx @apify/actors-mcp-server\n```\n\n### Backward compatibility\n\nThe v2 configuration preserves backward compatibility with v1 usage. Notes:\n\n- `actors` param (URL) and `--actors` flag (CLI) are still supported.\n  - Internally they are merged into `tools` selectors.\n  - Examples: `?actors=apify/rag-web-browser` ‚â° `?tools=apify/rag-web-browser`; `--actors apify/rag-web-browser` ‚â° `--tools apify/rag-web-browser`.\n- `enable-adding-actors` (CLI) and `enableAddingActors` (URL) are supported but deprecated.\n  - Prefer `tools=experimental` or including the specific tool `tools=add-actor`.",
        "start_pos": 17964,
        "end_pos": 19885,
        "token_count_estimate": 480,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 11,
        "text": "ls apify/rag-web-browser`.\n- `enable-adding-actors` (CLI) and `enableAddingActors` (URL) are supported but deprecated.\n  - Prefer `tools=experimental` or including the specific tool `tools=add-actor`.\n  - Behavior remains: when enabled with no `tools` specified, the server exposes only `add-actor`; when categories/tools are selected, `add-actor` is also included.\n- `enableActorAutoLoading` remains as a legacy alias for `enableAddingActors` and is mapped automatically.\n- Defaults remain compatible: when no `tools` are specified, the server loads `actors`, `docs`, and `apify/rag-web-browser`.\n  - If any `tools` are specified, the defaults are not added (same as v1 intent for explicit selection).\n- `call-actor` is now included by default via the `actors` category (additive change). To exclude it, specify an explicit `tools` list without `actors`.\n- `preview` category is deprecated and removed. Use specific tool names instead.\n\nExisting URLs and commands using `?actors=...` or `--actors` continue to work unchanged.\n\n### Prompts\n\nThe server provides a set of predefined example prompts to help you get started interacting with Apify through MCP. For example, there is a `GetLatestNewsOnTopic` prompt that allows you to easily retrieve the latest news on a specific topic using the [RAG Web Browser](https://apify.com/apify/rag-web-browser) Actor.\n\n### Resources\n\nThe server does not yet provide any resources.\n\n## üì° Telemetry\n\nThe Apify MCP Server collects telemetry data about tool calls to help Apify understand usage patterns and improve the service.\nBy default, telemetry is **enabled** for all tool calls.\n\nThe stdio transport also uses [Sentry](https://sentry.io) for error tracking, which helps us identify and fix issues faster.\nSentry is automatically disabled when telemetry is opted out.\n\n### Opting out of telemetry\n\nYou can opt out of telemetry (including Sentry error tracking) by setting the `--telemetry-enabled` CLI flag to `false` or the `TELEMETRY_ENABLED` environment variable to `false`.",
        "start_pos": 19685,
        "end_pos": 21704,
        "token_count_estimate": 504,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 12,
        "text": "ng out of telemetry\n\nYou can opt out of telemetry (including Sentry error tracking) by setting the `--telemetry-enabled` CLI flag to `false` or the `TELEMETRY_ENABLED` environment variable to `false`.\nCLI flags take precedence over environment variables.\n\n#### Examples\n\n**For the remote server (mcp.apify.com)**:\n```text\n# Disable via URL parameter\nhttps://mcp.apify.com?telemetry-enabled=false\n```\n\n**For the local stdio server**:\n```bash\n# Disable via CLI flag\nnpx @apify/actors-mcp-server --telemetry-enabled=false\n\n# Or set environment variable\nexport TELEMETRY_ENABLED=false\nnpx @apify/actors-mcp-server\n```\n\n# ‚öôÔ∏è Development\n\nPlease see the [CONTRIBUTING.md](./CONTRIBUTING.md) guide for contribution guidelines and commit message conventions.\n\nFor detailed development setup, project structure, and local testing instructions, see the [DEVELOPMENT.md](./DEVELOPMENT.md) guide.\n\n## Prerequisites\n\n- [Node.js](https://nodejs.org/en) (v18 or higher)\n\nCreate an environment file, `.env`, with the following content:\n```text\nAPIFY_TOKEN=\"your-apify-token\"\n```\n\nBuild the `actor-mcp-server` package:\n\n```bash\nnpm run build\n```\n\n## Start HTTP streamable MCP server\n\nRun using Apify CLI:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nexport APIFY_META_ORIGIN=STANDBY\napify run -p\n```\n\nOnce the server is running, you can use the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) to debug the server exposed at `http://localhost:3001`.\n\n## Start standard input/output (stdio) MCP server\n\nYou can launch the MCP Inspector with this command:\n\n```bash\nexport APIFY_TOKEN=\"your-apify-token\"\nnpx @modelcontextprotocol/inspector node ./dist/stdio.js\n```\n\nUpon launching, the Inspector will display a URL that you can open in your browser to begin debugging.\n\n## Unauthenticated access\n\nWhen the `tools` query parameter includes only tools explicitly enabled for unauthenticated use, the hosted server allows access without an API token.\nCurrently allowed tools: `search-actors`, `search-apify-docs`, `fetch-apify-docs`.",
        "start_pos": 21504,
        "end_pos": 23534,
        "token_count_estimate": 507,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 13,
        "text": "includes only tools explicitly enabled for unauthenticated use, the hosted server allows access without an API token.\nCurrently allowed tools: `search-actors`, `search-apify-docs`, `fetch-apify-docs`.\nExample: `https://mcp.apify.com?tools=search-actors`.\n\n## üê¶ Canary PR releases\n\nApify MCP is split across two repositories: this one for core MCP logic and the private `apify-mcp-server-internal` for the hosted server.\nChanges must be synchronized between both.\n\nTo create a canary release, add the `beta` tag to your PR branch.\nThis publishes the package to [pkg.pr.new](https://pkg.pr.new/) for staging and testing before merging.\nSee [the workflow file](.github/workflows/pre_release.yaml) for details.\n\n## üêã Docker Hub integration\nThe Apify MCP Server is also available on [Docker Hub](https://hub.docker.com/mcp/server/apify-mcp-server/overview), registered via the [mcp-registry](https://github.com/docker/mcp-registry) repository. The entry in `servers/apify-mcp-server/server.yaml` should be deployed automatically by the Docker Hub MCP registry (deployment frequency is unknown). **Before making major changes to the `stdio` server version, be sure to test it locally to ensure the Docker build passes.** To test, change the `source.branch` to your PR branch and run `task build -- apify-mcp-server`. For more details, see [CONTRIBUTING.md](https://github.com/docker/mcp-registry/blob/main/CONTRIBUTING.md).\n\n# üêõ Troubleshooting (local MCP server)\n\n- Make sure you have `node` installed by running `node -v`.\n- Make sure the `APIFY_TOKEN` environment variable is set.\n- Always use the latest version of the MCP server by using `@apify/actors-mcp-server@latest`.",
        "start_pos": 23334,
        "end_pos": 25005,
        "token_count_estimate": 417,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 14,
        "text": "oken\"\nnpx @modelcontextprotocol/inspector npx -y @apify/actors-mcp-server\n```\n\n## üí° Limitations\n\nThe Actor input schema is processed to be compatible with most MCP clients while adhering to [JSON Schema](https://json-schema.org/) standards. The processing includes:\n- **Descriptions** are truncated to 500 characters (as defined in `MAX_DESCRIPTION_LENGTH`).\n- **Enum fields** are truncated to a maximum combined length of 2000 characters for all elements (as defined in `ACTOR_ENUM_MAX_LENGTH`).\n- **Required fields** are explicitly marked with a `REQUIRED` prefix in their descriptions for compatibility with frameworks that may not handle the JSON schema properly.\n- **Nested properties** are built for special cases like proxy configuration and request list sources to ensure the correct input structure.\n- **Array item types** are inferred when not explicitly defined in the schema, using a priority order: explicit type in items > prefill type > default value type > editor type.\n- **Enum values and examples** are added to property descriptions to ensure visibility, even if the client doesn't fully support the JSON schema.\n- **Rental Actors** are only available for use with the hosted MCP server at https://mcp.apify.com. When running the server locally via stdio, you can only access Actors that are already added to your local toolset. To dynamically search for and use any Actor from the Apify Store‚Äîincluding rental Actors‚Äîconnect to the hosted endpoint.\n\n# ü§ù Contributing\n\nWe welcome contributions to improve the Apify MCP Server! Here's how you can help:\n\n- **üêõ Report issues**: Find a bug or have a feature request? [Open an issue](https://github.com/apify/apify-mcp-server/issues).\n- **üîß Submit pull requests**: Fork the repo and submit pull requests with enhancements or fixes.\n- **üìö Documentation**: Improvements to docs and examples are always welcome.\n- **üí° Share use cases**: Contribute examples to help other users.",
        "start_pos": 25182,
        "end_pos": 27121,
        "token_count_estimate": 484,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      },
      {
        "chunk_id": 15,
        "text": "repo and submit pull requests with enhancements or fixes.\n- **üìö Documentation**: Improvements to docs and examples are always welcome.\n- **üí° Share use cases**: Contribute examples to help other users.\n\nFor major changes, please open an issue first to discuss your proposal and ensure it aligns with the project's goals.\n\n# üìö Learn more\n\n- [Model Context Protocol](https://modelcontextprotocol.org/)\n- [What are AI Agents?](https://blog.apify.com/what-are-ai-agents/)\n- [What is MCP and why does it matter?](https://blog.apify.com/what-is-model-context-protocol/)\n- [How to use MCP with Apify Actors](https://blog.apify.com/how-to-use-mcp/)\n- [Tester MCP Client](https://apify.com/jiri.spilka/tester-mcp-client)\n- [Webinar: Building and Monetizing MCP Servers on Apify](https://www.youtube.com/watch?v=w3AH3jIrXXo)\n- [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)",
        "start_pos": 26921,
        "end_pos": 27836,
        "token_count_estimate": 228,
        "source_type": "readme",
        "agent_id": "f5297cd861cece8e"
      }
    ]
  }
]